import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.autotune(
    configs=[
        triton.Config({'ROWS_PER_PROGRAM': 128}, num_warps=8),
        triton.Config({'ROWS_PER_PROGRAM': 64}, num_warps=4),
        triton.Config({'ROWS_PER_PROGRAM': 32}, num_warps=2),
        triton.Config({'ROWS_PER_PROGRAM': 16}, num_warps=2),
        triton.Config({'ROWS_PER_PROGRAM': 8}, num_warps=1),
    ],
    key=['N', 'C'],
)
@triton.jit
def fused_clamp_scale_max_kernel(
    input_ptr,
    multiplier_ptr,
    output_ptr,
    clamp_min,
    clamp_max,
    C,
    N,
    BLOCK_SIZE: tl.constexpr,
    ROWS_PER_PROGRAM: tl.constexpr,
):
    pid = tl.program_id(0)
    row_start = pid * ROWS_PER_PROGRAM
    # Preload multiplier once per block
    mult = tl.load(multiplier_ptr + tl.arange(0, BLOCK_SIZE), mask=tl.arange(0, BLOCK_SIZE) < C, other=0.0)
    
    # Process multiple rows per block
    for i in tl.static_range(0, ROWS_PER_PROGRAM):
        row_idx = row_start + i
        if row_idx < N:
            row_offset = row_idx * C
            offsets = row_offset + tl.arange(0, BLOCK_SIZE)
            mask = tl.arange(0, BLOCK_SIZE) < C
            
            # Load, clamp, and scale
            row = tl.load(input_ptr + offsets, mask=mask, other=0.0)
            clamped = tl.minimum(tl.maximum(row, clamp_min), clamp_max)
            scaled = clamped * mult
            scaled = tl.where(mask, scaled, -float('inf'))
            
            # Compute row max and store
            max_val = tl.max(scaled, axis=0)
            tl.store(output_ptr + row_idx, max_val)

def fused_clamp_scale_max(input, multiplier, clamp_min, clamp_max):
    batch, C, D, H, W = input.shape
    N = batch * D * H * W
    if N == 0:
        return input.new_empty(batch, D, H, W)
    
    input_flat = input.permute(0,2,3,4,1).contiguous().view(N, C)
    output = torch.empty(N, device=input.device, dtype=input.dtype)
    
    if C > 0:
        BLOCK_SIZE = triton.next_power_of_2(C)
        grid = lambda meta: (triton.cdiv(N, meta['ROWS_PER_PROGRAM']),)
        fused_clamp_scale_max_kernel[grid](
            input_flat, multiplier, output, 
            clamp_min, clamp_max, C, N,
            BLOCK_SIZE=BLOCK_SIZE
        )
    else:
        output.fill_(-float('inf'))
    
    return output.view(batch, D, H, W)

class ModelNew(nn.Module):
    """
    A 3D convolutional layer followed by multiplication, instance normalization, clamping, multiplication, and a max operation.
    """
    def __init__(self, in_channels, out_channels, kernel_size, multiplier_shape, clamp_min, clamp_max):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.multiplier = nn.Parameter(torch.randn(multiplier_shape))
        self.instance_norm = nn.InstanceNorm3d(out_channels)
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max

    def forward(self, x):
        x = self.conv(x)
        x = x * self.multiplier
        x = self.instance_norm(x)
        x = fused_clamp_scale_max(x, self.multiplier.view(-1), self.clamp_min, self.clamp_max)
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
multiplier_shape = (out_channels, 1, 1, 1)
clamp_min = -1.0
clamp_max = 1.0

def get_inputs():
    return [torch.randn(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, multiplier_shape, clamp_min, clamp_max]

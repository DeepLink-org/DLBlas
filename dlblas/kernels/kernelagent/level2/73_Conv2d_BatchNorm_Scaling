import torch
import torch.nn as nn
import triton
import triton.language as tl
import math

@triton.autotune(
    configs=[
        triton.Config({"BLOCK_SIZE": 256, "VEC_SIZE": 4}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_SIZE": 256, "VEC_SIZE": 2}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_SIZE": 128, "VEC_SIZE": 4}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_SIZE": 128, "VEC_SIZE": 2}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_SIZE": 64, "VEC_SIZE": 4}, num_warps=2, num_stages=2),
        triton.Config({"BLOCK_SIZE": 256, "VEC_SIZE": 1}, num_warps=4, num_stages=2),
    ],
    key=['spatial_size'],
)
@triton.jit
def fused_bn_scale_kernel(
    x_ptr,
    output_ptr,
    scale_ptr,
    shift_ptr,
    spatial_size,
    n_channels,
    BLOCK_SIZE: tl.constexpr,
    VEC_SIZE: tl.constexpr,
):
    pid_batch = tl.program_id(0)
    pid_channel = tl.program_id(1)
    
    # Precompute channel parameters
    s = tl.load(scale_ptr + pid_channel)
    b = tl.load(shift_ptr + pid_channel)
    
    base_offset = pid_batch * n_channels * spatial_size + pid_channel * spatial_size
    tid = tl.program_id(2)
    start_idx = tid * BLOCK_SIZE * VEC_SIZE
    offsets = base_offset + start_idx + tl.arange(0, BLOCK_SIZE)[:, None] * VEC_SIZE + tl.arange(0, VEC_SIZE)[None, :]
    mask = (start_idx + tl.arange(0, BLOCK_SIZE)[:, None] * VEC_SIZE + tl.arange(0, VEC_SIZE)[None, :]) < spatial_size
    
    # Vectorized load
    x_vals = tl.load(x_ptr + offsets, mask=mask, other=0.0)
    
    # Compute fused operation
    output_vals = x_vals * s + b
    
    # Vectorized store
    tl.store(output_ptr + offsets, output_vals, mask=mask)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bn = nn.BatchNorm2d(out_channels)
        self.scaling_factor = scaling_factor
        self.fused_scale = None
        self.fused_shift = None

    def forward(self, x):
        x = self.conv(x)
        
        if not self.training:
            # Precompute fused parameters once
            if self.fused_scale is None or self.fused_shift is None:
                inv_std = 1.0 / torch.sqrt(self.bn.running_var + self.bn.eps)
                self.fused_scale = (self.bn.weight * inv_std * self.scaling_factor).contiguous()
                self.fused_shift = ((self.bn.bias - self.bn.weight * self.bn.running_mean * inv_std) * self.scaling_factor).contiguous()
            
            # Ensure contiguous memory layout
            x = x.contiguous()
            output = torch.empty_like(x)
            
            # Get tensor dimensions
            n, c, h, w = x.shape
            spatial_size = h * w
            
            # Configure kernel grid
            grid = (n, c, triton.cdiv(spatial_size, 256 * 4))
            
            # Launch optimized kernel
            fused_bn_scale_kernel[grid](
                x, output,
                self.fused_scale,
                self.fused_shift,
                spatial_size,
                c,
            )
            return output
        else:
            # Training path (unchanged)
            x = self.bn(x)
            return x * self.scaling_factor

batch_size = 128
in_channels = 3
out_channels = 16
height, width = 32, 32
kernel_size = 3
scaling_factor = 2.0

def get_inputs():
    return [torch.randn(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scaling_factor]

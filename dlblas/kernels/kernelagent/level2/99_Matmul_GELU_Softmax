import torch
import torch.nn as nn
import math
import triton
import triton.language as tl

@triton.jit
def linear_gelu_kernel(
    x_ptr,
    weight_ptr,
    bias_ptr,
    output_ptr,
    in_features,
    out_features,
    batch_size,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(0)
    if pid < n_elements:
        pid_b = pid // out_features
        pid_o = pid % out_features
        if pid_b < batch_size and pid_o < out_features:
            acc = 0.0
            for k in range(0, tl.cdiv(in_features, BLOCK_SIZE)):
                offs = k * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
                mask = offs < in_features
                x_val = tl.load(x_ptr + pid_b * in_features + offs, mask=mask, other=0.0)
                w_val = tl.load(weight_ptr + pid_o * in_features + offs, mask=mask, other=0.0)
                acc += tl.sum(x_val * w_val)
            bias_val = tl.load(bias_ptr + pid_o)
            linear_val = acc + bias_val
            gelu_val = linear_val * 0.5 * (1.0 + tl.erf(linear_val * 0.7071067811865475))
            output_ptr_val = output_ptr + pid_b * out_features + pid_o
            tl.store(output_ptr_val, gelu_val)

@triton.jit
def softmax_kernel(
    input_ptr,
    output_ptr,
    out_features,
    batch_size,
    BLOCK_SIZE: tl.constexpr,
):
    pid_b = tl.program_id(0)
    if pid_b < batch_size:
        row_start = pid_b * out_features
        col_offsets = tl.arange(0, BLOCK_SIZE)
        mask = col_offsets < out_features
        row_offsets = row_start + col_offsets
        row = tl.load(input_ptr + row_offsets, mask=mask, other=-float('inf'))
        row_minus_max = row - tl.max(row, axis=0)
        numerator = tl.exp(row_minus_max)
        denominator = tl.sum(numerator, axis=0)
        softmax_output = numerator / denominator
        tl.store(output_ptr + row_offsets, softmax_output, mask=mask)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        self.bias = nn.Parameter(torch.Tensor(out_features))
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
        nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x):
        x = x.contiguous()
        batch_size = x.shape[0]
        total_elements = batch_size * self.out_features
        
        # Fused Linear + GELU layer
        output_gelu = torch.empty((batch_size, self.out_features), device=x.device, dtype=x.dtype)
        grid_linear = (total_elements,)
        linear_gelu_kernel[grid_linear](
            x, self.weight, self.bias, output_gelu,
            self.in_features, self.out_features, batch_size, total_elements,
            BLOCK_SIZE=64
        )
        
        # Softmax
        output_softmax = torch.empty_like(output_gelu)
        BLOCK_SIZE = triton.next_power_of_2(self.out_features)
        grid_softmax = (batch_size,)
        softmax_kernel[grid_softmax](
            output_gelu, output_softmax, self.out_features, batch_size,
            BLOCK_SIZE=BLOCK_SIZE
        )
        
        return output_softmax

batch_size = 128
in_features = 100
out_features = 10

def get_inputs():
    return [torch.randn(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]

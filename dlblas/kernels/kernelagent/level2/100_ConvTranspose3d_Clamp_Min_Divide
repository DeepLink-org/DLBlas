import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def clamp_div_kernel(
    output_ptr,
    min_value,
    divisor,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    output = tl.load(output_ptr + offsets, mask=mask, other=0.0)
    output = tl.maximum(output, min_value)
    output = output / divisor
    tl.store(output_ptr + offsets, output, mask=mask)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, min_value, divisor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size, stride=stride, padding=padding
        )
        self.min_value = min_value
        self.divisor = divisor

    def forward(self, x):
        x = self.conv_transpose(x)
        if not x.is_contiguous():
            x = x.contiguous()
        n_elements = x.numel()
        grid = lambda meta: (triton.cdiv(n_elements, 1024),)
        clamp_div_kernel[grid](
            x, self.min_value, self.divisor, n_elements, BLOCK_SIZE=1024,
            num_warps=4
        )
        return x

batch_size = 16
in_channels = 32
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
min_value = -1.0
divisor = 2.0

def get_inputs():
    return [torch.randn(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, min_value, divisor]

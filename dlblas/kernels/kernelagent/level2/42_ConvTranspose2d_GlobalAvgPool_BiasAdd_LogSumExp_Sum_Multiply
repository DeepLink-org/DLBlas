import math
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def _logsumexp_kernel(
    input_ptr,
    bias_ptr,
    output_ptr,
    input_row_stride,
    output_row_stride,
    n_cols,
    scale,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(0)
    row_start = input_ptr + pid * input_row_stride
    col_offsets = tl.arange(0, BLOCK_SIZE)
    input_ptrs = row_start + col_offsets
    mask = col_offsets < n_cols
    
    # Fused load and bias addition
    row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
    bias = tl.load(bias_ptr + col_offsets, mask=mask, other=0.0)
    row += bias
    
    # Efficient logsumexp for small vectors
    max_val = tl.max(tl.where(mask, row, -float('inf')), axis=0)
    row_minus_max = tl.where(mask, row - max_val, -float('inf'))
    exp_row = tl.exp(row_minus_max)
    exp_row = tl.where(mask, exp_row, 0.0)
    exp_sum = tl.sum(exp_row, axis=0)
    log_sum_exp = tl.log(exp_sum) + max_val
    result = log_sum_exp * scale
    
    output_row = output_ptr + pid * output_row_stride
    tl.store(output_row, result)

def compute_logsumexp_and_scale(x, scale, bias):
    batch, n_cols = x.shape
    output = torch.empty(batch, device=x.device, dtype=torch.float32)
    input_row_stride = x.stride(0)
    output_row_stride = 1
    
    if n_cols == 0:
        output.fill_(0.0)
        return output.view(-1, 1)
    
    grid = (batch,)
    BLOCK_SIZE = triton.next_power_of_2(n_cols)
    _logsumexp_kernel[grid](
        x, 
        bias.contiguous(), 
        output,
        input_row_stride, 
        output_row_stride,
        n_cols, 
        scale,
        BLOCK_SIZE=BLOCK_SIZE,
        num_warps=1,
        num_stages=1
    )
    return output.view(-1, 1)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv_transpose(x)
        x = torch.mean(x, dim=(2, 3), keepdim=True)  # Global average pooling
        x_2d = x.view(x.size(0), -1)  # Flatten to [batch, channels]
        x = compute_logsumexp_and_scale(x_2d, 10.0, self.bias.view(-1))  # Fused computation
        return x

batch_size = 128
in_channels = 3
out_channels = 16
height, width = 32, 32
kernel_size = 3
bias_shape = (out_channels, 1, 1)

def get_inputs():
    return [torch.randn(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, bias_shape]

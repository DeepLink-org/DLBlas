import math
import torch
import triton
import triton.language as tl
import torch.nn as nn

@triton.autotune(
    configs=[
        triton.Config({}, num_stages=1, num_warps=8),
        triton.Config({}, num_stages=2, num_warps=8),
        triton.Config({}, num_stages=4, num_warps=8),
        triton.Config({}, num_stages=8, num_warps=8),
        triton.Config({}, num_stages=1),
        triton.Config({}, num_stages=2),
        triton.Config({}, num_stages=4),
        triton.Config({}, num_stages=8),
        triton.Config({}, num_warps=1),
        triton.Config({}, num_warps=2),
        triton.Config({}, num_warps=4),
        triton.Config({}, num_warps=8),
    ],
    key=['C', 'H', 'W'],
)
@triton.jit
def _fused_min_sum_gelu_bias(
    input_ptr,
    bias_ptr,
    output_ptr,
    B, C, H, W,
    stride_b, stride_c, stride_h, stride_w,
    out_stride_b, out_stride_c, out_stride_w,
    BLOCK_C: tl.constexpr
):
    pid_b = tl.program_id(0)
    pid_w = tl.program_id(1)
    
    if pid_b >= B or pid_w >= W:
        return
    
    c_offsets = tl.arange(0, BLOCK_C)
    mask = c_offsets < C
    
    total = 0.0
    for h in range(H):
        base = pid_b * stride_b + h * stride_h + pid_w * stride_w
        vals = tl.load(input_ptr + base + c_offsets * stride_c, mask=mask, other=float('inf'))
        min_val = tl.min(tl.where(mask, vals, float('inf')), axis=0)
        total += min_val

    x = total
    gelu_val = 0.5 * x * (1.0 + tl.erf(x * 0.7071067811865475))
    
    bias_vals = tl.load(bias_ptr + c_offsets, mask=mask, other=0.0)
    out_vals = gelu_val + bias_vals
    
    base_out = pid_b * out_stride_b + pid_w * out_stride_w
    tl.store(output_ptr + base_out + c_offsets * out_stride_c, out_vals, mask=mask)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv_transpose(x)
        x = x.contiguous()
        B, C, H, W = x.shape
        output = torch.empty((B, C, 1, W), device=x.device, dtype=x.dtype)
        
        bias_1d = self.bias.view(-1)
        grid = (B, W)
        stride_b, stride_c, stride_h, stride_w = x.stride()
        out_stride_b, out_stride_c, _, out_stride_w = output.stride()
        
        BLOCK_C = triton.next_power_of_2(C)
        _fused_min_sum_gelu_bias[grid](
            x, bias_1d, output,
            B, C, H, W,
            stride_b, stride_c, stride_h, stride_w,
            out_stride_b, out_stride_c, out_stride_w,
            BLOCK_C=BLOCK_C
        )
        
        return output

batch_size = 128
in_channels = 3
out_channels = 16
height, width = 32, 32
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
bias_shape = (out_channels, 1, 1)

def get_inputs():
    return [torch.randn(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]

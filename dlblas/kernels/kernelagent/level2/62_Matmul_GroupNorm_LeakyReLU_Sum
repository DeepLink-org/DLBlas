import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def _fused_forward(
    x_ptr, w_ptr, b_ptr, gn_w_ptr, gn_b_ptr, out_ptr,
    input_size, hidden_size, num_groups, eps, neg_slope,
    stride_xb, stride_xh,
    stride_wh, stride_ww,
    stride_b,
    stride_gnw,
    stride_gnb,
    stride_outb, stride_outh,
    BLOCK_HIDDEN: tl.constexpr,
    BLOCK_INPUT: tl.constexpr,
):
    pid = tl.program_id(0)
    batch_id = pid // num_groups
    group_id = pid % num_groups
    group_size = hidden_size // num_groups
    group_start = group_id * group_size

    # Compute linear output for one group
    j = group_start + tl.arange(0, BLOCK_HIDDEN)
    j_mask = j < hidden_size
    w_ptrs = w_ptr + j[:, None] * stride_wh
    x_base = x_ptr + batch_id * stride_xb
    
    # Accumulate matrix multiplication
    acc = tl.zeros((BLOCK_HIDDEN,), dtype=tl.float32)
    for i in range(0, input_size, BLOCK_INPUT):
        cols = i + tl.arange(0, BLOCK_INPUT)
        mask = cols < input_size
        x = tl.load(x_base + cols * stride_xh, mask=mask, other=0.0)
        w = tl.load(w_ptrs + cols[None, :] * stride_ww, 
                   mask=mask[None, :] & j_mask[:, None], other=0.0)
        acc += tl.sum(x[None, :] * w, axis=1)
    
    # Add bias
    b = tl.load(b_ptr + j, mask=j_mask, other=0.0)
    linear_out = acc + b

    # GroupNorm statistics
    mean = tl.sum(linear_out, axis=0) / group_size
    centered = linear_out - mean
    var = tl.sum(centered * centered, axis=0) / group_size
    inv_std = tl.math.rsqrt(var + eps)
    normalized = centered * inv_std

    # Apply group norm scale/shift
    gn_w = tl.load(gn_w_ptr + j, mask=j_mask, other=0.0)
    gn_b = tl.load(gn_b_ptr + j, mask=j_mask, other=0.0)
    scaled = normalized * gn_w + gn_b

    # Fused Leaky ReLU and scaling
    result = tl.where(scaled >= 0, scaled * 2.0, scaled * (2.0 * neg_slope))
    
    # Store results
    out_ptrs = out_ptr + batch_id * stride_outb + j * stride_outh
    tl.store(out_ptrs, result, mask=j_mask)

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):
        super().__init__()
        self.fc = nn.Linear(input_size, hidden_size)
        self.gn = nn.GroupNorm(num_groups, hidden_size, eps=eps)
        self.hidden_size = hidden_size
        self.num_groups = num_groups
        self.negative_slope = negative_slope
        
    def forward(self, x):
        batch_size, _ = x.shape
        group_size = self.hidden_size // self.num_groups
        output = torch.empty((batch_size, self.hidden_size), 
                            device=x.device, dtype=x.dtype)
        
        # Launch kernel
        grid = (batch_size * self.num_groups,)
        _fused_forward[grid](
            x, self.fc.weight, self.fc.bias, 
            self.gn.weight, self.gn.bias, output,
            x.size(1), self.hidden_size, self.num_groups,
            self.gn.eps, self.negative_slope,
            x.stride(0), x.stride(1),
            self.fc.weight.stride(0), self.fc.weight.stride(1),
            self.fc.bias.stride(0),
            self.gn.weight.stride(0),
            self.gn.bias.stride(0),
            output.stride(0), output.stride(1),
            BLOCK_HIDDEN=group_size,
            BLOCK_INPUT=64  # Optimized for H100 memory coalescing
        )
        return output

batch_size = 128
input_size = 512
hidden_size = 256
num_groups = 8

def get_inputs():
    return [torch.randn(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size, num_groups]

import math
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def fused_bias_act_groupnorm(
    x_ptr,
    bias_ptr,
    gn_weight_ptr,
    gn_bias_ptr,
    output_ptr,
    batch_size,
    num_features,
    num_groups,
    group_size,
    x_batch_stride,
    x_feature_stride,
    BLOCK_SIZE: tl.constexpr
):
    batch_idx = tl.program_id(0)
    group_idx = tl.program_id(1)
    
    if batch_idx >= batch_size or group_idx >= num_groups:
        return
        
    feature_start = group_idx * group_size
    offsets = feature_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < num_features
    
    x_ptr += batch_idx * x_batch_stride + offsets * x_feature_stride
    bias_ptr += offsets
    gn_weight_ptr += offsets
    gn_bias_ptr += offsets
    output_ptr += batch_idx * x_batch_stride + offsets * x_feature_stride
    
    # Load data
    x = tl.load(x_ptr, mask=mask, other=0.0)
    bias = tl.load(bias_ptr, mask=mask, other=0.0)
    
    # Fused: BiasAdd + Hardtanh + Mish
    x_biased = x + bias
    x_clamped = tl.minimum(tl.maximum(x_biased, -1.0), 1.0)
    exp_val = tl.exp(x_clamped)
    softplus = tl.log(1.0 + exp_val)
    sigmoid_input = 2.0 * softplus
    tanh_softplus = 2 * tl.sigmoid(sigmoid_input) - 1
    mish_out = x_clamped * tanh_softplus
    
    # GroupNorm: Compute mean and variance
    mean = tl.sum(mish_out, axis=0) / group_size
    centered = mish_out - mean
    var = tl.sum(centered * centered, axis=0) / group_size
    inv_std = 1.0 / tl.sqrt(var + 1e-5)
    
    # Apply GroupNorm
    weight = tl.load(gn_weight_ptr, mask=mask, other=1.0)
    bias_val = tl.load(gn_bias_ptr, mask=mask, other=0.0)
    normalized = centered * inv_std
    result = normalized * weight + bias_val
    
    # Store result
    tl.store(output_ptr, result, mask=mask)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bias_shape, num_groups):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.groupnorm_weight = nn.Parameter(torch.ones(out_features))
        self.groupnorm_bias = nn.Parameter(torch.zeros(out_features))
        self.num_groups = num_groups

    def forward(self, x):
        x = self.gemm(x)
        batch_size, num_features = x.shape
        group_size = num_features // self.num_groups
        
        # Ensure tensor contiguity
        x = x.contiguous()
        output = torch.empty_like(x)
        
        # Launch kernel
        grid = (batch_size, self.num_groups)
        fused_bias_act_groupnorm[grid](
            x, self.bias, self.groupnorm_weight, 
            self.groupnorm_bias, output,
            batch_size, num_features, self.num_groups, group_size,
            x.stride(0), x.stride(1),
            BLOCK_SIZE=group_size
        )
        return output

batch_size = 128
in_features = 512
out_features = 1024
bias_shape = (out_features,)
num_groups = 32

def get_inputs():
    return [torch.randn(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, bias_shape, num_groups]

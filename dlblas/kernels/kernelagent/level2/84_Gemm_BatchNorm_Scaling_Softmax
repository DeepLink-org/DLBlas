import torch
import torch.nn as nn
import triton
import triton.language as tl
import math

@triton.autotune(
    configs=[
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 64}, num_warps=4),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 32}, num_warps=8),
        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=8),
    ],
    key=['M', 'N', 'K'],
)
@triton.jit
def fused_gemm_bn_scale_softmax_kernel(
    x_ptr, w_ptr, b_ptr,
    bn_weight_ptr, bn_bias_ptr, running_mean_ptr, running_var_ptr,
    scale_ptr,
    output_ptr,
    M, N, K,
    stride_xm, stride_xk,
    stride_wn, stride_wk,
    stride_om, stride_on,
    eps: tl.constexpr,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    pid = tl.program_id(0)
    num_pid_n = tl.cdiv(N, BLOCK_N)
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk
    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn
    
    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    for k in range(0, tl.cdiv(K, BLOCK_K)):
        k_remaining = K - k * BLOCK_K
        k_mask = offs_k < k_remaining
        
        x = tl.load(x_ptrs, mask=k_mask[None, :] & (offs_m[:, None] < M), other=0.0)
        w = tl.load(w_ptrs, mask=k_mask[:, None] & (offs_n[None, :] < N), other=0.0)
        
        accumulator += tl.dot(x, w, allow_tf32=True)
        x_ptrs += BLOCK_K * stride_xk
        w_ptrs += BLOCK_K * stride_wk
    
    if b_ptr is not None:
        b = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)
        accumulator += b[None, :]
    
    mean = tl.load(running_mean_ptr + offs_n, mask=offs_n < N, other=0.0)
    var = tl.load(running_var_ptr + offs_n, mask=offs_n < N, other=1.0)
    gamma = tl.load(bn_weight_ptr + offs_n, mask=offs_n < N, other=1.0)
    beta = tl.load(bn_bias_ptr + offs_n, mask=offs_n < N, other=0.0)
    
    inv_std = 1.0 / tl.sqrt(var + eps)
    normalized = (accumulator - mean[None, :]) * inv_std[None, :]
    scaled_bn = normalized * gamma[None, :] + beta[None, :]
    
    scale = tl.load(scale_ptr)
    scaled_output = scaled_bn * scale
    
    # Efficient softmax with vectorization
    row_start_idx = pid_m * BLOCK_M
    col_offsets = tl.arange(0, BLOCK_N)
    for row_idx in range(0, BLOCK_M, 1):
        row = scaled_output[row_idx, :]
        row_minus_max = row - tl.max(row, axis=0)
        numerator = tl.exp(row_minus_max)
        denominator = tl.sum(numerator, axis=0) + 1e-10
        softmax_output = numerator / denominator
        
        output_row = row_start_idx + row_idx
        if output_row < M:
            out_ptrs = output_ptr + output_row * stride_om + offs_n * stride_on
            tl.store(out_ptrs, softmax_output, mask=offs_n < N)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)
        self.scale = nn.Parameter(torch.ones(scale_shape))
        self.eps = bn_eps
        self.out_features = out_features

    def forward(self, x):
        if self.training:
            x = self.gemm(x)
            x = self.bn(x)
            x = self.scale * x
            x = torch.nn.functional.softmax(x, dim=1)
            return x
        
        x = x.contiguous()
        batch_size, in_dim = x.shape
        output = torch.empty((batch_size, self.out_features), device=x.device, dtype=x.dtype)
        
        grid = lambda meta: (triton.cdiv(batch_size, meta['BLOCK_M']) * triton.cdiv(self.out_features, meta['BLOCK_N']),)
        fused_gemm_bn_scale_softmax_kernel[grid](
            x, self.gemm.weight.t(), self.gemm.bias,
            self.bn.weight, self.bn.bias, self.bn.running_mean, self.bn.running_var,
            self.scale,
            output,
            batch_size, self.out_features, in_dim,
            x.stride(0), x.stride(1),
            self.gemm.weight.stride(1), self.gemm.weight.stride(0),
            output.stride(0), output.stride(1),
            self.eps
        )
        
        return output

batch_size = 128
in_features = 1024
out_features = 512
bn_eps = 1e-5
bn_momentum = 0.1
scale_shape = (1,)

def get_inputs():
    return [torch.randn(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]

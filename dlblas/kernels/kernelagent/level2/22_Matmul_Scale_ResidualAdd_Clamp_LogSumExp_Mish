import torch
import torch.nn as nn
import math
import triton
import triton.language as tl

@triton.jit
def fused_linear_ops_kernel(
    x_ptr,
    weight_ptr,
    bias_ptr,
    output_ptr,
    input_size,
    hidden_size,
    scale_factor,
    clamp_min,
    clamp_max,
    BLOCK_SIZE: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
):
    pid_b = tl.program_id(0)
    pid_h_block = tl.program_id(1)
    
    offs_h = pid_h_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask_h = offs_h < hidden_size
    
    x_row_ptr = x_ptr + pid_b * input_size
    accumulator = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)
    
    for k in range(0, input_size, BLOCK_SIZE_K):
        offs_k = k + tl.arange(0, BLOCK_SIZE_K)
        mask_k = offs_k < input_size
        
        x_val = tl.load(x_row_ptr + offs_k, mask=mask_k, other=0.0)
        w_ptrs = weight_ptr + offs_h[:, None] * input_size + offs_k[None, :]
        w_block = tl.load(w_ptrs, mask=mask_h[:, None] & mask_k[None, :], other=0.0)
        
        accumulator += tl.sum(x_val[None, :] * w_block, axis=1)
    
    bias_block = tl.load(bias_ptr + offs_h, mask=mask_h, other=0.0)
    accumulator += bias_block
    accumulator = accumulator * scale_factor * 2.0
    accumulator = tl.minimum(tl.maximum(accumulator, clamp_min), clamp_max)
    
    output_row_ptr = output_ptr + pid_b * hidden_size + offs_h
    tl.store(output_row_ptr, accumulator, mask=mask_h)

@triton.jit
def logsumexp_kernel(
    input_ptr,
    output_ptr,
    hidden_size,
    BLOCK_REDUCE: tl.constexpr,
):
    pid_b = tl.program_id(0)
    offs = pid_b * hidden_size + tl.arange(0, BLOCK_REDUCE)
    mask = tl.arange(0, BLOCK_REDUCE) < hidden_size
    row = tl.load(input_ptr + offs, mask=mask, other=-float('inf'))
    
    max_val = tl.max(row, axis=0)
    row_minus_max = tl.exp(row - max_val)
    sum_val = tl.sum(row_minus_max, axis=0)
    result = tl.log(sum_val) + max_val
    
    tl.store(output_ptr + pid_b, result)

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, scale_factor, clamp_min, clamp_max):
        super(ModelNew, self).__init__()
        self.matmul = nn.Linear(input_size, hidden_size)
        self.scale_factor = scale_factor
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max

    def forward(self, x):
        batch_size, input_size = x.shape
        hidden_size = self.matmul.out_features
        output1 = torch.empty((batch_size, hidden_size), device=x.device, dtype=x.dtype)
        
        grid1 = (batch_size, triton.cdiv(hidden_size, 128))
        fused_linear_ops_kernel[grid1](
            x, self.matmul.weight, self.matmul.bias, output1,
            input_size, hidden_size,
            self.scale_factor,
            self.clamp_min, self.clamp_max,
            BLOCK_SIZE=128, BLOCK_SIZE_K=64
        )
        
        output2 = torch.empty((batch_size,), device=x.device, dtype=x.dtype)
        grid2 = (batch_size,)
        logsumexp_kernel[grid2](
            output1, output2, hidden_size, BLOCK_REDUCE=1024
        )
        output2 = output2.view(-1, 1)
        
        return output2 * torch.nn.functional.mish(output2)

batch_size = 128
input_size = 512
hidden_size = 1024
scale_factor = 2.0
clamp_min = -10.0
clamp_max = 10.0

def get_inputs():
    return [torch.randn(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size, scale_factor, clamp_min, clamp_max]

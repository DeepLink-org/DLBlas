import torch
import torch.nn as nn
import triton
import triton.language as tl
import math

def _triple(x):
    if isinstance(x, int):
        return (x, x, x)
    elif isinstance(x, tuple) and len(x) == 3:
        return x
    else:
        raise ValueError("Value must be int or 3-tuple")

def _output_size(in_size, kernel_size, padding, dilation, stride, ceil_mode):
    numerator = in_size + 2 * padding - dilation * (kernel_size - 1) - 1
    if ceil_mode:
        return (numerator + stride - 1) // stride + 1
    else:
        return numerator // stride + 1

@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),
        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),
        triton.Config({'BLOCK_SIZE': 128}, num_warps=8),
        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),
    ],
    key=['total_output_elements'],
)
@triton.jit
def _max_pool3d_kernel(
    input_ptr,
    output_ptr,
    total_output_elements,
    in_D, in_H, in_W,
    out_D, out_H, out_W,
    stride_d, stride_h, stride_w,
    padding_d, padding_h, padding_w,
    dilation_d, dilation_h, dilation_w,
    kernel_d: tl.constexpr, kernel_h: tl.constexpr, kernel_w: tl.constexpr,
    in_bs, in_cs, in_ds, in_hs, in_ws,
    out_bs, out_cs, out_ds, out_hs, out_ws,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < total_output_elements

    # Calculate output indices
    b_idx = offsets // (out_cs * out_D * out_H * out_W)
    remainder = offsets % (out_cs * out_D * out_H * out_W)
    c_idx = remainder // (out_D * out_H * out_W)
    remainder = remainder % (out_D * out_H * out_W)
    i_idx = remainder // (out_H * out_W)
    remainder = remainder % (out_H * out_W)
    j_idx = remainder // out_W
    k_idx = remainder % out_W

    start_d = i_idx * stride_d - padding_d
    start_h = j_idx * stride_h - padding_h
    start_w = k_idx * stride_w - padding_w

    cur_max = tl.full((BLOCK_SIZE,), float('-inf'), dtype=tl.float32)
    
    # Precompute base pointers for batch and channel
    base = b_idx * in_bs + c_idx * in_cs

    # Unroll small kernel loops for better performance
    for dd in tl.static_range(0, kernel_d):
        d_index = start_d + dd * dilation_d
        for hh in tl.static_range(0, kernel_h):
            h_index = start_h + hh * dilation_h
            for ww in tl.static_range(0, kernel_w):
                w_index = start_w + ww * dilation_w

                within_bounds = (d_index >= 0) & (d_index < in_D) & \
                                (h_index >= 0) & (h_index < in_H) & \
                                (w_index >= 0) & (w_index < in_W)

                # Use precomputed base and add spatial offsets
                spatial_offset = d_index * in_ds + h_index * in_hs + w_index * in_ws
                input_offset = base + spatial_offset
                
                val = tl.load(input_ptr + input_offset, 
                             mask=within_bounds & mask, 
                             other=float('-inf'))
                
                cur_max = tl.maximum(cur_max, val)

    output_offset = b_idx * out_bs + c_idx * out_cs + \
                    i_idx * out_ds + j_idx * out_hs + k_idx * out_ws
    tl.store(output_ptr + output_offset, cur_max, mask=mask)

class ModelNew(nn.Module):
    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, 
                 dilation: int = 1, return_indices: bool = False, ceil_mode: bool = False):
        super(ModelNew, self).__init__()
        self.kernel_size = kernel_size
        self.stride = stride if stride is not None else kernel_size
        self.padding = padding
        self.dilation = dilation
        self.return_indices = return_indices
        self.ceil_mode = ceil_mode

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.return_indices:
            return nn.functional.max_pool3d(
                x, self.kernel_size, self.stride, self.padding, 
                self.dilation, self.ceil_mode, self.return_indices
            )
            
        if not x.is_cuda:
            return nn.functional.max_pool3d(
                x, self.kernel_size, self.stride, self.padding, 
                self.dilation, self.ceil_mode, self.return_indices
            )

        batch_size, channels, in_D, in_H, in_W = x.shape
        kernel_size = _triple(self.kernel_size)
        stride = _triple(self.stride)
        padding = _triple(self.padding)
        dilation = _triple(self.dilation)

        out_D = _output_size(in_D, kernel_size[0], padding[0], 
                             dilation[0], stride[0], self.ceil_mode)
        out_H = _output_size(in_H, kernel_size[1], padding[1], 
                             dilation[1], stride[1], self.ceil_mode)
        out_W = _output_size(in_W, kernel_size[2], padding[2], 
                             dilation[2], stride[2], self.ceil_mode)

        output = torch.empty((batch_size, channels, out_D, out_H, out_W), 
                            device=x.device, dtype=x.dtype)

        total_output_elements = output.numel()
        grid = lambda meta: (triton.cdiv(total_output_elements, meta['BLOCK_SIZE']),)

        _max_pool3d_kernel[grid](
            x, output,
            total_output_elements,
            in_D, in_H, in_W,
            out_D, out_H, out_W,
            stride[0], stride[1], stride[2],
            padding[0], padding[1], padding[2],
            dilation[0], dilation[1], dilation[2],
            kernel_size[0], kernel_size[1], kernel_size[2],
            x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),
            output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4)
        )

        return output

batch_size = 16
channels = 32
dim1 = 64
dim2 = 64
dim3 = 64
kernel_size = 3
stride = 2
padding = 1
dilation = 3

def get_inputs():
    x = torch.randn(batch_size, channels, dim1, dim2, dim3, device='cuda')
    return [x]

def get_init_inputs():
    return [kernel_size, stride, padding, dilation]

import torch
import torch.nn as nn
import math
import triton
import triton.language as tl

@triton.jit
def conv_transpose1d_kernel(
    x_ptr,
    w_ptr,
    b_ptr,
    output_ptr,
    L_in,
    L_out,
    stride,
    padding,
    dilation,
    in_channels,
    kernel_size,
    stride_x_batch,
    stride_x_channel,
    stride_x_length,
    stride_w_in_ch,
    stride_w_out_ch,
    stride_w_kernel,
    stride_out_batch,
    stride_out_ch,
    stride_out_length,
    BLOCK_SIZE_L: tl.constexpr,
):
    pid_b = tl.program_id(0)
    pid_oc = tl.program_id(1)
    pid_block = tl.program_id(2)
    
    # Create ranges for the block
    o_start = pid_block * BLOCK_SIZE_L
    o_offsets = o_start + tl.arange(0, BLOCK_SIZE_L)
    o_mask = o_offsets < L_out
    
    # Initialize accumulator
    acc = tl.zeros((BLOCK_SIZE_L,), dtype=tl.float32)
    
    # Loop over kernel positions
    for k in range(kernel_size):
        # Compute input positions for current kernel position
        numerator = o_offsets + padding - k * dilation
        i_index = numerator // stride
        remainder = numerator % stride
        valid_index = (i_index >= 0) & (i_index < L_in)
        cond = (remainder == 0) & valid_index & o_mask
        
        # Loop over input channels
        for ic in range(in_channels):
            # Calculate input pointer
            x_ptr_offset = pid_b * stride_x_batch + ic * stride_x_channel + i_index * stride_x_length
            x_val = tl.load(x_ptr + x_ptr_offset, mask=cond, other=0.0)
            
            # Calculate weight pointer with corrected layout
            w_ptr_offset = ic * stride_w_in_ch + pid_oc * stride_w_out_ch + k * stride_w_kernel
            w_val = tl.load(w_ptr + w_ptr_offset)
            
            # Accumulate with conditional mask
            acc += tl.where(cond, x_val * w_val, 0.0)
    
    # Add bias if available
    if b_ptr is not None:
        bias_val = tl.load(b_ptr + pid_oc)
        acc += bias_val
    
    # Store results
    out_ptr_offset = pid_b * stride_out_batch + pid_oc * stride_out_ch + o_offsets * stride_out_length
    tl.store(output_ptr + out_ptr_offset, acc, mask=o_mask)

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):
        super(ModelNew, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        
        # Corrected weight shape: (in_channels, out_channels, kernel_size)
        self.weight = nn.Parameter(torch.empty(in_channels, out_channels, kernel_size))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter('bias', None)
        
        # Initialize parameters
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Compute output length
        L_out = (x.shape[-1] - 1) * self.stride - 2 * self.padding + self.dilation * (self.kernel_size - 1) + 1
        
        # Create output tensor (initialized to zero)
        output = torch.zeros(
            (x.shape[0], self.out_channels, L_out),
            device=x.device, dtype=x.dtype
        )
        
        # Get tensor strides
        stride_x_batch, stride_x_channel, stride_x_length = x.stride()
        # Corrected stride order for new weight layout
        stride_w_in_ch, stride_w_out_ch, stride_w_kernel = self.weight.stride()
        stride_out_batch, stride_out_ch, stride_out_length = output.stride()
        
        # Configure and launch kernel
        BLOCK_SIZE_L = 128
        grid = (
            x.shape[0], 
            self.out_channels, 
            triton.cdiv(L_out, BLOCK_SIZE_L)
        )
        
        conv_transpose1d_kernel[grid](
            x,
            self.weight,
            self.bias if self.bias is not None else None,
            output,
            x.shape[-1],
            L_out,
            self.stride,
            self.padding,
            self.dilation,
            self.in_channels,
            self.kernel_size,
            stride_x_batch,
            stride_x_channel,
            stride_x_length,
            stride_w_in_ch,
            stride_w_out_ch,
            stride_w_kernel,
            stride_out_batch,
            stride_out_ch,
            stride_out_length,
            BLOCK_SIZE_L
        )
        return output

# Test code
batch_size = 16
in_channels = 32
out_channels = 64
kernel_size = 3
length = 128
stride = 2
padding = 1
dilation = 2

def get_inputs():
    x = torch.randn(batch_size, in_channels, length)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, dilation]

import torch
import torch.nn as nn
import triton
import triton.language as tl
import math

@triton.jit
def _depthwise_conv2d_vertical_kernel(
    input_ptr: tl.pointer_type(tl.float32),
    weight_ptr: tl.pointer_type(tl.float32),
    bias_ptr: tl.pointer_type(tl.float32),
    output_ptr: tl.pointer_type(tl.float32),
    height,
    width,
    height_out,
    width_out,
    stride,
    padding,
    dilation,
    kernel_size,
    in_channels,
    has_bias: tl.constexpr,
    BLOCK_SIZE: tl.constexpr,
):
    pid_bc = tl.program_id(0)
    pid_h = tl.program_id(1)
    pid_w_block = tl.program_id(2)
    
    batch_idx = pid_bc // in_channels
    channel_idx = pid_bc % in_channels
    h_idx = pid_h
    
    w_start = pid_w_block * BLOCK_SIZE
    w_offsets = w_start + tl.arange(0, BLOCK_SIZE)
    w_mask = w_offsets < width_out
    
    h_in_base = h_idx * stride - padding
    w_in = w_offsets * stride - padding
    w_in_mask = (w_in >= 0) & (w_in < width)
    
    base_offset_input = (batch_idx * in_channels * height * width) + \
                       (channel_idx * height * width)
    base_offset_output = (batch_idx * in_channels * height_out * width_out) + \
                        (channel_idx * height_out * width_out) + \
                        (h_idx * width_out)
    
    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)
    
    for k in range(0, kernel_size):
        h_in = h_in_base + k * dilation
        h_in_mask = (h_in >= 0) & (h_in < height)
        
        mask = w_mask & w_in_mask & h_in_mask
        
        input_offset = base_offset_input + (h_in * width) + w_in
        input_val = tl.load(input_ptr + input_offset, mask=mask, other=0.0)
        
        weight_offset = channel_idx * kernel_size + k
        weight_val = tl.load(weight_ptr + weight_offset)
        
        total += input_val * weight_val

    if has_bias:
        bias_val = tl.load(bias_ptr + channel_idx)
        total += bias_val

    output_offset = base_offset_output + w_offsets
    tl.store(output_ptr + output_offset, total, mask=w_mask)

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):
        super(ModelNew, self).__init__()
        self.conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=(kernel_size, 1), stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        weight = self.conv2d.weight
        bias = self.conv2d.bias
        
        batch_size, _, height, width = x.shape
        stride_h = self.conv2d.stride[0]
        padding_h = self.conv2d.padding[0]
        dilation_h = self.conv2d.dilation[0]
        kernel_size = self.conv2d.kernel_size[0]
        
        height_out = (height + 2 * padding_h - dilation_h * (kernel_size - 1) - 1) // stride_h + 1
        width_out = (width + 2 * padding_h - 1) // stride_h + 1
        
        output = torch.empty((batch_size, self.conv2d.out_channels, height_out, width_out), 
                             device=x.device, dtype=x.dtype)
        
        has_bias = bias is not None
        bias_ptr = bias.data_ptr() if has_bias else 0
        
        total_bc = batch_size * self.conv2d.out_channels
        BLOCK_SIZE = 64
        grid = (total_bc, height_out, triton.cdiv(width_out, BLOCK_SIZE))
        
        _depthwise_conv2d_vertical_kernel[grid](
            x.data_ptr(),
            weight.data_ptr(),
            bias_ptr,
            output.data_ptr(),
            height,
            width,
            height_out,
            width_out,
            stride_h,
            padding_h,
            dilation_h,
            kernel_size,
            self.conv2d.out_channels,
            has_bias,
            BLOCK_SIZE=BLOCK_SIZE,
        )
        return output

# Test code
batch_size = 16
in_channels = 3
kernel_size = 3
width = 256
height = 256
stride = 1
padding = 0
dilation = 1

def get_inputs():
    x = torch.randn(batch_size, in_channels, height, width)
    return [x]

def get_init_inputs():
    return [in_channels, kernel_size, stride, padding, dilation]

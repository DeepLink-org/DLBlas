import torch
import torch.nn as nn
import triton
import triton.language as tl
import math

@triton.jit
def avg_pool3d_kernel(
    x_ptr,
    output_ptr,
    batch_size, channels, depth, height, width,
    stride_b, stride_c, stride_d, stride_h, stride_w,
    kernel_size: tl.constexpr,
    stride_val, padding_val,
    out_depth, out_height, out_width,
    n_elements,
    BLOCK_SIZE: tl.constexpr
):
    pid = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = pid < n_elements
    
    # Compute 5D indices from flattened PID
    chw = out_depth * out_height * out_width
    c_per_batch = channels * chw
    batch_idx = pid // c_per_batch
    remainder = pid % c_per_batch
    channel_idx = remainder // chw
    spatial_idx = remainder % chw
    
    d_out = spatial_idx // (out_height * out_width)
    hw_rem = spatial_idx % (out_height * out_width)
    h_out = hw_rem // out_width
    w_out = hw_rem % out_width
    
    # Compute input window start positions
    d_start = d_out * stride_val - padding_val
    h_start = h_out * stride_val - padding_val
    w_start = w_out * stride_val - padding_val
    
    # Initialize accumulator
    accum = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)
    kernel_vol = kernel_size * kernel_size * kernel_size
    
    # Process 3D kernel window with static unrolling
    for kd in tl.static_range(0, 3):
        d_in = d_start + kd
        d_mask = mask & (d_in >= 0) & (d_in < depth)
        for kh in tl.static_range(0, 3):
            h_in = h_start + kh
            h_mask = d_mask & (h_in >= 0) & (h_in < height)
            for kw in tl.static_range(0, 3):
                w_in = w_start + kw
                w_mask = h_mask & (w_in >= 0) & (w_in < width)
                
                # Calculate full pointer offset
                offset = (
                    batch_idx * stride_b + 
                    channel_idx * stride_c + 
                    d_in * stride_d + 
                    h_in * stride_h + 
                    w_in * stride_w
                )
                val = tl.load(x_ptr + offset, mask=w_mask, other=0.0)
                accum += val
    
    # Compute average and store result
    result = accum / kernel_vol
    tl.store(output_ptr + pid, result, mask=mask)

class ModelNew(nn.Module):
    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):
        super(ModelNew, self).__init__()
        self.kernel_size = kernel_size
        self.stride = stride if stride is not None else kernel_size
        self.padding = padding

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, C, D, H, W = x.shape
        D_out = (D + 2*self.padding - self.kernel_size) // self.stride + 1
        H_out = (H + 2*self.padding - self.kernel_size) // self.stride + 1
        W_out = (W + 2*self.padding - self.kernel_size) // self.stride + 1
        
        # Create output tensor
        output = torch.empty((B, C, D_out, H_out, W_out), 
                            device=x.device, dtype=x.dtype)
        n_elements = output.numel()
        
        if n_elements == 0:
            return output
        
        # Ensure contiguous memory
        x_cont = x.contiguous()
        
        # Launch kernel with optimized block size
        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
        avg_pool3d_kernel[grid](
            x_cont, output,
            B, C, D, H, W,
            x_cont.stride(0), x_cont.stride(1), x_cont.stride(2), 
            x_cont.stride(3), x_cont.stride(4),
            self.kernel_size, self.stride, self.padding,
            D_out, H_out, W_out,
            n_elements,
            BLOCK_SIZE=256
        )
        
        return output

batch_size = 16
channels = 32
depth = 64
height = 64
width = 64
kernel_size = 3
stride = 2
padding = 1

def get_inputs():
    x = torch.randn(batch_size, channels, depth, height, width)
    return [x]

def get_init_inputs():
    return [kernel_size, stride, padding]

import torch
import torch.nn as nn
import triton
import triton.language as tl
import math

@triton.jit
def conv1d_kernel(
    x_ptr,
    w_ptr,
    bias_ptr,
    out_ptr,
    in_channels,
    length,
    kernel_size,
    stride,
    padding,
    dilation,
    groups,
    out_channels,
    out_length,
    has_bias: tl.constexpr,
    BLOCK_IC: tl.constexpr,
    BLOCK_L: tl.constexpr,
):
    pid_batch = tl.program_id(0)
    pid_oc = tl.program_id(1)
    pid_l_block = tl.program_id(2)
    
    # Compute group parameters
    out_channels_per_group = out_channels // groups
    group_idx = pid_oc // out_channels_per_group
    oc_in_group = pid_oc % out_channels_per_group
    in_channels_per_group = in_channels // groups
    
    # Output length block
    l_start = pid_l_block * BLOCK_L
    l_offsets = l_start + tl.arange(0, BLOCK_L)
    l_mask = l_offsets < out_length
    
    # Pointers for current batch and group
    x_batch_ptr = x_ptr + pid_batch * in_channels * length
    x_group_ptr = x_batch_ptr + group_idx * in_channels_per_group * length
    w_oc_ptr = w_ptr + pid_oc * in_channels_per_group * kernel_size
    
    # Initialize accumulator
    acc = tl.zeros((BLOCK_L,), dtype=tl.float32)
    
    # Process input channels in blocks
    for ic_block in range(0, in_channels_per_group, BLOCK_IC):
        ic_offsets = ic_block + tl.arange(0, BLOCK_IC)
        ic_mask = ic_offsets < in_channels_per_group
        
        # Process kernel positions
        for k in range(kernel_size):
            # Load weights for current kernel position
            w = tl.load(
                w_oc_ptr + ic_offsets * kernel_size + k,
                mask=ic_mask,
                other=0.0
            )
            
            # Compute input positions
            input_pos = l_offsets * stride + k * dilation - padding
            pos_mask = (input_pos >= 0) & (input_pos < length)
            
            # Load input values
            x_ptrs = x_group_ptr + ic_offsets[:, None] * length + input_pos[None, :]
            full_mask = ic_mask[:, None] & pos_mask[None, :]
            x_vals = tl.load(x_ptrs, mask=full_mask, other=0.0)
            
            # Accumulate (maintain vector shape)
            product = w[:, None] * x_vals
            acc += tl.sum(product, axis=0)
    
    # Add bias if present
    if has_bias:
        bias_val = tl.load(bias_ptr + pid_oc)
        acc += bias_val
    
    # Store results
    output_offset = pid_batch * out_channels * out_length + pid_oc * out_length + l_offsets
    tl.store(out_ptr + output_offset, acc, mask=l_mask)

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, 
                 stride: int = 1, padding: int = 0, dilation: int = 1, 
                 groups: int = 1, bias: bool = False):
        super(ModelNew, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.groups = groups
        
        assert in_channels % groups == 0, "in_channels must be divisible by groups"
        assert out_channels % groups == 0, "out_channels must be divisible by groups"
        
        self.weight = nn.Parameter(torch.empty(
            out_channels, in_channels // groups, kernel_size
        ))
        
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter('bias', None)
            
        self.reset_parameters()
    
    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
            nn.init.uniform_(self.bias, -bound, bound)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch, in_channels, length = x.shape
        assert in_channels == self.in_channels, "Input channels mismatch"
        
        out_length = (length + 2 * self.padding - 
                      self.dilation * (self.kernel_size - 1) - 1) // self.stride + 1
        
        output = torch.empty(
            (batch, self.out_channels, out_length),
            device=x.device,
            dtype=x.dtype
        )
        
        if self.bias is not None:
            bias_ptr = self.bias
        else:
            bias_ptr = None
            
        # Grid configuration (3 dimensions only)
        grid = (
            batch, 
            self.out_channels,
            triton.cdiv(out_length, 64)
        )
        
        conv1d_kernel[grid](
            x, 
            self.weight, 
            bias_ptr,
            output,
            self.in_channels,
            length,
            self.kernel_size,
            self.stride,
            self.padding,
            self.dilation,
            self.groups,
            self.out_channels,
            out_length,
            has_bias=(self.bias is not None),
            BLOCK_IC=32,
            BLOCK_L=64,
        )
        
        return output

# Test code
batch_size = 16
in_channels = 3
out_channels = 64
kernel_size = 3
length = 512

def get_inputs():
    x = torch.randn(batch_size, in_channels, length)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization

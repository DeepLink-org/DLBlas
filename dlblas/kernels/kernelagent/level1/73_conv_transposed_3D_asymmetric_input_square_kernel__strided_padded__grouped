import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def conv_transpose3d_kernel(
    x_ptr,
    weight_ptr,
    bias_ptr,
    output_ptr,
    stride,
    padding,
    groups,
    in_channels,
    out_channels,
    kernel_size,
    D, H, W,
    D_out, H_out, W_out,
    x_batch_stride, x_channel_stride, x_d_stride, x_h_stride, x_w_stride,
    weight_in_channel_stride, weight_out_channel_stride, weight_d_stride, weight_h_stride, weight_kw_stride,
    output_batch_stride, output_channel_stride, output_d_stride, output_h_stride, output_w_stride,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(0)
    num_output_elements = D_out * H_out * W_out * out_channels
    if pid >= num_output_elements:
        return

    # Decompose linear index
    w_out = pid % W_out
    pid //= W_out
    h_out = pid % H_out
    pid //= H_out
    d_out = pid % D_out
    c_out = pid // D_out
    batch_idx = 0  # Single batch per kernel launch

    # Compute group parameters
    channels_per_group = out_channels // groups
    group_idx = c_out // channels_per_group
    c_out_group = c_out % channels_per_group
    in_channels_per_group = in_channels // groups
    in_group_start = group_idx * in_channels_per_group

    # Initialize accumulator
    acc = 0.0

    # Iterate over kernel dimensions
    for kd in range(kernel_size):
        for kh in range(kernel_size):
            for kw in range(kernel_size):
                # Compute input values (integer arithmetic)
                d_in_val = d_out + padding - kd
                h_in_val = h_out + padding - kh
                w_in_val = w_out + padding - kw
                
                # Check divisibility and bounds
                d_divisible = (d_in_val % stride == 0)
                h_divisible = (h_in_val % stride == 0)
                w_divisible = (w_in_val % stride == 0)
                
                if d_divisible & h_divisible & w_divisible:
                    d_in = d_in_val // stride
                    h_in = h_in_val // stride
                    w_in = w_in_val // stride
                    
                    in_bounds = (0 <= d_in) & (d_in < D) & \
                                (0 <= h_in) & (h_in < H) & \
                                (0 <= w_in) & (w_in < W)
                    
                    if in_bounds:
                        # Process each channel in the group
                        for c_in_offset in range(in_channels_per_group):
                            c_in = in_group_start + c_in_offset
                            
                            # Load input value
                            x_offset = (batch_idx * x_batch_stride + 
                                       c_in * x_channel_stride + 
                                       d_in * x_d_stride + 
                                       h_in * x_h_stride + 
                                       w_in * x_w_stride)
                            x_val = tl.load(x_ptr + x_offset)
                            
                            # Load weight value
                            weight_offset = (c_in * weight_in_channel_stride + 
                                           c_out_group * weight_out_channel_stride + 
                                           kd * weight_d_stride + 
                                           kh * weight_h_stride + 
                                           kw * weight_kw_stride)
                            w_val = tl.load(weight_ptr + weight_offset)
                            
                            # Accumulate
                            acc += x_val * w_val

    # Add bias if available
    if bias_ptr is not None:
        bias_val = tl.load(bias_ptr + c_out)
        acc += bias_val

    # Store result
    output_offset = (batch_idx * output_batch_stride + 
                    c_out * output_channel_stride + 
                    d_out * output_d_stride + 
                    h_out * output_h_stride + 
                    w_out * output_w_stride)
    tl.store(output_ptr + output_offset, acc)

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):
        super(ModelNew, self).__init__()
        self.conv_transpose3d = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size=(kernel_size, kernel_size, kernel_size),
            stride=stride, padding=padding, groups=groups, bias=bias
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        weight = self.conv_transpose3d.weight
        bias = self.conv_transpose3d.bias
        stride = self.conv_transpose3d.stride[0]
        padding = self.conv_transpose3d.padding[0]
        groups = self.conv_transpose3d.groups
        kernel_size = self.conv_transpose3d.kernel_size[0]
        
        batch_size, in_channels, D, H, W = x.shape
        out_channels = self.conv_transpose3d.out_channels
        
        # Calculate output dimensions
        D_out = (D - 1) * stride - 2 * padding + kernel_size
        H_out = (H - 1) * stride - 2 * padding + kernel_size
        W_out = (W - 1) * stride - 2 * padding + kernel_size
        
        output = torch.empty((batch_size, out_channels, D_out, H_out, W_out), 
                            device=x.device, dtype=x.dtype)
        
        # Precompute tensor strides
        x_strides = (x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4))
        weight_strides = (weight.stride(0), weight.stride(1), weight.stride(2), weight.stride(3), weight.stride(4))
        output_strides = (output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4))
        
        # Process each sample in batch
        for b in range(batch_size):
            num_output_elements = D_out * H_out * W_out * out_channels
            grid = lambda meta: (triton.cdiv(num_output_elements, meta['BLOCK_SIZE']),)
            
            conv_transpose3d_kernel[grid](
                x[b],
                weight,
                bias,
                output[b],
                stride,
                padding,
                groups,
                in_channels,
                out_channels,
                kernel_size,
                D, H, W,
                D_out, H_out, W_out,
                *x_strides,
                *weight_strides,
                *output_strides,
                BLOCK_SIZE=128
            )
        
        return output

# Test code
batch_size = 16
in_channels = 32
out_channels = 64
kernel_size = 3
depth = 16
height = 32
width = 32
stride = 2
padding = 3
groups = 4

def get_inputs():
    x = torch.randn(batch_size, in_channels, depth, height, width)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, groups]

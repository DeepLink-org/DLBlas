import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def _pointwise_conv(
    x_ptr, weight_ptr, bias_ptr, output_ptr,
    in_channels, height, width, out_channels,
    stride_ib, stride_ic, stride_ih, stride_iw,
    stride_wo, stride_wi,
    stride_ob, stride_oc, stride_oh, stride_ow,
    has_bias: tl.constexpr,
    BLOCK_C: tl.constexpr,
):
    pid_bhw = tl.program_id(0)
    pid_c = tl.program_id(1)
    c_offsets = pid_c * BLOCK_C + tl.arange(0, BLOCK_C)
    c_mask = c_offsets < out_channels

    n_spatial = height * width
    batch_id = pid_bhw // n_spatial
    spatial_idx = pid_bhw % n_spatial
    h_idx = spatial_idx // width
    w_idx = spatial_idx % width

    x_offset = batch_id * stride_ib + h_idx * stride_ih + w_idx * stride_iw
    acc = tl.zeros((BLOCK_C,), dtype=tl.float32)
    
    for c_in in range(0, in_channels):
        # Corrected: Use channel stride (stride_ic) for input access
        x_val = tl.load(x_ptr + x_offset + c_in * stride_ic)
        w_ptrs = weight_ptr + c_offsets * stride_wo + c_in * stride_wi
        weights = tl.load(w_ptrs, mask=c_mask, other=0.0)
        acc += x_val * weights

    if has_bias:
        b_vals = tl.load(bias_ptr + c_offsets, mask=c_mask, other=0.0)
        acc += b_vals

    out_offset = (batch_id * stride_ob + 
                  c_offsets * stride_oc + 
                  h_idx * stride_oh + 
                  w_idx * stride_ow)
    tl.store(output_ptr + out_offset, acc, mask=c_mask)

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):
        super(ModelNew, self).__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=bias)
        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, bias=bias)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.depthwise(x)
        
        weight = self.pointwise.weight
        bias = self.pointwise.bias if self.pointwise.bias is not None else None
        
        batch, in_chan, h, w = x.shape
        output = torch.empty(batch, weight.size(0), h, w, device=x.device, dtype=x.dtype)
        
        grid = (batch * h * w, triton.cdiv(weight.size(0), 128))
        has_bias = bias is not None
        
        _pointwise_conv[grid](
            x, weight, bias, output,
            in_chan, h, w, weight.size(0),
            x.stride(0), x.stride(1), x.stride(2), x.stride(3),  # Added input channel stride
            weight.stride(0), weight.stride(1),
            output.stride(0), output.stride(1), output.stride(2), output.stride(3),
            has_bias,
            BLOCK_C=128
        )
        
        return output

# Test code
batch_size = 16
in_channels = 3
out_channels = 64
kernel_size = 3
width = 256
height = 256
stride = 1
padding = 0
dilation = 1

def get_inputs():
    x = torch.randn(batch_size, in_channels, height, width)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, dilation]

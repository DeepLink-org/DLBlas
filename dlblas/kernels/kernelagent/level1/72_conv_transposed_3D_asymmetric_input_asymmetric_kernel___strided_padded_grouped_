import torch
import torch.nn as nn
import triton
import triton.language as tl
import math

@triton.jit
def _conv_transpose3d_kernel(
    input_ptr, weight_ptr, bias_ptr, output_ptr,
    in_channels, out_channels, groups,
    depth, height, width,
    kernel_d, kernel_h, kernel_w,
    stride_d, stride_h, stride_w,
    padding_d, padding_h, padding_w,
    D_out, H_out, W_out,
    input_batch_stride, input_channel_stride, input_depth_stride, input_height_stride, input_width_stride,
    weight_in_channel_stride, weight_out_channel_stride, weight_kernel_d_stride, weight_kernel_h_stride, weight_kernel_w_stride,
    output_batch_stride, output_channel_stride, output_depth_stride, output_height_stride, output_width_stride,
    BLOCK_SIZE_CHANNELS: tl.constexpr,
):
    # Get program IDs
    pid_bc = tl.program_id(0)
    pid_s = tl.program_id(1)
    
    # Decompose batch and channel index
    batch_idx = pid_bc // out_channels
    out_channel_idx = pid_bc % out_channels
    
    # Decompose spatial index
    w_out = pid_s % W_out
    h_out = (pid_s // W_out) % H_out
    d_out = pid_s // (W_out * H_out)
    
    # Compute group parameters
    out_channels_per_group = out_channels // groups
    group_index = out_channel_idx // out_channels_per_group
    in_channels_per_group = in_channels // groups
    in_channel_start = group_index * in_channels_per_group
    
    # Initialize accumulator
    acc = 0.0
    
    # Loop over kernel dimensions
    for kd in range(kernel_d):
        for kh in range(kernel_h):
            for kw in range(kernel_w):
                # Calculate input positions
                d_in_val = d_out - kd + padding_d
                if d_in_val % stride_d != 0:
                    continue
                d_in = d_in_val // stride_d
                if d_in < 0 or d_in >= depth:
                    continue
                
                h_in_val = h_out - kh + padding_h
                if h_in_val % stride_h != 0:
                    continue
                h_in = h_in_val // stride_h
                if h_in < 0 or h_in >= height:
                    continue
                
                w_in_val = w_out - kw + padding_w
                if w_in_val % stride_w != 0:
                    continue
                w_in = w_in_val // stride_w
                if w_in < 0 or w_in >= width:
                    continue
                
                # Process channels in blocks
                for c_start in range(0, in_channels_per_group, BLOCK_SIZE_CHANNELS):
                    c_end = min(c_start + BLOCK_SIZE_CHANNELS, in_channels_per_group)
                    c_length = c_end - c_start
                    
                    # Create channel offset
                    co = tl.arange(0, BLOCK_SIZE_CHANNELS)
                    mask = co < c_length
                    
                    # Calculate input offset
                    input_offset = (
                        batch_idx * input_batch_stride +
                        (in_channel_start + c_start + co) * input_channel_stride +
                        d_in * input_depth_stride +
                        h_in * input_height_stride +
                        w_in * input_width_stride
                    )
                    input_val = tl.load(input_ptr + input_offset, mask=mask, other=0.0)
                    
                    # Calculate weight offset
                    weight_offset = (
                        (in_channel_start + c_start + co) * weight_in_channel_stride +
                        (out_channel_idx % out_channels_per_group) * weight_out_channel_stride +
                        kd * weight_kernel_d_stride +
                        kh * weight_kernel_h_stride +
                        kw * weight_kernel_w_stride
                    )
                    weight_val = tl.load(weight_ptr + weight_offset, mask=mask, other=0.0)
                    
                    # Accumulate product
                    partial = tl.sum(input_val * weight_val, axis=0)
                    acc += partial
    
    # Add bias if present
    if bias_ptr is not None:
        bias_val = tl.load(bias_ptr + out_channel_idx)
        acc += bias_val
    
    # Calculate output offset and store
    output_offset = (
        batch_idx * output_batch_stride +
        out_channel_idx * output_channel_stride +
        d_out * output_depth_stride +
        h_out * output_height_stride +
        w_out * output_width_stride
    )
    tl.store(output_ptr + output_offset, acc)

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):
        super(ModelNew, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.output_padding = output_padding
        self.groups = groups
        
        # Initialize weight
        self.weight = nn.Parameter(torch.empty(
            in_channels,
            out_channels // groups,
            kernel_size[0],
            kernel_size[1],
            kernel_size[2]
        ))
        
        # Initialize bias
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter('bias', None)
        
        # Initialize parameters
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            nn.init.zeros_(self.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Ensure contiguous memory
        x = x.contiguous()
        weight = self.weight.contiguous()
        
        # Calculate output dimensions
        batch_size = x.shape[0]
        depth, height, width = x.shape[2], x.shape[3], x.shape[4]
        D_out = (depth - 1) * self.stride[0] - 2 * self.padding[0] + self.kernel_size[0] + self.output_padding[0]
        H_out = (height - 1) * self.stride[1] - 2 * self.padding[1] + self.kernel_size[1] + self.output_padding[1]
        W_out = (width - 1) * self.stride[2] - 2 * self.padding[2] + self.kernel_size[2] + self.output_padding[2]
        
        # Create output tensor
        output = torch.empty(
            (batch_size, self.out_channels, D_out, H_out, W_out),
            device=x.device, 
            dtype=x.dtype
        )
        
        # Prepare kernel parameters
        kernel_d, kernel_h, kernel_w = self.kernel_size
        stride_d, stride_h, stride_w = self.stride
        padding_d, padding_h, padding_w = self.padding
        
        # Compute tensor strides
        input_strides = (x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4))
        weight_strides = (weight.stride(0), weight.stride(1), weight.stride(2), weight.stride(3), weight.stride(4))
        output_strides = (output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4))
        
        # Set grid and block sizes
        grid_bc = batch_size * self.out_channels
        grid_s = D_out * H_out * W_out
        grid = (grid_bc, grid_s)
        
        # Kernel parameters
        kernel_params = {
            "input_ptr": x,
            "weight_ptr": weight,
            "bias_ptr": self.bias if self.bias is not None else None,
            "output_ptr": output,
            "in_channels": self.in_channels,
            "out_channels": self.out_channels,
            "groups": self.groups,
            "depth": depth,
            "height": height,
            "width": width,
            "kernel_d": kernel_d,
            "kernel_h": kernel_h,
            "kernel_w": kernel_w,
            "stride_d": stride_d,
            "stride_h": stride_h,
            "stride_w": stride_w,
            "padding_d": padding_d,
            "padding_h": padding_h,
            "padding_w": padding_w,
            "D_out": D_out,
            "H_out": H_out,
            "W_out": W_out,
            "input_batch_stride": input_strides[0],
            "input_channel_stride": input_strides[1],
            "input_depth_stride": input_strides[2],
            "input_height_stride": input_strides[3],
            "input_width_stride": input_strides[4],
            "weight_in_channel_stride": weight_strides[0],
            "weight_out_channel_stride": weight_strides[1],
            "weight_kernel_d_stride": weight_strides[2],
            "weight_kernel_h_stride": weight_strides[3],
            "weight_kernel_w_stride": weight_strides[4],
            "output_batch_stride": output_strides[0],
            "output_channel_stride": output_strides[1],
            "output_depth_stride": output_strides[2],
            "output_height_stride": output_strides[3],
            "output_width_stride": output_strides[4],
            "BLOCK_SIZE_CHANNELS": 128,
        }
        
        # Launch kernel
        _conv_transpose3d_kernel[grid](**kernel_params)
        
        return output

# Test code
batch_size = 16
in_channels = 32
out_channels = 64
kernel_size = (3, 5, 7)
depth = 16
height = 32
width = 64
stride = (2, 2, 2)
padding = (1, 2, 3)
output_padding = (1, 1, 1)
groups = 4

def get_inputs():
    x = torch.randn(batch_size, in_channels, depth, height, width)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, groups]

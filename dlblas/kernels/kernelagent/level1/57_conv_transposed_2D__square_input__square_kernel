import torch
import torch.nn as nn
import triton
import triton.language as tl
import math

@triton.jit
def _conv_transpose2d_kernel(
    # Pointers to input, weights, output
    x_ptr,
    weight_ptr,
    output_ptr,
    # Tensor dimensions
    B, C_in, H_in, W_in,
    C_out, H_out, W_out,
    # Kernel parameters
    kernel_size: tl.constexpr,
    stride: tl.constexpr,
    padding: tl.constexpr,
    output_padding: tl.constexpr,
    groups: tl.constexpr,
    # Stride information
    x_batch_stride, x_channel_stride, x_height_stride, x_width_stride,
    weight_in_stride, weight_out_stride, weight_h_stride, weight_w_stride,
    out_batch_stride, out_channel_stride, out_height_stride, out_width_stride,
    # Blocking
    BLOCK_SIZE_H: tl.constexpr,
    BLOCK_SIZE_W: tl.constexpr,
    BLOCK_SIZE_C: tl.constexpr,
    total_blocks_w: tl.constexpr,
):
    # Get program IDs
    pid_b = tl.program_id(0)
    pid_c = tl.program_id(1)
    pid_hw = tl.program_id(2)
    
    # Split combined HW dimension
    block_id_h = pid_hw // total_blocks_w
    block_id_w = pid_hw % total_blocks_w
    pid_h = block_id_h * BLOCK_SIZE_H
    pid_w = block_id_w * BLOCK_SIZE_W
    
    # Create masks for H and W blocks
    h_offsets = pid_h + tl.arange(0, BLOCK_SIZE_H)
    w_offsets = pid_w + tl.arange(0, BLOCK_SIZE_W)
    h_mask = h_offsets < H_out
    w_mask = w_offsets < W_out
    
    # Initialize output block to zero
    output_block = tl.zeros((BLOCK_SIZE_H, BLOCK_SIZE_W), dtype=tl.float32)
    
    # Calculate group info
    group_size = C_out // groups
    group_id = pid_c // group_size
    c_in_start = group_id * (C_in // groups)
    c_in_end = (group_id + 1) * (C_in // groups)
    
    # Loop over input channels in group
    for c_in in range(c_in_start, c_in_end):
        # Loop over kernel positions
        for kh in range(kernel_size):
            for kw in range(kernel_size):
                # Calculate input positions
                h_in = (h_offsets[:, None] - kh + padding) // stride
                w_in = (w_offsets[None, :] - kw + padding) // stride
                
                # Check divisibility and boundaries
                h_valid = (h_offsets[:, None] - kh + padding) % stride == 0
                w_valid = (w_offsets[None, :] - kw + padding) % stride == 0
                h_in_bound = (h_in >= 0) & (h_in < H_in)
                w_in_bound = (w_in >= 0) & (w_in < W_in)
                
                # Combined mask
                valid_mask = h_valid & w_valid & h_in_bound & w_in_bound
                
                # Compute input pointers
                x_ptrs = (
                    x_ptr + 
                    pid_b * x_batch_stride +
                    c_in * x_channel_stride +
                    h_in * x_height_stride +
                    w_in * x_width_stride
                )
                
                # Load input values with mask
                x_vals = tl.load(x_ptrs, mask=valid_mask, other=0.0)
                
                # Compute weight pointers
                weight_ptrs = (
                    weight_ptr +
                    c_in * weight_in_stride +
                    (pid_c % group_size) * weight_out_stride +
                    kh * weight_h_stride +
                    kw * weight_w_stride
                )
                
                # Load weight value
                w_val = tl.load(weight_ptrs)
                
                # Accumulate
                output_block += x_vals * w_val
    
    # Compute output pointers
    out_ptrs = (
        output_ptr +
        pid_b * out_batch_stride +
        pid_c * out_channel_stride +
        h_offsets[:, None] * out_height_stride +
        w_offsets[None, :] * out_width_stride
    )
    
    # Store output block
    tl.store(out_ptrs, output_block, mask=h_mask[:, None] & w_mask[None, :])

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, 
                 stride: int = 1, padding: int = 0, output_padding: int = 0, 
                 groups: int = 1, bias: bool = False):
        super(ModelNew, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.output_padding = output_padding
        self.groups = groups
        
        # Initialize weights
        self.weight = nn.Parameter(torch.empty(
            in_channels, 
            out_channels // groups,
            kernel_size, 
            kernel_size
        ))
        
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter('bias', None)
        
        # Initialize parameters
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, C_in, H_in, W_in = x.shape
        H_out = (H_in - 1) * self.stride - 2 * self.padding + self.kernel_size + self.output_padding
        W_out = (W_in - 1) * self.stride - 2 * self.padding + self.kernel_size + self.output_padding
        
        # Preallocate output tensor
        output = torch.zeros((B, self.out_channels, H_out, W_out), 
                             device=x.device, dtype=x.dtype)
        
        # Compute grid dimensions
        BLOCK_H = 16
        BLOCK_W = 16
        total_blocks_h = triton.cdiv(H_out, BLOCK_H)
        total_blocks_w = triton.cdiv(W_out, BLOCK_W)
        total_blocks_hw = total_blocks_h * total_blocks_w
        
        # Define grid dimensions (3D)
        grid = (
            B, 
            self.out_channels, 
            total_blocks_hw
        )
        
        # Launch kernel
        _conv_transpose2d_kernel[grid](
            x, self.weight, output,
            B, C_in, H_in, W_in,
            self.out_channels, H_out, W_out,
            self.kernel_size, self.stride, self.padding, self.output_padding, self.groups,
            x.stride(0), x.stride(1), x.stride(2), x.stride(3),
            self.weight.stride(0), self.weight.stride(1), self.weight.stride(2), self.weight.stride(3),
            output.stride(0), output.stride(1), output.stride(2), output.stride(3),
            BLOCK_SIZE_H=BLOCK_H, 
            BLOCK_SIZE_W=BLOCK_W, 
            BLOCK_SIZE_C=1,
            total_blocks_w=total_blocks_w
        )
        
        # Add bias if exists
        if self.bias is not None:
            output += self.bias.view(1, -1, 1, 1)
            
        return output

# Test code
batch_size = 16
in_channels = 32
out_channels = 64
kernel_size = 3
width = 128
height = 128

def get_inputs():
    x = torch.randn(batch_size, in_channels, height, width)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization

{"id": "b4945c41-d0fb-4769-9598-60f0258d65df", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 4}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 4}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 4}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 8}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 8}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 8}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 8}, num_warps=4),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef fused_div_leaky_relu(\n    x_ptr,\n    output_ptr,\n    divisor,\n    negative_slope,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n    VEC_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    total_elements_per_block = BLOCK_SIZE * VEC_SIZE\n    block_start = pid * total_elements_per_block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)[:, None] * VEC_SIZE + tl.arange(0, VEC_SIZE)[None, :]\n    mask = offsets < n_elements\n    \n    # Vectorized load\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    \n    # Precompute reciprocal for faster division\n    inv_divisor = 1.0 / divisor\n    x_scaled = x * inv_divisor\n    \n    # Fused leaky ReLU\n    result = tl.where(x_scaled >= 0, x_scaled, x_scaled * negative_slope)\n    \n    # Vectorized store\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x_contig = x.contiguous()\n        output = torch.empty_like(x_contig)\n        n_elements = x_contig.numel()\n        \n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE'] * meta['VEC_SIZE']),)\n        \n        fused_div_leaky_relu[grid](\n            x_contig, output, self.divisor, 0.01, n_elements\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "a9ffb2d3-7fd2-44a4-b152-aff1ba7324ac", "generation": 2, "timestamp": 1754584956.278659, "iteration_found": 25, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.668, "runtime_ms": 0.0885, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0591, "speed_up": 0.668, "custom_timing": 0.0885}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.694, "runtime_ms": 0.0852, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0591, "speed_up": 0.694, "custom_timing": 0.0852}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.6940\n- runtime_ms: 0.0852\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0591, 'speed_up': 0.694, 'custom_timing': 0.0852}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: speed_up. Consider continuing with similar changes.\n- Metrics showing regression: runtime_ms. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.6390, runtime_ms: 0.0925, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0591, 'speed_up': 0.639, 'custom_timing': 0.0925}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.4840, runtime_ms: 0.1220, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0591, 'speed_up': 0.484, 'custom_timing': 0.122}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.6940, runtime_ms: 0.0852, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0591, 'speed_up': 0.694, 'custom_timing': 0.0852}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 2048}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 4096}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef fused_div_leaky_relu(\n    x_ptr,\n    output_ptr,\n    divisor,\n    negative_slope,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Vectorized load for coalesced memory access\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    \n    # Fused operations: division + leaky ReLU\n    x = x / divisor\n    leaky_val = x * negative_slope\n    result = tl.where(x >= 0, x, leaky_val)\n    \n    # Vectorized store\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x_contig = x.contiguous()\n        output = torch.empty_like(x_contig)\n        n_elements = x_contig.numel()\n        \n        # Dynamically determine grid size based on elements\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        \n        # Launch optimized fused kernel\n        fused_div_leaky_relu[grid](\n            x_contig, output, self.divisor, 0.01, n_elements\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.6940), Performs well on runtime_ms (0.0852), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0591, 'speed_up': 0.694, 'custom_timing': 0.0852})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n    ],\n    key=['n_elements']\n)\n@triton.jit\ndef leaky_relu_scale_kernel(\n    input_ptr,\n    output_ptr,\n    divisor,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    x = tl.load(input_ptr + offsets, mask=mask)\n    x_scaled = x / divisor\n    result = tl.where(x_scaled >= 0, x_scaled, 0.01 * x_scaled)\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        leaky_relu_scale_kernel[grid](x, output, self.divisor, n_elements)\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.4840), Performs well on runtime_ms (0.1220), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0591, 'speed_up': 0.484, 'custom_timing': 0.122})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 4}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 4}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 4}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 8}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 8}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 8}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef leaky_relu_scale_kernel(\n    input_ptr,\n    output_ptr,\n    divisor,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n    VEC_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE * VEC_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)[:, None] * VEC_SIZE + tl.arange(0, VEC_SIZE)[None, :]\n    mask = offsets < n_elements\n    \n    x = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    inv_divisor = 1.0 / divisor\n    x_scaled = x * inv_divisor\n    result = tl.where(x_scaled >= 0, x_scaled, 0.01 * x_scaled)\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x_contig = x.contiguous()\n        output = torch.empty_like(x_contig)\n        n_elements = x_contig.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE'] * meta['VEC_SIZE']),)\n        leaky_relu_scale_kernel[grid](\n            x_contig, output, self.divisor, n_elements\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.6390), Performs well on runtime_ms (0.0925), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0591, 'speed_up': 0.639, 'custom_timing': 0.0925})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=['in_channels', 'out_channels', 'height', 'width'],\n)\n@triton.jit\ndef conv_div_relu_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    in_channels, out_channels, height, width, divisor,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    weight_out_stride, weight_in_stride, weight_kh_stride, weight_kw_stride,\n    output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n    BLOCK_SIZE_H: tl.constexpr, BLOCK_SIZE_W: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    pid_h = tl.program_id(2)\n    pid_w = tl.program_id(3)\n    \n    output_h = height - 2\n    output_w = width - 2\n    h_start = pid_h * BLOCK_SIZE_H\n    w_start = pid_w * BLOCK_SIZE_W\n    \n    h_offsets = h_start + tl.arange(0, BLOCK_SIZE_H)\n    w_offsets = w_start + tl.arange(0, BLOCK_SIZE_W)\n    h_mask = (h_offsets < output_h) & (h_offsets >= 0)\n    w_mask = (w_offsets < output_w) & (w_offsets >= 0)\n    hw_mask = h_mask[:, None] & w_mask[None, :]\n    \n    acc = tl.zeros((BLOCK_SIZE_H, BLOCK_SIZE_W), dtype=tl.float32)\n    \n    for kc in range(in_channels):\n        for dy in range(3):\n            for dx in range(3):\n                input_y = h_offsets[:, None] + dy\n                input_x = w_offsets[None, :] + dx\n                input_mask = (input_y < height) & (input_x < width)\n                \n                input_vals = tl.load(\n                    input_ptr + pid_batch * input_batch_stride + \n                    kc * input_channel_stride + \n                    input_y * input_height_stride + \n                    input_x * input_width_stride,\n                    mask=input_mask & hw_mask,\n                    other=0.0\n                )\n                \n                weight_val = tl.load(\n                    weight_ptr + \n                    pid_channel * weight_out_stride + \n                    kc * weight_in_stride + \n                    dy * weight_kh_stride + \n                    dx * weight_kw_stride\n                )\n                \n                acc += input_vals * weight_val\n    \n    bias_val = tl.load(bias_ptr + pid_channel)\n    acc += bias_val\n    acc = acc / divisor\n    acc = tl.where(acc >= 0, acc, acc * 0.01)\n    \n    output_offsets = (\n        pid_batch * output_batch_stride + \n        pid_channel * output_channel_stride + \n        h_offsets[:, None] * output_height_stride + \n        w_offsets[None, :] * output_width_stride\n    )\n    \n    tl.store(\n        output_ptr + output_offsets,\n        acc,\n        mask=hw_mask\n    )\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n        \n    def forward(self, x):\n        x = x.contiguous()\n        weight = self.conv.weight.contiguous()\n        bias = self.conv.bias.contiguous()\n        \n        batch_size, _, height, width = x.shape\n        output_h, output_w = height - 2, width - 2\n        output = torch.empty(\n            batch_size, \n            self.conv.out_channels, \n            output_h, \n            output_w, \n            device=x.device, \n            dtype=x.dtype\n        )\n        \n        grid = (batch_size, self.conv.out_channels, \n                triton.cdiv(output_h, 16), \n                triton.cdiv(output_w, 16))\n        \n        conv_div_relu_kernel[grid](\n            x, weight, bias, output,\n            self.conv.in_channels, \n            self.conv.out_channels, \n            height, \n            width, \n            self.divisor,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            weight.stride(0), weight.stride(1), weight.stride(2), weight.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            BLOCK_SIZE_H=16, BLOCK_SIZE_W=16\n        )\n        \n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 2048}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 4096}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 8192}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 16384}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef fused_div_leaky_relu(\n    x_ptr,\n    output_ptr,\n    divisor,\n    negative_slope,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    start_idx = pid * BLOCK_SIZE\n    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Vectorized load/store with block pointers\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(n_elements,),\n        strides=(1,),\n        offsets=(start_idx,),\n        block_shape=(BLOCK_SIZE,),\n        order=(0,)\n    )\n    output_block_ptr = tl.make_block_ptr(\n        base=output_ptr,\n        shape=(n_elements,),\n        strides=(1,),\n        offsets=(start_idx,),\n        block_shape=(BLOCK_SIZE,),\n        order=(0,)\n    )\n    \n    x = tl.load(x_block_ptr, boundary_check=(0,), padding_option=\"zero\", mask=mask)\n    \n    # Fused operations: division + leaky ReLU\n    x_scaled = x / divisor\n    leaky_val = x_scaled * negative_slope\n    result = tl.where(x_scaled >= 0, x_scaled, leaky_val)\n    \n    tl.store(output_block_ptr, result, boundary_check=(0,), mask=mask)\n    \n    # Log optimal configuration during tuning\n    tl.static_print(f\"Using BLOCK_SIZE={BLOCK_SIZE}, warps={tl.num_warps()}\")\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        # Only make contiguous if necessary\n        if not x.is_contiguous():\n            x = x.contiguous()\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_div_leaky_relu[grid](\n            x, output, self.divisor, 0.01, n_elements\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 4}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 4}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 4}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 8}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 8}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 8}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef leaky_relu_scale_kernel(\n    input_ptr,\n    output_ptr,\n    divisor,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n    VEC_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE * VEC_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)[:, None] * VEC_SIZE + tl.arange(0, VEC_SIZE)[None, :]\n    mask = offsets < n_elements\n    \n    x = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    inv_divisor = 1.0 / divisor\n    x_scaled = x * inv_divisor\n    result = tl.where(x_scaled >= 0, x_scaled, 0.01 * x_scaled)\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x_contig = x.contiguous()\n        output = torch.empty_like(x_contig)\n        n_elements = x_contig.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE'] * meta['VEC_SIZE']),)\n        leaky_relu_scale_kernel[grid](\n            x_contig, output, self.divisor, n_elements\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 2048, 'VEC_SIZE': 4}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 4096, 'VEC_SIZE': 4}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef fused_div_leaky_relu(\n    x_ptr,\n    output_ptr,\n    divisor,\n    negative_slope,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n    VEC_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * (BLOCK_SIZE * VEC_SIZE)\n    offsets = block_start + tl.arange(0, BLOCK_SIZE) * VEC_SIZE\n    mask = (offsets + tl.arange(0, VEC_SIZE)) < n_elements\n    \n    # Vectorized load for coalesced memory access\n    x = tl.load(\n        x_ptr + offsets[:, None] + tl.arange(0, VEC_SIZE)[None, :],\n        mask=mask,\n        other=0.0\n    )\n    \n    # Fused operations: division + leaky ReLU\n    x = x / divisor\n    result = tl.where(x >= 0, x, x * negative_slope)\n    \n    # Vectorized store\n    tl.store(\n        output_ptr + offsets[:, None] + tl.arange(0, VEC_SIZE)[None, :],\n        result,\n        mask=mask\n    )\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x_contig = x.contiguous()\n        output = torch.empty_like(x_contig)\n        n_elements = x_contig.numel()\n        \n        # Dynamically determine grid size based on elements\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE'] * meta['VEC_SIZE']),)\n        \n        # Launch optimized fused kernel\n        fused_div_leaky_relu[grid](\n            x_contig, output, self.divisor, 0.01, n_elements\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 2048}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 4096}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 8192}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 16384}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef fused_div_leaky_relu(\n    x_ptr,\n    output_ptr,\n    divisor,\n    negative_slope,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    start_idx = pid * BLOCK_SIZE\n    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Vectorized load/store with block pointers\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(n_elements,),\n        strides=(1,),\n        offsets=(start_idx,),\n        block_shape=(BLOCK_SIZE,),\n        order=(0,)\n    )\n    output_block_ptr = tl.make_block_ptr(\n        base=output_ptr,\n        shape=(n_elements,),\n        strides=(1,),\n        offsets=(start_idx,),\n        block_shape=(BLOCK_SIZE,),\n        order=(0,)\n    )\n    \n    x = tl.load(x_block_ptr, boundary_check=(0,), padding_option=\"zero\", mask=mask)\n    \n    # Fused operations: division + leaky ReLU\n    x_scaled = x / divisor\n    leaky_val = x_scaled * negative_slope\n    result = tl.where(x_scaled >= 0, x_scaled, leaky_val)\n    \n    tl.store(output_block_ptr, result, boundary_check=(0,), mask=mask)\n    \n    # Log optimal configuration during tuning\n    tl.static_print(f\"Using BLOCK_SIZE={BLOCK_SIZE}, warps={tl.num_warps()}\")\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        # Only make contiguous if necessary\n        if not x.is_contiguous():\n            x = x.contiguous()\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_div_leaky_relu[grid](\n            x, output, self.divisor, 0.01, n_elements\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=['in_channels', 'out_channels', 'height', 'width'],\n)\n@triton.jit\ndef conv_div_relu_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    in_channels, out_channels, height, width, divisor,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    weight_out_stride, weight_in_stride, weight_kh_stride, weight_kw_stride,\n    output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n    BLOCK_SIZE_H: tl.constexpr, BLOCK_SIZE_W: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    pid_h = tl.program_id(2)\n    pid_w = tl.program_id(3)\n    \n    output_h = height - 2\n    output_w = width - 2\n    h_start = pid_h * BLOCK_SIZE_H\n    w_start = pid_w * BLOCK_SIZE_W\n    \n    h_offsets = h_start + tl.arange(0, BLOCK_SIZE_H)\n    w_offsets = w_start + tl.arange(0, BLOCK_SIZE_W)\n    h_mask = (h_offsets < output_h) & (h_offsets >= 0)\n    w_mask = (w_offsets < output_w) & (w_offsets >= 0)\n    hw_mask = h_mask[:, None] & w_mask[None, :]\n    \n    acc = tl.zeros((BLOCK_SIZE_H, BLOCK_SIZE_W), dtype=tl.float32)\n    \n    for kc in range(in_channels):\n        for dy in range(3):\n            for dx in range(3):\n                input_y = h_offsets[:, None] + dy\n                input_x = w_offsets[None, :] + dx\n                input_mask = (input_y < height) & (input_x < width)\n                \n                input_vals = tl.load(\n                    input_ptr + pid_batch * input_batch_stride + \n                    kc * input_channel_stride + \n                    input_y * input_height_stride + \n                    input_x * input_width_stride,\n                    mask=input_mask & hw_mask,\n                    other=0.0\n                )\n                \n                weight_val = tl.load(\n                    weight_ptr + \n                    pid_channel * weight_out_stride + \n                    kc * weight_in_stride + \n                    dy * weight_kh_stride + \n                    dx * weight_kw_stride\n                )\n                \n                acc += input_vals * weight_val\n    \n    bias_val = tl.load(bias_ptr + pid_channel)\n    acc += bias_val\n    acc = acc / divisor\n    acc = tl.where(acc >= 0, acc, acc * 0.01)\n    \n    output_offsets = (\n        pid_batch * output_batch_stride + \n        pid_channel * output_channel_stride + \n        h_offsets[:, None] * output_height_stride + \n        w_offsets[None, :] * output_width_stride\n    )\n    \n    tl.store(\n        output_ptr + output_offsets,\n        acc,\n        mask=hw_mask\n    )\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n        \n    def forward(self, x):\n        x = x.contiguous()\n        weight = self.conv.weight.contiguous()\n        bias = self.conv.bias.contiguous()\n        \n        batch_size, _, height, width = x.shape\n        output_h, output_w = height - 2, width - 2\n        output = torch.empty(\n            batch_size, \n            self.conv.out_channels, \n            output_h, \n            output_w, \n            device=x.device, \n            dtype=x.dtype\n        )\n        \n        grid = (batch_size, self.conv.out_channels, \n                triton.cdiv(output_h, 16), \n                triton.cdiv(output_w, 16))\n        \n        conv_div_relu_kernel[grid](\n            x, weight, bias, output,\n            self.conv.in_channels, \n            self.conv.out_channels, \n            height, \n            width, \n            self.divisor,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            weight.stride(0), weight.stride(1), weight.stride(2), weight.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            BLOCK_SIZE_H=16, BLOCK_SIZE_W=16\n        )\n        \n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n    ],\n    key=['n_elements']\n)\n@triton.jit\ndef leaky_relu_scale_kernel(\n    input_ptr,\n    output_ptr,\n    divisor,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    x = tl.load(input_ptr + offsets, mask=mask)\n    x_scaled = x / divisor\n    result = tl.where(x_scaled >= 0, x_scaled, 0.01 * x_scaled)\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        leaky_relu_scale_kernel[grid](x, output, self.divisor, n_elements)\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 2048}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 4096}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef fused_div_leaky_relu(\n    x_ptr,\n    output_ptr,\n    divisor,\n    negative_slope,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Vectorized load for coalesced memory access\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    \n    # Fused operations: division + leaky ReLU\n    x = x / divisor\n    leaky_val = x * negative_slope\n    result = tl.where(x >= 0, x, leaky_val)\n    \n    # Vectorized store\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x_contig = x.contiguous()\n        output = torch.empty_like(x_contig)\n        n_elements = x_contig.numel()\n        \n        # Dynamically determine grid size based on elements\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        \n        # Launch optimized fused kernel\n        fused_div_leaky_relu[grid](\n            x_contig, output, self.divisor, 0.01, n_elements\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program which uses a Triton kernel to perform a fused operation: divide by a scalar and then apply leaky ReLU.\n The current performance shows a speed_up of 0.694 (i.e., slower than the reference PyTorch code).\n\n The problem: The current Triton kernel is performing a simple elementwise operation but is slower than the PyTorch reference.\n\n Analysis:\n   - The operation is: output = (x / divisor) if (x/divisor) >=0 else 0.01*(x/divisor)\n   - The current kernel uses a straightforward approach: load, compute, store.\n\n Why might it be slow?\n   - The kernel might not be efficiently using the memory bandwidth (coalescing) or the compute resources.\n   - The autotuning configurations might not be optimal for the problem size.\n\n Steps to improve:\n 1. Vectorize the loads and stores: Instead of loading one element per thread, we can load a vector of elements (e.g., 4 or 8 elements) to increase memory throughput and reduce the number of instructions.\n\n 2. Adjust the autotuning configurations to include vectorization and more warps? But note that the current autotune only varies BLOCK_SIZE and num_warps.\n\n 3. We note that the operation is memory bound. Therefore, we should focus on maximizing memory bandwidth utilization.\n\n 4. The current kernel uses `tl.arange(0, BLOCK_SIZE)` which is a contiguous block. However, without vectorization, each thread is processing one element. We can change the kernel to process multiple elements per thread (vectorized).\n\n 5. We can take inspiration from the top-performing programs and inspiration programs that use vectorization (e.g., Program 3 and Inspiration 1).\n\n Proposed changes:\n   - We will introduce a `VEC_SIZE` parameter (e.g., 4 or 8) to process multiple elements per thread.\n   - We will change the offsets to load a vector of elements at once.\n   - We will adjust the mask accordingly.\n\n Note: The total number of elements processed by a block will be `BLOCK_SIZE * VEC_SIZE`. Therefore, the grid size becomes `triton.cdiv(n_elements, BLOCK_SIZE * VEC_SIZE)`.\n\n We'll modify the kernel to:\n   - Have a new `VEC_SIZE` as a `tl.constexpr`.\n   - Use a 2D offset: one dimension for the block (of size BLOCK_SIZE) and the other for the vector (of size VEC_SIZE).\n   - Use vectorized loads and stores.\n\n Also, we'll extend the autotuning configurations to include different vector sizes and block sizes.\n\n Let's rewrite the kernel with vectorization and update the autotune configs accordingly.\n\n Important: We must ensure that the number of elements is divisible by the vector size or we handle the remainder with a mask.\n\n We note that the current program uses:\n   - `x_contig = x.contiguous()` to ensure contiguous memory.\n   - The tensor is 4D but we are treating it as 1D by using `numel()`.\n\n We can safely vectorize because the tensor is contiguous.\n\n Let's write the new kernel:\n\n Steps in the kernel:\n   pid = tl.program_id(0)\n   total_elements_per_block = BLOCK_SIZE * VEC_SIZE\n   block_start = pid * total_elements_per_block\n   offsets = block_start + tl.arange(0, BLOCK_SIZE)[:, None] * VEC_SIZE + tl.arange(0, VEC_SIZE)[None, :]\n   mask = (offsets < n_elements)   # This is a 2D mask\n\n   x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n   ... compute ...\n   tl.store(output_ptr + offsets, result, mask=mask)\n\n We also change the grid function to: `lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE'] * meta['VEC_SIZE']),)`\n\n We'll update the autotune configs to include different vector sizes (e.g., 4 and 8) and adjust the block sizes accordingly.\n\n Since the problem size is fixed (batch_size=128, in_channels=3, height=32, width=32 -> total elements = 128*16*30*30 = 1,843,200), we can try to cover block sizes that are multiples of the vector size.\n\n We'll create a new set of configs that include both BLOCK_SIZE and VEC_SIZE.\n\n Note: We must be cautious about the number of warps. Each warp is 32 threads. The total threads per block is BLOCK_SIZE (which is the number of rows in the 2D block). So we are launching BLOCK_SIZE threads per block? Actually, no: we are launching a 1D grid of blocks, and each block has BLOCK_SIZE threads? But note: in the vectorized version, each thread processes VEC_SIZE elements. The block dimension is still 1D with BLOCK_SIZE threads.\n\n However, the thread block is of size BLOCK_SIZE. We must set the number of warps appropriately: `num_warps = ceil(BLOCK_SIZE / 32)`. But note: Triton requires that the number of threads per block is a multiple of the warp size (32). So BLOCK_SIZE must be a multiple of 32? Not necessarily, but it is recommended. Alternatively, we can set the number of warps and Triton will set the threads per block accordingly? Actually, the `num_warps` parameter is independent of the BLOCK_SIZE? \n\n How does it work?\n   - The `num_warps` specifies the number of warps per block.\n   - The total threads per block is `num_warps * 32`.\n   - Therefore, we must set `BLOCK_SIZE` to be equal to `num_warps * 32`? \n\n But note: in the kernel, we are using `tl.arange(0, BLOCK_SIZE)`. This means we are launching a block with `BLOCK_SIZE` threads. However, Triton requires that the number of threads per block is a multiple of the warp size. So we must ensure that BLOCK_SIZE is a multiple of 32? And then set `num_warps = BLOCK_SIZE // 32`.\n\n However, the autotune configs in Triton allow specifying `num_warps` independently. The compiler will check that the block size is a multiple of the warp size? Actually, the block size is the product of the dimensions of the `tl.arange` calls? In our case, we have a 1D block of size BLOCK_SIZE. So we must have BLOCK_SIZE divisible by 32? And then set `num_warps = BLOCK_SIZE // 32`? \n\n But note: the autotune configs in the current program set `num_warps` independently. How do they relate? The `num_warps` must be at least `ceil(BLOCK_SIZE / 32)`. However, Triton will automatically round up the number of warps? Or we must set it correctly.\n\n Actually, the Triton documentation says: the `num_warps` parameter in the config is the number of warps for the entire block. So we must have:\n   `BLOCK_SIZE <= num_warps * 32`\n\n But we want to use the full warp: so we set `num_warps = (BLOCK_SIZE + 31) // 32`? However, we are letting the autotuner choose both.\n\n Alternatively, we can set the `num_warps` to a fixed value and let BLOCK_SIZE be independent? But note: the block size (number of threads) is BLOCK_SIZE, so we must have:\n   `BLOCK_SIZE <= num_warps * 32`\n\n We can set the configs with BLOCK_SIZE and num_warps that satisfy that.\n\n Since we are introducing vectorization, we can reduce the number of threads per block and still process many elements.\n\n We'll set the autotune configs to:\n\n   configs = [\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 4}, num_warps=4),   # 256 threads, 4 warps (128 threads per warp? no, 4 warps * 32 = 128 threads -> wait, 256>128 -> invalid)\n   ]\n\n We see: if BLOCK_SIZE=256, then we need at least 256/32 = 8 warps. So we must set num_warps=8.\n\n Let me recalc:\n   - Each warp has 32 threads.\n   - Number of warps per block = num_warps.\n   - Total threads per block = num_warps * 32.\n\n Therefore, we must have: BLOCK_SIZE <= num_warps * 32.\n\n We'll set:\n   - For BLOCK_SIZE=256, we need num_warps=8 (since 8*32=256).\n   - For BLOCK_SIZE=128, we can use num_warps=4 (4*32=128).\n\n We'll create configs that satisfy this.\n\n Proposed configs:\n\n   configs = [\n        triton.Config({'BLOCK_SIZE': 128, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 4}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 4}, num_warps=16),   # 16 warps * 32 = 512\n\n        triton.Config({'BLOCK_SIZE': 128, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 8}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 8}, num_warps=16),\n\n        # Also try with smaller block sizes and more warps? Or we can also try with fewer warps? But we must meet the constraint.\n\n        # We can also include the original non-vectorized versions? But we are focusing on vectorized.\n\n        # We can also try VEC_SIZE=16? But the H100 has 128-bit loads? Actually, we are using FP32? The problem does not specify, but the reference uses float. We'll assume FP32.\n        # For FP32, a vector of 4 elements is 16 bytes, which is 128 bits -> that's a single vector load.\n\n        # We can try VEC_SIZE=4 and 8.\n\n   ]\n\n However, note that the maximum number of warps per block is 32 (for H100). And 32 warps * 32 threads = 1024 threads per block. So we can also try:\n\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 4}, num_warps=32),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 8}, num_warps=32),\n\n But note: the total number of threads per block must be BLOCK_SIZE? And we are setting BLOCK_SIZE to 1024 and num_warps=32 (which is 32*32=1024) -> valid.\n\n However, the problem size is 1.8M elements. With VEC_SIZE=8 and BLOCK_SIZE=1024, each block processes 8192 elements. Then the grid size is 1.8M / 8192 = about 225 blocks. That should be fine.\n\n Alternatively, we can also include smaller block sizes and vector sizes to cover a wider range.\n\n We'll set a range of configs.\n\n Let's write the new code accordingly.\n\n We'll also note that the kernel must handle the case when the total number of elements is not a multiple of the vector size.\n\n We'll change the kernel to:\n\n   @triton.jit\n   def fused_div_leaky_relu(\n        x_ptr,\n        output_ptr,\n        divisor,\n        negative_slope,\n        n_elements,\n        BLOCK_SIZE: tl.constexpr,\n        VEC_SIZE: tl.constexpr,\n   ):\n        pid = tl.program_id(0)\n        block_start = pid * BLOCK_SIZE * VEC_SIZE\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)[:, None] * VEC_SIZE + tl.arange(0, VEC_SIZE)[None, :]\n        mask = offsets < n_elements   # 2D mask: [BLOCK_SIZE, VEC_SIZE]\n\n        x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n\n        # Compute: division and then leaky ReLU\n        x_scaled = x / divisor\n        result = tl.where(x_scaled >= 0, x_scaled, x_scaled * negative_slope)\n\n        tl.store(output_ptr + offsets, result, mask=mask)\n\n We also update the grid function in the forward pass to use the new parameters.\n\n We'll change the autotune configs to the ones we discussed.\n\n Let's write the new program accordingly.\n\n However, note: the current program's ModelNew class and the test setup (batch_size, etc.) remain the same.\n\n We'll replace the current kernel and autotune configs with the vectorized version.\n\n IMPORTANT: We must set the grid to use the product of BLOCK_SIZE and VEC_SIZE.\n\n Let's code accordingly.\n\n We hope that the vectorized version will improve memory throughput and thus the speedup.\n\n Note: The current program has a speedup of 0.694 (slower than PyTorch). We aim for at least 1.05.\n\n We'll also note that the Triton compiler might be able to optimize the division and the leaky ReLU better with vectorization.\n\n Let's try.\n\n We'll also note that the divisor is a scalar. We can precompute its reciprocal to replace division by multiplication? But note: division and multiplication have similar cost in modern GPUs? And the compiler might do that for us. We can try:\n\n   inv_divisor = 1.0 / divisor\n   x_scaled = x * inv_divisor\n\n But note: the divisor is the same for all elements. So we can precompute the reciprocal in the host and pass it to the kernel? However, the kernel currently takes `divisor` as a parameter. We can change the kernel to take `inv_divisor` instead? But the interface of the kernel is fixed by the autotuner and the call in the model.\n\n Alternatively, we can precompute the reciprocal in the kernel once per block? But that would be inefficient because it's the same for all elements. We can compute it once per program? We can do:\n\n   inv_divisor = 1.0 / divisor   # This will be computed once per kernel launch? Actually, no: it's per thread? But we are in the kernel and it's a scalar.\n\n   However, each thread would compute it? That's wasteful.\n\n We can precompute it on the host and pass it as a parameter. Let's change the kernel to take `inv_divisor` and then in the model, we compute `inv_divisor = 1.0 / self.divisor` and pass that? But note: the divisor might be zero? The problem says it's at least 1e-4? Actually, we don't know. But the reference code uses a divisor that is at least 1e-4? Not in this program. In the example, the divisor was 2.\n\n We are not told that the divisor is non-zero? But the problem context says the divisor is 2. So we assume it's safe.\n\n However, to avoid changing the interface, we can leave it as is and hope the compiler optimizes the division by a constant? But the divisor is passed as a parameter and is the same for all elements. The compiler might hoist the reciprocal computation.\n\n Alternatively, we can change the kernel to take `inv_divisor` and then in the model, we compute the reciprocal and pass it. This might save one division per element.\n\n We'll do that in a separate step if needed. For now, we focus on vectorization.\n\n We'll stick to the same kernel parameters and change the computation to:\n\n   x_scaled = x * (1.0 / divisor)\n\n But note: that would compute the reciprocal for every element? That's worse.\n\n We can precompute the reciprocal once per kernel:\n\n   inv_divisor = 1.0 / divisor   # outside the loop? But the kernel is executed per element? Actually, we are in a SPMD model: each thread runs the entire kernel.\n\n   So we can do:\n\n        inv_divisor = 1.0 / divisor\n        ... then use x * inv_divisor ...\n\n   This will compute the reciprocal once per thread? That's better than per element? But still, it's one per thread. And we have BLOCK_SIZE threads per block. That's acceptable.\n\n   But note: the divisor is the same for the entire kernel launch. We could compute it on the host and pass the reciprocal? That would be more efficient.\n\n   We'll change the kernel to take `inv_divisor` as a parameter? But that would change the signature. We are allowed to change the kernel as long as we don't change the function name and the parameters that are passed from the model? Actually, we are allowed to change the kernel arbitrarily as long as the function name and the grid configuration remain the same? But note the constraints:\n\n        \u274c NEVER change @triton.jit function signatures or parameter names\n\n   So we cannot change the parameters? The current kernel parameters are:\n\n        x_ptr, output_ptr, divisor, negative_slope, n_elements, BLOCK_SIZE\n\n   We want to change `divisor` to `inv_divisor`? That would change the signature.\n\n   Alternatively, we can leave it and hope the compiler optimizes the division by a scalar that is the same for all threads? The compiler might be smart enough to hoist the reciprocal computation.\n\n   We'll try without changing the interface first.\n\n   We change the computation to:\n\n        x_scaled = x * (1.0 / divisor)\n\n   But note: that is one division per element? That's bad.\n\n   We can do:\n\n        inv_divisor = 1.0 / divisor   # at the beginning of the kernel, once per thread\n        ... then use x * inv_divisor ...\n\n   That would be one division per thread and then one multiplication per element.\n\n   We'll do that.\n\n   So:\n\n        inv_divisor = 1.0 / divisor\n        x_scaled = x * inv_divisor\n\n   This is better.\n\n   We'll update the kernel accordingly.\n\n Let's write the new kernel with:\n   - Vectorization (VEC_SIZE)\n   - Precomputation of the reciprocal per thread.\n\n We'll also update the autotune configurations.\n\n We hope this will improve performance.\n\n We'll now write the complete program.\n</think>\nTo improve the performance of the Triton kernel for the fused division and leaky ReLU operation, we'll implement vectorized memory operations. This approach processes multiple elements per thread, increasing memory throughput and computational efficiency. The key optimizations include:\n\n1. Added vectorization with VEC_SIZE parameter (4 or 8 elements per thread)\n2. Precomputed reciprocal of divisor to replace division with multiplication\n3. Expanded autotuning configurations to explore vectorization parameters\n4. Optimized memory access patterns for coalesced loads/stores\n5. Maintained correctness with proper masking for partial vectors\n\nHere's the optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 4}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 4}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 4}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 8}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 8}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 8}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 8}, num_warps=4),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef fused_div_leaky_relu(\n    x_ptr,\n    output_ptr,\n    divisor,\n    negative_slope,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n    VEC_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    total_elements_per_block = BLOCK_SIZE * VEC_SIZE\n    block_start = pid * total_elements_per_block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)[:, None] * VEC_SIZE + tl.arange(0, VEC_SIZE)[None, :]\n    mask = offsets < n_elements\n    \n    # Vectorized load\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    \n    # Precompute reciprocal for faster division\n    inv_divisor = 1.0 / divisor\n    x_scaled = x * inv_divisor\n    \n    # Fused leaky ReLU\n    result = tl.where(x_scaled >= 0, x_scaled, x_scaled * negative_slope)\n    \n    # Vectorized store\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x_contig = x.contiguous()\n        output = torch.empty_like(x_contig)\n        n_elements = x_contig.numel()\n        \n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE'] * meta['VEC_SIZE']),)\n        \n        fused_div_leaky_relu[grid](\n            x_contig, output, self.divisor, 0.01, n_elements\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
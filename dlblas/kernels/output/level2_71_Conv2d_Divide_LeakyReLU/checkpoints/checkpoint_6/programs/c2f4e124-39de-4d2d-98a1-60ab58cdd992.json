{"id": "c2f4e124-39de-4d2d-98a1-60ab58cdd992", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef leaky_relu_scale_kernel(\n    input_ptr,\n    output_ptr,\n    divisor,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    x = tl.load(input_ptr + offsets, mask=mask)\n    x_scaled = x / divisor\n    # LeakyReLU: max(0, x) + negative_slope * min(0, x)\n    result = tl.where(x_scaled >= 0, x_scaled, 0.01 * x_scaled)\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        leaky_relu_scale_kernel[grid](x, output, self.divisor, n_elements, BLOCK_SIZE=1024)\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "cd547b95-75a2-456e-9e92-271e361664bc", "generation": 1, "timestamp": 1754580365.892144, "iteration_found": 3, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.843, "runtime_ms": 0.0701, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0591, "speed_up": 0.843, "custom_timing": 0.0701}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0602, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0591, "speed_up": 0.982, "custom_timing": 0.0602}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0602\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0591, 'speed_up': 0.982, 'custom_timing': 0.0602}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0602, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0591, 'speed_up': 0.982, 'custom_timing': 0.0602}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, divides by a constant, and applies LeakyReLU.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x / self.divisor\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0602), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0591, 'speed_up': 0.982, 'custom_timing': 0.0602})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, divides by a constant, and applies LeakyReLU.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x / self.divisor\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Convolution\n  2. Division by a constant divisor\n  3. LeakyReLU (with negative_slope=0.01)\n\nThe current implementation is in pure PyTorch. We are to rewrite it using Triton to speed up the operations.\n\nHowever, note that the convolution operation is currently done by `nn.Conv2d`. We cannot easily replace the convolution with a Triton kernel because:\n  - The convolution operation is complex and requires im2col or other techniques.\n  - The problem states that we are to rewrite the entire model, but the convolution is already optimized by PyTorch (using cuDNN).\n\nBut note: the operations after convolution (division and LeakyReLU) are pointwise. We can fuse these operations to reduce memory bandwidth.\n\nHowever, the current program does not show a significant performance issue (speed_up is 0.982, which is about 1). But we are required to get at least 5% speedup.\n\nBut note: the entire model is currently running in 0.0602 ms (custom_timing) vs reference 0.0591 ms. The convolution is the most expensive part.\n\nAlternative approach:\n  We can try to fuse the pointwise operations (division and LeakyReLU) with the convolution. However, PyTorch does not provide a direct way to do this.\n\nAnother idea: replace the entire model with a Triton kernel that does the convolution and the pointwise operations? That would be very complex.\n\nBut note: the problem says we are to generate Triton implementations. We are allowed to change the internal implementation.\n\nGiven the complexity of writing a convolution in Triton (and the fact that it might not beat cuDNN), we might focus on the pointwise operations.\n\nHowever, the pointwise operations are very lightweight and might be memory bound. Fusing them might save memory bandwidth.\n\nBut note: the current model is:\n  x = self.conv(x)   -> produces a tensor of shape [batch_size, out_channels, height_out, width_out]\n  x = x / self.divisor\n  x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n\nWe can write a Triton kernel that does the division and LeakyReLU in one pass, and then we hope that by doing so we avoid writing the intermediate tensor for the convolution output and then reading it again for the pointwise ops.\n\nBut note: the convolution output is already in GPU memory. The pointwise operations are element-wise and can be done in-place. However, the current PyTorch implementation might already be fusing these pointwise operations via the JIT.\n\nSo we must be cautious.\n\nAlternatively, we can write a kernel that does the entire operation (convolution + pointwise) but that is a big task.\n\nGiven the constraints, let's break down:\n\n  We are allowed to change only the Triton kernel source within the EVOLVE-BLOCK. But note that the current code does not have a Triton kernel.\n\nWe are going to restructure:\n\n  Instead of having the convolution in PyTorch and then the pointwise in PyTorch, we can do:\n    - Convolution in PyTorch (we leave it because it's efficient)\n    - Then we call a Triton kernel for the pointwise operations (division and LeakyReLU).\n\nBut note: the pointwise operations are very cheap. The convolution is the bottleneck. So even if we speed up the pointwise part, we might not see an overall speedup.\n\nBut let's try because the current pointwise operations are two passes (division then LeakyReLU) and we can do one pass.\n\nWe'll write a Triton kernel that does:\n  output = leaky_relu(input / divisor, negative_slope=0.01)\n\nWe then replace:\n  x = x / self.divisor\n  x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n\nwith:\n  x = leaky_relu_scale_kernel(x, self.divisor)\n\nWe hope that by fusing the two operations we reduce the memory traffic by half (because we only read and write once).\n\nBut note: the convolution output is already in memory. Without fusion, the division writes a new tensor and then the LeakyReLU reads that and writes another. So we are writing two tensors and reading two tensors (if we count intermediate). With fusion, we only write one tensor and read one tensor.\n\nHowever, in PyTorch, the intermediate might be avoided by fusion. So we need to test.\n\nGiven the small size of the tensor (batch_size=128, out_channels=16, height=32, width=32 -> 128*16*32*32 = 2^20 = 1M elements, 4MB for float32) and the fact that the convolution is 3x3, the convolution is the dominant cost.\n\nBut let's try to write the pointwise kernel and see if it helps.\n\nSteps:\n\n  1. We will write a Triton kernel that takes a tensor and a divisor (scalar) and does:\n        output = leaky_relu(input / divisor, negative_slope=0.01)\n\n  2. We will replace the two lines in the forward with a call to this kernel.\n\n  3. We must be cautious about the grid and block size.\n\nWe note that the tensor is 4D, but we can flatten it to 1D for the kernel.\n\nWe'll do:\n\n    total_elements = x.numel()\n    grid = (triton.cdiv(total_elements, BLOCK_SIZE),)\n\n    kernel[grid](x, divisor, ...)\n\nBut note: we cannot change the class signature arbitrarily. We must maintain the same inputs and outputs.\n\nWe are going to define the kernel inside the class? Actually, we can define it as a helper function in the same module.\n\nHowever, note the constraints: we must output the entire code block for the EVOLVE-BLOCK.\n\nWe are going to restructure the code:\n\n  - Import triton and triton.language as tl.\n  - Write the kernel function (leaky_relu_scale_kernel) with the appropriate decorators.\n  - In the forward, we call this kernel.\n\nBut note: the kernel must be compiled and the first run might have overhead. We are only measuring the forward pass.\n\nWe must be cautious: the kernel must support any tensor shape (as long as the total number of elements is within the grid).\n\nWe'll write:\n\n    @triton.jit\n    def _leaky_relu_scale_kernel(x_ptr, output_ptr, divisor, n_elements, BLOCK_SIZE: tl.constexpr):\n        pid = tl.program_id(axis=0)\n        block_start = pid * BLOCK_SIZE\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_elements\n        x = tl.load(x_ptr + offsets, mask=mask)\n        # Compute: x = x / divisor, then leaky_relu\n        x_scaled = x / divisor\n        # LeakyReLU: if x_scaled >=0 then x_scaled else 0.01 * x_scaled\n        # But note: LeakyReLU formula: max(0, x_scaled) + negative_slope * min(0, x_scaled)\n        # We can do: \n        #   below_zero = (x_scaled < 0) * (0.01 * x_scaled)\n        #   above_zero = (x_scaled >=0) * x_scaled\n        #   output = below_zero + above_zero\n        # Alternatively: \n        #   output = tl.where(x_scaled >= 0, x_scaled, 0.01 * x_scaled)\n        output = tl.where(x_scaled >= 0, x_scaled, 0.01 * x_scaled)\n        tl.store(output_ptr + offsets, output, mask=mask)\n\n    Then we create a helper function:\n\n    def leaky_relu_scale(x, divisor):\n        output = torch.empty_like(x)\n        n_elements = output.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        # We need to choose a block size. We can set it to 1024 and then autotune later?\n        BLOCK_SIZE = 1024\n        _leaky_relu_scale_kernel[grid](x, output, divisor, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n        return output\n\nBut note: the kernel uses the same input and output. We are not doing in-place? We could do in-place to save memory, but let's do out-of-place to be safe.\n\nHowever, we are going to replace:\n\n    x = x / self.divisor\n    x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n\nwith:\n\n    x = leaky_relu_scale(x, self.divisor)\n\nThis will create a new tensor. But the original code also creates an intermediate for the division. So we are replacing two intermediates with one? Actually, the original code:\n\n    x = self.conv(x)   # output tensor A\n    x = x / self.divisor   # creates tensor B\n    x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)   # creates tensor C\n\nWe are replacing:\n\n    x = self.conv(x)   # tensor A\n    x = leaky_relu_scale(x, self.divisor)   # creates tensor D\n\nSo we are saving one intermediate (tensor B). But note: the leaky_relu_scale kernel still writes to a new tensor (D). So we are not saving memory for the output, but we are saving the intermediate B.\n\nBut the total memory traffic for the pointwise part is reduced by half? Because without fusion, we do:\n  Read A, write B (size N)\n  Read B, write C (size N)\n\nTotal: 2N reads, 2N writes.\n\nWith fusion:\n  Read A, write D (size N)\nTotal: N reads, N writes.\n\nSo we save 2N memory operations.\n\nBut note: the convolution output (A) is still in memory. We are reading A and writing D, then A can be freed. So the peak memory is the same (A and D at the same time) but the total memory bandwidth for the pointwise part is halved.\n\nGiven that the pointwise part is memory bound, we expect a speedup in the pointwise part. But since the convolution is the dominant cost, the overall speedup might be small.\n\nBut let's try.\n\nWe must also note that the kernel launch has overhead. For a small tensor (1M elements), the overhead might be significant.\n\nWe'll use a large block size to reduce the grid size.\n\nAlternatively, we can autotune the block size. But for simplicity, we start with 1024.\n\nWe must also note that the kernel is very simple and might be bandwidth bound.\n\nNow, let's write the code.\n\nWe are going to restructure the EVOLVE-BLOCK.\n\nWe must include the necessary imports (triton, tl) and define the kernel and the helper function.\n\nWe note: the original code had a class ModelNew and some global variables (batch_size, etc.) and functions get_inputs and get_init_inputs.\n\nWe are going to keep the same structure.\n\nWe'll do:\n\n  import triton\n  import triton.language as tl\n\n  Then define the kernel and the helper.\n\n  Then the class ModelNew will use the helper in forward.\n\nBut note: the kernel must be defined at the top level? Actually, we can define it inside the module.\n\nWe are going to write:\n\n    @triton.jit\n    def _leaky_relu_scale_kernel(...)\n\n    def leaky_relu_scale(...)\n\n  and then in the ModelNew.forward, we call leaky_relu_scale.\n\nWe must be cautious about the device: the kernel only runs on CUDA.\n\nWe can do:\n\n    if x.is_cuda:\n        return leaky_relu_scale(x, divisor)\n    else:\n        # fallback to PyTorch, but the problem is for GPU\n        ...\n\nBut the problem states that the GPU is used. So we can assume CUDA.\n\nAlternatively, we can make the kernel device-agnostic by having a fallback? But the problem is for Triton on GPU.\n\nSo we write the kernel for CUDA only.\n\nNow, let's code.\n\nWe must note: the divisor is a float. We pass it as a scalar to the kernel.\n\nWe are going to change the forward of ModelNew.\n\nBut note: the original code uses a module. We are replacing the two operations with one.\n\nWe must be cautious: the original leaky_relu had negative_slope=0.01. We hardcode that in the kernel.\n\nWe are going to write the entire EVOLVE-BLOCK.\n\nWe are going to remove the two lines and replace with one.\n\nLet's write the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef leaky_relu_scale_kernel(\n    input_ptr,\n    output_ptr,\n    divisor,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    x = tl.load(input_ptr + offsets, mask=mask)\n    x_scaled = x / divisor\n    # LeakyReLU: max(0, x) + negative_slope * min(0, x)\n    result = tl.where(x_scaled >= 0, x_scaled, 0.01 * x_scaled)\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divisor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        x = self.conv(x)\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        leaky_relu_scale_kernel[grid](x, output, self.divisor, n_elements, BLOCK_SIZE=1024)\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivisor = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
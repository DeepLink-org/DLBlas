{"id": "22c75abf-0d83-4fda-901f-69ad50a9ae9d", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE_H\": 256, \"BLOCK_SIZE_K\": 64}, num_warps=4, num_stages=3),\n        triton.Config({\"BLOCK_SIZE_H\": 128, \"BLOCK_SIZE_K\": 128}, num_warps=4, num_stages=3),\n        triton.Config({\"BLOCK_SIZE_H\": 512, \"BLOCK_SIZE_K\": 32}, num_warps=8, num_stages=3),\n        triton.Config({\"BLOCK_SIZE_H\": 64, \"BLOCK_SIZE_K\": 256}, num_warps=2, num_stages=3),\n        triton.Config({\"BLOCK_SIZE_H\": 256, \"BLOCK_SIZE_K\": 128}, num_warps=8, num_stages=3),\n        triton.Config({\"BLOCK_SIZE_H\": 512, \"BLOCK_SIZE_K\": 64}, num_warps=8, num_stages=3),\n        triton.Config({\"BLOCK_SIZE_H\": 128, \"BLOCK_SIZE_K\": 32}, num_warps=4, num_stages=3),\n        triton.Config({\"BLOCK_SIZE_H\": 64, \"BLOCK_SIZE_K\": 64}, num_warps=2, num_stages=3),\n    ],\n    key=[\"hidden_size\", \"input_size\"],\n)\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    out_ptr,\n    input_size,\n    hidden_size,\n    scaling_factor,\n    stride_x,\n    stride_w0,\n    stride_w1,\n    stride_out,\n    BLOCK_SIZE_H: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_hidden = tl.program_id(1)\n    \n    # Create block offsets with vectorization\n    hidden_offsets = pid_hidden * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n    hidden_mask = hidden_offsets < hidden_size\n    \n    acc = tl.zeros((BLOCK_SIZE_H,), dtype=tl.float32)\n    \n    # Vectorized input loading with efficient memory access\n    for k in range(0, input_size, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < input_size\n        \n        # Coalesced input loading\n        x_block = tl.load(\n            x_ptr + pid_batch * stride_x + k_offsets,\n            mask=k_mask,\n            other=0.0\n        )\n        \n        # Coalesced weight loading\n        w_block = tl.load(\n            w_ptr + hidden_offsets[:, None] * stride_w0 + k_offsets[None, :] * stride_w1,\n            mask=hidden_mask[:, None] & k_mask[None, :],\n            other=0.0\n        )\n        \n        # Compute partial dot product\n        acc += tl.sum(w_block * x_block[None, :], axis=1)\n    \n    # Load bias with vectorization\n    bias = tl.load(b_ptr + hidden_offsets, mask=hidden_mask, other=0.0)\n    y = acc + bias\n    \n    # Fused operations with efficient sigmoid\n    sig = tl.sigmoid(y)\n    result = y + scaling_factor * sig\n    \n    # Store output with coalesced access\n    tl.store(\n        out_ptr + pid_batch * stride_out + hidden_offsets,\n        result,\n        mask=hidden_mask\n    )\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super().__init__()\n        self.gemm = nn.Linear(input_size, hidden_size)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        batch_size, input_size = x.shape\n        hidden_size = self.gemm.out_features\n        output = torch.empty((batch_size, hidden_size), device=x.device, dtype=x.dtype)\n        \n        # Get tensor strides\n        stride_x = x.stride(0)\n        stride_w0 = self.gemm.weight.stride(0)\n        stride_w1 = self.gemm.weight.stride(1)\n        stride_out = output.stride(0)\n        \n        # Adaptive grid using autotuned block size\n        grid = lambda meta: (\n            batch_size,\n            triton.cdiv(hidden_size, meta['BLOCK_SIZE_H'])\n        )\n        \n        # Launch optimized kernel\n        _forward_kernel[grid](\n            x, \n            self.gemm.weight,\n            self.gemm.bias,\n            output,\n            input_size,\n            hidden_size,\n            self.scaling_factor,\n            stride_x,\n            stride_w0,\n            stride_w1,\n            stride_out\n        )\n        return output\n\nbatch_size = 128\ninput_size = 1024\nhidden_size = 512\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "505397a8-0c90-4707-b39d-6458e6992fd3", "generation": 3, "timestamp": 1754581309.0197868, "iteration_found": 26, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.633, "runtime_ms": 0.0886, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0561, "speed_up": 0.633, "custom_timing": 0.0886}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.683, "runtime_ms": 0.0821, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0561, "speed_up": 0.683, "custom_timing": 0.0821}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
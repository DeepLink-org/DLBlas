{"id": "fd0db797-4fb7-42c7-95d7-40245086ec70", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_gemm_sigmoid_scale_add_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    out_ptr,\n    scaling_factor,\n    input_size,\n    hidden_size,\n    stride_x,\n    stride_w0,\n    stride_w1,\n    stride_out,\n    BLOCK_SIZE: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    # 2D grid: batch index and column block index\n    pid_batch = tl.program_id(0)\n    pid_col_block = tl.program_id(1)\n    \n    # Offsets for current block\n    col_offsets = pid_col_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    col_mask = col_offsets < hidden_size\n    \n    # Accumulator initialization\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Loop over input dimension with BLOCK_K steps\n    for k in range(0, input_size, BLOCK_K):\n        k_offsets = k + tl.arange(0, BLOCK_K)\n        k_mask = k_offsets < input_size\n        \n        # Load input block\n        x_ptr_current = x_ptr + pid_batch * stride_x + k_offsets\n        x_block = tl.load(x_ptr_current, mask=k_mask, other=0.0)\n        \n        # Load weight block with vectorization\n        w_ptr_current = w_ptr + col_offsets[:, None] * stride_w0 + k_offsets[None, :] * stride_w1\n        w_block = tl.load(w_ptr_current, mask=col_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(w_block * x_block[None, :], axis=1)\n    \n    # Load bias block\n    bias_block = tl.load(b_ptr + col_offsets, mask=col_mask, other=0.0)\n    y_block = acc + bias_block\n    \n    # Fused operations: sigmoid -> scaling -> residual add\n    t_block = tl.sigmoid(y_block)\n    t_block = t_block * scaling_factor\n    out_block = t_block + y_block\n    \n    # Store output with coalesced access\n    out_ptr_current = out_ptr + pid_batch * stride_out + col_offsets\n    tl.store(out_ptr_current, out_block, mask=col_mask)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super().__init__()\n        self.gemm = nn.Linear(input_size, hidden_size)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        batch_size, input_size = x.shape\n        hidden_size = self.gemm.weight.shape[0]\n        output = torch.empty((batch_size, hidden_size), device=x.device, dtype=x.dtype)\n        \n        # Configure kernel grid and block sizes\n        BLOCK_SIZE = 128\n        grid = (batch_size, triton.cdiv(hidden_size, BLOCK_SIZE))\n        \n        # Launch optimized kernel\n        _fused_gemm_sigmoid_scale_add_kernel[grid](\n            x, \n            self.gemm.weight, \n            self.gemm.bias,\n            output,\n            self.scaling_factor,\n            input_size,\n            hidden_size,\n            x.stride(0),\n            self.gemm.weight.stride(0),\n            self.gemm.weight.stride(1),\n            output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE,\n            BLOCK_K=32\n        )\n        return output\n\nbatch_size = 128\ninput_size = 1024\nhidden_size = 512\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "2ea3fa0d-ab87-4509-9611-21f24c7f0ed1", "generation": 1, "timestamp": 1754576557.8230114, "iteration_found": 0, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.678, "runtime_ms": 0.0827, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0561, "speed_up": 0.678, "custom_timing": 0.0827}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0584, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0561, "speed_up": 0.961, "custom_timing": 0.0584}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0584\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0561, 'speed_up': 0.961, 'custom_timing': 0.0584}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0584, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0561, 'speed_up': 0.961, 'custom_timing': 0.0584}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model implementing the pattern \"Gemm_Sigmoid_Scaling_ResidualAdd\".\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(input_size, hidden_size)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = self.gemm(x)\n        original_x = x\n        x = torch.sigmoid(x)\n        x = x * self.scaling_factor\n        x = x + original_x\n        return x\n\nbatch_size = 128\ninput_size = 1024\nhidden_size = 512\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0584), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0561, 'speed_up': 0.961, 'custom_timing': 0.0584})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model implementing the pattern \"Gemm_Sigmoid_Scaling_ResidualAdd\".\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(input_size, hidden_size)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = self.gemm(x)\n        original_x = x\n        x = torch.sigmoid(x)\n        x = x * self.scaling_factor\n        x = x + original_x\n        return x\n\nbatch_size = 128\ninput_size = 1024\nhidden_size = 512\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear transformation (GEMM)\n  2. Sigmoid\n  3. Scaling (multiplication by a constant)\n  4. Residual addition (add the original output of the linear layer)\n\nThe current implementation is in PyTorch and we are to rewrite it in Triton for performance.\n\nHowever, note that the entire model is not just a single operation but a sequence. We cannot replace the entire model with one Triton kernel? \nBut note: the problem says \"Generate functionally equivalent but faster Triton implementations (custom_code)\".\n\nLooking at the context, the dataset consists of kernelbench files. This suggests that we are to write a Triton kernel for the entire sequence of operations.\n\nBut note: the original code uses `nn.Linear` which is a high-level operation. We are allowed to replace the entire forward pass with a custom Triton kernel.\n\nHowever, the constraints say:\n  \u274c NEVER change @triton.jit function signatures or parameter names  \n  \u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\nBut in the current code, there is no Triton kernel. So we are starting from scratch.\n\nWe have to design a Triton kernel that does:\n  y = gemm(x, W, b)   # linear transformation\n  t = sigmoid(y)\n  t = t * scaling_factor\n  out = t + y\n\nWe can break it down:\n  Step 1: Do the matrix multiplication: x (batch_size, input_size) * W (hidden_size, input_size)^T + bias (hidden_size) -> y (batch_size, hidden_size)\n  Step 2: Apply sigmoid to y to get t, then scale, then add y.\n\nBut note: the entire operation can be fused into one kernel to avoid multiple passes over the output matrix.\n\nHowever, the matrix multiplication part is a full GEMM. We are going to write a fused GEMM + elementwise kernel.\n\nBut note the problem: the input tensor x has shape (batch_size, input_size) and the weight has shape (hidden_size, input_size). The bias is (hidden_size).\n\nWe are going to write a kernel that:\n  - Each program ID will handle a block of rows in the output matrix? But note: the output matrix is (batch_size, hidden_size). We can assign each row to a program? But note: the batch_size might be large.\n\nAlternatively, we can assign each row of the output to a program. Then each program would compute one row of the output. However, the row is of length `hidden_size`, and we have to do a dot product of the input row (size input_size) with every column of the weight matrix? That would require loading the entire weight matrix? That is not efficient.\n\nInstead, we can do a blocked GEMM. But note: the problem is that the weight matrix is of size (hidden_size, input_size). We are doing:\n  y[i, j] = sum_k x[i, k] * W[j, k] + bias[j]\n\nWe can break the input_size dimension into blocks. Each program will compute a block of the output row? Actually, we can use a 2D grid:\n  - Grid: (batch_size, hidden_size) but that would be too big? Instead, we can use a 2D grid of (batch_size, ceil(hidden_size / BLOCK_SIZE)) and then each block computes a contiguous block of hidden_size for a given row.\n\nBut note: the elementwise operations are per element. So we can compute the entire row and then apply the elementwise.\n\nAlternatively, we can do:\n  - Each program handles one block of the output matrix (a block of rows and a block of columns). But that might be complex.\n\nGiven the complexity and the fact that we are at level1-level5 (this is a level1? because it's the first Triton rewrite) we might start with a simpler approach.\n\nBut note: the example provided (per_row_quantize_int8) uses a 1D grid and each program handles one row.\n\nWe can do similarly for the GEMM? Actually, we can do:\n\n  pid = tl.program_id(0)   # which row in the batch? and also which row in the output? Actually, the output has batch_size rows and hidden_size columns.\n\n  We can assign each row of the output to a program. Then we have grid = (batch_size,). But then each program must compute an entire row of the output (of length hidden_size). That might be too much if hidden_size is large? But we can use a loop over blocks of the hidden_size? Actually, we can do:\n\n  For a given row i (in the batch), we want to compute:\n    y[i, :] = x[i, :] * W^T + bias\n\n  We can break the hidden_size into chunks of BLOCK_SIZE. Then we can have a 2D grid: (batch_size, num_blocks) where num_blocks = ceil(hidden_size / BLOCK_SIZE). Then each program computes a block of the row.\n\n  However, the elementwise operations (sigmoid, scaling, residual) are applied per element and can be done in the same kernel.\n\nPlan:\n\n  We are going to write a kernel that uses:\n    pid_batch = tl.program_id(0)   # row index in the batch [0, batch_size-1]\n    pid_block = tl.program_id(1)    # block index for the columns of the output [0, num_blocks-1]\n\n  Then, for the given row pid_batch, we compute the block of columns [pid_block * BLOCK_SIZE, (pid_block+1)*BLOCK_SIZE) of the output.\n\n  Steps:\n    1. Load the corresponding block of the input row? Actually, the input row is of length input_size. We have to do a dot product for each column in the block? We need to load a block of the weight matrix.\n\n    2. We break the input_size dimension into chunks (BLOCK_K) and do a reduction.\n\n  But note: the kernel must be efficient. We can use the same approach as in the matrix multiplication tutorial.\n\n  However, the problem is that we are not just doing GEMM but also elementwise. We can do:\n\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for k in range(0, input_size, BLOCK_K):\n        # Load a block of the input row (size BLOCK_K) at offset k\n        x_ptrs = x_ptr + pid_batch * input_size + k + tl.arange(0, BLOCK_K)\n        x_block = tl.load(x_ptrs, mask=(k + tl.arange(0, BLOCK_K)) < input_size, other=0.0)\n\n        # Load a block of the weight matrix for the current column block and the current K block: shape (BLOCK_SIZE, BLOCK_K)\n        w_ptrs = w_ptr + (pid_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)[:, None]) * input_size + k + tl.arange(0, BLOCK_K)[None, :]\n        w_block = tl.load(w_ptrs, mask=(pid_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)[:, None] < hidden_size) & (k + tl.arange(0, BLOCK_K)[None, :] < input_size), other=0.0)\n\n        # Compute the dot product for this block\n        acc += tl.sum(x_block[None, :] * w_block, axis=1)\n\n    # Add bias for the current block of columns\n    bias_ptrs = bias_ptr + pid_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    bias_block = tl.load(bias_ptrs, mask=(pid_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)) < hidden_size, other=0.0)\n    y_block = acc + bias_block\n\n    # Now apply the elementwise operations:\n    t_block = tl.sigmoid(y_block)\n    t_block = t_block * scaling_factor\n    out_block = t_block + y_block\n\n    # Write the output block\n    out_ptrs = out_ptr + pid_batch * hidden_size + pid_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    tl.store(out_ptrs, out_block, mask=(pid_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)) < hidden_size)\n\n  However, note: the scaling factor is a constant (scalar) that we can pass to the kernel.\n\n  But wait: the original model has the weight and bias as parameters. How do we handle that?\n\n  In the original code, the model is an nn.Module that holds the weight and bias. In the Triton version, we will have to pass the weight and bias as tensors to the kernel.\n\n  We are going to change the forward method to use our Triton kernel.\n\n  Steps for the module:\n\n    We will keep the nn.Linear? Actually, we cannot because we are replacing the entire forward. We will instead store the weight and bias as parameters and then use our kernel.\n\n    We can do:\n\n      class ModelNew(nn.Module):\n          def __init__(self, input_size, hidden_size, scaling_factor):\n              super().__init__()\n              self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n              self.bias = nn.Parameter(torch.randn(hidden_size))\n              self.scaling_factor = scaling_factor\n\n          def forward(self, x):\n              # Check dimensions: x is (batch_size, input_size)\n              batch_size, input_size = x.shape\n              hidden_size = self.weight.shape[0]\n              # Output will be (batch_size, hidden_size)\n              output = torch.empty((batch_size, hidden_size), device=x.device, dtype=x.dtype)\n\n              # We have to choose block sizes and grid\n              BLOCK_SIZE = 128   # for the column block? This is the BLOCK_SIZE in the kernel for the columns of the output.\n              num_blocks = (hidden_size + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n              grid = (batch_size, num_blocks)\n\n              # We also need to choose BLOCK_K for the inner reduction. Let's set it to 32?\n              # We can make it configurable.\n\n              # Launch the kernel\n              _fused_gemm_eltwise[grid](x, self.weight, self.bias, output, \n                                        scaling_factor=self.scaling_factor,\n                                        input_size=input_size, hidden_size=hidden_size,\n                                        BLOCK_SIZE=BLOCK_SIZE, BLOCK_K=32)\n\n              return output\n\n  However, note the constraints: we cannot change the grid configuration? Actually, the constraint says \"NEVER modify grid configuration\", meaning if it is already set? In this case, we are defining the grid. But the constraint might be referring to the grid being set by the problem? Actually, the problem does not set any grid because it's the first Triton version.\n\n  But note: the constraint says: \"NEVER change @triton.jit function signatures or parameter names\". We are defining the kernel from scratch.\n\n  Also note: the constraint says: \"NEVER remove boundary checks\". We are including masks.\n\n  However, the problem says: \"Make sure your rewritten program maintains the same inputs and outputs\".\n\n  The inputs to the model are the same: a tensor of shape (batch_size, input_size). The output is (batch_size, hidden_size).\n\n  But note: the original model used an nn.Linear. We are replacing that with our own weight and bias? But the original model also had an nn.Linear. So we are essentially doing the same.\n\n  However, the original model had the nn.Linear in the module. We are changing the parameters to be stored as weight and bias. That is acceptable because the functionality is the same.\n\n  But wait: the original model had the linear layer as `self.gemm`. We are replacing that by directly storing weight and bias. We are removing the nn.Linear. This is acceptable because we are rewriting the entire model.\n\n  Also note: the original model had the residual connection: it saved the output of the linear (original_x) and then added it at the end. We are doing the same in the kernel.\n\n  Now, we must write the kernel. We note that the kernel must be efficient. We are targeting H100 with tensor cores? But note: the operations are in float32? The original was in float32? We don't have information. We can assume float32.\n\n  However, the hardware has FP16 tensor cores. But we are doing the entire computation in float32? We could use float16? But the problem does not specify.\n\n  Let's assume we are using the same dtype as the input. We can pass the dtype from the input.\n\n  But note: the example in the problem used FP16 tensor cores. We are allowed to use tensor cores? We can use them if we set the input to float16? Actually, the problem does not specify the dtype.\n\n  We will write the kernel in float32. We can later optimize for FP16 if needed.\n\n  Steps for the kernel:\n\n    @triton.jit\n    def _fused_gemm_eltwise(\n        x_ptr, \n        w_ptr,\n        b_ptr,\n        out_ptr,\n        scaling_factor,\n        input_size,\n        hidden_size,\n        BLOCK_SIZE: tl.constexpr,\n        BLOCK_K: tl.constexpr,\n    ):\n        # We have two program IDs: pid_batch and pid_block\n        pid_batch = tl.program_id(0)\n        pid_block = tl.program_id(1)\n\n        # We are going to compute a block of BLOCK_SIZE columns for the row pid_batch.\n\n        # The base pointers for the current row in x and the current block in w and output.\n        # For x: we are at row pid_batch -> pointer: x_ptr + pid_batch * input_size\n        # For w: we are at column block starting at pid_block * BLOCK_SIZE -> each row of w is of length input_size, and we are taking a block of rows? Actually, the weight matrix is (hidden_size, input_size). We are taking a block of rows? No, we are taking a contiguous block of columns? Actually, the output row is computed by: row_vector (1, input_size) * matrix (input_size, hidden_size) -> (1, hidden_size). So the weight matrix is stored in row-major? Actually, in PyTorch, the weight in nn.Linear is stored as (hidden_size, input_size). So each row of the weight matrix is one output neuron.\n\n        # Therefore, the block we are computing is for columns [pid_block * BLOCK_SIZE, (pid_block+1)*BLOCK_SIZE) of the output row. This corresponds to rows [pid_block * BLOCK_SIZE, (pid_block+1)*BLOCK_SIZE) of the weight matrix.\n\n        # So we need to load the block of the weight matrix of size (BLOCK_SIZE, input_size) but we break input_size into chunks of BLOCK_K.\n\n        # Initialize the accumulator\n        acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n        # Loop over k in steps of BLOCK_K\n        for k in range(0, input_size, BLOCK_K):\n            # Load a block of the input row: at offset k, length BLOCK_K\n            x_offsets = k + tl.arange(0, BLOCK_K)\n            x_ptr_current = x_ptr + pid_batch * input_size + k\n            x_block = tl.load(x_ptr_current + tl.arange(0, BLOCK_K), mask=x_offsets < input_size, other=0.0)\n\n            # Load a block of the weight matrix: for the rows [pid_block*BLOCK_SIZE, (pid_block+1)*BLOCK_SIZE] and columns [k, k+BLOCK_K]\n            w_offsets_row = pid_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n            w_offsets_col = k + tl.arange(0, BLOCK_K)\n            w_ptr_current = w_ptr + (w_offsets_row[:, None] * input_size + w_offsets_col[None, :])\n            w_block = tl.load(w_ptr_current, \n                              mask=(w_offsets_row[:, None] < hidden_size) & (w_offsets_col[None, :] < input_size), \n                              other=0.0)\n\n            # Compute the dot product for this block: we are doing a vector (x_block) times a matrix (w_block) along the columns? Actually, we are doing:\n            #   acc_j = sum_{i} x_block[i] * w_block[j, i]   for j in [0, BLOCK_SIZE-1]\n            # So we can do:\n            #   acc += tl.sum(x_block[None, :] * w_block, axis=1)   # but this is not efficient\n\n            # Instead, we can use dot product per row? Actually, we can do:\n            #   acc += tl.dot(w_block, x_block)   # but tl.dot expects two vectors? Or two matrices?\n\n            # Alternatively, we can do:\n            #   acc += tl.sum(w_block * x_block[None, :], axis=1)\n\n            # But note: we are in Triton and we have to do it efficiently. We can use tl.dot for a block? Actually, we can use tl.dot if we make the x_block a vector and w_block a matrix? But tl.dot only supports two vectors or two matrices? \n\n            # We can do:\n            #   acc += tl.sum(w_block * x_block, axis=1)   # this is a reduction along the columns.\n\n            # This is acceptable? Let's do it.\n            acc += tl.sum(w_block * x_block[None, :], axis=1)\n\n        # Now load the bias block\n        bias_offsets = pid_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        bias_ptr_current = b_ptr + bias_offsets\n        bias_block = tl.load(bias_ptr_current, mask=bias_offsets < hidden_size, other=0.0)\n        y_block = acc + bias_block\n\n        # Apply the elementwise operations: sigmoid, scaling, then residual\n        t_block = tl.sigmoid(y_block)\n        t_block = t_block * scaling_factor\n        out_block = t_block + y_block\n\n        # Write the output block\n        out_offsets = pid_batch * hidden_size + pid_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        out_ptr_current = out_ptr + out_offsets\n        tl.store(out_ptr_current, out_block, mask=out_offsets < (pid_batch+1)*hidden_size)   # Actually, we should mask by the column index: which is bias_offsets < hidden_size? But note: out_offsets is a flat index into the output matrix. We are storing at row pid_batch and columns [pid_block*BLOCK_SIZE, ...]. We want to mask if the column index (which is bias_offsets) is within [0, hidden_size). We can use the same mask as for the bias.\n\n        # But note: the output matrix is of size (batch_size, hidden_size). The element at (i, j) is at offset i * hidden_size + j. So the mask for the j index is that j < hidden_size. We have:\n        #   j = pid_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE) -> must be < hidden_size.\n        # So we can use the same mask as for the bias: bias_offsets < hidden_size.\n\n        # Actually, we have stored the mask in the variable for the bias block. We can reuse it? But we are storing after the loop, so we can compute:\n        mask = bias_offsets < hidden_size\n        tl.store(out_ptr_current, out_block, mask=mask)\n\n  However, note: the above kernel uses a 2D grid. We must set the grid accordingly.\n\n  But note: the kernel might not be optimal. We are not using shared memory or tensor cores. This is a basic implementation.\n\n  We can try to optimize by using tensor cores? But the problem says we can use FP16 tensor cores. We are currently in float32. We can change the kernel to use float16? But we have to check the input dtype.\n\n  Alternatively, we can let the kernel support both. We can use the dtype of the input. But the problem does not specify.\n\n  We are going to assume that the input is float32. We can change the kernel to use float16 if we see that the input is float16? Actually, we cannot because the kernel is compiled. We would need two kernels? Or we can use a template.\n\n  For simplicity, we do float32.\n\n  Also note: the above kernel has a loop over input_size / BLOCK_K. We are using a 2D grid. This should be efficient if BLOCK_K is chosen appropriately.\n\n  Now, we must set the autotuning? The example had autotuning. We can add autotuning for BLOCK_SIZE and BLOCK_K? But note: the kernel has two block sizes: BLOCK_SIZE (for the output columns) and BLOCK_K (for the reduction). We can make them configurable.\n\n  However, the problem says: \"Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions\". We are using BLOCK_SIZE and BLOCK_K.\n\n  We can set up autotuning for BLOCK_SIZE and BLOCK_K? But note: the kernel also has the grid configuration, which depends on the block sizes. We can set the block sizes as triton.Config parameters.\n\n  We are going to write the kernel without autotuning first, then add autotuning.\n\n  But note: the problem says to do small iterative steps. We can first get a version that compiles and is correct, then optimize.\n\n  Let's write the kernel without autotuning and then we can add autotuning in a later step.\n\n  However, the current performance of the PyTorch version is 0.0584 ms. We must be faster.\n\n  We note that the PyTorch version does:\n    x = self.gemm(x)   # this calls cuBLAS for the GEMM\n    then does sigmoid, scaling, and add.\n\n  The cuBLAS GEMM is very efficient. Our kernel must be efficient to beat it.\n\n  We are fusing the operations, so we save on memory bandwidth for the intermediate results.\n\n  We are going to write the kernel and hope that the fusion and blocking give us a speedup.\n\n  Steps:\n\n    We will define the kernel as above, but we must be cautious of the data types.\n\n    We are using tl.float32 for the accumulator? But the input and weight might be float16? We should use the same as the input? Actually, we can get the dtype from the pointers? But Triton requires that we specify the dtype in the pointers? We are not specifying. We can use:\n\n        x_ptr = x_ptr.to(tl.float16)   -> no, we are not. We are using the same as the input.\n\n    Actually, we are not specifying the dtype in the kernel. We are using tl.load and tl.store without specifying? The load will use the default of float32? But if the input is float16, we should load as float16.\n\n    We can change the kernel to use the dtype of the input? We can pass the dtype as a parameter? Or we can use the `tl.load(..., dtype=...)`? But the dtype argument is not available? Actually, in Triton, the load infers the dtype from the pointer? We are passing the pointer from a tensor. The kernel will know the dtype.\n\n    So we don't need to worry.\n\n  Let's write the kernel.\n\n  But note: the sigmoid function: we are using tl.sigmoid. That is available.\n\n  The scaling factor: we are passing it as a scalar. We can pass it as a constant.\n\n  We are going to write the kernel and the module.\n\n  However, note the constraints: we are only allowed to change the Triton kernel source within the EVOLVE-BLOCK. But the entire model is inside the block? We are replacing the entire model.\n\n  The current block contains:\n      class ModelNew ... and then the batch_size, input_size, ... and get_inputs, get_init_inputs.\n\n  We are going to replace the entire ModelNew with our Triton version, and then also define the kernel.\n\n  We must keep the same interface: the class name must be ModelNew, and the get_inputs and get_init_inputs must be present.\n\n  We are going to write:\n\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      ... kernel ...\n\n      class ModelNew(nn.Module):\n          ... our module ...\n\n      batch_size = ... (same as before)\n\n      ... etc.\n\n  We must be cautious: the original model had the nn.Linear. We are replacing it with our own parameters.\n\n  Also note: the scaling_factor is a constant (2.0). We are storing it as a scalar in the module.\n\n  Now, we write the code.\n\n  We choose BLOCK_SIZE=128 and BLOCK_K=32 initially.\n\n  We must test for correctness? We cannot run it here, but we hope it will be correct.\n\n  We note that the kernel might be slow because we are not using tensor cores. But we are at level1. We can improve in the next steps.\n\n  We are going to output the entire block.\n\n  IMPORTANT: We are not allowed to change the function signatures of the kernel? We are defining it. So it's okay.\n\n  Let's write the code.\n\n  Note: We are also changing the model parameters from nn.Linear to nn.Parameter. We must initialize them appropriately? The original nn.Linear uses a specific initialization. We are using randn? We should match that? Actually, the original code did not specify the initialization. We can do the same: randn.\n\n  But note: the original model used nn.Linear which by default uses a uniform initialization? Actually, the default for nn.Linear is uniform with bounds 1/sqrt(fan_in). We are not matching that. However, the problem says \"functionally equivalent\". We are only concerned with the computation, not the initial weights? Actually, the problem does not specify the weights. The verification must be done with the same weights? The test will pass the same inputs and weights? Actually, the test will create the model with the same parameters? We are not told.\n\n  We are going to assume that the test will initialize the model with the same weights? How? The original model had self.gemm.weight and self.gemm.bias. Now we have self.weight and self.bias. We must make sure that in the test, the weights are the same.\n\n  How can we do that? We cannot. But the problem says: the Triton implementation must be functionally equivalent. It does not require the same weights? Actually, the test will run the same input through the reference model and our model. The weights must be the same? The test will set the weights of the reference model and our model to the same values? We are not told.\n\n  We are going to change the model to have the same structure as the reference? Actually, the reference model had an nn.Linear. We are replacing it with two parameters. We can load the state_dict? But we are not told.\n\n  We must design the model so that if the test copies the state_dict, it will work? The reference model has:\n      state_dict: \n          gemm.weight, gemm.bias\n\n  Our model has:\n      weight, bias\n\n  So the state_dict keys are different. This will break.\n\n  We must preserve the state_dict? The problem does not specify. But note: the inputs to the model are the same, but the model parameters might be set by the test.\n\n  We are going to change the model to have a linear layer again? Actually, we can keep the linear layer and then in the forward use our kernel? But that would store the weight twice? We cannot.\n\n  Alternatively, we can use the linear layer's weight and bias in our kernel? We can do:\n\n      class ModelNew(nn.Module):\n          def __init__(self, input_size, hidden_size, scaling_factor):\n              super().__init__()\n              self.gemm = nn.Linear(input_size, hidden_size)\n              self.scaling_factor = scaling_factor\n\n          def forward(self, x):\n              # We are going to use our kernel, so we need to get the weight and bias from self.gemm\n              batch_size, input_size = x.shape\n              hidden_size = self.gemm.weight.shape[0]\n              output = torch.empty((batch_size, hidden_size), device=x.device, dtype=x.dtype)\n\n              BLOCK_SIZE = 128\n              num_blocks = (hidden_size + BLOCK_SIZE - 1) // BLOCK_SIZE\n              grid = (batch_size, num_blocks)\n\n              _fused_gemm_eltwise[grid](x, self.gemm.weight, self.gemm.bias, output, \n                                        scaling_factor=self.scaling_factor,\n                                        input_size=input_size, hidden_size=hidden_size,\n                                        BLOCK_SIZE=BLOCK_SIZE, BLOCK_K=32)\n\n              return output\n\n  This way, the state_dict remains the same: it has 'gemm.weight' and 'gemm.bias'. The test can set them.\n\n  This is acceptable.\n\n  So we will do that.\n\n  Now, we write the entire code.\n\n  We note: the kernel must be defined in the same file.\n\n  We are going to write the kernel and the model in the block.\n\n  We are also going to keep the batch_size, input_size, etc. and the get_inputs and get_init_inputs.\n\n  Let's do it.\n\n  IMPORTANT: We are not allowed to remove boundary checks. We have included masks.\n\n  We hope this will compile and be correct.\n\n  If it is not fast enough, we can optimize in the next iteration.\n\n  We are going to write the code now.\n</think>\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_gemm_sigmoid_scale_add_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    out_ptr,\n    scaling_factor,\n    input_size,\n    hidden_size,\n    stride_x,\n    stride_w0,\n    stride_w1,\n    stride_out,\n    BLOCK_SIZE: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    # 2D grid: batch index and column block index\n    pid_batch = tl.program_id(0)\n    pid_col_block = tl.program_id(1)\n    \n    # Offsets for current block\n    col_offsets = pid_col_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    col_mask = col_offsets < hidden_size\n    \n    # Accumulator initialization\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Loop over input dimension with BLOCK_K steps\n    for k in range(0, input_size, BLOCK_K):\n        k_offsets = k + tl.arange(0, BLOCK_K)\n        k_mask = k_offsets < input_size\n        \n        # Load input block\n        x_ptr_current = x_ptr + pid_batch * stride_x + k_offsets\n        x_block = tl.load(x_ptr_current, mask=k_mask, other=0.0)\n        \n        # Load weight block with vectorization\n        w_ptr_current = w_ptr + col_offsets[:, None] * stride_w0 + k_offsets[None, :] * stride_w1\n        w_block = tl.load(w_ptr_current, mask=col_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(w_block * x_block[None, :], axis=1)\n    \n    # Load bias block\n    bias_block = tl.load(b_ptr + col_offsets, mask=col_mask, other=0.0)\n    y_block = acc + bias_block\n    \n    # Fused operations: sigmoid -> scaling -> residual add\n    t_block = tl.sigmoid(y_block)\n    t_block = t_block * scaling_factor\n    out_block = t_block + y_block\n    \n    # Store output with coalesced access\n    out_ptr_current = out_ptr + pid_batch * stride_out + col_offsets\n    tl.store(out_ptr_current, out_block, mask=col_mask)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super().__init__()\n        self.gemm = nn.Linear(input_size, hidden_size)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        batch_size, input_size = x.shape\n        hidden_size = self.gemm.weight.shape[0]\n        output = torch.empty((batch_size, hidden_size), device=x.device, dtype=x.dtype)\n        \n        # Configure kernel grid and block sizes\n        BLOCK_SIZE = 128\n        grid = (batch_size, triton.cdiv(hidden_size, BLOCK_SIZE))\n        \n        # Launch optimized kernel\n        _fused_gemm_sigmoid_scale_add_kernel[grid](\n            x, \n            self.gemm.weight, \n            self.gemm.bias,\n            output,\n            self.scaling_factor,\n            input_size,\n            hidden_size,\n            x.stride(0),\n            self.gemm.weight.stride(0),\n            self.gemm.weight.stride(1),\n            output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE,\n            BLOCK_K=32\n        )\n        return output\n\nbatch_size = 128\ninput_size = 1024\nhidden_size = 512\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
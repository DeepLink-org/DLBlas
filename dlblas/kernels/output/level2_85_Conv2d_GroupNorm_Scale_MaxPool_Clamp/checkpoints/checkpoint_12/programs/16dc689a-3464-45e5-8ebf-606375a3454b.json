{"id": "16dc689a-3464-45e5-8ebf-606375a3454b", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_gn_scale_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    s_ptr,\n    y_ptr,\n    C, G, H, W,\n    eps: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    n = tl.program_id(0)\n    g = tl.program_id(1)\n    \n    group_size = C // G\n    start = g * group_size\n    offsets = tl.arange(0, BLOCK_SIZE)\n    c_idx = offsets // (H * W)\n    spatial_idx = offsets % (H * W)\n    \n    mask = offsets < group_size * H * W\n    base_ptr = n * C * H * W + start * H * W\n    \n    # Preload parameters for the entire group\n    ch_offsets = tl.arange(0, group_size)\n    mask_ch = ch_offsets < group_size\n    w_ch = tl.load(w_ptr + start + ch_offsets, mask=mask_ch, other=0.0)\n    b_ch = tl.load(b_ptr + start + ch_offsets, mask=mask_ch, other=0.0)\n    scale_ch = tl.load(s_ptr + start + ch_offsets, mask=mask_ch, other=0.0)\n    \n    # Load data\n    x = tl.load(x_ptr + base_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean\n    sum_x = tl.sum(x, axis=0)\n    mean = sum_x / (group_size * H * W)\n    \n    # Compute variance\n    x_centered = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / (group_size * H * W)\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Normalize and scale\n    safe_c_idx = tl.where(c_idx < group_size, c_idx, 0)\n    w_val = w_ch[safe_c_idx]\n    b_val = b_ch[safe_c_idx]\n    scale_val = scale_ch[safe_c_idx]\n    \n    y = (x_centered * rstd) * w_val + b_val\n    y = y * scale_val\n    tl.store(y_ptr + base_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        \n        # Fused group norm and scaling with Triton\n        N, C, H, W = x.shape\n        G = self.group_norm.num_groups\n        y = torch.empty_like(x)\n        group_elements = (C // G) * H * W\n        grid = (N, G)\n        fused_gn_scale_kernel[grid](\n            x, \n            self.group_norm.weight, \n            self.group_norm.bias, \n            self.scale.view(-1), \n            y,\n            C, G, H, W,\n            self.group_norm.eps,\n            BLOCK_SIZE=triton.next_power_of_2(group_elements)\n        )\n        \n        x = self.maxpool(y)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "0e6c0cca-1cf7-454e-b1cb-c9d407cd1fd0", "generation": 2, "timestamp": 1754582412.3057404, "iteration_found": 7, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_85_Conv2d_GroupNorm_Scale_MaxPool_Clamp", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 574, in arange\n    raise ValueError(\"arange's arguments must be of type tl.constexpr\")\nValueError: arange's arguments must be of type tl.constexpr\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpbvyty3ha.py\", line 78, in forward\n    fused_gn_scale_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 24:17:\n\n    group_size = C // G\n    start = g * group_size\n    offsets = tl.arange(0, BLOCK_SIZE)\n    c_idx = offsets // (H * W)\n    spatial_idx = offsets % (H * W)\n\n    mask = offsets < group_size * H * W\n    base_ptr = n * C * H * W + start * H * W\n\n    # Preload parameters for the entire group\n    ch_offsets = tl.arange(0, group_size)\n                 ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 574, in arange\n    raise ValueError(\"arange's arguments must be of type tl.constexpr\")\nValueError: arange's arguments must be of type tl.constexpr\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.102, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.895, "runtime_ms": 0.114, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.102, "speed_up": 0.895, "custom_timing": 0.114}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.8950\n- runtime_ms: 0.1140\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 0.895, 'custom_timing': 0.114}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.8950, runtime_ms: 0.1140, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 0.895, 'custom_timing': 0.114}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_gn_scale_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    s_ptr,\n    y_ptr,\n    C, G, H, W,\n    eps: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    n = tl.program_id(0)\n    g = tl.program_id(1)\n    \n    group_size = C // G\n    start = g * group_size\n    offsets = tl.arange(0, BLOCK_SIZE)\n    c_idx = offsets // (H * W)\n    spatial_idx = offsets % (H * W)\n    ch_offset = start + c_idx\n    \n    mask = offsets < group_size * H * W\n    base_ptr = n * C * H * W + start * H * W\n    \n    # Load data\n    x = tl.load(x_ptr + base_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean\n    sum_x = tl.sum(x, axis=0)\n    mean = sum_x / (group_size * H * W)\n    \n    # Compute variance\n    x_centered = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / (group_size * H * W)\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Normalize and scale\n    w = tl.load(w_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    b = tl.load(b_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    scale = tl.load(s_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    \n    y = (x_centered * rstd) * w + b\n    y = y * scale\n    tl.store(y_ptr + base_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        \n        # Fused group norm and scaling with Triton\n        N, C, H, W = x.shape\n        G = self.group_norm.num_groups\n        y = torch.empty_like(x)\n        group_elements = (C // G) * H * W\n        grid = (N, G)\n        fused_gn_scale_kernel[grid](\n            x, \n            self.group_norm.weight, \n            self.group_norm.bias, \n            self.scale.view(-1), \n            y,\n            C, G, H, W,\n            self.group_norm.eps,\n            BLOCK_SIZE=triton.next_power_of_2(group_elements)\n        )\n        \n        x = self.maxpool(y)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.8950), Performs well on runtime_ms (0.1140), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 0.895, 'custom_timing': 0.114})\n\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_gn_scale_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    s_ptr,\n    y_ptr,\n    C, G, H, W,\n    eps: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    n = tl.program_id(0)\n    g = tl.program_id(1)\n    \n    group_size = C // G\n    start = g * group_size\n    offsets = tl.arange(0, BLOCK_SIZE)\n    c_idx = offsets // (H * W)\n    spatial_idx = offsets % (H * W)\n    ch_offset = start + c_idx\n    \n    mask = offsets < group_size * H * W\n    base_ptr = n * C * H * W + start * H * W\n    \n    # Load data\n    x = tl.load(x_ptr + base_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean\n    sum_x = tl.sum(x, axis=0)\n    mean = sum_x / (group_size * H * W)\n    \n    # Compute variance\n    x_centered = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / (group_size * H * W)\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Normalize and scale\n    w = tl.load(w_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    b = tl.load(b_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    scale = tl.load(s_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    \n    y = (x_centered * rstd) * w + b\n    y = y * scale\n    tl.store(y_ptr + base_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        \n        # Fused group norm and scaling with Triton\n        N, C, H, W = x.shape\n        G = self.group_norm.num_groups\n        y = torch.empty_like(x)\n        group_elements = (C // G) * H * W\n        grid = (N, G)\n        fused_gn_scale_kernel[grid](\n            x, \n            self.group_norm.weight, \n            self.group_norm.bias, \n            self.scale.view(-1), \n            y,\n            C, G, H, W,\n            self.group_norm.eps,\n            BLOCK_SIZE=triton.next_power_of_2(group_elements)\n        )\n        \n        x = self.maxpool(y)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a Triton kernel for fused group normalization and scaling that is currently achieving a speedup of 0.895 (which is a slowdown) compared to the reference.\n The goal is to improve the speedup to at least 1.05 (5% improvement) and ideally more.\n\n Analysis of the current kernel:\n  - The kernel is launched with grid (N, G) where N is batch size and G is number of groups.\n  - Each program processes one group in one sample.\n  - The block size is set to the next power of two of (group_elements) = (C//G)*H*W.\n\n Potential issues:\n  1. The kernel does a lot of work per block: loading the entire group (which is (C//G)*H*W elements), then computing mean and variance by reduction over that entire group.\n  2. The reductions (sum for mean and sum of squares for variance) are done sequentially in one thread block. This might not be efficient for large groups.\n\n How to improve?\n  - We can break the group into smaller chunks and use parallel reduction techniques.\n  - However, note that the group size might not be very large. For the given parameters: C=16, G=8, H=32, W=32 -> group_elements = (16//8)*32*32 = 2*1024=2048. The next power of two is 2048.\n  - The current kernel uses a block size of 2048, which is acceptable for Triton (it can handle up to 4096 per block on some architectures). But the reductions might be done more efficiently.\n\n  - Another idea: use multiple blocks per group? But note that the grid is (N, G) and we cannot easily split a group across multiple blocks without atomic operations or a two-pass reduction.\n\n  - Alternatively, we can restructure the kernel to use a two-stage reduction. However, that would complicate the kernel and require temporary memory.\n\n  - Let's focus on optimizing the memory access pattern and the reduction.\n\n  - We note that the current reduction is done by a single thread block for the entire group. The reduction operations (tl.sum) are efficient because they are done in hardware. But the problem is that we are loading the entire group in one go and then doing two passes (one for mean and one for variance). This might be memory bound.\n\n  - We can try to compute the mean and variance in one pass? There is an online algorithm (Welford's algorithm) but it is not easily parallelizable.\n\n  - Alternatively, we can use the fact that the group is contiguous in memory and try to vectorize the loads.\n\n  - However, the current code already uses a vectorized load via `tl.arange(0, BLOCK_SIZE)`. But note: the group might not be contiguous in the original tensor? Actually, the group is contiguous because we are processing a contiguous chunk of `group_size * H * W` elements.\n\n  - Another idea: the current kernel does two reductions (for mean and for variance) and then a normalization and scaling. We might try to fuse the two reductions into one to reduce the number of passes? But note that the variance requires the mean, so we cannot avoid two passes.\n\n  - We can precompute the mean and variance in a separate kernel? But that would break fusion.\n\n  - Alternatively, we can use shared memory to avoid reloading the data? But Triton does not have shared memory in the same way as CUDA. It uses registers and the L1 cache.\n\n  - We note that the current kernel does:\n      x = load ...\n      compute mean (reduction pass 1)\n      then compute x_centered = x - mean (so we have to reload x? Actually, we have it in registers, so that's okay)\n      then compute variance (reduction pass 2)\n\n  - So the data is kept in registers. That should be efficient.\n\n  - Why is it slow then? Maybe because the group size is 2048 and we are using a single thread block for that, and the reduction is done sequentially? Actually, Triton's reduction is parallel within the block using tree reduction.\n\n  - But note: the current kernel does two reductions and then an elementwise operation. The main bottleneck might be the memory bandwidth.\n\n  - We are loading the entire group once for the mean, then we do the variance, and then we do the normalization. Then we store the entire group. So we do 1 load and 1 store for the group, plus two reductions.\n\n  - How can we reduce the memory traffic? We cannot avoid the loads and stores.\n\n  - Another idea: the current kernel does not use vectorized loads/stores. We are using `tl.load` and `tl.store` for each element. We can try to use vectorized memory operations by processing multiple elements per thread.\n\n  - Let's restructure the kernel to process multiple elements per thread. We can break the group into chunks that are processed by multiple threads. Currently, the block size is 2048 and we are using 1 thread? Actually, no: the `tl.arange(0, BLOCK_SIZE)` creates a vector of 2048 elements, and the reductions are done over that vector. This is using a single thread? Actually, no: Triton maps the vector to multiple threads. But the reduction is done in parallel across the block.\n\n  - However, the current kernel does not explicitly break the group into smaller chunks for parallel processing. The entire group is loaded in one block and reduced in parallel. This should be efficient.\n\n  - Another possibility: the grid configuration might not be optimal. We are using (N, G) blocks. For the given parameters: N=128, G=8 -> 1024 blocks. Each block processes 2048 elements. This is 2M elements, which is the same as the total data (128*16*32*32=128*16384=2M). So it's correct.\n\n  - But note: the current kernel uses a fixed block size of the next power of two. This might lead to some threads processing out-of-bound elements (masked). We are already using a mask.\n\n  - What if we change the block size to a smaller value and have multiple blocks per group? We would need to do a two-pass reduction. This would require temporary memory to store partial sums and then a second kernel to combine them. This might be too heavy.\n\n  - Alternatively, we can use the existing Triton reduction features by writing a reduction kernel that uses multiple blocks? But that would break the fusion.\n\n  - Considering the constraints (we cannot change the grid configuration or the output tensor shapes), we must keep the grid as (N, G). So we cannot use multiple blocks per group.\n\n  - Therefore, we must optimize the current single-block-per-group approach.\n\n  - We can try to optimize the reduction by using a more efficient method? The current method (tl.sum) is already optimized.\n\n  - Another idea: precompute the mean and variance in a separate kernel and then do the normalization? But that would require an extra kernel launch and temporary memory, and we are not allowed to change the output.\n\n  - We note that the current kernel does not use Tensor Cores because it is FP32. The operations are not matrix multiplies.\n\n  - How about we change the data type? The reference is in float32. We are using float32 in the kernel. We cannot change to FP16 without checking correctness.\n\n  - But note: the problem says we are on H100 which has FP16 Tensor Cores. However, the operations we are doing (reductions, elementwise) are not tensor core operations.\n\n  - Another idea: we can try to fuse the clamp and maxpool? But the current code does the clamp after the maxpool. And the maxpool is done in PyTorch. We cannot change that because the clamp is after the maxpool.\n\n  - We are allowed to change the Triton kernel only. So we must focus on the fused_gn_scale_kernel.\n\n  - Let's try to break the group into smaller chunks and do the reduction in two steps without leaving the kernel? We can use shared memory? But Triton does not have explicit shared memory. It has `tl.static` for allocating shared memory, but it is limited.\n\n  - We can try to use a two-stage reduction within the same block? We are already using a block of 2048. We can break the 2048 elements into 64 chunks of 32 elements? Then we do a two-stage reduction: first reduce the 32 elements in each chunk, then reduce the 64 chunks.\n\n  - But Triton's built-in reductions (tl.sum) already do this under the hood.\n\n  - Therefore, we must look at the memory access pattern.\n\n  - The current memory access for loading x is contiguous and should be coalesced. Similarly for storing y.\n\n  - The loads for w, b, scale are by channel. We are loading one value per channel for the entire group. We are using `ch_offset = start + c_idx` and then loading with `c_idx < group_size`. This is a gather operation? Because the same channel is used for H*W elements. We are loading the same weight and bias for every spatial position. We could broadcast the same value to all spatial positions? But we are loading it once per element. That is inefficient.\n\n  - We can change the way we load the parameters: load the weight, bias, and scale once per channel and then broadcast to the entire spatial block. Currently, we are doing:\n        w = tl.load(w_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    and then we use `w` in `y = (x_centered * rstd) * w + b`. But note: `w` is a vector of length BLOCK_SIZE? Actually, no: `ch_offset` is a vector of the same length as `offsets` (BLOCK_SIZE). So we are loading one weight per element? But the same channel index should have the same weight.\n\n  - This is inefficient because we are loading the same weight value multiple times (once per spatial element in that channel). We should load the weight for each channel once and then broadcast it to the spatial elements.\n\n  - How to do that?\n    We can restructure the kernel to have two levels:\n        - The outer loop over channels in the group (there are `group_size` channels)\n        - The inner loop over spatial positions (H*W)\n\n    But note: the current block is 1D and we are processing the entire group. We want to vectorize over spatial positions.\n\n  - Alternatively, we can load the weights in a separate array of length `group_size` and then expand to the spatial positions. We can do:\n\n        w_ch = tl.load(w_ptr + start + tl.arange(0, group_size), mask=tl.arange(0, group_size) < group_size, other=0.0)\n        # then we need to expand w_ch to the entire group: for each channel, repeat H*W times.\n\n    But note: the block size is group_size * H * W. We can compute the channel index for each element: `c_idx = offsets // (H*W)`. Then we can index into `w_ch` by `c_idx`.\n\n  - So we can preload the weights for the entire group (for the channels in the group) and then use `c_idx` to get the weight for each element.\n\n  - This would change the weight load from:\n        w = tl.load(w_ptr + ch_offset, ...)   # ch_offset = start + c_idx, and we have one load per element (non contiguous, gather)\n    to:\n        w_ch = tl.load(w_ptr + start + tl.arange(0, group_size), ...)   # one contiguous load for group_size elements\n        then w = w_ch[c_idx]\n\n  - This reduces the number of memory transactions for weights from (group_size * H * W) to (group_size). Similarly for bias and scale.\n\n  - This could be a significant improvement because the weights are reused for H*W elements.\n\n  - Let's do the same for bias and scale.\n\n  - We must be cautious: the group_size might be small (2 in the example) so the load of `group_size` elements is contiguous and efficient.\n\n  - Steps:\n    1. Precompute the channel indices for the entire block? We already have `c_idx = offsets // (H*W)`.\n    2. Preload the weight, bias, scale for the group (for channels in [start, start+group_size)) into vectors of length `group_size`.\n    3. Then, for each element, we use `c_idx` to index into these vectors.\n\n  - We must mask the preload for group_size that might not be a power of two? We are using `tl.arange(0, group_size)` and a mask. But note: group_size might be small and we can round up to the next power of two? Actually, we are already using a mask in the load.\n\n  - We can do:\n\n        c_offsets = tl.arange(0, group_size)\n        mask_ch = c_offsets < group_size\n        w_ch = tl.load(w_ptr + start + c_offsets, mask=mask_ch, other=0.0)\n        # similarly for b and scale.\n\n        Then in the elementwise part:\n        w = w_ch[c_idx]\n        b = b_ch[c_idx]\n        scale_val = scale_ch[c_idx]\n\n  - But note: `c_idx` is an integer vector in the range [0, group_size). We must ensure that `c_idx` is within [0, group_size). We are already computing `c_idx = offsets // (H*W)`, and since `offsets` goes from 0 to group_size*H*W-1, then `c_idx` goes from 0 to group_size-1.\n\n  - This is safe.\n\n  - We also note that we are doing the same for the parameters of the group norm and the scale. The scale is of shape (out_channels, 1, 1). We are using `self.scale.view(-1)` so it is a 1D tensor of length out_channels. So we can load it the same way.\n\n  - Let's change the kernel accordingly.\n\n  - We must be cautious about the indices: `start` is the starting channel index for the group.\n\n  - We'll preload the parameters for the group (for the channels in the group) and then use `c_idx` to expand.\n\n  - We also note that the current kernel uses `ch_offset` for loading the parameters. We can remove that.\n\n  - We also note that the current mask for the parameters is `c_idx < group_size`. But since `c_idx` is in [0, group_size-1], it's always true. So we can remove that mask? Actually, no: because the block might have extra elements (due to rounding to power of two) that are masked by `mask`. We are already using `mask` for the main load and store. So for the parameters, we are expanding to the entire block, but for the extra elements (where mask is false) we don't care. So we can do without masking the parameter expansion? Actually, we are indexing `c_idx` which for out-of-bound offsets might be out of [0, group_size). But note: we have `mask = offsets < group_size * H * W`. For offsets beyond the group, `mask` is false and we don't use the computed y. So it's safe.\n\n  - However, to avoid out-of-bound indices in `c_idx` for the extra padded elements, we note that `c_idx = offsets // (H*W)`. For offsets beyond the group, `c_idx` might be >= group_size. We must mask the loading of the parameters? Actually, we are using `mask` in the main store so it's safe. But to avoid invalid memory access in `w_ch[c_idx]` when `c_idx>=group_size`, we can clamp or mask. However, the `w_ch` vector is of length `group_size` (exactly). So if `c_idx>=group_size`, it will be out of bounds.\n\n  - Therefore, we must mask the parameter expansion to only within the group. We can do:\n\n        w = tl.where(c_idx < group_size, w_ch[c_idx], 0.0)\n\n    but note: for the extra elements, we are going to mask the store anyway. But the computation of `y` must not crash. Alternatively, we can extend the `w_ch` vector to the next power of two? We preloaded only `group_size` elements. We cannot index beyond.\n\n  - We can instead preload a vector of length `group_size_padded` (the next power of two of group_size) and then use `c_idx` without masking? But note: the group_size might be 2, then we preload 2 elements. The `c_idx` for the padded elements might be 2, 3, ... which are beyond the loaded vector.\n\n  - Alternatively, we can pad the preload vector to the next power of two of group_size? But we don't know the next power of two of group_size in the kernel. We can pass it as a constant? We don't want to overcomplicate.\n\n  - Instead, we can use the existing `mask` (which is for the entire group) to avoid using invalid `c_idx`? Actually, when `mask` is false, we don't care about the value of `w`, `b`, `scale_val` because we won't store the result. So we can set them to 0 arbitrarily. We can do:\n\n        w = tl.where(mask, w_ch[c_idx], 0.0)\n\n    but note: `mask` is for the entire group, and the channel index `c_idx` might be beyond `group_size` only when `mask` is false. So we can use:\n\n        w = w_ch[c_idx]   # but this might be out-of-bound\n\n  - We cannot do that. We must avoid out-of-bound access.\n\n  - Therefore, we must either preload a padded vector or use a conditional. We can preload a vector of length `max_group_size`? But we don't know the maximum group size.\n\n  - Alternatively, we can change the way we index: we note that `c_idx` for the padded elements might be group_size, group_size+1, ... but we only preloaded [0, group_size-1]. So we can do:\n\n        w = tl.load(w_ptr + start + c_idx, mask=c_idx < group_size, other=0.0)\n\n    which is what we had originally. So we are back to square one.\n\n  - How about we preload the parameters for the entire group in a contiguous way and then use a gather? But that's what we are doing now.\n\n  - We must find a way to avoid the gather.\n\n  - Another idea: use broadcasting. We can load the weight for the channel and then broadcast to the spatial positions. We can do:\n\n        w_ch = tl.load(w_ptr + start + c, ...)   # for each channel c in the group\n        then we want to create a vector of length H*W for that channel with w_ch.\n\n    Then we can concatenate these vectors for all channels in the group to form a vector of length group_size * H * W.\n\n  - How to do that without a loop? We can use:\n\n        w_ch = tl.load(w_ptr + start + tl.arange(0, group_size), ...)   # (group_size,)\n        # Now we want to expand to (group_size, H*W) and then flatten? But we are in 1D.\n\n        # We can do:\n        #   w_expanded = w_ch[:, None] * tl.ones((1, H*W), dtype=w_ch.dtype)   # but we don't have 2D arrays.\n\n  - Triton does not support 2D arrays. We are working in 1D.\n\n  - We can do:\n\n        w_expanded = tl.broadcast(w_ch, [H*W])   # but no such function.\n\n  - Alternatively, we can do:\n\n        w_expanded = w_ch[c_idx]\n\n    which is the same as before.\n\n  - Therefore, we cannot avoid the gather if we want to expand the weights to the entire group.\n\n  - But note: the original kernel already does a gather. We are trying to reduce the number of memory transactions. The original gather does:\n        - group_size * H * W transactions, but they are not contiguous.\n\n    The new method does:\n        - group_size contiguous transactions for the entire group of weights.\n        - then we do a local expansion (register) to the entire group by indexing.\n\n    This is still a win because the group_size * H * W gather is non contiguous and might be expensive, while the contiguous load of group_size elements is efficient.\n\n  - However, the expansion by indexing (w_ch[c_idx]) is done in registers? It does not generate memory traffic. So the memory traffic for weights is reduced from (group_size * H * W) to (group_size).\n\n  - Similarly for bias and scale.\n\n  - Therefore, we should try this.\n\n  - Steps in the kernel:\n\n        # Precompute the channel index for each element in the block (as before)\n        c_idx = offsets // (H * W)\n\n        # Preload the weight, bias, scale for the group (channels in the group)\n        ch_offsets = tl.arange(0, group_size)   # [0, 1, ..., group_size-1]\n        mask_ch = ch_offsets < group_size   # in case group_size is not a power of two? Actually, group_size is integer and ch_offsets goes from 0 to group_size-1, so all true.\n        # But we load with mask to be safe.\n\n        w_ch = tl.load(w_ptr + start + ch_offsets, mask=mask_ch, other=0.0)\n        b_ch = tl.load(b_ptr + start + ch_offsets, mask=mask_ch, other=0.0)\n        scale_ch = tl.load(s_ptr + start + ch_offsets, mask=mask_ch, other=0.0)\n\n        # Then later, when we compute y:\n        w_val = w_ch[c_idx]   # but c_idx might be out of [0, group_size) for the padded elements? We must mask to avoid out-of-bound access in the vector w_ch which has length group_size.\n\n        # How to mask? We can use:\n        #   w_val = tl.where(c_idx < group_size, w_ch[c_idx], 0.0)\n        # But note: for the padded elements, we are not going to store, so it doesn't matter. However, if c_idx is out of bounds, the kernel will crash.\n\n        # We must ensure that for the padded elements, c_idx is less than group_size? Actually, no: because the padded elements are beyond the group, so c_idx might be group_size or more.\n\n        # How is c_idx computed? \n        #   offsets = [0, 1, ..., BLOCK_SIZE-1]\n        #   c_idx = offsets // (H*W)\n        #   and the condition for mask is: offsets < group_size * H * W.\n        #   Therefore, when offsets >= group_size * H * W, then c_idx >= group_size.\n\n        # So we must condition the indexing of w_ch to only when c_idx < group_size. We can do:\n\n        w_val = tl.zeros_like(c_idx, dtype=w_ch.dtype)\n        # But we don't have a way to conditionally index without causing out-of-bound.\n\n        # Instead, we can do:\n        #   w_val = tl.load(w_ptr + start + c_idx, mask=c_idx < group_size, other=0.0)   # but that's the original method.\n\n        # We want to avoid that.\n\n        # Alternative: pad the w_ch vector to the maximum possible c_idx? We don't know the maximum.\n\n        # We can use a trick: for c_idx >= group_size, we set w_val=0.0, but note that we are going to mask the store anyway. So we can do:\n\n        w_val = w_ch[c_idx]   # This will crash if c_idx >= group_size.\n\n        # To avoid crash, we can use a conditional move? But Triton doesn't have conditional indexing.\n\n        # Instead, we can use:\n        safe_c_idx = tl.minimum(c_idx, group_size-1)\n        w_val = w_ch[safe_c_idx]\n        # but then we would be using the last channel's weight for the padded elements? That is incorrect, but we are going to mask the store so it doesn't matter.\n\n        # But note: the padded elements might be very many. We don't want to cause out-of-bound. So we do:\n\n        safe_c_idx = tl.where(c_idx < group_size, c_idx, 0)\n        w_val = w_ch[safe_c_idx]\n        # and then set to 0 for the padded elements? Actually, we don't care because we mask the store. But we are using the weight for channel0 for the padded elements? That might be incorrect for the valid elements that happen to be in channel0? No, because for the valid elements, c_idx is in [0, group_size-1] and we use the correct weight.\n\n        # But for the padded elements, we are setting safe_c_idx=0 and then using w_ch[0]. Then we compute y, but then we mask the store. So it doesn't matter.\n\n        # However, we are doing unnecessary computation for the padded elements. But that's acceptable.\n\n        # But note: the padded elements are only at the end of the block. The number of padded elements is (next_power_of_two - group_size*H*W). We are already computing them and then masking the store.\n\n        # We can do the same for b and scale.\n\n  - Alternatively, we can avoid the safe_c_idx by ensuring that the preload vector is long enough? We can preload a vector of length (group_size_padded) where group_size_padded is the next power of two of group_size? But we don't know the next power of two of group_size in the kernel.\n\n  - We can pass the next power of two of group_size? But we don't have that. We can compute it? But it's expensive.\n\n  - Given the complexity, and since the padded elements are only a small fraction (if group_size*H*W is 2048 and we are using 2048, then there are no padded elements; if it is 2000, then 48 padded elements) we can do the safe_c_idx trick.\n\n  - We decide to do:\n\n        safe_c_idx = tl.where(c_idx < group_size, c_idx, 0)\n        w_val = w_ch[safe_c_idx]\n        b_val = b_ch[safe_c_idx]\n        scale_val = scale_ch[safe_c_idx]\n\n  - Then use these in the computation.\n\n  - We must note: this is safe because the padded elements (where mask is false) are not stored, and we are setting the parameters arbitrarily (to the first channel) for these elements.\n\n  - Now, let's change the kernel accordingly.\n\n  - We'll preload w_ch, b_ch, scale_ch at the beginning of the kernel.\n\n  - We remove the old loads of w, b, scale.\n\n  - Then in the normalization step, we use w_val, b_val, scale_val.\n\n  - We also note: the old code used `scale` for the scale, but now we call it `scale_val` to avoid conflict.\n\n  - We must be cautious about the data types: w_ch, b_ch, scale_ch are loaded as float32? Yes.\n\n  - We'll change the kernel step by step.\n\n  - We also note: the current kernel has a variable `scale` which we are changing to `scale_val`. We must change the multiplication at the end.\n\n  - The line: `y = y * scale` becomes `y = y * scale_val`\n\n  - We'll make these changes.\n\n  - We hope that by reducing the memory transactions for the parameters, we will improve performance.\n\n  - Additionally, we can try to optimize the reduction by using a more efficient method? But we leave that for later.\n\n  - We also note: the current kernel uses `tl.sum` which is efficient.\n\n  - Let's rewrite the kernel accordingly.\n\n  - We are going to change the kernel in the following way:\n\n        # Precompute the channel index for each element in the block\n        c_idx = offsets // (H * W)\n\n        # Preload the parameters for the entire group (channels in the group)\n        ch_offsets = tl.arange(0, group_size)   # indices for the channels in the group\n        mask_ch = ch_offsets < group_size   # always true, but for safety\n        w_ch = tl.load(w_ptr + start + ch_offsets, mask=mask_ch, other=0.0)\n        b_ch = tl.load(b_ptr + start + ch_offsets, mask=mask_ch, other=0.0)\n        scale_ch = tl.load(s_ptr + start + ch_offsets, mask=mask_ch, other=0.0)\n\n        # ... rest of the kernel until the normalization step\n\n        # Instead of loading w, b, scale per element, we do:\n        safe_c_idx = tl.where(c_idx < group_size, c_idx, 0)\n        w_val = w_ch[safe_c_idx]\n        b_val = b_ch[safe_c_idx]\n        scale_val = scale_ch[safe_c_idx]\n\n        y = (x_centered * rstd) * w_val + b_val\n        y = y * scale_val\n\n  - We remove the old lines that loaded w, b, scale.\n\n  - We hope this improves performance.\n\n  - We also note: the current kernel might be memory bound. This change reduces the memory traffic for the parameters from (3 * group_size * H * W) to (3 * group_size). This is a significant reduction.\n\n  - For the example: group_size=2, H*W=1024, then the old method did 3*2*1024=6144 loads, the new method does 3*2=6 loads.\n\n  - This should be a big win.\n\n  - Let's implement it.\n\n  - We must be cautious: the kernel must still compile and be correct.\n\n  - We'll change the kernel and hope for a speedup.\n\n  - We also note: the current kernel might be slowed down by the reduction operations? But we are not changing those.\n\n  - We'll make this change and hope it improves the speedup.\n\n  - We are only changing the Triton kernel, so we output the entire EVOLVE-BLOCK.\n\n  - We note: the class ModelNew and the rest of the code remains the same.\n\n  - We do not change the grid or the block size.\n\n  - We output the entire code block.\n\n  - IMPORTANT: We must not change the function signature.\n\n  - Let's code accordingly.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_gn_scale_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    s_ptr,\n    y_ptr,\n    C, G, H, W,\n    eps: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    n = tl.program_id(0)\n    g = tl.program_id(1)\n    \n    group_size = C // G\n    start = g * group_size\n    offsets = tl.arange(0, BLOCK_SIZE)\n    c_idx = offsets // (H * W)\n    spatial_idx = offsets % (H * W)\n    \n    mask = offsets < group_size * H * W\n    base_ptr = n * C * H * W + start * H * W\n    \n    # Preload parameters for the entire group\n    ch_offsets = tl.arange(0, group_size)\n    mask_ch = ch_offsets < group_size\n    w_ch = tl.load(w_ptr + start + ch_offsets, mask=mask_ch, other=0.0)\n    b_ch = tl.load(b_ptr + start + ch_offsets, mask=mask_ch, other=0.0)\n    scale_ch = tl.load(s_ptr + start + ch_offsets, mask=mask_ch, other=0.0)\n    \n    # Load data\n    x = tl.load(x_ptr + base_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean\n    sum_x = tl.sum(x, axis=0)\n    mean = sum_x / (group_size * H * W)\n    \n    # Compute variance\n    x_centered = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / (group_size * H * W)\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Normalize and scale\n    safe_c_idx = tl.where(c_idx < group_size, c_idx, 0)\n    w_val = w_ch[safe_c_idx]\n    b_val = b_ch[safe_c_idx]\n    scale_val = scale_ch[safe_c_idx]\n    \n    y = (x_centered * rstd) * w_val + b_val\n    y = y * scale_val\n    tl.store(y_ptr + base_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        \n        # Fused group norm and scaling with Triton\n        N, C, H, W = x.shape\n        G = self.group_norm.num_groups\n        y = torch.empty_like(x)\n        group_elements = (C // G) * H * W\n        grid = (N, G)\n        fused_gn_scale_kernel[grid](\n            x, \n            self.group_norm.weight, \n            self.group_norm.bias, \n            self.scale.view(-1), \n            y,\n            C, G, H, W,\n            self.group_norm.eps,\n            BLOCK_SIZE=triton.next_power_of_2(group_elements)\n        )\n        \n        x = self.maxpool(y)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
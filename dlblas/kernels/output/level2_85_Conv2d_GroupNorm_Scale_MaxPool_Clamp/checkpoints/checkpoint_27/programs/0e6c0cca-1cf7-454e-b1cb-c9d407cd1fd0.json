{"id": "0e6c0cca-1cf7-454e-b1cb-c9d407cd1fd0", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_gn_scale_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    s_ptr,\n    y_ptr,\n    C, G, H, W,\n    eps: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    n = tl.program_id(0)\n    g = tl.program_id(1)\n    \n    group_size = C // G\n    start = g * group_size\n    offsets = tl.arange(0, BLOCK_SIZE)\n    c_idx = offsets // (H * W)\n    spatial_idx = offsets % (H * W)\n    ch_offset = start + c_idx\n    \n    mask = offsets < group_size * H * W\n    base_ptr = n * C * H * W + start * H * W\n    \n    # Load data\n    x = tl.load(x_ptr + base_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean\n    sum_x = tl.sum(x, axis=0)\n    mean = sum_x / (group_size * H * W)\n    \n    # Compute variance\n    x_centered = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / (group_size * H * W)\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Normalize and scale\n    w = tl.load(w_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    b = tl.load(b_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    scale = tl.load(s_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    \n    y = (x_centered * rstd) * w + b\n    y = y * scale\n    tl.store(y_ptr + base_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        \n        # Fused group norm and scaling with Triton\n        N, C, H, W = x.shape\n        G = self.group_norm.num_groups\n        y = torch.empty_like(x)\n        group_elements = (C // G) * H * W\n        grid = (N, G)\n        fused_gn_scale_kernel[grid](\n            x, \n            self.group_norm.weight, \n            self.group_norm.bias, \n            self.scale.view(-1), \n            y,\n            C, G, H, W,\n            self.group_norm.eps,\n            BLOCK_SIZE=triton.next_power_of_2(group_elements)\n        )\n        \n        x = self.maxpool(y)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "7eba1fd4-0264-4fae-aa83-628d57ea6306", "generation": 1, "timestamp": 1754580823.5506194, "iteration_found": 2, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.895, "runtime_ms": 0.114, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.102, "speed_up": 0.895, "custom_timing": 0.114}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.1, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.102, "speed_up": 1.02, "custom_timing": 0.1}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.1000\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 1.02, 'custom_timing': 0.1}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.1000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 1.02, 'custom_timing': 0.1}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs convolution, group normalization, scaling, max pooling, and clamping.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, height, width).\n        Returns:\n            Output tensor of shape (batch_size, out_channels, height', width').\n        \"\"\"\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = self.maxpool(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.1000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 1.02, 'custom_timing': 0.1})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs convolution, group normalization, scaling, max pooling, and clamping.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, height, width).\n        Returns:\n            Output tensor of shape (batch_size, out_channels, height', width').\n        \"\"\"\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = self.maxpool(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Convolution\n  2. Group Normalization\n  3. Scaling by a learnable parameter\n  4. Max Pooling\n  5. Clamping\n\nThe goal is to rewrite the model to be faster, but note that the current implementation is using high-level PyTorch operations.\n\nHowever, the instructions are to rewrite using Triton. But note: the current code does not use any custom Triton kernel.\n\nWe are to replace the entire EVOLVE-BLOCK. The current block is a PyTorch model and data setup.\n\nWe must note that the Triton rewrite must be for a kernel that can replace one of the operations? But the entire model is a sequence of operations.\n\nBut note: the problem says we are given a dataset of kernelbench files. We are to write Triton for the custom_code (which is the forward function?).\n\nHowever, the example provided was a standalone function (per_row_quantize_int8) rewritten in Triton.\n\nLooking at the current ModelNew, the forward function has multiple operations. We cannot rewrite the entire forward in one Triton kernel? \n\nBut note: the instructions say \"Generate functionally equivalent but faster Triton implementations (custom_code)\".\n\nWe must focus on the most compute-intensive part? But the problem does not specify.\n\nAlternatively, we might be expected to write a Triton kernel for the entire forward pass? That might be too complex.\n\nBut note: the constraints say \"NEVER change @triton.jit function signatures or parameter names\" and \"NEVER modify grid configuration, output tensor shapes, or PID logic\". This implies we are writing a Triton kernel for a specific operation.\n\nHowever, the current program is a full model. We must reconsider.\n\nLooking at the history, the current program is the reference code. We are to replace it with a custom Triton implementation.\n\nBut note: the example replaced a single function (per_row_quantize_int8) with a Triton kernel. So we must identify a function that we can write in Triton.\n\nIn the current ModelNew, the entire forward is a sequence of PyTorch operations. There is no single function that we can easily replace? \n\nAlternatively, we can try to fuse multiple operations? For example, fuse convolution, group normalization, scaling, and clamping? But maxpool might be tricky.\n\nBut note: the problem says \"Each file contains PyTorch reference (reference_code)\" and we are to write Triton (custom_code). The reference_code is the entire model? Then the custom_code should be the entire model implemented in Triton? That is a big task.\n\nAlternatively, we might be expected to write a Triton kernel for one of the operations? But the problem says \"functional equivalence\" of the entire model.\n\nGiven the complexity, and that the current model is a reference code, I think we are to write a Triton kernel for the entire forward pass? But that is a huge kernel.\n\nAlternatively, note that the problem says \"small iterative steps: Modify \u226420 lines per revision\". So we cannot rewrite everything at once.\n\nBut the current code is 0 lines of Triton. We must start.\n\nHowever, the example provided a Triton kernel for a specific function. So we must break down the model into functions and replace one?\n\nBut the model is currently implemented as a sequence of operations in the forward. We can try to replace the most expensive one? But without profiling, we don't know.\n\nAlternatively, we can write a Triton kernel for the group normalization? Because group normalization is a common operation that can be optimized.\n\nBut note: the model also does convolution. Convolution is very complex and already optimized by PyTorch. Writing a convolution in Triton is non-trivial.\n\nGiven the constraints, we might focus on the group normalization and the scaling? And leave convolution and maxpool as they are? But that would not be the entire model.\n\nAlternatively, we can write a kernel that does group normalization and scaling together? And then leave the rest.\n\nBut the problem says \"functional equivalence\" for the entire model. We cannot change the model structure arbitrarily? We must replace the entire forward with a Triton kernel? That seems too broad.\n\nAlternatively, note that the problem says \"custom_code\" is the Triton implementation. So we are to write a custom implementation of the entire model? But that is a huge task.\n\nBut wait, the example only replaced one function. So perhaps the reference_code is a function, not a module? \n\nLooking at the current program: it defines a class ModelNew and then some global variables and helper functions (get_inputs, get_init_inputs). \n\nThe problem says: \"Name the output structure class as ModelNew.\" So we are to keep the class ModelNew.\n\nBut the example did not have a class. It had a function.\n\nThis suggests that the problem might be inconsistent? Or perhaps we are to write a Triton kernel for a specific operation within the model? \n\nGiven the ambiguity, I propose the following:\n\nWe will replace the entire forward function with a custom Triton kernel that does the entire sequence? But that is too complex for a first step.\n\nAlternatively, we can try to write a fused kernel for the group normalization and scaling? Then in the forward, we do:\n  1. Convolution (PyTorch)\n  2. Then our custom groupnorm_scale kernel (Triton)\n  3. MaxPool (PyTorch)\n  4. Clamp (PyTorch)\n\nThis way, we are replacing two operations with one Triton kernel. That might give speedup.\n\nLet me design:\n\nWe write a Triton kernel for group normalization and scaling.\n\nThe group normalization in PyTorch does:\n  - Splits the channels into groups, then normalizes each group by mean and variance.\n  - Then it applies affine transformation (with learnable weight and bias). But note: in our model, we do:\n        x = self.group_norm(x)\n        x = x * self.scale\n\nThere is no bias? And the scaling is by a separate parameter.\n\nBut note: the group normalization in PyTorch already has a weight and bias. In our model, we are using nn.GroupNorm without specifying if we are using affine? \n\nLooking at the current code: \n    self.group_norm = nn.GroupNorm(num_groups, out_channels)\n\nBy default, nn.GroupNorm has affine=True? Let me check: \n    https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html\n\nYes, so it has learnable affine parameters (weight and bias). Then we are multiplying by an extra scale.\n\nSo the operations are:\n  1. GroupNorm: which does (x - mean) / sqrt(var + eps) * weight + bias\n  2. Then multiply by self.scale\n\nWe can fuse these two into one operation: \n    ( (x - mean) / sqrt(var + eps) * weight + bias ) * scale\n\nBut note: the self.scale is of shape (out_channels, 1, 1). So it's per-channel.\n\nWe can write a Triton kernel that does group normalization and then multiplies by the scale.\n\nHowever, note that the group normalization requires computing mean and variance per group and per channel? Actually, per group and per spatial location? \n\nActually, group normalization: \n    Input: [N, C, H, W]\n    Groups: G\n    Then, the channels are divided into G groups (each group has C/G channels). For each group and for each sample, we compute mean and variance over (C/G * H * W).\n\nWe can write a Triton kernel that does:\n\n  - For each group in parallel? \n  - We need to compute the mean and variance for each group and each sample.\n\nThis is a two-pass kernel? Or we can use online computation? \n\nAlternatively, we can use the existing Triton example for group normalization? \n\nBut note: the problem says we are to write the entire model? But we are only replacing one part.\n\nWe are allowed to leave the convolution and maxpool in PyTorch.\n\nSo the plan:\n\n  We will write a Triton kernel for the group normalization and scaling step. We will call it `group_norm_scale_kernel`.\n\n  Then in the forward:\n        x = self.conv(x)\n        # Instead of:\n        #   x = self.group_norm(x)\n        #   x = x * self.scale\n        # We do:\n        x = group_norm_scale_kernel(x, self.group_norm.weight, self.group_norm.bias, self.scale, self.group_norm.num_groups, self.group_norm.eps)\n\n  Then continue with maxpool and clamp.\n\nBut note: the group_norm in PyTorch has affine parameters (weight and bias). We are using them in the kernel.\n\nHowever, the current model also has a separate scaling (self.scale). We are fusing that.\n\nBut note: the model might be trained? We are in inference? The problem doesn't specify. But we are given a forward.\n\nWe must also consider that the group_norm and scaling are element-wise operations? But the normalization requires reduction.\n\nGiven the complexity, we write the kernel for group_norm_scale.\n\nWe design the kernel:\n\n  Inputs:\n      x: tensor of shape [N, C, H, W] (after convolution)\n      weight: from group_norm, shape [C]\n      bias: from group_norm, shape [C]\n      scale: tensor of shape [C, 1, 1] -> we can reshape to [C]\n      num_groups: int\n      eps: float\n\n  Steps in kernel:\n\n      We split the channels into G groups. Each group has group_size = C // G.\n\n      We have to compute for each sample n, each group g, the mean and variance over the group (which is a block of channels and the entire spatial domain).\n\n      Then normalize each element in the group and then apply: \n          normalized * weight[c] + bias[c]   and then multiply by scale[c]\n\n      We can do:\n          output = ( (x - mean) * rstd ) * weight + bias ) * scale\n\n      But note: we can combine weight and scale: \n          output = (x - mean) * rstd * (weight * scale) + (bias * scale)   ??? \n          Actually, no: because the scale is applied after the affine.\n\n      Alternatively, we can do:\n          output = ((x - mean) * rstd * weight + bias) * scale\n\n      But that is two multiplications and one addition.\n\n      We can precompute: \n          factor = rstd * weight * scale\n          bias_term = bias * scale\n\n          output = (x - mean) * factor + bias_term\n\n      However, note that the scale might be applied to the entire result? So yes.\n\n  Implementation:\n\n      We can have each program handle one group in one sample? \n\n      Let:\n          total_elements_per_group = (C // G) * H * W\n\n      We can have grid of (N, G).\n\n      For each program (n, g), we compute the mean and variance of the group.\n\n      Then we normalize each element in the group.\n\n      But note: we need to store the entire group? We can do two passes: first to compute mean and variance, then to normalize.\n\n      Alternatively, we can use two kernels? One to compute the mean and variance per group, and then one to normalize? \n\n      But the problem says we are to write one kernel? \n\n      We can do it in one kernel with two passes? But that would require storing intermediate data? \n\n      Alternatively, we can use a reduction within the kernel? \n\n      We can do:\n\n          pid0 = program_id(0)  # n from 0 to N-1\n          pid1 = program_id(1)  # g from 0 to G-1\n\n          Then, we have a block that loads the entire group (C//G * H * W) and then computes the mean and variance? \n\n      But the group might be too big? We can use multiple programs per group? \n\n      This is complex.\n\n  Alternatively, we can use the Triton reduction for mean and variance? \n\n  But note: the Triton tutorial has a layer norm example. GroupNorm is similar to LayerNorm but on groups of channels.\n\n  We can adapt the LayerNorm Triton kernel for GroupNorm? \n\n  Let me recall the LayerNorm Triton kernel: it uses one program per row (where row is the normalized dimension). For GroupNorm, we have:\n\n      Input: [N, C, H, W] -> we can view as [N, G, C', H, W] where C' = C//G.\n\n      Then we want to normalize over the dimensions [C', H, W] for each (N, G).\n\n      So the normalized dimension has size = C' * H * W.\n\n  We can write a kernel that has:\n\n      grid = (N, G)\n\n      BLOCK_SIZE = the number of elements we can handle per block? But we might have to use multiple blocks per group? \n\n      However, if the group size (C'*H*W) is too big, we use multiple blocks? \n\n      We can use a reduction across multiple blocks? \n\n  Alternatively, we can use the existing Triton function for normalization? \n\n  But note: we are to write the kernel.\n\n  Given the complexity and the 20-line constraint, we might not be able to write a full group norm kernel in one step.\n\n  Alternatively, we can leave the group normalization in PyTorch and only write the scaling in Triton? That would be a simple element-wise kernel. But that is unlikely to give speedup.\n\n  Considering the time, and since the current model is the reference, I think we are expected to rewrite the entire model? \n\n  But that is not feasible.\n\n  Alternatively, note that the problem says \"small iterative steps\". We are to start with Phase 1 (compilation) and then Phase 2 (correctness). \n\n  We are currently at reference code. We are to write the first Triton kernel for one part.\n\n  I propose to start with the scaling operation? It is a simple element-wise multiplication. We can write a Triton kernel for that.\n\n  We can write a kernel for scaling that is fused with the clamp? But the clamp comes after maxpool.\n\n  Alternatively, we can write a kernel for the entire forward? That is too much.\n\n  Given the confusion, I will stick to a simple approach: write a Triton kernel for the scaling operation (which is just element-wise multiplication by a per-channel scale) and then leave the rest. \n\n  But note: the scaling operation is already very fast in PyTorch. We are unlikely to get speedup.\n\n  Alternatively, we can fuse the group normalization and the scaling? But we don't have a group norm kernel.\n\n  Since the problem says we are at the reference code, and we are to start with a Triton implementation, I think we are expected to write a Triton kernel for the entire model? \n\n  But the model includes convolution, which is very complex.\n\n  I see that the example was a simple quantization function. So perhaps the current program is not the entire model? \n\n  Looking at the current program: it defines a model and then some constants and helper functions. The model is small: 128 batch, 3x32x32 input, 16 output channels.\n\n  Given the complexity of writing a convolution in Triton, I think we might have to reconsider the approach.\n\n  Alternatively, the problem might be that we are to write a Triton kernel for the clamping operation? That is even simpler.\n\n  I think there might be a misunderstanding.\n\n  Let me re-read the problem: \"Dataset: 250 kernelbench/level1-level5 *.py files\"\n\n  The current program is one of these files. It is a PyTorch reference. We are to replace the reference_code with custom_code written in Triton.\n\n  The example showed a function being replaced. But here the reference_code is a module.\n\n  We must look at the structure of the benchmark. The problem says: \"Each file contains PyTorch reference (reference_code)\".\n\n  In this file, the reference_code is the ModelNew and the data setup.\n\n  We are to write a Triton implementation that is functionally equivalent to ModelNew.forward.\n\n  So we are to write a custom implementation of the entire forward pass using Triton? \n\n  This is a level 1-5 difficulty? This one might be level 5.\n\n  Given the constraints, we might write a Triton kernel for the entire forward pass? \n\n  We will write a kernel that does:\n\n      - Convolution: with weight from self.conv\n      - Group normalization: with parameters from self.group_norm\n      - Scaling: by self.scale\n      - MaxPooling: with kernel_size=maxpool_kernel_size\n      - Clamping\n\n  This is a huge kernel.\n\n  Alternatively, we can break it down into multiple kernels? But the problem says one EVOLVE-BLOCK.\n\n  Given the time, and since we are only to modify <=20 lines, we cannot write the entire kernel in one step.\n\n  Therefore, I think we are expected to write a Triton kernel for one of the operations. But which one?\n\n  The problem says: \"Areas identified for improvement: Consider simplifying the code to improve readability and maintainability\"\n\n  This is not helpful.\n\n  Given the above, I will take a different approach: \n\n  We will write a Triton kernel for the group normalization and scaling, and leave the convolution and maxpool in PyTorch. This is a common fusion.\n\n  We will write a group_norm_scale kernel.\n\n  Steps for group_norm_scale kernel:\n\n      Input: \n          x: tensor of shape [N, C, H, W]\n          weight: [C] (from group_norm)\n          bias: [C] (from group_norm)\n          scale: [C] (we will squeeze the given scale tensor to 1D)\n          num_groups: int\n          eps: float\n\n      Output: tensor of shape [N, C, H, W]\n\n      Process:\n\n          Let G = num_groups, and group_size = C // G.\n\n          We'll have a grid of (N, G, H, W) ? But then we would have to do reductions per group? That doesn't work.\n\n          Alternatively, grid of (N, G). For each (n, g), we compute the mean and variance over the group (which is group_size * H * W elements).\n\n          Then we want to normalize every element in the group.\n\n          How to parallelize? We can have each block handle one group in one sample. Then within the block, we can use multiple warps to reduce.\n\n          We will have:\n\n              pid0 = tl.program_id(0)  # n\n              pid1 = tl.program_id(1)  # g\n\n          Then, we need to load the entire group? But the group might be too large. We can use a loop.\n\n          We can use a two-pass method: first to compute mean and variance, then to normalize.\n\n          We'll have to use shared memory to store intermediate sums? Or use atomic operations? \n\n          But Triton does not have shared memory? It has static_shared, but it's limited.\n\n          Alternatively, we can use online parallel variance algorithm? \n\n          Given the complexity, and since we are limited to 20 lines, we cannot implement a full group norm kernel here.\n\n  Given the above, I think we might have to start with a simpler operation: the scaling and clamping? But they are after the maxpool.\n\n  The maxpool reduces the spatial dimensions. After maxpool, the tensor is [N, C, H//2, W//2] (if kernel_size=2 and stride=2).\n\n  Then we do scaling (per-channel) and then clamp.\n\n  We can fuse the scaling and clamping into one kernel.\n\n  Let me write a kernel for scaling and clamping:\n\n      Input: \n          x: tensor of shape [N, C, H, W]  (after maxpool)\n          scale: tensor of shape [C]   (per-channel scale)\n          clamp_min, clamp_max: float\n\n      Output: tensor of the same shape.\n\n      Operation: for each element at (n, c, i, j):\n          out = clamp(x * scale[c], clamp_min, clamp_max)\n\n      This is an element-wise kernel.\n\n  We can write a very simple Triton kernel for this.\n\n  This is likely to be memory bound, but we can try.\n\n  We'll have:\n\n      @triton.jit\n      def scale_clamp_kernel(\n          x_ptr,\n          scale_ptr,\n          output_ptr,\n          clamp_min,\n          clamp_max,\n          C: tl.constexpr,\n          H: tl.constexpr,\n          W: tl.constexpr,\n          BLOCK_SIZE: tl.constexpr,\n      ):\n          pid = tl.program_id(0)\n          num_pixels = C * H * W\n          offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n          mask = offsets < num_pixels\n\n          # We have to compute the channel index for each offset? \n          # We can do: \n          #   c = (offsets // (H*W)) % C   -> but note: the tensor is [N, C, H, W]\n          #   n = offsets // (C * H * W)\n          #   offset_in_channel = offsets % (H*W)\n          #   i = offset_in_channel // W\n          #   j = offset_in_channel % W\n\n          # Alternatively, we can use a 4D grid? But the problem says not to change grid.\n\n          # We are using 1D grid. The total number of elements is N*C*H*W.\n\n          # Let's do 1D grid over all elements.\n\n          x = tl.load(x_ptr + offsets, mask=mask)\n          # We need the scale for the channel of each element.\n          # We can compute the channel index for each element in the block.\n          #   channel_index = (offsets // (H*W)) % C\n\n          # But note: the order of the tensor is [N, C, H, W]. So the stride for C is H*W, for H is W, for W is 1.\n\n          # How to get the channel index for an element at offset 'o' in the flattened array?\n\n          #   o = n*(C*H*W) + c*(H*W) + i*W + j\n\n          #   c = (o // (H*W)) % C\n\n          # We can do:\n\n          channel_index = (offsets // (H * W)) % C\n\n          s = tl.load(scale_ptr + channel_index, mask=mask)\n\n          scaled = x * s\n          clamped = scaled\n          if clamp_min is not None:\n              clamped = tl.where(scaled < clamp_min, clamp_min, clamped)\n          if clamp_max is not None:\n              clamped = tl.where(scaled > clamp_max, clamp_max, clamped)\n\n          tl.store(output_ptr + offsets, clamped, mask=mask)\n\n  Then in the forward, we replace:\n\n        # x = self.maxpool(x)\n        # x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        # with:\n\n        x = self.maxpool(x)\n        # Now x is [N, C, H', W']\n        # We want to do scale and clamp in Triton\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        # We can set block size to 1024\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        scale_clamp_kernel[grid](x, self.scale.view(-1), output, self.clamp_min, self.clamp_max, x.shape[1], x.shape[2], x.shape[3], BLOCK_SIZE=1024)\n        x = output\n\n  But note: the self.scale is [out_channels, 1, 1] -> we can flatten to [out_channels].\n\n  This might not give speedup because it's memory bound, but it's a start.\n\n  Given the constraints, and that we are to start with compilation and correctness, we will write this kernel.\n\n  We will do:\n\n      - Write the scale_clamp_kernel as above.\n      - Modify the forward to use this kernel after maxpool.\n\n  But note: the clamping in the original is after the scaling and after the maxpool. We are only replacing the scaling and clamping after the maxpool.\n\n  This is a safe because the maxpool is done in PyTorch.\n\n  Let's write the code accordingly.\n\n  We must be cautious about the grid and the block.\n\n  However, note: the problem says \"NEVER change grid configuration\". But in this kernel, we are using a 1D grid. We are not given a grid in the reference, so we are defining it.\n\n  But the constraint says: \"NEVER modify grid configuration, output tensor shapes, or PID logic\" for the Triton kernel we are replacing? But here we are adding a new kernel.\n\n  We are allowed.\n\n  We'll do it.\n\n  But note: the current program does not have any Triton kernel. We are introducing one.\n\n  We must also consider that the kernel might be slower than PyTorch's element-wise operations.\n\n  Given the above, I will proceed.\n\n  We will replace the EVOLVE-BLOCK with:\n\n      - The class ModelNew, but in the forward, we replace the last two lines (maxpool and clamp) with maxpool and then our Triton kernel.\n      - We will define the Triton kernel in the same file.\n\n  But note: the EVOLVE-BLOCK must contain the entire code that is to be evaluated.\n\n  We will include the kernel definition and the modified ModelNew.\n\n  Let's write the code.\n\n  We must also consider that the Triton kernel must compile and be correct.\n\n  We will use:\n\n      import triton\n      import triton.language as tl\n\n  So we must add those imports.\n\n  We are also to name the class as ModelNew.\n\n  We will keep the same __init__.\n\n  We will change the forward.\n\n  We must also consider that the clamping might be None for min or max? But in the model, clamp_min and clamp_max are provided.\n\n  We will assume they are floats.\n\n  Let's write the code.\n\n  Note: The current program has batch_size=128, and the tensor after maxpool is [128, 16, 16, 16] (because 32x32 with kernel=2 -> 16x16).\n\n  This has 128*16*16*16 = 524,288 elements.\n\n  We'll use a block size of 1024, then grid of  ceil(524288/1024)=513.\n\n  This should be fine.\n\n  We will write the kernel and hope for the best.\n\n  If it doesn't give speedup, we might try to fuse more operations in the future.\n\n  But note: the problem says small steps.\n\n  This is our first step.\n\n  Code structure:\n\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def scale_clamp_kernel(...):\n          ... \n\n      class ModelNew(nn.Module):\n          ... same __init__ ...\n\n          def forward(self, x):\n              x = self.conv(x)\n              x = self.group_norm(x)\n              x = x * self.scale   # This is the original scaling, but note: we are not replacing this scaling.\n              x = self.maxpool(x)\n\n              # Now we do the clamping with a Triton kernel for the scaling and clamping? \n              # But wait: in the original, there is no scaling after maxpool. \n              # The scaling before maxpool is already done.\n\n              # In the original forward, the scaling (by self.scale) is before the maxpool.\n\n              # And the clamping is after the maxpool.\n\n              # So in the original, there is no scaling after maxpool.\n\n              # Therefore, we are only doing the clamping after maxpool.\n\n              # But our kernel is for scaling and clamping.\n\n              # This is a mistake.\n\n  Let me double-check the original forward:\n\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale   # scaling happens here, before maxpool\n        x = self.maxpool(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)   # only clamping after maxpool.\n\n  So we only need to do the clamping after maxpool.\n\n  Therefore, we can write a kernel for just the clamping? That is even simpler.\n\n  But then why would we get speedup? It's a simple operation.\n\n  Alternatively, we can fuse the scaling that is before the maxpool with the group norm? But the scaling before the maxpool is already an element-wise operation.\n\n  Given the above, I think we should revisit the group norm fusion.\n\n  But note: the group norm is more expensive.\n\n  Given the time constraints, I will write a Triton kernel for the entire group normalization and the scaling (which is before the maxpool) and leave the rest.\n\n  So in the forward:\n\n        x = self.conv(x)   # in PyTorch\n        # Then we do group_norm and scaling in Triton\n        x = group_norm_scale_triton(x, self.group_norm.weight, self.group_norm.bias, self.scale, self.group_norm.num_groups, self.group_norm.eps)\n        x = self.maxpool(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)   # in PyTorch\n\n  This might give speedup because group norm is compute-intensive.\n\n  We will write the group_norm_scale_triton kernel.\n\n  We will base it on the Triton layer norm example.\n\n  Here is a plan for the kernel:\n\n      We view the input as [N, G, C//G, H, W] -> normalize over [C//G, H, W].\n\n      Grid: (N, G)\n\n      For each (n, g), we want to load the entire group and compute mean and variance.\n\n      BLOCK_SIZE: the number of elements in the group might be large, so we will use a reduction with multiple blocks? \n\n      Alternatively, we can use one block per group and within the block, reduce with multiple warps.\n\n      We will let the group be reduced in one block if possible, but if the group is too large, we use multiple blocks.\n\n      This is complex.\n\n  Given the 20-line constraint, I will provide a kernel that only works for groups that are not too large.\n\n  We will assume that the group fits in one block.\n\n  Steps kernel for group_norm_scale for one group in one sample:\n\n      pid0 = n = tl.program_id(0)\n      pid1 = g = tl.program_id(1)\n\n      group_size = C // G\n      group_start = g * group_size\n      range = tl.arange(0, group_size * H * W)\n\n      # Create pointers to the input for this group\n      x_ptr + n * (C*H*W) + group_start * (H*W) + range\n\n      Load the group elements.\n\n      Then compute mean and variance.\n\n      Then normalize: (x - mean) * rstd\n\n      Then apply the group norm weight and bias for the channel: \n          for an element at channel c = group_start + offset_channel, we use weight[c] and bias[c]\n\n      Then multiply by scale[c] (which is the extra scale).\n\n      But note: the scale is per-channel, so for each element, we know its channel index.\n\n      However, within the group, the channel index is group_start + (offset // (H*W)).\n\n      We can precompute the channel index for each element in the group.\n\n  This kernel will have grid (N, G) and we assume the group fits in one block.\n\n  The block size will be group_size * H * W, which might be too big. For example, group_size=16//8=2, H=32, W=32 -> 2*32*32=2048, which is acceptable.\n\n  We will do:\n\n      @triton.jit\n      def group_norm_scale_kernel(\n          x_ptr,\n          weight_ptr,\n          bias_ptr,\n          scale_ptr,\n          output_ptr,\n          C, G, H, W,\n          eps: tl.constexpr,\n          BLOCK_SIZE: tl.constexpr,   # the product group_size * H * W, but we will not use it directly\n      ):\n          pid0 = tl.program_id(0)  # n\n          pid1 = tl.program_id(1)  # g\n\n          group_size = C // G\n          group_start = pid1 * group_size\n\n          # The number of elements in the group\n          num_group_elements = group_size * H * W\n\n          # Start of the sample\n          sample_start = pid0 * C * H * W\n          group_start_offset = sample_start + group_start * H * W\n\n          # Offsets for the group within the sample\n          offsets = group_start_offset + tl.arange(0, num_group_elements)\n\n          # Load the group\n          x = tl.load(x_ptr + offsets)\n\n          # Compute mean and variance\n          mean = tl.sum(x, axis=0) / num_group_elements\n          x_centered = x - mean\n          var = tl.sum(x_centered * x_centered, axis=0) / num_group_elements\n          rstd = 1.0 / tl.sqrt(var + eps)\n\n          # Normalize\n          x_normalized = x_centered * rstd\n\n          # Now apply weight, bias, and scale.\n          # For each element, we need its channel index within the group.\n          # The channel index in the whole tensor is group_start + channel_in_group.\n          # The element's offset in the group is 'i' in [0, num_group_elements-1]\n          #   channel_in_group = i // (H*W)\n          #   channel_index = group_start + channel_in_group\n\n          # We can precompute the channel_in_group for each element:\n          channel_in_group = tl.arange(0, group_size)\n          channel_in_group = channel_in_group[:, None, None]  # [group_size, 1, 1]\n          # We then need to expand to [group_size, H, W] and then flatten\n          # Instead, we can compute for each element its channel_in_group by integer division.\n\n          # Create an array of the same length as x_normalized for channel_in_group.\n          element_indices = tl.arange(0, num_group_elements)\n          channel_in_group = element_indices // (H * W)   # integer division\n          channel_index = group_start + channel_in_group\n\n          # Load weight, bias, scale for these channels\n          w = tl.load(weight_ptr + channel_index)\n          b = tl.load(bias_ptr + channel_index)\n          s = tl.load(scale_ptr + channel_index)   # scale_ptr is [C] \n\n          # Apply: (x_normalized * w + b) * s\n          result = (x_normalized * w + b) * s\n\n          # Store\n          tl.store(output_ptr + offsets, result)\n\n  Then in the forward:\n\n        x = self.conv(x)\n        # For group_norm and scale, use our kernel\n        N, C, H, W = x.shape\n        G = self.group_norm.num_groups\n        # Flatten the weight and bias of group_norm to [C]\n        weight = self.group_norm.weight.view(-1)\n        bias = self.group_norm.bias.view(-1)\n        scale = self.scale.view(-1)   # [C]\n        output = torch.empty_like(x)\n        # Grid: (N, G)\n        group_size = C // G\n        num_group_elements = group_size * H * W\n        # We must ensure num_group_elements is within the block size limit? \n        # We will set the block size to the next power of two of num_group_elements? \n        # But the kernel expects a 1D array for the group.\n\n        group_norm_scale_kernel[(N, G)](x, weight, bias, scale, output, C, G, H, W, self.group_norm.eps, BLOCK_SIZE=num_group_elements)\n\n        x = output\n        x = self.maxpool(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n\n  This might work for small groups.\n\n  Given the current parameters: \n        batch_size=128, in_channels=3, out_channels=16, height=32, width=32.\n        num_groups=8 -> group_size = 16//8=2.\n        num_group_elements = 2*32*32 = 2048.\n\n  The block size of 2048 is acceptable.\n\n  We will try this.\n\n  Note: This kernel is not optimized for performance. It does a lot of redundant work (e.g., integer division for each element). \n\n  But it should be correct.\n\n  We will include it in the EVOLVE-BLOCK.\n\n  We must also import triton and tl.\n\n  Let's write the code.\n\n  We must be cautious: the kernel might be slow, but it's a start.\n\n  We are to return only the code within the EVOLVE-BLOCK.\n\n  We will replace the entire current EVOLVE-BLOCK with our new code.\n\n  Note: The current EVOLVE-BLOCK has the class ModelNew and the data setup. We will keep the data setup.\n\n  We will also define the kernel inside the EVOLVE-BLOCK.\n\n  Let's do it.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_gn_scale_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    s_ptr,\n    y_ptr,\n    C, G, H, W,\n    eps: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    n = tl.program_id(0)\n    g = tl.program_id(1)\n    \n    group_size = C // G\n    start = g * group_size\n    offsets = tl.arange(0, BLOCK_SIZE)\n    c_idx = offsets // (H * W)\n    spatial_idx = offsets % (H * W)\n    ch_offset = start + c_idx\n    \n    mask = offsets < group_size * H * W\n    base_ptr = n * C * H * W + start * H * W\n    \n    # Load data\n    x = tl.load(x_ptr + base_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean\n    sum_x = tl.sum(x, axis=0)\n    mean = sum_x / (group_size * H * W)\n    \n    # Compute variance\n    x_centered = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / (group_size * H * W)\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Normalize and scale\n    w = tl.load(w_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    b = tl.load(b_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    scale = tl.load(s_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    \n    y = (x_centered * rstd) * w + b\n    y = y * scale\n    tl.store(y_ptr + base_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        \n        # Fused group norm and scaling with Triton\n        N, C, H, W = x.shape\n        G = self.group_norm.num_groups\n        y = torch.empty_like(x)\n        group_elements = (C // G) * H * W\n        grid = (N, G)\n        fused_gn_scale_kernel[grid](\n            x, \n            self.group_norm.weight, \n            self.group_norm.bias, \n            self.scale.view(-1), \n            y,\n            C, G, H, W,\n            self.group_norm.eps,\n            BLOCK_SIZE=triton.next_power_of_2(group_elements)\n        )\n        \n        x = self.maxpool(y)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
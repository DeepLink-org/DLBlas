{"id": "78aceb7d-a3d2-433f-a098-2eff9badf372", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_max_pool_clamp_kernel(\n    input_ptr,\n    output_ptr,\n    clamp_min,\n    clamp_max,\n    input_n_stride, input_c_stride, input_h_stride, input_w_stride,\n    output_n_stride, output_c_stride, output_h_stride, output_w_stride,\n    N, C, H, W,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_i = tl.program_id(1)\n    pid_j = tl.program_id(2)\n    \n    pid_n = pid // C\n    pid_c = pid % C\n    \n    if pid_n >= N or pid_c >= C or pid_i >= (H//2) or pid_j >= (W//2):\n        return\n    \n    input_offset = pid_n * input_n_stride + pid_c * input_c_stride\n    output_offset = pid_n * output_n_stride + pid_c * output_c_stride + pid_i * output_h_stride + pid_j * output_w_stride\n    \n    h_start = pid_i * 2\n    w_start = pid_j * 2\n    \n    # Optimized vectorized loading for 2x2 window\n    h0 = h_start\n    h1 = h_start + 1\n    w0 = w_start\n    \n    # Check row validity\n    valid_row0 = h0 < H\n    valid_row1 = h1 < H\n    \n    # Vectorized column mask\n    w_offsets = tl.arange(0, 2)\n    w_mask = w0 + w_offsets < W\n    \n    # Compute masks for both rows\n    mask0 = valid_row0 & w_mask\n    mask1 = valid_row1 & w_mask\n    \n    # Base pointers for the two rows\n    base0 = input_offset + h0 * input_h_stride\n    base1 = input_offset + h1 * input_h_stride\n    \n    # Vectorized loads for row0 and row1\n    row0 = tl.load(input_ptr + base0 + (w0 + w_offsets) * input_w_stride, mask=mask0, other=float('-inf'))\n    row1 = tl.load(input_ptr + base1 + (w0 + w_offsets) * input_w_stride, mask=mask1, other=float('-inf'))\n    \n    # Combine into window and compute max\n    window = tl.concat([row0, row1])\n    max_val = tl.max(window)\n    \n    # Apply clamping and store\n    clamped = tl.minimum(tl.maximum(max_val, clamp_min), clamp_max)\n    tl.store(output_ptr + output_offset, clamped)\n\ndef fused_max_pool_clamp(x, clamp_min, clamp_max):\n    N, C, H, W = x.shape\n    output = torch.empty((N, C, H//2, W//2), device=x.device, dtype=x.dtype)\n    grid = (N * C, H//2, W//2)\n    \n    strides = x.stride()\n    fused_max_pool_clamp_kernel[grid](\n        x,\n        output,\n        clamp_min,\n        clamp_max,\n        strides[0], strides[1], strides[2], strides[3],\n        output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n        N, C, H, W,\n        BLOCK_SIZE=1024\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "bbbaf44f-00a1-406b-95b7-856cb9f67055", "generation": 4, "timestamp": 1754586768.9001548, "iteration_found": 27, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_85_Conv2d_GroupNorm_Scale_MaxPool_Clamp", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpqwuyqz73.py\", line 98, in forward\n    x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max)\n  File \"/tmp/tmpqwuyqz73.py\", line 73, in fused_max_pool_clamp\n    fused_max_pool_clamp_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'concat'. Did you mean: 'const'?\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 0.102, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.258, "runtime_ms": 0.395, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.102, "speed_up": 0.258, "custom_timing": 0.395}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.2580\n- runtime_ms: 0.3950\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 0.258, 'custom_timing': 0.395}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: runtime_ms. Consider continuing with similar changes.\n- Metrics showing regression: speed_up. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.2610, runtime_ms: 0.3910, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 0.261, 'custom_timing': 0.391}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7560, runtime_ms: 0.1350, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 0.756, 'custom_timing': 0.135}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.2580, runtime_ms: 0.3950, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 0.258, 'custom_timing': 0.395}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_max_pool_clamp_kernel(\n    input_ptr,\n    output_ptr,\n    clamp_min,\n    clamp_max,\n    input_n_stride, input_c_stride, input_h_stride, input_w_stride,\n    output_n_stride, output_c_stride, output_h_stride, output_w_stride,\n    N, C, H, W,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_i = tl.program_id(1)\n    pid_j = tl.program_id(2)\n    \n    pid_n = pid // C\n    pid_c = pid % C\n    \n    if pid_n >= N or pid_c >= C or pid_i >= (H//2) or pid_j >= (W//2):\n        return\n    \n    input_offset = pid_n * input_n_stride + pid_c * input_c_stride\n    output_offset = pid_n * output_n_stride + pid_c * output_c_stride + pid_i * output_h_stride + pid_j * output_w_stride\n    \n    h_start = pid_i * 2\n    w_start = pid_j * 2\n    \n    # Generate coordinates for 2x2 window\n    idx = tl.arange(0, 4)\n    h_offsets = h_start + idx // 2\n    w_offsets = w_start + idx % 2\n    \n    # Check boundaries for each point\n    mask = (h_offsets < H) & (w_offsets < W)\n    input_ptrs = input_offset + h_offsets * input_h_stride + w_offsets * input_w_stride\n    \n    # Load with boundary-safe mask\n    window = tl.load(input_ptr + input_ptrs, mask=mask, other=float('-inf'))\n    max_val = tl.max(window)\n    clamped = tl.minimum(tl.maximum(max_val, clamp_min), clamp_max)\n    \n    tl.store(output_ptr + output_offset, clamped)\n\ndef fused_max_pool_clamp(x, clamp_min, clamp_max):\n    N, C, H, W = x.shape\n    output = torch.empty((N, C, H//2, W//2), device=x.device, dtype=x.dtype)\n    grid = (N * C, H//2, W//2)\n    \n    strides = x.stride()\n    fused_max_pool_clamp_kernel[grid](\n        x,\n        output,\n        clamp_min,\n        clamp_max,\n        strides[0], strides[1], strides[2], strides[3],\n        output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n        N, C, H, W,\n        BLOCK_SIZE=1024\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.2580), Performs well on runtime_ms (0.3950), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 0.258, 'custom_timing': 0.395})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef group_norm_scale_kernel(\n    x_ptr,\n    y_ptr,\n    weight_ptr,\n    bias_ptr,\n    C, G, H, W,\n    eps: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    n = tl.program_id(0)\n    g = tl.program_id(1)\n    group_size = C // G\n    start = g * group_size\n    offsets = tl.arange(0, BLOCK_SIZE)\n    c_idx = offsets // (H * W)\n    spatial_idx = offsets % (H * W)\n    ch_offset = start + c_idx\n    \n    mask = offsets < group_size * H * W\n    base_ptr = n * C * H * W + start * H * W\n    \n    # Load data\n    x_val = tl.load(x_ptr + base_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean\n    sum_x = tl.sum(x_val, axis=0)\n    mean = sum_x / (group_size * H * W)\n    \n    # Compute variance\n    x_centered = tl.where(mask, x_val - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / (group_size * H * W)\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Load parameters\n    w = tl.load(weight_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    b = tl.load(bias_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    \n    # Normalize and store\n    y = (x_centered * rstd) * w + b\n    tl.store(y_ptr + base_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.contiguous()\n        \n        # Fused groupnorm + scaling\n        combined_weight = self.group_norm.weight * self.scale.squeeze()\n        combined_bias = self.group_norm.bias * self.scale.squeeze()\n        N, C, H, W = x.shape\n        G = self.group_norm.num_groups\n        y = torch.empty_like(x)\n        \n        group_elements = (C // G) * H * W\n        BLOCK_SIZE = triton.next_power_of_2(group_elements)\n        grid = (N, G)\n        \n        group_norm_scale_kernel[grid](\n            x, y, combined_weight, combined_bias,\n            C, G, H, W,\n            self.group_norm.eps,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        x = self.maxpool(y)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7560), Performs well on runtime_ms (0.1350), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 0.756, 'custom_timing': 0.135})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_max_pool_clamp_kernel(\n    input_ptr,\n    output_ptr,\n    clamp_min,\n    clamp_max,\n    input_n_stride, input_c_stride, input_h_stride, input_w_stride,\n    output_n_stride, output_c_stride, output_h_stride, output_w_stride,\n    N, C, H, W,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_i = tl.program_id(1)\n    pid_j = tl.program_id(2)\n    \n    pid_n = pid // C\n    pid_c = pid % C\n    \n    if pid_n >= N or pid_c >= C or pid_i >= (H//2) or pid_j >= (W//2):\n        return\n    \n    input_offset = pid_n * input_n_stride + pid_c * input_c_stride\n    output_offset = pid_n * output_n_stride + pid_c * output_c_stride + pid_i * output_h_stride + pid_j * output_w_stride\n    \n    h_start = pid_i * 2\n    w_start = pid_j * 2\n    \n    # Compute exact coordinates for each element in 2x2 window\n    h_coords = h_start + tl.arange(0, 2)\n    w_coords = w_start + tl.arange(0, 2)\n    \n    # Generate mask for valid coordinates\n    h_mask = h_coords < H\n    w_mask = w_coords < W\n    mask = h_mask[:, None] & w_mask[None, :]\n    mask = mask.reshape((4,))\n    \n    # Compute pointers using 2D coordinates\n    input_ptrs = input_offset + h_coords[:, None] * input_h_stride + w_coords[None, :] * input_w_stride\n    input_ptrs = input_ptrs.reshape((4,))\n    \n    # Load with boundary-aware mask\n    window = tl.load(input_ptr + input_ptrs, mask=mask, other=-float('inf'))\n    max_val = tl.max(window)\n    clamped = tl.minimum(tl.maximum(max_val, clamp_min), clamp_max)\n    \n    tl.store(output_ptr + output_offset, clamped)\n\ndef fused_max_pool_clamp(x, clamp_min, clamp_max):\n    N, C, H, W = x.shape\n    output = torch.empty((N, C, H//2, W//2), device=x.device, dtype=x.dtype)\n    grid = (N * C, H//2, W//2)\n    \n    strides = x.stride()\n    fused_max_pool_clamp_kernel[grid](\n        x,\n        output,\n        clamp_min,\n        clamp_max,\n        strides[0], strides[1], strides[2], strides[3],\n        output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n        N, C, H, W,\n        BLOCK_SIZE=1024\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.2610), Performs well on runtime_ms (0.3910), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 0.261, 'custom_timing': 0.391})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n    ],\n    key=['group_elements'],\n)\n@triton.jit\ndef fused_gn_scale_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    s_ptr,\n    y_ptr,\n    C, G, H, W,\n    group_elements: tl.constexpr,\n    eps: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    n = tl.program_id(0)\n    g = tl.program_id(1)\n    \n    group_size = C // G\n    start = g * group_size\n    offsets = tl.arange(0, BLOCK_SIZE)\n    c_idx = offsets // (H * W)\n    spatial_idx = offsets % (H * W)\n    ch_offset = start + c_idx\n    \n    mask = offsets < group_elements\n    base_ptr = n * C * H * W + start * H * W\n    \n    # Load data\n    x = tl.load(x_ptr + base_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean using efficient reduction\n    sum_x = tl.sum(x, axis=0)\n    mean = sum_x / group_elements\n    \n    # Compute variance\n    x_centered = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / group_elements\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Normalize and scale\n    w = tl.load(w_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    b = tl.load(b_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    scale = tl.load(s_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    \n    y = (x_centered * rstd) * w + b\n    y = y * scale\n    tl.store(y_ptr + base_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        \n        # Fused group norm and scaling with Triton\n        N, C, H, W = x.shape\n        G = self.group_norm.num_groups\n        y = torch.empty_like(x)\n        group_elements = (C // G) * H * W\n        grid = (N, G)\n        fused_gn_scale_kernel[grid](\n            x, \n            self.group_norm.weight, \n            self.group_norm.bias, \n            self.scale.view(-1), \n            y,\n            C, G, H, W,\n            group_elements,\n            self.group_norm.eps,\n            BLOCK_SIZE=triton.next_power_of_2(group_elements)\n        )\n        \n        x = self.maxpool(y)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_max_pool_clamp_kernel(\n    input_ptr,\n    output_ptr,\n    clamp_min,\n    clamp_max,\n    input_n_stride, input_c_stride, input_h_stride, input_w_stride,\n    output_n_stride, output_c_stride, output_h_stride, output_w_stride,\n    N, C, H, W,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_i = tl.program_id(1)\n    pid_j = tl.program_id(2)\n    \n    pid_n = pid // C\n    pid_c = pid % C\n    \n    if pid_n >= N or pid_c >= C or pid_i >= (H//2) or pid_j >= (W//2):\n        return\n    \n    input_offset = pid_n * input_n_stride + pid_c * input_c_stride\n    output_offset = pid_n * output_n_stride + pid_c * output_c_stride + pid_i * output_h_stride + pid_j * output_w_stride\n    \n    h_start = pid_i * 2\n    w_start = pid_j * 2\n    \n    input_ptrs = input_offset + tl.arange(0, 2) * input_h_stride + tl.arange(0, 2) * input_w_stride\n    input_ptrs = tl.expand_dims(input_ptrs, axis=0) + tl.expand_dims(tl.arange(0, 2) * input_h_stride, axis=1)\n    input_ptrs = input_ptrs.reshape(-1)\n    \n    window = tl.load(input_ptr + input_ptrs, mask=input_ptrs < input_offset + H * input_h_stride + W * input_w_stride)\n    max_val = tl.max(window)\n    clamped = tl.minimum(tl.maximum(max_val, clamp_min), clamp_max)\n    \n    tl.store(output_ptr + output_offset, clamped)\n\ndef fused_max_pool_clamp(x, clamp_min, clamp_max):\n    N, C, H, W = x.shape\n    output = torch.empty((N, C, H//2, W//2), device=x.device, dtype=x.dtype)\n    grid = (N * C, H//2, W//2)\n    \n    strides = x.stride()\n    fused_max_pool_clamp_kernel[grid](\n        x,\n        output,\n        clamp_min,\n        clamp_max,\n        strides[0], strides[1], strides[2], strides[3],\n        output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n        N, C, H, W,\n        BLOCK_SIZE=1024\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n    ],\n    key=['group_elements'],\n)\n@triton.jit\ndef fused_gn_scale_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    s_ptr,\n    y_ptr,\n    C, G, H, W,\n    group_elements: tl.constexpr,\n    eps: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    n = tl.program_id(0)\n    g = tl.program_id(1)\n    \n    group_size = C // G\n    start = g * group_size\n    offsets = tl.arange(0, BLOCK_SIZE)\n    c_idx = offsets // (H * W)\n    spatial_idx = offsets % (H * W)\n    ch_offset = start + c_idx\n    \n    mask = offsets < group_elements\n    base_ptr = n * C * H * W + start * H * W\n    \n    # Load data\n    x = tl.load(x_ptr + base_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean using efficient reduction\n    sum_x = tl.sum(x, axis=0)\n    mean = sum_x / group_elements\n    \n    # Compute variance\n    x_centered = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / group_elements\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Normalize and scale\n    w = tl.load(w_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    b = tl.load(b_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    scale = tl.load(s_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    \n    y = (x_centered * rstd) * w + b\n    y = y * scale\n    tl.store(y_ptr + base_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        \n        # Fused group norm and scaling with Triton\n        N, C, H, W = x.shape\n        G = self.group_norm.num_groups\n        y = torch.empty_like(x)\n        group_elements = (C // G) * H * W\n        grid = (N, G)\n        fused_gn_scale_kernel[grid](\n            x, \n            self.group_norm.weight, \n            self.group_norm.bias, \n            self.scale.view(-1), \n            y,\n            C, G, H, W,\n            group_elements,\n            self.group_norm.eps,\n            BLOCK_SIZE=triton.next_power_of_2(group_elements)\n        )\n        \n        x = self.maxpool(y)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_max_pool_clamp_kernel(\n    input_ptr,\n    output_ptr,\n    clamp_min,\n    clamp_max,\n    input_n_stride, input_c_stride, input_h_stride, input_w_stride,\n    output_n_stride, output_c_stride, output_h_stride, output_w_stride,\n    N, C, H, W,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_n = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    pid_i = tl.program_id(2)\n    pid_j = tl.program_id(3)\n    \n    if pid_n >= N or pid_c >= C or pid_i >= (H//2) or pid_j >= (W//2):\n        return\n    \n    input_offset = pid_n * input_n_stride + pid_c * input_c_stride\n    output_offset = pid_n * output_n_stride + pid_c * output_c_stride + pid_i * output_h_stride + pid_j * output_w_stride\n    \n    h_start = pid_i * 2\n    w_start = pid_j * 2\n    \n    input_ptrs = input_offset + tl.arange(0, 2) * input_h_stride + tl.arange(0, 2) * input_w_stride\n    input_ptrs = tl.expand_dims(input_ptrs, axis=0) + tl.expand_dims(tl.arange(0, 2) * input_h_stride, axis=1)\n    input_ptrs = input_ptrs.reshape(-1)\n    \n    window = tl.load(input_ptr + input_ptrs, mask=input_ptrs < input_offset + H * input_h_stride + W * input_w_stride)\n    max_val = tl.max(window)\n    clamped = tl.minimum(tl.maximum(max_val, clamp_min), clamp_max)\n    \n    tl.store(output_ptr + output_offset, clamped)\n\ndef fused_max_pool_clamp(x, clamp_min, clamp_max):\n    N, C, H, W = x.shape\n    output = torch.empty((N, C, H//2, W//2), device=x.device, dtype=x.dtype)\n    grid = (N, C, H//2, W//2)\n    \n    strides = x.stride()\n    fused_max_pool_clamp_kernel[grid](\n        x,\n        output,\n        clamp_min,\n        clamp_max,\n        strides[0], strides[1], strides[2], strides[3],\n        output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n        N, C, H, W,\n        BLOCK_SIZE=1024\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_max_pool_clamp_kernel(\n    input_ptr,\n    output_ptr,\n    clamp_min,\n    clamp_max,\n    input_n_stride, input_c_stride, input_h_stride, input_w_stride,\n    output_n_stride, output_c_stride, output_h_stride, output_w_stride,\n    N, C, H, W,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_i = tl.program_id(1)\n    pid_j = tl.program_id(2)\n    \n    pid_n = pid // C\n    pid_c = pid % C\n    \n    if pid_n >= N or pid_c >= C or pid_i >= (H//2) or pid_j >= (W//2):\n        return\n    \n    input_offset = pid_n * input_n_stride + pid_c * input_c_stride\n    output_offset = pid_n * output_n_stride + pid_c * output_c_stride + pid_i * output_h_stride + pid_j * output_w_stride\n    \n    h_start = pid_i * 2\n    w_start = pid_j * 2\n    \n    # Compute exact coordinates for each element in 2x2 window\n    h_coords = h_start + tl.arange(0, 2)\n    w_coords = w_start + tl.arange(0, 2)\n    \n    # Generate mask for valid coordinates\n    h_mask = h_coords < H\n    w_mask = w_coords < W\n    mask = h_mask[:, None] & w_mask[None, :]\n    mask = mask.reshape((4,))\n    \n    # Compute pointers using 2D coordinates\n    input_ptrs = input_offset + h_coords[:, None] * input_h_stride + w_coords[None, :] * input_w_stride\n    input_ptrs = input_ptrs.reshape((4,))\n    \n    # Load with boundary-aware mask\n    window = tl.load(input_ptr + input_ptrs, mask=mask, other=-float('inf'))\n    max_val = tl.max(window)\n    clamped = tl.minimum(tl.maximum(max_val, clamp_min), clamp_max)\n    \n    tl.store(output_ptr + output_offset, clamped)\n\ndef fused_max_pool_clamp(x, clamp_min, clamp_max):\n    N, C, H, W = x.shape\n    output = torch.empty((N, C, H//2, W//2), device=x.device, dtype=x.dtype)\n    grid = (N * C, H//2, W//2)\n    \n    strides = x.stride()\n    fused_max_pool_clamp_kernel[grid](\n        x,\n        output,\n        clamp_min,\n        clamp_max,\n        strides[0], strides[1], strides[2], strides[3],\n        output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n        N, C, H, W,\n        BLOCK_SIZE=1024\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_max_pool_clamp_kernel(\n    input_ptr,\n    output_ptr,\n    clamp_min,\n    clamp_max,\n    input_n_stride, input_c_stride, input_h_stride, input_w_stride,\n    output_n_stride, output_c_stride, output_h_stride, output_w_stride,\n    N, C, H, W,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_i = tl.program_id(1)\n    pid_j = tl.program_id(2)\n    \n    pid_n = pid // C\n    pid_c = pid % C\n    \n    if pid_n >= N or pid_c >= C or pid_i >= (H//2) or pid_j >= (W//2):\n        return\n    \n    input_offset = pid_n * input_n_stride + pid_c * input_c_stride\n    output_offset = pid_n * output_n_stride + pid_c * output_c_stride + pid_i * output_h_stride + pid_j * output_w_stride\n    \n    h_start = pid_i * 2\n    w_start = pid_j * 2\n    \n    input_ptrs = input_offset + tl.arange(0, 2) * input_h_stride + tl.arange(0, 2) * input_w_stride\n    input_ptrs = tl.expand_dims(input_ptrs, axis=0) + tl.expand_dims(tl.arange(0, 2) * input_h_stride, axis=1)\n    input_ptrs = input_ptrs.reshape(-1)\n    \n    window = tl.load(input_ptr + input_ptrs, mask=input_ptrs < input_offset + H * input_h_stride + W * input_w_stride)\n    max_val = tl.max(window)\n    clamped = tl.minimum(tl.maximum(max_val, clamp_min), clamp_max)\n    \n    tl.store(output_ptr + output_offset, clamped)\n\ndef fused_max_pool_clamp(x, clamp_min, clamp_max):\n    N, C, H, W = x.shape\n    output = torch.empty((N, C, H//2, W//2), device=x.device, dtype=x.dtype)\n    grid = (N * C, H//2, W//2)\n    \n    strides = x.stride()\n    fused_max_pool_clamp_kernel[grid](\n        x,\n        output,\n        clamp_min,\n        clamp_max,\n        strides[0], strides[1], strides[2], strides[3],\n        output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n        N, C, H, W,\n        BLOCK_SIZE=1024\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef group_norm_scale_kernel(\n    x_ptr,\n    y_ptr,\n    weight_ptr,\n    bias_ptr,\n    C, G, H, W,\n    eps: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    n = tl.program_id(0)\n    g = tl.program_id(1)\n    group_size = C // G\n    start = g * group_size\n    offsets = tl.arange(0, BLOCK_SIZE)\n    c_idx = offsets // (H * W)\n    spatial_idx = offsets % (H * W)\n    ch_offset = start + c_idx\n    \n    mask = offsets < group_size * H * W\n    base_ptr = n * C * H * W + start * H * W\n    \n    # Load data\n    x_val = tl.load(x_ptr + base_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean\n    sum_x = tl.sum(x_val, axis=0)\n    mean = sum_x / (group_size * H * W)\n    \n    # Compute variance\n    x_centered = tl.where(mask, x_val - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / (group_size * H * W)\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Load parameters\n    w = tl.load(weight_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    b = tl.load(bias_ptr + ch_offset, mask=c_idx < group_size, other=0.0)\n    \n    # Normalize and store\n    y = (x_centered * rstd) * w + b\n    tl.store(y_ptr + base_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.contiguous()\n        \n        # Fused groupnorm + scaling\n        combined_weight = self.group_norm.weight * self.scale.squeeze()\n        combined_bias = self.group_norm.bias * self.scale.squeeze()\n        N, C, H, W = x.shape\n        G = self.group_norm.num_groups\n        y = torch.empty_like(x)\n        \n        group_elements = (C // G) * H * W\n        BLOCK_SIZE = triton.next_power_of_2(group_elements)\n        grid = (N, G)\n        \n        group_norm_scale_kernel[grid](\n            x, y, combined_weight, combined_bias,\n            C, G, H, W,\n            self.group_norm.eps,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        x = self.maxpool(y)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_max_pool_clamp_kernel(\n    input_ptr,\n    output_ptr,\n    clamp_min,\n    clamp_max,\n    input_n_stride, input_c_stride, input_h_stride, input_w_stride,\n    output_n_stride, output_c_stride, output_h_stride, output_w_stride,\n    N, C, H, W,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_i = tl.program_id(1)\n    pid_j = tl.program_id(2)\n    \n    pid_n = pid // C\n    pid_c = pid % C\n    \n    if pid_n >= N or pid_c >= C or pid_i >= (H//2) or pid_j >= (W//2):\n        return\n    \n    input_offset = pid_n * input_n_stride + pid_c * input_c_stride\n    output_offset = pid_n * output_n_stride + pid_c * output_c_stride + pid_i * output_h_stride + pid_j * output_w_stride\n    \n    h_start = pid_i * 2\n    w_start = pid_j * 2\n    \n    # Generate coordinates for 2x2 window\n    idx = tl.arange(0, 4)\n    h_offsets = h_start + idx // 2\n    w_offsets = w_start + idx % 2\n    \n    # Check boundaries for each point\n    mask = (h_offsets < H) & (w_offsets < W)\n    input_ptrs = input_offset + h_offsets * input_h_stride + w_offsets * input_w_stride\n    \n    # Load with boundary-safe mask\n    window = tl.load(input_ptr + input_ptrs, mask=mask, other=float('-inf'))\n    max_val = tl.max(window)\n    clamped = tl.minimum(tl.maximum(max_val, clamp_min), clamp_max)\n    \n    tl.store(output_ptr + output_offset, clamped)\n\ndef fused_max_pool_clamp(x, clamp_min, clamp_max):\n    N, C, H, W = x.shape\n    output = torch.empty((N, C, H//2, W//2), device=x.device, dtype=x.dtype)\n    grid = (N * C, H//2, W//2)\n    \n    strides = x.stride()\n    fused_max_pool_clamp_kernel[grid](\n        x,\n        output,\n        clamp_min,\n        clamp_max,\n        strides[0], strides[1], strides[2], strides[3],\n        output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n        N, C, H, W,\n        BLOCK_SIZE=1024\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program that implements a fused max-pool and clamp operation using Triton.\n The current performance shows a speed_up of 0.258 (which is a slowdown) and runtime_ms of 0.395 (reference is 0.102).\n We are to improve the performance (speed_up) of the Triton kernel.\n\n The current kernel:\n   - Uses a grid of (N * C, H//2, W//2)\n   - Each program loads a 2x2 window (4 elements) and computes the max, then clamps.\n\n The problem: The current implementation might be inefficient because:\n   - The grid has three dimensions, but the kernel uses a 1D thread organization for the channel and batch, and then 2D for the spatial.\n   - The loading of the 4 elements is done by generating 4 indices and then loading with a mask. This might not be coalesced.\n\n How can we improve?\n  1. We note that the grid is (N*C, H//2, W//2). This means we have one program for each output element.\n  2. The current way of loading the 2x2 window uses 4 separate loads. Since the elements in the 2x2 window are adjacent in memory (in the width dimension), we can try to load them in a vectorized way.\n\n However, note that the input tensor is in NCHW format. In the same channel, the elements in a 2x2 window are at:\n     [h_start, w_start], [h_start, w_start+1], [h_start+1, w_start], [h_start+1, w_start+1]\n The strides for H and W are given. The difference between the two adjacent elements in the same row is 1 * input_w_stride (if the tensor is contiguous, then input_w_stride=1).\n\n Since the window is small (only 4 elements), we can try to load them with a single vector load if the data is contiguous in the width dimension? But note: the elements in the window are not contiguous in memory because of the row stride? Actually, the two elements in the same row are contiguous, but the next row is offset by input_h_stride.\n\n However, we can use tl.load with a block of 4 elements? But the addresses are not consecutive. So we have to do 4 separate loads.\n\n Alternative: Use a 2D vector load? But Triton doesn't directly support 2D vectors. We can use tl.make_block_ptr and then advance in two dimensions? But the window is small.\n\n Another idea: Instead of having each program handle one output, we can have each program handle a block of outputs? But the window for each output is independent.\n\n However, note that the current kernel does not have any data reuse. Each program loads 4 elements and then writes one. This is memory bound, and the arithmetic intensity is very low.\n\n How to improve?\n   - We might try to process multiple outputs per program to increase the arithmetic intensity and reuse the loaded rows? For example, we could load a block of rows and then compute multiple output windows.\n\n But note: the current grid is already very large (N*C * H//2 * W//2). If we make each program do more, we reduce the grid size and might have better occupancy? But we have to be careful with shared memory and registers.\n\n However, given the simplicity of the operation, it might be that the main bottleneck is the memory access pattern.\n\n Let's analyze the memory access pattern:\n   - The input is accessed in a 2x2 window. The accesses for adjacent output elements in the same row (same i, j and j+1) will have windows that are adjacent in the input? For example, the window for (i, j) covers [2*i, 2*j] to [2*i+1, 2*j+1] and the window for (i, j+1) covers [2*i, 2*j+2] to [2*i+1, 2*j+3]. These two windows are separated by 2 elements in the input. So they are not contiguous.\n\n   - However, if we process a row of outputs in the same j-loop, we might be able to load the two rows of input once and then compute multiple windows? This would require processing a block of outputs in the width dimension.\n\n Proposed optimization:\n   - Change the kernel to process a row of output elements (or a block of output elements) per program.\n   - We can have the grid as (N*C, H//2) and then each program processes a row of W//2 outputs.\n   - Then we can load two rows of the input (for the two rows in the window) and then compute the max for each 2x2 window in that row.\n\n Steps for the optimized kernel:\n   - pid = [program_id(0), program_id(1)] for (batch*channel, output_row)\n   - Each program will process one row of the output (which corresponds to two rows in the input: 2*i and 2*i+1) and the entire width of the output (W//2) in the j dimension.\n\n   How to load the data?\n     - We can load two full rows of the input for the two rows that are needed? But the row might be long, and we might not be able to load the entire row at once.\n\n   Alternatively, we can load the two rows in blocks. We can have an inner loop over the output columns in blocks.\n\n   But note: the output row might be long. We can have each program process a fixed block of the row? Then the grid would be 3D: (N*C, H//2, W//2 // BLOCK_J) or something.\n\n   However, we are constrained by the grid size and the number of threads per block. We are using Triton and we can have 1D blocks? Actually, the kernel is written in Triton and we are using the grid for scheduling.\n\n   Given the complexity, and since we are only allowed to change the Triton kernel and not the grid configuration (the problem says: \"\u274c NEVER modify grid configuration, output tensor shapes, or PID logic\"), we must be cautious.\n\n   But note: the constraint says \"NEVER modify grid configuration\", meaning we cannot change the grid that is passed? However, the grid is computed in the Python function. We are allowed to change the grid as long as the output tensor is the same? Actually, the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" So we cannot change the grid.\n\n   Therefore, we must work with the existing grid: (N*C, H//2, W//2). We are stuck with one program per output element.\n\n   Then the only way to improve is to optimize the memory access for the 2x2 window.\n\n   Observation: The current way of computing the pointers for the 4 points is:\n\n        idx = tl.arange(0, 4)\n        h_offsets = h_start + idx // 2\n        w_offsets = w_start + idx % 2\n\n        input_ptrs = input_offset + h_offsets * input_h_stride + w_offsets * input_w_stride\n\n        window = tl.load(input_ptr + input_ptrs, mask=mask, other=float('-inf'))\n\n   This results in 4 separate loads. We can try to load the two rows separately and then form the 2x2 window? For example:\n\n        ptr_row0 = input_offset + h_start * input_h_stride + w_start * input_w_stride\n        ptr_row1 = input_offset + (h_start+1) * input_h_stride + w_start * input_w_stride\n\n        # Load two vectors of two elements from row0 and row1? But we need two elements per row.\n\n        row0 = tl.load(input_ptr + ptr_row0 + tl.arange(0,2) * input_w_stride, mask = (w_start + tl.arange(0,2) < W), other=float('-inf'))\n        row1 = tl.load(input_ptr + ptr_row1 + tl.arange(0,2) * input_w_stride, mask = (w_start + tl.arange(0,2) < W), other=float('-inf'))\n\n        Then we can form the window as [row0[0], row0[1], row1[0], row1[1]]\n\n   This would be two vector loads of 2 elements each, which is better than 4 scalar loads? And it might be that the two elements in a row are contiguous and can be loaded in one instruction.\n\n   Also, we must check the boundaries for the rows: we have two rows (h_start and h_start+1) and we must check that h_start < H and h_start+1 < H? Actually, the current mask is per element. We can do:\n\n        mask_row0 = (h_start < H) and (w_start + tl.arange(0,2) < W)\n        mask_row1 = (h_start+1 < H) and (w_start + tl.arange(0,2) < W)\n\n   But note: the mask for row0 and row1 might be different.\n\n   Alternatively, we can do:\n\n        row0 = tl.load(input_ptr + ptr_row0 + tl.arange(0,2) * input_w_stride, mask = (w_start + tl.arange(0,2) < W) and (h_start < H), other=float('-inf'))\n        row1 = tl.load(input_ptr + ptr_row1 + tl.arange(0,2) * input_w_stride, mask = (w_start + tl.arange(0,2) < W) and (h_start+1 < H), other=float('-inf'))\n\n   Then we can form the window as:\n\n        window = tl.concat([row0, row1])\n\n   Then compute the max.\n\n   However, note: the mask for row0 and row1 are independent. Also, if the entire row0 is out of bounds, we set to -inf? Similarly for row1.\n\n   This approach does two loads (each of 2 elements) instead of 4 loads. This might be more efficient.\n\n   Let's change the kernel accordingly.\n\n   Steps in the kernel:\n\n        h0 = h_start\n        h1 = h_start + 1\n        w0 = w_start\n        w1 = w_start + 1   # but note: we are loading two contiguous elements starting at w0.\n\n        # Check row boundaries for the two rows\n        valid_row0 = h0 < H\n        valid_row1 = h1 < H\n\n        # Pointers for the two rows\n        ptr_row0 = input_offset + h0 * input_h_stride + w0 * input_w_stride\n        ptr_row1 = input_offset + h1 * input_h_stride + w0 * input_w_stride\n\n        # For row0: we want to load two elements [w0, w0+1] if they are within the width\n        # Create a mask for the two elements in row0: if valid_row0 and within width\n        mask_row0 = valid_row0 and (w0 + tl.arange(0,2) < W)   # This is a vector of two booleans\n\n        row0 = tl.load(input_ptr + ptr_row0 + tl.arange(0,2) * input_w_stride, mask=mask_row0, other=float('-inf'))\n\n        mask_row1 = valid_row1 and (w0 + tl.arange(0,2) < W)\n        row1 = tl.load(input_ptr + ptr_row1 + tl.arange(0,2) * input_w_stride, mask=mask_row1, other=float('-inf'))\n\n        window = tl.concat([row0, row1])\n\n        max_val = tl.max(window)\n\n        ... clamp and store ...\n\n   But note: what if the window is at the very right edge? Then w0+1 might be out of bounds. We are handling that with the mask.\n\n   However, note: the current kernel uses a single mask for all four points. We are now doing two vector loads, each of two points, and we are masking each vector load appropriately.\n\n   This should be safe.\n\n   We hope that by reducing the number of load instructions, we can improve performance.\n\n   Also, note that the two vector loads are for contiguous addresses (if the tensor is contiguous in the width dimension) and hence can be coalesced.\n\n   Let's rewrite the kernel accordingly.\n\n   Important: We must keep the same grid and the same parameters.\n\n   We are only changing the way the 2x2 window is loaded.\n\n   We'll replace the code that generates the four separate pointers and loads with the two vector loads.\n\n   Note: We are not changing the signature of the kernel.\n\n   Also, we must be cautious about the boundary conditions.\n\n   We'll do:\n\n        h0 = h_start\n        h1 = h_start + 1\n        w0 = w_start\n\n        # We are going to load two elements from row0: at [h0, w0] and [h0, w0+1]\n        # and two from row1: at [h1, w0] and [h1, w0+1]\n\n        # Check if the rows are valid\n        valid_row0 = (h0 < H)\n        valid_row1 = (h1 < H)\n\n        # For the width: we'll create a vector of two booleans for the two columns\n        # Note: w0 is the start, and we are loading at w0 and w0+1\n        w_mask = tl.arange(0,2) < (W - w0)   # This is equivalent to (w0 + tl.arange(0,2)) < W\n\n        # For row0: the mask is valid_row0 and w_mask\n        mask0 = w_mask if valid_row0 else [False, False]   # Actually, we can use logical and with a scalar and a vector? In Triton, we can do: valid_row0 & w_mask\n\n        # Similarly for row1: mask1 = valid_row1 & w_mask\n\n        # Now, pointers for row0 and row1\n        base0 = input_offset + h0 * input_h_stride\n        base1 = input_offset + h1 * input_h_stride\n\n        # Load row0: two elements starting at base0 + w0 * input_w_stride\n        row0 = tl.load(input_ptr + base0 + (w0 + tl.arange(0,2)) * input_w_stride, mask=mask0, other=float('-inf'))\n        row1 = tl.load(input_ptr + base1 + (w0 + tl.arange(0,2)) * input_w_stride, mask=mask1, other=float('-inf'))\n\n        window = tl.concat([row0, row1])\n\n   But note: the above expression for the pointer: \n        base0 + (w0 + tl.arange(0,2)) * input_w_stride\n   is equivalent to:\n        base0 + w0*input_w_stride + tl.arange(0,2)*input_w_stride\n\n   We can precompute the base pointer for the row and then add the column offset.\n\n   Alternatively, we can do:\n\n        ptr0 = input_ptr + base0 + w0 * input_w_stride\n        row0 = tl.load(ptr0 + tl.arange(0,2) * input_w_stride, mask=mask0, other=float('-inf'))\n\n   But note: the pointer arithmetic must be done carefully.\n\n   We'll do:\n\n        ptr0 = input_ptr + base0 + w0 * input_w_stride\n        ptr1 = input_ptr + base1 + w0 * input_w_stride\n\n        row0 = tl.load(ptr0 + tl.arange(0,2) * input_w_stride, mask=mask0, other=float('-inf'))\n        row1 = tl.load(ptr1 + tl.arange(0,2) * input_w_stride, mask=mask1, other=float('-inf'))\n\n   Then window = tl.concat([row0, row1])\n\n   Then max_val = tl.max(window)\n\n   And then clamp and store.\n\n   Let's update the kernel accordingly.\n\n   We are replacing the following lines:\n\n        # Generate coordinates for 2x2 window\n        idx = tl.arange(0, 4)\n        h_offsets = h_start + idx // 2\n        w_offsets = w_start + idx % 2\n\n        # Check boundaries for each point\n        mask = (h_offsets < H) & (w_offsets < W)\n        input_ptrs = input_offset + h_offsets * input_h_stride + w_offsets * input_w_stride\n\n        # Load with boundary-safe mask\n        window = tl.load(input_ptr + input_ptrs, mask=mask, other=float('-inf'))\n\n   With the new code that does two vector loads.\n\n   We hope this will be faster because we are doing two vector loads (each of two elements) instead of four scalar loads.\n\n   Also, note that the two vector loads might be more efficient if the data is contiguous.\n\n   We'll make this change.\n\n   Important: We must test the boundaries carefully.\n\n   We are also changing the mask: the old mask was per element. The new mask for row0 is for two elements and similarly for row1.\n\n   This should be equivalent.\n\n   Let's write the new code.\n\n   Note: We must not change the rest of the kernel.\n\n   We'll keep the same grid and the same parameters.\n\n   We are only changing the loading part.\n\n   We'll also note: if the entire window is out of bounds, we set the loaded values to -inf. Then the max of -inf and -inf is -inf? And then we clamp: clamp_min (0.0) and clamp_max (1.0) will clamp -inf to 0.0? Actually, the clamp function: tl.maximum(max_val, clamp_min) would be max(-inf, 0.0) -> 0.0, then tl.minimum(0.0, clamp_max) -> 0.0.\n\n   This is the same as the old behavior? The old behavior: if the entire window is out of bounds, the mask is all false and we load -inf for all, so the max is -inf and then clamped to 0.0.\n\n   So the behavior is the same.\n\n   We'll make the change.\n\n   Also, note: the old code used a 1D array of 4 elements. The new window is also 4 elements (row0[0], row0[1], row1[0], row1[1]).\n\n   So the max computation is the same.\n\n   We'll rewrite the kernel accordingly.\n\n   We are allowed to change up to 20 lines.\n\n   We are replacing about 8 lines of code with about 20? But we are within the limit.\n\n   Let's write the new kernel code for the loading part.\n\n   We'll do:\n\n        # Instead of generating 4 separate points, we load two rows of two elements.\n\n        # First, compute the two row bases and the column indices.\n        h0 = h_start\n        h1 = h_start + 1\n        w0 = w_start\n\n        # Check if the rows are within the height\n        valid_row0 = (h0 < H)\n        valid_row1 = (h1 < H)\n\n        # For the columns: we want to load two elements starting at w0, if they are within the width.\n        # Create a vector of two booleans for the two columns: [w0, w0+1] < W\n        w_offsets = tl.arange(0, 2)\n        w_mask = w0 + w_offsets < W\n\n        # Mask for row0: only if the row is valid and the column is valid.\n        mask0 = valid_row0 & w_mask\n        mask1 = valid_row1 & w_mask\n\n        # Base pointers for the two rows\n        base0 = input_offset + h0 * input_h_stride\n        base1 = input_offset + h1 * input_h_stride\n\n        # Pointers for the start of the two rows at column w0\n        ptr0 = input_ptr + base0 + w0 * input_w_stride\n        ptr1 = input_ptr + base1 + w0 * input_w_stride\n\n        # Load two elements from row0 and two from row1\n        row0 = tl.load(ptr0 + w_offsets * input_w_stride, mask=mask0, other=float('-inf'))\n        row1 = tl.load(ptr1 + w_offsets * input_w_stride, mask=mask1, other=float('-inf'))\n\n        # Combine the two rows to form a window of 4 elements\n        window = tl.concat([row0, row1])\n\n        # Now compute the max over the window\n\n   Then proceed as before.\n\n   We'll replace the old code with this.\n\n   Note: We are using w_offsets = tl.arange(0,2). We must define that.\n\n   Let's do it.\n\n   We are also removing the old idx, h_offsets, w_offsets, and mask.\n\n   We'll write the new code.\n\n   We hope this improves performance.\n\n   We are also adding a few more lines, but we are within the 20-line limit.\n\n   Let's update the kernel accordingly.\n\n   IMPORTANT: We are not changing the rest of the kernel (the clamp and store).\n\n   We are only changing the way the window is loaded.\n\n   We'll output the entire kernel code block.\n\n   Also, note: the kernel currently has a parameter BLOCK_SIZE that is not used. We leave it.\n\n   We are also not changing the Python function that calls the kernel.\n\n   We are only changing the Triton kernel.\n\n   We output the entire EVOLVE-BLOCK with the updated kernel.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_max_pool_clamp_kernel(\n    input_ptr,\n    output_ptr,\n    clamp_min,\n    clamp_max,\n    input_n_stride, input_c_stride, input_h_stride, input_w_stride,\n    output_n_stride, output_c_stride, output_h_stride, output_w_stride,\n    N, C, H, W,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_i = tl.program_id(1)\n    pid_j = tl.program_id(2)\n    \n    pid_n = pid // C\n    pid_c = pid % C\n    \n    if pid_n >= N or pid_c >= C or pid_i >= (H//2) or pid_j >= (W//2):\n        return\n    \n    input_offset = pid_n * input_n_stride + pid_c * input_c_stride\n    output_offset = pid_n * output_n_stride + pid_c * output_c_stride + pid_i * output_h_stride + pid_j * output_w_stride\n    \n    h_start = pid_i * 2\n    w_start = pid_j * 2\n    \n    # Optimized vectorized loading for 2x2 window\n    h0 = h_start\n    h1 = h_start + 1\n    w0 = w_start\n    \n    # Check row validity\n    valid_row0 = h0 < H\n    valid_row1 = h1 < H\n    \n    # Vectorized column mask\n    w_offsets = tl.arange(0, 2)\n    w_mask = w0 + w_offsets < W\n    \n    # Compute masks for both rows\n    mask0 = valid_row0 & w_mask\n    mask1 = valid_row1 & w_mask\n    \n    # Base pointers for the two rows\n    base0 = input_offset + h0 * input_h_stride\n    base1 = input_offset + h1 * input_h_stride\n    \n    # Vectorized loads for row0 and row1\n    row0 = tl.load(input_ptr + base0 + (w0 + w_offsets) * input_w_stride, mask=mask0, other=float('-inf'))\n    row1 = tl.load(input_ptr + base1 + (w0 + w_offsets) * input_w_stride, mask=mask1, other=float('-inf'))\n    \n    # Combine into window and compute max\n    window = tl.concat([row0, row1])\n    max_val = tl.max(window)\n    \n    # Apply clamping and store\n    clamped = tl.minimum(tl.maximum(max_val, clamp_min), clamp_max)\n    tl.store(output_ptr + output_offset, clamped)\n\ndef fused_max_pool_clamp(x, clamp_min, clamp_max):\n    N, C, H, W = x.shape\n    output = torch.empty((N, C, H//2, W//2), device=x.device, dtype=x.dtype)\n    grid = (N * C, H//2, W//2)\n    \n    strides = x.stride()\n    fused_max_pool_clamp_kernel[grid](\n        x,\n        output,\n        clamp_min,\n        clamp_max,\n        strides[0], strides[1], strides[2], strides[3],\n        output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n        N, C, H, W,\n        BLOCK_SIZE=1024\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "7e038ed8-cc1c-4198-a3a6-03aeb1e7fd43", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_max_pool_clamp_kernel(\n    input_ptr,\n    output_ptr,\n    clamp_min,\n    clamp_max,\n    input_n_stride, input_c_stride, input_h_stride, input_w_stride,\n    output_n_stride, output_c_stride, output_h_stride, output_w_stride,\n    N, C, H, W,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_n = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    pid_i = tl.program_id(2)\n    pid_j = tl.program_id(3)\n    \n    if pid_n >= N or pid_c >= C or pid_i >= (H//2) or pid_j >= (W//2):\n        return\n    \n    input_offset = pid_n * input_n_stride + pid_c * input_c_stride\n    output_offset = pid_n * output_n_stride + pid_c * output_c_stride + pid_i * output_h_stride + pid_j * output_w_stride\n    \n    h_start = pid_i * 2\n    w_start = pid_j * 2\n    \n    input_ptrs = input_offset + tl.arange(0, 2) * input_h_stride + tl.arange(0, 2) * input_w_stride\n    input_ptrs = tl.expand_dims(input_ptrs, axis=0) + tl.expand_dims(tl.arange(0, 2) * input_h_stride, axis=1)\n    input_ptrs = input_ptrs.reshape(-1)\n    \n    window = tl.load(input_ptr + input_ptrs, mask=input_ptrs < input_offset + H * input_h_stride + W * input_w_stride)\n    max_val = tl.max(window)\n    clamped = tl.minimum(tl.maximum(max_val, clamp_min), clamp_max)\n    \n    tl.store(output_ptr + output_offset, clamped)\n\ndef fused_max_pool_clamp(x, clamp_min, clamp_max):\n    N, C, H, W = x.shape\n    output = torch.empty((N, C, H//2, W//2), device=x.device, dtype=x.dtype)\n    grid = (N, C, H//2, W//2)\n    \n    strides = x.stride()\n    fused_max_pool_clamp_kernel[grid](\n        x,\n        output,\n        clamp_min,\n        clamp_max,\n        strides[0], strides[1], strides[2], strides[3],\n        output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n        N, C, H, W,\n        BLOCK_SIZE=1024\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "7eba1fd4-0264-4fae-aa83-628d57ea6306", "generation": 1, "timestamp": 1754580933.2210667, "iteration_found": 0, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_85_Conv2d_GroupNorm_Scale_MaxPool_Clamp", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1630, in program_id\n    return _semantic.program_id(axis)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 40, in program_id\n    raise ValueError(f\"program_id axis must be 0, 1, or 2 but got {axis}\")\nValueError: program_id axis must be 0, 1, or 2 but got 3\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp5avpdk2v.py\", line 73, in forward\n    x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max)\n  File \"/tmp/tmp5avpdk2v.py\", line 48, in fused_max_pool_clamp\n    fused_max_pool_clamp_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 14:12:\n    output_ptr,\n    clamp_min,\n    clamp_max,\n    input_n_stride, input_c_stride, input_h_stride, input_w_stride,\n    output_n_stride, output_c_stride, output_h_stride, output_w_stride,\n    N, C, H, W,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_n = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    pid_i = tl.program_id(2)\n    pid_j = tl.program_id(3)\n            ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1630, in program_id\n    return _semantic.program_id(axis)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 40, in program_id\n    raise ValueError(f\"program_id axis must be 0, 1, or 2 but got {axis}\")\nValueError: program_id axis must be 0, 1, or 2 but got 3\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.102, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.1, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.102, "speed_up": 1.02, "custom_timing": 0.1}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.1000\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 1.02, 'custom_timing': 0.1}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.1000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 1.02, 'custom_timing': 0.1}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs convolution, group normalization, scaling, max pooling, and clamping.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, height, width).\n        Returns:\n            Output tensor of shape (batch_size, out_channels, height', width').\n        \"\"\"\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = self.maxpool(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.1000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.102, 'speed_up': 1.02, 'custom_timing': 0.1})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs convolution, group normalization, scaling, max pooling, and clamping.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, height, width).\n        Returns:\n            Output tensor of shape (batch_size, out_channels, height', width').\n        \"\"\"\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = self.maxpool(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Convolution\n  2. Group Normalization\n  3. Scaling (element-wise multiplication by a parameter)\n  4. Max Pooling\n  5. Clamping\n\nThe current implementation uses standard PyTorch modules (nn.Conv2d, nn.GroupNorm, etc.)\n\nHowever, note that the evaluation pipeline is set up to run on Triton and we are to generate a Triton implementation.\n\nBut wait: the current program is entirely in PyTorch and does not use Triton. The task is to rewrite the forward pass (or parts of it) using Triton kernels to improve performance.\n\nHowever, note the constraints:\n  - We cannot change the function signatures (so the forward function must have the same input and output)\n  - We cannot change the grid configuration or output shapes.\n\nBut the current code is a PyTorch module. We are allowed to replace the internal operations with Triton kernels if we can do so without changing the inputs and outputs.\n\nGiven the operations:\n  - Convolution: This is complex and may be left to PyTorch (unless we can write a faster Triton convolution). However, note that Triton does have examples for convolution but it is non-trivial.\n  - Group Normalization: We can write a Triton kernel for this.\n  - Scaling: simple element-wise, but note that the scale is a parameter of shape (out_channels, 1, 1). So it's a channel-wise scale.\n  - Max Pooling: We can write a Triton kernel for max pooling.\n  - Clamping: simple element-wise.\n\nGiven the complexity, we might start by replacing the GroupNorm with a Triton kernel because GroupNorm is known to be expensive and can be optimized.\n\nBut note: the entire model is small and running on small tensors (batch=128, channels=16, height/width=32). So the overhead of launching multiple kernels might be high.\n\nAlternatively, we can try to fuse some operations. For example, we can fuse the group normalization and the scaling into one kernel. Then we can also fuse the clamping with the max pooling? But note: max pooling and clamping are independent. However, we can do clamping after max pooling.\n\nBut note: the max pooling is done on a 2x2 window with stride 2 (by default). Then we clamp.\n\nWe have to be cautious: we cannot change the order of operations.\n\nPlan:\n  Step 1: Replace the group normalization and scaling with a custom Triton kernel.\n  Step 2: Replace the max pooling with a Triton kernel? (optional, but max pooling is already efficient in PyTorch? Maybe not worth it for small tensors)\n\nHowever, note that the problem states: \"Generate functionally equivalent but faster Triton implementations\". So we are allowed to write Triton for any part.\n\nBut note: the convolution is done first. We cannot change the convolution because it's implemented by PyTorch and we don't have a Triton kernel for it.\n\nSo we will do:\n  - Use the existing convolution (leave as is)\n  - Then for group norm + scaling: write a Triton kernel that does:\n        x = group_norm(x) * scale   [scale is per-channel]\n\n  - Then max pooling (we leave it as PyTorch for now, but if we have time we can write a Triton kernel for max pooling and clamp fusion? but note: clamp is after max pooling)\n\n  - Then clamp: we can fuse with max pooling? Actually, no because max pooling and clamp are separate. But we can do max pooling and then clamp in the same kernel? But note: the clamp is after max pooling.\n\nAlternatively, we can write a kernel that does group norm, scaling, and then max pooling? But that would be non-standard because max pooling is a reduction. It's better to separate.\n\nGiven the complexity, let's start by replacing group norm and scaling.\n\nHow GroupNorm works:\n  - Input: (N, C, H, W)\n  - Groups: the channels are divided into groups. Here, num_groups=8 and out_channels=16, so 2 channels per group? (16/8=2)\n  - For each group, we compute mean and variance over the group channels and spatial dimensions (so over (C//num_groups)*H*W elements per group per sample).\n\nWe can write a Triton kernel for group norm that also multiplies by the scale (per channel). Note: the scale is applied per channel, so after normalization we do:\n    x = (x * weight + bias)   [in standard GroupNorm, but in our code there is no bias?]\nBut in the given code, we have:\n    self.group_norm = nn.GroupNorm(num_groups, out_channels)\n    then x = x * self.scale\n\nSo the group norm has its own weight and bias? By default, nn.GroupNorm has learnable affine parameters (weight and bias) per channel. But note: in our model, we are doing:\n    x = self.group_norm(x)\n    then x = x * self.scale\n\nSo the model has:\n    group norm (with its own weight and bias) and then an extra scaling by a separate parameter.\n\nWe have to check the GroupNorm documentation: \n    The GroupNorm module does: \n        y = (x - E[x]) / sqrt(Var[x] + eps) * weight + bias\n\nThen we do: y * scale.\n\nSo we can write a kernel that does:\n    group normalization (with the weight and bias of the group norm) and then multiplies by the scale.\n\nBut note: the current model uses the default GroupNorm which has weight and bias. We are not told if we can remove them? Actually, the model is defined with:\n    self.group_norm = nn.GroupNorm(num_groups, out_channels)\n\nWe are going to replace the group_norm and the scaling with a single kernel. Therefore, we must account for:\n    - The group norm's weight (let's call it `gn_weight`) and bias (`gn_bias`)\n    - Then multiply by the scale parameter.\n\nSo the entire operation is:\n    normalized = (x - mean) * rstd * gn_weight + gn_bias\n    then: result = normalized * scale\n\nBut note: the scale is of shape (out_channels, 1, 1). So it's per channel.\n\nWe can combine the linear operations:\n    result = [ (x - mean) * (rstd * gn_weight) + gn_bias ] * scale\n            = (x - mean) * (rstd * gn_weight * scale) + (gn_bias * scale)\n\nBut wait: the scale is applied element-wise, so it multiplies both the main term and the bias term.\n\nAlternatively, we can do:\n    normalized = (x - mean) * rstd\n    then: result = normalized * (gn_weight * scale) + (gn_bias * scale)\n\nBut note: the group norm's weight and bias are learnable parameters, and the scale is also a learnable parameter. So we can precompute:\n    combined_weight = gn_weight * scale   [per channel]\n    combined_bias = gn_bias * scale        [per channel]\n\nThen the operation becomes:\n    result = normalized * combined_weight + combined_bias\n\nBut note: the normalization step (computing mean and variance) is the same.\n\nSo the kernel would:\n  1. Load the input tile (for a group and a sample) and compute the mean and variance for that group in the sample.\n  2. Normalize the input: (x - mean) * rstd\n  3. Then do the affine transform with the combined_weight and combined_bias.\n\nBut note: the group norm in PyTorch is implemented by grouping the channels. Our kernel must respect the groups.\n\nWe are going to write a kernel that processes one group per program? But note: each sample has multiple groups.\n\nWe can assign one program per group per sample. The input tensor has shape (N, C, H, W). We have G groups. Then we have N * G programs.\n\nEach program will process a block of data: (C // G) * H * W.\n\nSteps in the kernel:\n  - Let pid = program id. We can have two-dimensional grid: (N, G). So pid0 = sample index, pid1 = group index.\n  - The group index will tell us which group of channels to process.\n\nBut note: the group norm in PyTorch groups consecutive channels. So group 0: channels [0, C//G - 1], group 1: [C//G, 2*C//G-1], etc.\n\nWe have to be cautious: the number of groups must divide the number of channels.\n\nIn the provided example: out_channels=16, num_groups=8 -> 16/8=2 channels per group.\n\nWe can precompute the combined_weight and combined_bias in the forward function and pass them to the kernel.\n\nSo in the forward function, we do:\n    combined_weight = self.group_norm.weight * self.scale.squeeze()   # [out_channels]\n    combined_bias = self.group_norm.bias * self.scale.squeeze()       # [out_channels]\n\nBut note: the scale has shape (out_channels, 1, 1). We can squeeze to [out_channels].\n\nThen we call our Triton kernel for group norm and affine with the combined parameters.\n\nWe will write a Triton kernel that:\n  - Takes: input, output, combined_weight, combined_bias, num_channels, height, width, group_size (channels per group), eps (for stability)\n  - The grid is (N, num_groups)\n\nBut note: the convolution output is the input to this kernel.\n\nAfter the kernel, we do max pooling and clamp. We leave max pooling and clamp in PyTorch for now.\n\nSo the forward function becomes:\n    x = self.conv(x)\n    # Instead of group_norm and scaling, we do:\n    combined_weight = self.group_norm.weight * self.scale.squeeze()\n    combined_bias = self.group_norm.bias * self.scale.squeeze()\n    x = group_norm_scale_kernel(x, combined_weight, combined_bias, self.group_norm.num_groups, self.group_norm.eps)\n    x = self.maxpool(x)\n    x = torch.clamp(x, self.clamp_min, self.clamp_max)\n\nWe must write the group_norm_scale_kernel.\n\nLet's design the kernel:\n\n  Inputs:\n    x: [N, C, H, W]\n    weight: [C]  (the combined weight)\n    bias: [C]    (the combined bias)\n    num_groups: int\n    eps: float\n\n  Steps:\n    Let group_size = C // num_groups\n\n    We'll use a 2D grid: (N, num_groups)\n\n    For each program (n, g):\n        - The group of channels: [g * group_size, (g+1)*group_size]\n        - We need to compute the mean and variance over the channels in this group and the spatial dimensions H, W.\n\n    We can use a two-pass method? But that would be inefficient. Instead, we can use online mean and variance or use two reductions.\n\n    Alternatively, we can do:\n        Step 1: Compute the sum and sum of squares for the group.\n        Step 2: Then compute mean and variance.\n\n    We'll use a reduction in Triton.\n\n    We'll have:\n        BLOCK_SIZE = group_size * H * W   [but note: group_size is small (2) and H=32, W=32 -> 2048, which is acceptable?]\n\n    Actually, we can have a block for the entire group? But note: the group has group_size * H * W elements.\n\n    We can set the block size to the next power of two of (H*W*group_size). In this case, 32*32*2=2048, which is 2^11. We can set BLOCK_SIZE_REDUCTION = 2048? But note: the maximum block size in Triton is 1024? Actually, we can use multiple blocks per group? But that complicates.\n\n    Alternatively, we can do one reduction per group and use the entire group as the reduction axis. We can set the block to be 1024 and then have multiple blocks? But note: the group has 2048 elements. We can have 2 blocks? Then we do a two-stage reduction.\n\n    However, note that the group size might vary. We want a generic kernel.\n\n    We can do:\n\n        num_elements = group_size * H * W\n        num_blocks = triton.cdiv(num_elements, BLOCK_SIZE)\n\n        We'll use a 1D block for the reduction per group. Then we do a two-pass: first each block computes a partial sum, then we combine.\n\n    But note: the kernel must be efficient for small groups. We are constrained by the example.\n\n    Alternatively, we can use a single block per group if the group has <= 1024 elements? But 2048>1024. So we have to handle multiple blocks.\n\n    We are going to write a kernel that uses multiple blocks per group? But then we have to use shared memory to communicate between blocks? Actually, we can use atomic operations? But Triton doesn't have atomics for floats? We can use a two-stage reduction: first we compute per-block sum and then we do a second kernel to combine the blocks.\n\n    This becomes complex.\n\n    There is an example in Triton for layer norm. We can adapt the layer norm to group norm? But note: group norm is similar to layer norm but for a subset of channels.\n\n    We can use the same approach as the layer norm example: use one program per row (where a row is the group and spatial dimensions). But in our case, the group is 2 channels and 32x32, so 2048 elements.\n\n    The Triton layer norm example uses one program per row and then a reduction over the row. They use a block size of a power of two that is at least the row size? But if the row is too big, they split the row across multiple blocks? Actually, the example uses a fixed BLOCK_SIZE and then loops.\n\n    We can do:\n\n        BLOCK_SIZE = 1024   (maximum number of threads per block)\n\n        row = the entire group for a sample (so group_size * H * W elements)\n\n        We can have multiple blocks per row? Then we have to do a two-stage reduction.\n\n    Given the complexity and the fact that the group size is fixed and small (2048 in the example), we can set BLOCK_SIZE to 1024 and then have two blocks per row? Then we need to store the intermediate sums in a temporary buffer.\n\n    Alternatively, we can use the online mean/variance algorithm? But that would require multiple passes.\n\n    Considering the time, we might start with a kernel that uses a single block per group. Then we require that the group_size * H * W <= 1024? But in our example, 2*32*32=2048>1024. So we cannot.\n\n    We can break the group into two parts? Then we do two reductions? Actually, we can use a two-stage reduction within the same kernel? We can use the method from the Triton layer norm example that uses multiple blocks and then a second kernel for the final reduction? But that would be two kernel launches per group norm.\n\n    This might be too heavy.\n\n    Alternatively, we can use the existing PyTorch group norm and just fuse the scaling? But note: the scaling is a simple element-wise operation, so it might be fused by PyTorch automatically. But we are not sure.\n\n    Given the complexity and the small size, it might be that the Triton kernel for group norm is not faster than the PyTorch one. But we are to try.\n\n    Another idea: we can use the Triton layer norm kernel as a base and modify it for group norm. The layer norm example in Triton is designed for rows of a matrix. We can reshape the input to (N*G, group_size*H*W) and then do a layer norm? Then we would have one row per group per sample.\n\n    Then we can use the Triton layer norm kernel? But note: the layer norm kernel does per-row normalization. That is exactly what we want.\n\n    We can do:\n\n        x_reshaped = x.reshape(N*G, -1)   # [N*G, group_size * H * W]\n        normalized = layer_norm_triton(x_reshaped, weight, bias, eps)   # but note: the layer norm kernel expects a 2D input and does normalization per row.\n\n    But then we have to write the layer norm kernel? Actually, Triton has a layer norm example: https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n\n    We can use that kernel and then reshape back.\n\n    However, note that the layer norm kernel does:\n        y = (x - mean) * rstd * weight + bias\n\n    And we want:\n        normalized = (x - mean) * rstd\n        then: y = normalized * combined_weight + combined_bias\n\n    So we can set the weight and bias of the layer norm to be the combined parameters.\n\n    But note: the layer norm kernel in the example does not support per-element weight and bias? It supports per-row weight and bias? Actually, it supports per-feature weight and bias? In the example, the weight and bias are of shape [M] where M is the number of columns? But in our case, the combined_weight is per channel, not per element in the row.\n\n    How does that work?\n\n        Our row has group_size * H * W elements. The combined_weight is of shape [C] = [num_groups * group_size]. But for a given row (which corresponds to one group and one sample), the row uses a contiguous set of group_size channels. So the weight for that row should be a vector of length group_size? But we have a single weight per channel. So for the row, we have a vector of weight of length (group_size) for the channels in that group.\n\n    The layer norm kernel in the example does:\n\n        y = normalized * weight[None, :] + bias[None, :]\n\n    But note: the normalized tensor is of shape [M] (the row) and the weight is of shape [M]. So it does per-element scaling.\n\n    But we want per-channel scaling. In the row, the first group_size elements are the first group of channels? Actually, no: the row is of length group_size * H * W. The elements are arranged as:\n        [channel0_pixel0, channel0_pixel1, ... channel0_pixel(H*W-1), channel1_pixel0, ...]\n\n    And we want to scale each element by the weight of the channel it belongs to. So for element at position i in the row, the channel index is i // (H*W). Then the weight for that element is combined_weight[g * group_size + channel_index_in_group]?\n\n    Actually, in the row we have:\n        group_index = g\n        channels in the group: from g*group_size to (g+1)*group_size-1.\n\n        The row is arranged as: all spatial positions for channel0 in the group, then channel1, etc.\n\n    So the row has group_size blocks of H*W.\n\n    We want to scale each block by the same weight? Actually, no: each channel has its own weight. So the first block (channel0 in the group) should be multiplied by combined_weight[g*group_size+0], the next block by combined_weight[g*group_size+1], etc.\n\n    But the layer norm kernel in the example does per-element scaling. So we can pass a weight vector for the row that has:\n        weight_row = [ combined_weight[g*group_size+0] repeated H*W times, \n                       combined_weight[g*group_size+1] repeated H*W times, ... ]\n\n    Similarly for bias.\n\n    But note: the size of the row is group_size * H * W, and we have to create a weight vector of that size? That would be expensive in memory.\n\n    Alternatively, we can modify the layer norm kernel to accept per-channel weights and replicate the weight per spatial element inside the kernel? But that would be inefficient.\n\n    Given the complexity and the fact that the group is small (only 2 channels in the example), we can do:\n\n        We can have the weight and bias for the row as a vector of length (group_size * H * W) but we can compute it on the fly by expanding the per-channel weight to the entire row.\n\n    But creating a tensor of that size would be (N*G * group_size * H * W) which is the same as the input size. So we are doubling the memory.\n\n    This might not be acceptable.\n\n    Therefore, we decide to write a custom group norm kernel that does:\n\n        For each program (n, g):\n            - Load the entire group data (group_size * H * W) in chunks (using multiple blocks if necessary) and compute the mean and variance.\n            - Then normalize and apply per-channel weight and bias.\n\n    This is complex and beyond the scope of a simple example.\n\nGiven the time and complexity, and the fact that the tensor is small, we might not get a speedup. Therefore, we should reconsider.\n\nAlternative plan: fuse the group norm and the scaling with the convolution? But that would require writing a convolution and group norm together, which is very complex.\n\nWe might start by replacing only the scaling? But the scaling is a simple element-wise multiplication and is probably already fused by PyTorch.\n\nTherefore, we might skip the group norm and focus on other parts? Or we might try to fuse the group norm and the scaling into one PyTorch operation? But PyTorch doesn't have a fused group norm with extra scaling.\n\nGiven the constraints of the problem, we are allowed to write Triton for any part. We might try to write a fused kernel for group norm and scaling for the specific case of small groups and small spatial size.\n\nBut note: the example has only 2 channels per group and 32x32, so 2048 elements per group per sample. We can process each group with two blocks (each block of 1024 threads). We need to do:\n\n    Stage 1: Each block computes the sum and sum of squares for its part of the group.\n    Stage 2: A second kernel or a single kernel with two passes? We can do a two-pass kernel: first pass to compute partial sums, store in a temporary array, then second pass to combine and normalize.\n\n    We can do:\n\n        We allocate a temporary buffer of shape (N, num_groups, 2, num_blocks) for the partial sums (sum and sum of squares).\n\n    This is complex.\n\nGiven the above, and since the current performance is already at speed_up=1.02 (which is above 1) but we need at least 5% (1.05) for points, we must try.\n\nBut note: the current program is the reference (PyTorch) and the speed_up is 1.02 (so 2% faster) but that might be because of the initial run. We are to replace the reference with Triton.\n\nWe might be overcomplicating.\n\nAnother idea: replace the entire sequence from convolution to the end with a single Triton kernel? But the convolution is complex.\n\nConsidering the time, we decide to replace only the group norm and scaling with a Triton kernel that uses the layer norm approach but with the correct per-channel weight and bias expansion done in the kernel.\n\nWe will write a kernel that does:\n\n    Input: \n        x: pointer to the input tensor of shape (N, C, H, W)\n        weight: pointer to the combined weight of shape (C,)\n        bias: pointer to the combined bias of shape (C,)\n        output: pointer to the output tensor of shape (N, C, H, W)\n        stride_n, stride_c, stride_h, stride_w: strides for the input\n        N, C, H, W, G: num_groups\n        eps: float\n\n    We'll reshape the input to (N, G, group_size, H, W) and then to (N*G, group_size, H, W) and then to (N*G, group_size*H*W) for the normalization.\n\n    But note: we don't physically reshape, we just compute indices.\n\n    We'll assign one program per row (N*G rows) and per feature element in the row? But that would be too many. Actually, we can have one program per row to compute the mean and variance? Then we need another set of programs to do the normalization? Or we can do it in one kernel.\n\n    We can do:\n\n        Kernel 1: compute mean and variance per row (N*G rows)\n        Kernel 2: normalize and scale per element\n\n    But that would be two kernels.\n\n    Alternatively, we can do it in one kernel by first computing the mean and variance for the row and then normalizing the row. But we have to use two passes: one to compute the mean and variance (with reduction) and then one to normalize.\n\n    We can do this in one kernel by having the program load the data twice? Or we can store the data in shared memory? But the row is 2048 elements and shared memory is limited (48KB per SM, so 2048*4=8KB per block, which is acceptable).\n\n    Steps in one kernel per row:\n\n        pid = program id (1D: from 0 to N*G-1)\n        n = pid // G\n        g = pid % G\n\n        We then compute the start of the row: the group of channels for sample n and group g.\n\n        We allocate shared memory for the entire row? But the row has group_size * H * W elements. In our example: 2*32*32=2048. We can have a block of 2048 threads? But the maximum block size is 1024. So we need two blocks per row? Then we cannot use shared memory for the entire row because shared memory is per block.\n\n    We can do:\n\n        We break the row into two parts. We have two blocks per row. Then we have to do:\n\n            Block 0: loads the first 1024 elements, computes partial sum and partial sum of squares.\n            Block 1: loads the next 1024 elements, computes partial sum and partial sum of squares.\n\n        Then we need to combine these two blocks. We can use atomic operations? But Triton doesn't support atomic add on float32 in shared memory across blocks? We have to use global memory and a second kernel to combine.\n\n    This is getting very complex.\n\nGiven the complexity and the time, we might start with a simpler approach: leave the group norm and scaling in PyTorch and try to replace the max pooling and clamp with a Triton kernel.\n\nMax pooling is a well-known operation and can be optimized. We can write a Triton kernel for max pooling that also does the clamp? Or does the max pooling and then in the same kernel does the clamp.\n\nThe max pooling is 2x2 with stride 2. The output size is (N, C, H//2, W//2).\n\nWe can write a kernel that:\n\n    Input: (N, C, H, W)\n    Output: (N, C, H//2, W//2)\n\n    And then clamp the output to [clamp_min, clamp_max].\n\n    We can assign one program per output element.\n\n    The grid would be (N, C, H//2, W//2).\n\n    Each program:\n        n, c, i, j = indices\n        h_start = i * 2\n        w_start = j * 2\n        window = input[n, c, h_start:h_start+2, w_start:w_start+2]\n        max_val = max(window[0,0], window[0,1], window[1,0], window[1,1])\n        # then clamp\n        if max_val < clamp_min: max_val = clamp_min\n        if max_val > clamp_max: max_val = clamp_max\n        output[n, c, i, j] = max_val\n\n    This is straightforward.\n\n    We can write this kernel.\n\n    The advantages:\n        - Fuses two operations: max pooling and clamp.\n        - Avoids creating an intermediate tensor for the max pooling output.\n\n    We will replace:\n        x = self.maxpool(x)\n        x = torch.clamp(x, self.clamp_min, self.clamp_max)\n\n    with:\n        x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max, kernel_size=2, stride=2)\n\n    We must write the kernel.\n\n    Note: the kernel size and stride are fixed to 2 in the model.\n\n    We assume H and W are even.\n\n    We'll write the kernel for kernel_size=2 and stride=2.\n\n    Let's do it.\n\n    Kernel signature:\n\n        @triton.jit\n        def max_pool_clamp_2x2(\n            x_ptr,\n            output_ptr,\n            clamp_min,\n            clamp_max,\n            stride_xn, stride_xc, stride_xh, stride_xw,\n            stride_on, stride_oc, stride_oh, stride_ow,\n            N, C, H, W,\n            BLOCK_SIZE: tl.constexpr,\n        ):\n            pid_n = tl.program_id(0)\n            pid_c = tl.program_id(1)\n            pid_i = tl.program_id(2)\n            pid_j = tl.program_id(3)\n\n            # We might have more programs than needed, so check bounds:\n            if pid_n >= N or pid_c >= C or pid_i >= (H//2) or pid_j >= (W//2):\n                return\n\n            # Compute the base pointer for the input for this batch and channel\n            x_ptr += pid_n * stride_xn + pid_c * stride_xc\n            # The spatial starting point\n            h_start = pid_i * 2\n            w_start = pid_j * 2\n\n            # Load the 2x2 window\n            # We have four elements:\n            #   [h_start, w_start], [h_start, w_start+1], [h_start+1, w_start], [h_start+1, w_start+1]\n            # We can use four tl.load\n\n            off00 = h_start * stride_xh + w_start * stride_xw\n            off01 = off00 + stride_xw\n            off10 = off00 + stride_xh\n            off11 = off01 + stride_xh\n\n            val00 = tl.load(x_ptr + off00)\n            val01 = tl.load(x_ptr + off01)\n            val10 = tl.load(x_ptr + off10)\n            val11 = tl.load(x_ptr + off11)\n\n            # Compute max\n            max_val = val00\n            if val01 > max_val: max_val = val01\n            if val10 > max_val: max_val = val10\n            if val11 > max_val: max_val = val11\n\n            # Clamp\n            if max_val < clamp_min:\n                max_val = clamp_min\n            elif max_val > clamp_max:\n                max_val = clamp_max\n\n            # Compute the output pointer\n            output_ptr += pid_n * stride_on + pid_c * stride_oc + pid_i * stride_oh + pid_j * stride_ow\n            tl.store(output_ptr, max_val)\n\n    In the forward function:\n\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        # Instead of maxpool and clamp, we do:\n        #   x = self.maxpool(x)\n        #   x = torch.clamp(x, self.clamp_min, self.clamp_max)\n        # Replace with:\n        x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max)\n\n    But note: the input to maxpool is of shape (N, C, H, W). The output is (N, C, H//2, W//2).\n\n    We have to create the output tensor.\n\n    We can do:\n\n        output = torch.empty(N, C, H//2, W//2, device=x.device, dtype=x.dtype)\n\n    And then launch the kernel with grid (N, C, H//2, W//2).\n\n    We assume H and W are even.\n\n    This kernel is simple and should be fast.\n\n    Let's hope it gives speedup.\n\n    We'll implement this.\n\n    Note: we must be cautious of the strides. The input tensor might not be contiguous.\n\n    We can do:\n\n        x = x.contiguous()   # before passing to the kernel to ensure we know the strides.\n\n    But that might add overhead. Alternatively, we can use the stride information.\n\n    We'll assume the input is contiguous in the kernel for simplicity. Or we can make it contiguous in the function.\n\n    In the function fused_max_pool_clamp:\n\n        if not x.is_contiguous():\n            x = x.contiguous()\n        N, C, H, W = x.shape\n        output = torch.empty(N, C, H//2, W//2, device=x.device, dtype=x.dtype)\n        # and then use the kernel with the strides from x.stride()\n\n    We'll do that.\n\n    We'll write the function fused_max_pool_clamp.\n\n    Then in the forward function, replace the maxpool and clamp with this function.\n\n    We leave the convolution, group_norm, and scaling as is.\n\n    This is a safer first step.\n\n    If this gives speedup, we can then try to fuse more.\n\n    Given the above, we now write the code.\n\n    We must be within the EVOLVE-BLOCK.\n\n    We'll define the ModelNew class and also the helper function fused_max_pool_clamp.\n\n    But note: the EVOLVE-BLOCK must contain the entire program. So we will include the Triton kernel and the function.\n\n    We'll also adjust the forward function of ModelNew.\n\n    Let's do it.\n</think>\nTo improve performance, I'll implement a fused Triton kernel for max pooling and clamping operations. This replaces the sequential PyTorch operations with a single optimized kernel, reducing memory access overhead and leveraging GPU parallelism efficiently.\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_max_pool_clamp_kernel(\n    input_ptr,\n    output_ptr,\n    clamp_min,\n    clamp_max,\n    input_n_stride, input_c_stride, input_h_stride, input_w_stride,\n    output_n_stride, output_c_stride, output_h_stride, output_w_stride,\n    N, C, H, W,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_n = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    pid_i = tl.program_id(2)\n    pid_j = tl.program_id(3)\n    \n    if pid_n >= N or pid_c >= C or pid_i >= (H//2) or pid_j >= (W//2):\n        return\n    \n    input_offset = pid_n * input_n_stride + pid_c * input_c_stride\n    output_offset = pid_n * output_n_stride + pid_c * output_c_stride + pid_i * output_h_stride + pid_j * output_w_stride\n    \n    h_start = pid_i * 2\n    w_start = pid_j * 2\n    \n    input_ptrs = input_offset + tl.arange(0, 2) * input_h_stride + tl.arange(0, 2) * input_w_stride\n    input_ptrs = tl.expand_dims(input_ptrs, axis=0) + tl.expand_dims(tl.arange(0, 2) * input_h_stride, axis=1)\n    input_ptrs = input_ptrs.reshape(-1)\n    \n    window = tl.load(input_ptr + input_ptrs, mask=input_ptrs < input_offset + H * input_h_stride + W * input_w_stride)\n    max_val = tl.max(window)\n    clamped = tl.minimum(tl.maximum(max_val, clamp_min), clamp_max)\n    \n    tl.store(output_ptr + output_offset, clamped)\n\ndef fused_max_pool_clamp(x, clamp_min, clamp_max):\n    N, C, H, W = x.shape\n    output = torch.empty((N, C, H//2, W//2), device=x.device, dtype=x.dtype)\n    grid = (N, C, H//2, W//2)\n    \n    strides = x.stride()\n    fused_max_pool_clamp_kernel[grid](\n        x,\n        output,\n        clamp_min,\n        clamp_max,\n        strides[0], strides[1], strides[2], strides[3],\n        output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n        N, C, H, W,\n        BLOCK_SIZE=1024\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.group_norm = nn.GroupNorm(num_groups, out_channels)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.group_norm(x)\n        x = x * self.scale\n        x = fused_max_pool_clamp(x, self.clamp_min, self.clamp_max)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nnum_groups = 8\nscale_shape = (out_channels, 1, 1)\nmaxpool_kernel_size = 2\nclamp_min = 0.0\nclamp_max = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
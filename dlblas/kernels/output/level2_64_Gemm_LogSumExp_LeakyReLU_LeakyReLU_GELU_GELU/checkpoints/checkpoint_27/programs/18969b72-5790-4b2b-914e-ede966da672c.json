{"id": "18969b72-5790-4b2b-914e-ede966da672c", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    y_ptr,\n    stride_x,\n    M, N,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    row_start = pid * stride_x\n    \n    # First pass: compute row max\n    row_max = float('-inf')\n    for col_offset in range(0, N, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < N\n        ptr = x_ptr + row_start + col_idx\n        vals = tl.load(ptr, mask=mask, other=float('-inf'))\n        row_max = tl.maximum(row_max, tl.max(vals, 0))\n    \n    # Second pass: compute exp sum\n    exp_sum = 0.0\n    for col_offset in range(0, N, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < N\n        ptr = x_ptr + row_start + col_idx\n        vals = tl.load(ptr, mask=mask, other=0.0)\n        exp_vals = tl.exp(vals - row_max)\n        exp_sum += tl.sum(exp_vals, 0)\n    \n    # Compute logsumexp\n    log_sum_exp = row_max + tl.log(exp_sum)\n    \n    # Optimized activations\n    x1 = tl.maximum(0.01 * log_sum_exp, log_sum_exp)\n    x1 = tl.maximum(0.01 * x1, x1)\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n\n    tl.store(y_ptr + pid, x1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        x = self.linear(x)\n        M, N = x.shape\n        y = torch.empty(M, 1, device=x.device, dtype=x.dtype)\n        grid = (M,)\n        # Use dynamic block size based on column count\n        BLOCK_SIZE = triton.next_power_of_2(N)\n        _forward_kernel[grid](x, y, x.stride(0), M, N, BLOCK_SIZE=BLOCK_SIZE)\n        return y\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "72f511f0-759f-4049-8567-34edad6de6a7", "generation": 5, "timestamp": 1754585014.2703557, "iteration_found": 27, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 100.0, "speed_up": 1.8, "runtime_ms": 0.07, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.126, "speed_up": 1.8, "custom_timing": 0.07}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 88.9, "speed_up": 1.689, "runtime_ms": 0.0746, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.126, "speed_up": 1.689, "custom_timing": 0.0746}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 1.0000\n- score: 88.9000\n- speed_up: 1.6890\n- runtime_ms: 0.0746\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.126, 'speed_up': 1.689, 'custom_timing': 0.0746}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: runtime_ms. Consider continuing with similar changes.\n- Metrics showing regression: score, speed_up. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 94.8000, speed_up: 1.7480, runtime_ms: 0.0721, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.126, 'speed_up': 1.748, 'custom_timing': 0.0721}\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 95.5000, speed_up: 1.7550, runtime_ms: 0.0718, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.126, 'speed_up': 1.755, 'custom_timing': 0.0718}\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 101.6000, speed_up: 1.8160, runtime_ms: 0.0694, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.126, 'speed_up': 1.816, 'custom_timing': 0.0694}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 101.6000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _logsumexp_activations_kernel(\n    x_ptr,\n    output_ptr,\n    row_stride,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    row_start = row_idx * row_stride\n    \n    # Initialize max and sum\n    row_max = float('-inf')\n    exp_sum = 0.0\n    \n    # First pass: compute max\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        ptr = x_ptr + row_start + col_idx\n        vals = tl.load(ptr, mask=mask, other=float('-inf'))\n        row_max = tl.maximum(row_max, tl.max(vals, 0))\n    \n    # Second pass: compute exp sum\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        ptr = x_ptr + row_start + col_idx\n        vals = tl.load(ptr, mask=mask, other=0)\n        exp_vals = tl.exp(vals - row_max)\n        exp_sum += tl.sum(exp_vals, 0)\n    \n    # Compute logsumexp\n    log_sum_exp = tl.log(exp_sum) + row_max\n    \n    # LeakyReLU twice\n    y = tl.maximum(0.01 * log_sum_exp, log_sum_exp)\n    y = tl.maximum(0.01 * y, y)\n    \n    # GELU twice using erf approximation\n    gelu_coeff = 0.5 * (1.0 + tl.erf(y * 0.7071067811865475))\n    y = y * gelu_coeff\n    y = y * 0.5 * (1.0 + tl.erf(y * 0.7071067811865475))\n    \n    # Store final result\n    tl.store(output_ptr + row_idx, y)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n    \n    def forward(self, x):\n        # GEMM operation\n        x = self.linear(x)\n        \n        # Prepare for kernel launch\n        x = x.contiguous()\n        output = torch.empty(x.size(0), 1, device=x.device, dtype=x.dtype)\n        \n        # Kernel configuration\n        grid = (x.size(0),)\n        n_cols = x.size(1)\n        \n        # Launch kernel with optimized block size\n        _logsumexp_activations_kernel[grid](\n            x,\n            output,\n            x.stride(0),\n            n_cols,\n            BLOCK_SIZE=triton.next_power_of_2(n_cols)\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (101.6000), Performs well on speed_up (1.8160), Performs well on runtime_ms (0.0694), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.126, 'speed_up': 1.816, 'custom_timing': 0.0694})\n\n\n### Program 2 (Score: 95.5000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    y_ptr,\n    stride_x,\n    M, N,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    offs_n = tl.arange(0, BLOCK_SIZE)\n    row_start = pid * stride_x\n    x_ptrs = x_ptr + row_start + offs_n\n    mask = offs_n < N\n    x_vals = tl.load(x_ptrs, mask=mask, other=-float('inf'))\n\n    row_max = tl.max(x_vals, axis=0)\n    x_vals_minus_max = tl.where(mask, x_vals - row_max, -float('inf'))\n    exp_vals = tl.exp(x_vals_minus_max)\n    row_sum = tl.sum(exp_vals, axis=0)\n    log_sum_exp = row_max + tl.log(row_sum)\n\n    # Apply activations\n    leaky_val = 0.01 * log_sum_exp\n    x1 = tl.where(log_sum_exp > 0, log_sum_exp, leaky_val)\n    x1 = tl.where(x1 > 0, x1, 0.01 * x1)\n    # Replace tl.gelu with erf-based GELU implementation\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n\n    tl.store(y_ptr + pid, x1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        x = self.linear(x)\n        M, N = x.shape\n        y = torch.empty(M, 1, device=x.device, dtype=x.dtype)\n        BLOCK_SIZE = triton.next_power_of_2(N)\n        grid = (M,)\n        _forward_kernel[grid](x, y, x.stride(0), M, N, BLOCK_SIZE=BLOCK_SIZE)\n        return y\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (95.5000), Performs well on speed_up (1.7550), Performs well on runtime_ms (0.0718), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.126, 'speed_up': 1.755, 'custom_timing': 0.0718})\n\n\n### Program 3 (Score: 94.8000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    y_ptr,\n    stride_x,\n    M, N,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    offs_n = tl.arange(0, BLOCK_SIZE)\n    row_start = pid * stride_x\n    x_ptrs = x_ptr + row_start + offs_n\n    mask = offs_n < N\n    x_vals = tl.load(x_ptrs, mask=mask, other=-float('inf'))\n\n    row_max = tl.max(x_vals, axis=0)\n    x_vals_minus_max = tl.where(mask, x_vals - row_max, -float('inf'))\n    exp_vals = tl.exp(x_vals_minus_max)\n    row_sum = tl.sum(exp_vals, axis=0)\n    log_sum_exp = row_max + tl.log(row_sum)\n\n    # Apply activations\n    leaky_val = 0.01 * log_sum_exp\n    x1 = tl.where(log_sum_exp > 0, log_sum_exp, leaky_val)\n    x1 = tl.where(x1 > 0, x1, 0.01 * x1)\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n\n    tl.store(y_ptr + pid, x1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        x = self.linear(x)\n        M, N = x.shape\n        y = torch.empty(M, 1, device=x.device, dtype=x.dtype)\n        BLOCK_SIZE = triton.next_power_of_2(N)\n        grid = (M,)\n        _forward_kernel[grid](x, y, x.stride(0), M, N, BLOCK_SIZE=BLOCK_SIZE)\n        return y\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (94.8000), Performs well on speed_up (1.7480), Performs well on runtime_ms (0.0721), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.126, 'speed_up': 1.748, 'custom_timing': 0.0721})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 93.1000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _logsumexp_activations_kernel(\n    x_ptr,\n    output_ptr,\n    row_stride,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    row_start = row_idx * row_stride\n    \n    # Initialize for online logsumexp computation\n    row_max = float('-inf')\n    exp_sum = 0.0\n    \n    # Single-pass online logsumexp computation\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        ptr = x_ptr + row_start + col_idx\n        vals = tl.load(ptr, mask=mask, other=float('-inf'))\n        \n        # Compute local max and update global max\n        local_max = tl.max(vals, axis=0)\n        new_max = tl.maximum(row_max, local_max)\n        \n        # Compute exp values and block sum\n        exp_vals = tl.exp(vals - new_max)\n        exp_vals = tl.where(mask, exp_vals, 0.0)\n        block_sum = tl.sum(exp_vals, axis=0)\n        \n        # Scale previous sum and add new block\n        exp_sum = exp_sum * tl.exp(row_max - new_max) + block_sum\n        row_max = new_max\n    \n    # Compute final logsumexp\n    log_sum_exp = row_max + tl.log(exp_sum)\n    \n    # Optimized LeakyReLU (two steps combined)\n    y = tl.where(log_sum_exp >= 0, log_sum_exp, 0.0001 * log_sum_exp)\n    \n    # GELU twice using erf approximation\n    gelu_coeff = 0.5 * (1.0 + tl.erf(y * 0.7071067811865475))\n    y = y * gelu_coeff\n    y = y * 0.5 * (1.0 + tl.erf(y * 0.7071067811865475))\n    \n    # Store final result\n    tl.store(output_ptr + row_idx, y)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n    \n    def forward(self, x):\n        # GEMM operation\n        x = self.linear(x)\n        \n        # Prepare for kernel launch\n        x = x.contiguous()\n        output = torch.empty(x.size(0), 1, device=x.device, dtype=x.dtype)\n        \n        # Kernel configuration\n        grid = (x.size(0),)\n        n_cols = x.size(1)\n        \n        # Launch kernel with optimized block size\n        _logsumexp_activations_kernel[grid](\n            x,\n            output,\n            x.stride(0),\n            n_cols,\n            BLOCK_SIZE=triton.next_power_of_2(n_cols)\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 88.9000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    y_ptr,\n    stride_x,\n    M, N,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    # Initialize for online logsumexp\n    row_max = float('-inf')\n    exp_sum = 0.0\n    \n    # Process row in fixed-size blocks\n    for col_offset in range(0, N, BLOCK_SIZE):\n        col_idx = tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < (N - col_offset)\n        ptr = x_ptr + pid * stride_x + col_offset + col_idx\n        vals = tl.load(ptr, mask=mask, other=float('-inf'))\n        \n        # Update max and sum incrementally\n        local_max = tl.max(vals, axis=0)\n        new_max = tl.maximum(row_max, local_max)\n        exp_vals = tl.exp(vals - new_max)\n        block_sum = tl.sum(exp_vals, axis=0)\n        \n        # Scale previous sum and add new block\n        exp_sum = exp_sum * tl.exp(row_max - new_max) + block_sum\n        row_max = new_max\n    \n    # Final logsumexp calculation\n    log_sum_exp = row_max + tl.log(exp_sum)\n    \n    # Optimized activations\n    # First LeakyReLU\n    x1 = tl.maximum(0.01 * log_sum_exp, log_sum_exp)\n    # Second LeakyReLU\n    x1 = tl.maximum(0.01 * x1, x1)\n    # First GELU (erf approximation)\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n    # Second GELU (erf approximation)\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n\n    tl.store(y_ptr + pid, x1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        x = self.linear(x)\n        M, N = x.shape\n        y = torch.empty(M, 1, device=x.device, dtype=x.dtype)\n        grid = (M,)\n        # Use fixed block size for better cache utilization\n        _forward_kernel[grid](x, y, x.stride(0), M, N, BLOCK_SIZE=128)\n        return y\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 101.6000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _logsumexp_activations_kernel(\n    x_ptr,\n    output_ptr,\n    row_stride,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    row_start = row_idx * row_stride\n    \n    # Initialize max and sum\n    row_max = float('-inf')\n    exp_sum = 0.0\n    \n    # First pass: compute max\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        ptr = x_ptr + row_start + col_idx\n        vals = tl.load(ptr, mask=mask, other=float('-inf'))\n        row_max = tl.maximum(row_max, tl.max(vals, 0))\n    \n    # Second pass: compute exp sum\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        ptr = x_ptr + row_start + col_idx\n        vals = tl.load(ptr, mask=mask, other=0)\n        exp_vals = tl.exp(vals - row_max)\n        exp_sum += tl.sum(exp_vals, 0)\n    \n    # Compute logsumexp\n    log_sum_exp = tl.log(exp_sum) + row_max\n    \n    # LeakyReLU twice\n    y = tl.maximum(0.01 * log_sum_exp, log_sum_exp)\n    y = tl.maximum(0.01 * y, y)\n    \n    # GELU twice using erf approximation\n    gelu_coeff = 0.5 * (1.0 + tl.erf(y * 0.7071067811865475))\n    y = y * gelu_coeff\n    y = y * 0.5 * (1.0 + tl.erf(y * 0.7071067811865475))\n    \n    # Store final result\n    tl.store(output_ptr + row_idx, y)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n    \n    def forward(self, x):\n        # GEMM operation\n        x = self.linear(x)\n        \n        # Prepare for kernel launch\n        x = x.contiguous()\n        output = torch.empty(x.size(0), 1, device=x.device, dtype=x.dtype)\n        \n        # Kernel configuration\n        grid = (x.size(0),)\n        n_cols = x.size(1)\n        \n        # Launch kernel with optimized block size\n        _logsumexp_activations_kernel[grid](\n            x,\n            output,\n            x.stride(0),\n            n_cols,\n            BLOCK_SIZE=triton.next_power_of_2(n_cols)\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 94.8000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    y_ptr,\n    stride_x,\n    M, N,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    offs_n = tl.arange(0, BLOCK_SIZE)\n    row_start = pid * stride_x\n    x_ptrs = x_ptr + row_start + offs_n\n    mask = offs_n < N\n    x_vals = tl.load(x_ptrs, mask=mask, other=-float('inf'))\n\n    row_max = tl.max(x_vals, axis=0)\n    x_vals_minus_max = tl.where(mask, x_vals - row_max, -float('inf'))\n    exp_vals = tl.exp(x_vals_minus_max)\n    row_sum = tl.sum(exp_vals, axis=0)\n    log_sum_exp = row_max + tl.log(row_sum)\n\n    # Apply activations\n    leaky_val = 0.01 * log_sum_exp\n    x1 = tl.where(log_sum_exp > 0, log_sum_exp, leaky_val)\n    x1 = tl.where(x1 > 0, x1, 0.01 * x1)\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n\n    tl.store(y_ptr + pid, x1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        x = self.linear(x)\n        M, N = x.shape\n        y = torch.empty(M, 1, device=x.device, dtype=x.dtype)\n        BLOCK_SIZE = triton.next_power_of_2(N)\n        grid = (M,)\n        _forward_kernel[grid](x, y, x.stride(0), M, N, BLOCK_SIZE=BLOCK_SIZE)\n        return y\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 3 (Score: 93.1000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _logsumexp_activations_kernel(\n    x_ptr,\n    output_ptr,\n    row_stride,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    row_start = row_idx * row_stride\n    \n    # Initialize for online logsumexp computation\n    row_max = float('-inf')\n    exp_sum = 0.0\n    \n    # Single-pass online logsumexp computation\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        ptr = x_ptr + row_start + col_idx\n        vals = tl.load(ptr, mask=mask, other=float('-inf'))\n        \n        # Compute local max and update global max\n        local_max = tl.max(vals, axis=0)\n        new_max = tl.maximum(row_max, local_max)\n        \n        # Compute exp values and block sum\n        exp_vals = tl.exp(vals - new_max)\n        exp_vals = tl.where(mask, exp_vals, 0.0)\n        block_sum = tl.sum(exp_vals, axis=0)\n        \n        # Scale previous sum and add new block\n        exp_sum = exp_sum * tl.exp(row_max - new_max) + block_sum\n        row_max = new_max\n    \n    # Compute final logsumexp\n    log_sum_exp = row_max + tl.log(exp_sum)\n    \n    # Optimized LeakyReLU (two steps combined)\n    y = tl.where(log_sum_exp >= 0, log_sum_exp, 0.0001 * log_sum_exp)\n    \n    # GELU twice using erf approximation\n    gelu_coeff = 0.5 * (1.0 + tl.erf(y * 0.7071067811865475))\n    y = y * gelu_coeff\n    y = y * 0.5 * (1.0 + tl.erf(y * 0.7071067811865475))\n    \n    # Store final result\n    tl.store(output_ptr + row_idx, y)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n    \n    def forward(self, x):\n        # GEMM operation\n        x = self.linear(x)\n        \n        # Prepare for kernel launch\n        x = x.contiguous()\n        output = torch.empty(x.size(0), 1, device=x.device, dtype=x.dtype)\n        \n        # Kernel configuration\n        grid = (x.size(0),)\n        n_cols = x.size(1)\n        \n        # Launch kernel with optimized block size\n        _logsumexp_activations_kernel[grid](\n            x,\n            output,\n            x.stride(0),\n            n_cols,\n            BLOCK_SIZE=triton.next_power_of_2(n_cols)\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 4 (Score: 95.5000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    y_ptr,\n    stride_x,\n    M, N,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    offs_n = tl.arange(0, BLOCK_SIZE)\n    row_start = pid * stride_x\n    x_ptrs = x_ptr + row_start + offs_n\n    mask = offs_n < N\n    x_vals = tl.load(x_ptrs, mask=mask, other=-float('inf'))\n\n    row_max = tl.max(x_vals, axis=0)\n    x_vals_minus_max = tl.where(mask, x_vals - row_max, -float('inf'))\n    exp_vals = tl.exp(x_vals_minus_max)\n    row_sum = tl.sum(exp_vals, axis=0)\n    log_sum_exp = row_max + tl.log(row_sum)\n\n    # Apply activations\n    leaky_val = 0.01 * log_sum_exp\n    x1 = tl.where(log_sum_exp > 0, log_sum_exp, leaky_val)\n    x1 = tl.where(x1 > 0, x1, 0.01 * x1)\n    # Replace tl.gelu with erf-based GELU implementation\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n\n    tl.store(y_ptr + pid, x1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        x = self.linear(x)\n        M, N = x.shape\n        y = torch.empty(M, 1, device=x.device, dtype=x.dtype)\n        BLOCK_SIZE = triton.next_power_of_2(N)\n        grid = (M,)\n        _forward_kernel[grid](x, y, x.stride(0), M, N, BLOCK_SIZE=BLOCK_SIZE)\n        return y\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 5 (Score: 54.9000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1, num_stages=1),\n        triton.Config({}, num_warps=1, num_stages=2),\n        triton.Config({}, num_warps=1, num_stages=4),\n        triton.Config({}, num_warps=2, num_stages=1),\n        triton.Config({}, num_warps=2, num_stages=2),\n        triton.Config({}, num_warps=4, num_stages=1),\n        triton.Config({}, num_warps=4, num_stages=2),\n        triton.Config({}, num_warps=8, num_stages=1),\n        triton.Config({}, num_warps=8, num_stages=2),\n        triton.Config({}, num_warps=16, num_stages=1),\n        triton.Config({}, num_warps=32, num_stages=1),\n    ],\n    key=['n_cols'],\n)\n@triton.jit\ndef _logsumexp_activations_kernel(\n    x_ptr,\n    output_ptr,\n    row_stride,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    row_start = row_idx * row_stride\n    \n    row_max = float('-inf')\n    exp_sum = 0.0\n    \n    # First pass: compute max\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        ptr = x_ptr + row_start + col_idx\n        vals = tl.load(ptr, mask=mask, other=float('-inf'))\n        row_max = tl.maximum(row_max, tl.max(vals, 0))\n    \n    # Second pass: compute exp sum\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        ptr = x_ptr + row_start + col_idx\n        vals = tl.load(ptr, mask=mask, other=0)\n        exp_vals = tl.exp(vals - row_max)\n        exp_sum += tl.sum(exp_vals, 0)\n    \n    log_sum_exp = tl.log(exp_sum) + row_max\n    \n    # LeakyReLU twice\n    y = tl.maximum(0.01 * log_sum_exp, log_sum_exp)\n    y = tl.maximum(0.01 * y, y)\n    \n    # GELU twice\n    gelu_coeff = 0.5 * (1.0 + tl.erf(y * 0.7071067811865475))\n    y = y * gelu_coeff\n    y = y * 0.5 * (1.0 + tl.erf(y * 0.7071067811865475))\n    \n    tl.store(output_ptr + row_idx, y)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n    \n    def forward(self, x):\n        x = self.linear(x)\n        x = x.contiguous()\n        output = torch.empty(x.size(0), 1, device=x.device, dtype=x.dtype)\n        \n        grid = (x.size(0),)\n        n_cols = x.size(1)\n        max_block_size = 1024\n        block_size_val = min(max_block_size, triton.next_power_of_2(n_cols))\n        \n        _logsumexp_activations_kernel[grid](\n            x,\n            output,\n            x.stride(0),\n            n_cols,\n            BLOCK_SIZE=block_size_val\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    y_ptr,\n    stride_x,\n    M, N,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    # Initialize for online logsumexp\n    row_max = float('-inf')\n    exp_sum = 0.0\n    \n    # Process row in fixed-size blocks\n    for col_offset in range(0, N, BLOCK_SIZE):\n        col_idx = tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < (N - col_offset)\n        ptr = x_ptr + pid * stride_x + col_offset + col_idx\n        vals = tl.load(ptr, mask=mask, other=float('-inf'))\n        \n        # Update max and sum incrementally\n        local_max = tl.max(vals, axis=0)\n        new_max = tl.maximum(row_max, local_max)\n        exp_vals = tl.exp(vals - new_max)\n        block_sum = tl.sum(exp_vals, axis=0)\n        \n        # Scale previous sum and add new block\n        exp_sum = exp_sum * tl.exp(row_max - new_max) + block_sum\n        row_max = new_max\n    \n    # Final logsumexp calculation\n    log_sum_exp = row_max + tl.log(exp_sum)\n    \n    # Optimized activations\n    # First LeakyReLU\n    x1 = tl.maximum(0.01 * log_sum_exp, log_sum_exp)\n    # Second LeakyReLU\n    x1 = tl.maximum(0.01 * x1, x1)\n    # First GELU (erf approximation)\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n    # Second GELU (erf approximation)\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n\n    tl.store(y_ptr + pid, x1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        x = self.linear(x)\n        M, N = x.shape\n        y = torch.empty(M, 1, device=x.device, dtype=x.dtype)\n        grid = (M,)\n        # Use fixed block size for better cache utilization\n        _forward_kernel[grid](x, y, x.stride(0), M, N, BLOCK_SIZE=128)\n        return y\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program and its performance metrics. The current program has a score of 88.9 and a speed_up of 1.689. We note that the top-performing program (Program 1) achieved a score of 101.6 (speed_up=1.816). The current program uses an online logsumexp computation in a single pass but with a fixed block size of 128. The top program uses a two-pass approach (first for max, then for sum) and uses a block size that is the next power of two of the number of columns.\n\nThe current program's kernel uses:\n  - Online logsumexp (single pass, updating max and sum incrementally)\n  - Fixed block size of 128\n\nThe top program (Program 1) uses:\n  - Two passes: one for max and one for the exp sum\n  - Block size set to the next power of two of the number of columns\n\nWe also note that the current program's performance is lower than the top program. Therefore, we should consider adopting the two-pass approach and using a variable block size (next power of two) as in the top program.\n\nHowever, we must be cautious because the top program (Program 1) does not use the online method and may be more efficient for the given problem.\n\nLet's compare the current program and the top program:\n\nCurrent program (D2) kernel:\n  - Single pass: for each block, update max and sum incrementally (with scaling of the previous sum)\n  - Fixed block size (128)\n\nTop program (Program 1) kernel:\n  - First pass: read the entire row in blocks to compute the row max.\n  - Second pass: read the entire row again to compute the exp sum (using the row max from the first pass).\n\nThe two-pass method might be more efficient because:\n  - It avoids the scaling operations (exp(row_max - new_max)) which can be expensive.\n  - It allows for more straightforward memory access patterns.\n\nGiven that the top program has a higher speedup, we will change the current program to use the two-pass method and set the block size to the next power of two.\n\nWe will also note that the top program uses:\n  - Two LeakyReLUs: `y = tl.maximum(0.01 * log_sum_exp, log_sum_exp)` and then again on `y`.\n  - Two GELUs: using the erf approximation.\n\nThe current program does the same activations.\n\nPlan for the rewrite:\n  1. Change the kernel to use two passes: first for max, then for sum.\n  2. Set the block size to `triton.next_power_of_2(n_cols)` (or a maximum if needed, but the top program doesn't cap it).\n  3. We note that the top program does not cap the block size, but we should be cautious of the maximum block size (1024 is typical). However, the top program did not cap and worked. We can try without a cap first.\n\nAdditionally, the current program uses a fixed block size of 128. We will change that to the next power of two.\n\nWe also note that the top program uses a different kernel name and structure. We will adapt the current kernel to that structure.\n\nLet's rewrite the kernel in the current program to match the two-pass approach.\n\nWe will keep the class and forward method structure, but update the kernel.\n\nImportant: The current program's kernel signature is:\n    _forward_kernel(x_ptr, y_ptr, stride_x, M, N, BLOCK_SIZE: tl.constexpr)\n\nWe will change the kernel to:\n\n    @triton.jit\n    def _forward_kernel(\n        x_ptr,\n        y_ptr,\n        stride_x,\n        M, N,\n        BLOCK_SIZE: tl.constexpr\n    ):\n        pid = tl.program_id(0)\n        if pid >= M:\n            return\n\n        row_start = pid * stride_x\n\n        # First pass: find row_max\n        row_max = float('-inf')\n        for col_offset in range(0, N, BLOCK_SIZE):\n            col_idx = tl.arange(0, BLOCK_SIZE)\n            mask = col_idx < (N - col_offset)   # Note: This mask is incorrect for the general case.\n\nWait, the current mask in the loop of the online method was:\n        mask = col_idx < (N - col_offset)\n\nBut note: the loop runs for col_offset in [0, N, BLOCK_SIZE]. The number of elements left in the row is (N - col_offset). However, the block we are reading is of size BLOCK_SIZE, but we only want to read up to the remaining elements.\n\nAlternatively, the top program does:\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols   # n_cols is the total columns (N)\n\nWe should use the same mask as the top program: `mask = col_idx < N` but note that our col_idx is `col_offset + tl.arange(0, BLOCK_SIZE)`, and we are iterating by BLOCK_SIZE.\n\nActually, the current program in the online method uses:\n        col_idx = tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < (N - col_offset)\n\nThis is equivalent to: we are at offset `col_offset` and we have `N - col_offset` elements left. We want to read at most `BLOCK_SIZE` or the remaining.\n\nBut the top program does: `mask = col_idx < n_cols` but note that `col_idx` is set to `col_offset + tl.arange(0, BLOCK_SIZE)`, so it checks if the absolute index is less than N.\n\nWe should change the mask to the top program's style because it is simpler and correct.\n\nSo, let's restructure the two-pass kernel:\n\nPass 1 for max:\n    for col_offset in range(0, N, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < N\n        ptr = x_ptr + row_start + col_idx\n        vals = tl.load(ptr, mask=mask, other=float('-inf'))\n        row_max = tl.maximum(row_max, tl.max(vals, 0))\n\nPass 2 for exp sum:\n    exp_sum = 0.0\n    for col_offset in range(0, N, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < N\n        ptr = x_ptr + row_start + col_idx\n        vals = tl.load(ptr, mask=mask, other=0.0)\n        exp_vals = tl.exp(vals - row_max)\n        exp_sum += tl.sum(exp_vals, 0)\n\nThen compute log_sum_exp = row_max + tl.log(exp_sum)\n\nThen apply the activations as in the current program.\n\nWe note that the current program's activations are the same as the top program.\n\nWe will also change the block size in the forward call to be the next power of two of N (the number of columns). We remove the fixed block size.\n\nIn the forward method:\n    Instead of fixed 128, we set:\n        BLOCK_SIZE = triton.next_power_of_2(N)\n\nBut note: if N is very large, we might want to cap the block size? The top program did not cap and it worked. However, the maximum block size in Triton is typically 1024 or 2048? But note that the block size here is the size of the column block, and we are using a 1D grid over rows. The block size in this context is the number of elements processed in one iteration of the loop. It is not the CUDA thread block size.\n\nIn Triton, the `tl.arange(0, BLOCK_SIZE)` will create a vector of length BLOCK_SIZE, which must be within the hardware limits. The maximum is 1024 for some architectures? But the top program used `triton.next_power_of_2(n_cols)` without a cap and it worked. We'll do the same.\n\nLet's write the new kernel.\n\nWe are going to replace the entire kernel function and change the block size in the forward call.\n\nWe'll keep the same kernel name and function signature to minimize changes.\n\nNew kernel code:\n\n    @triton.jit\n    def _forward_kernel(\n        x_ptr,\n        y_ptr,\n        stride_x,\n        M, N,\n        BLOCK_SIZE: tl.constexpr\n    ):\n        pid = tl.program_id(0)\n        if pid >= M:\n            return\n\n        row_start = pid * stride_x\n\n        # First pass: compute max\n        row_max = float('-inf')\n        for col_offset in range(0, N, BLOCK_SIZE):\n            col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n            mask = col_idx < N\n            ptr = x_ptr + row_start + col_idx\n            vals = tl.load(ptr, mask=mask, other=float('-inf'))\n            row_max = tl.maximum(row_max, tl.max(vals, 0))\n\n        # Second pass: compute exp sum\n        exp_sum = 0.0\n        for col_offset in range(0, N, BLOCK_SIZE):\n            col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n            mask = col_idx < N\n            ptr = x_ptr + row_start + col_idx\n            vals = tl.load(ptr, mask=mask, other=0.0)\n            exp_vals = tl.exp(vals - row_max)\n            exp_sum += tl.sum(exp_vals, 0)\n\n        log_sum_exp = row_max + tl.log(exp_sum)\n\n        # Activations: two LeakyReLU and two GELU\n        x1 = tl.maximum(0.01 * log_sum_exp, log_sum_exp)\n        x1 = tl.maximum(0.01 * x1, x1)\n        x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n        x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n\n        tl.store(y_ptr + pid, x1)\n\nIn the forward method, we change the block size from 128 to `triton.next_power_of_2(N)`.\n\nWe note that the top program had an extra parameter `n_cols` (which is the same as our `N`). We are passing `N` to the kernel as the number of columns.\n\nWe are also passing `BLOCK_SIZE` as the next power of two of `N`.\n\nWe must be cautious: the top program used `BLOCK_SIZE=triton.next_power_of_2(n_cols)`. We do the same.\n\nWe'll update the forward method:\n\n    def forward(self, x):\n        x = self.linear(x)\n        M, N = x.shape\n        y = torch.empty(M, 1, device=x.device, dtype=x.dtype)\n        grid = (M,)\n        BLOCK_SIZE = triton.next_power_of_2(N)\n        _forward_kernel[grid](x, y, x.stride(0), M, N, BLOCK_SIZE=BLOCK_SIZE)\n\nBut note: the kernel uses a loop over columns with step BLOCK_SIZE. If N is not a multiple of BLOCK_SIZE, the mask handles the remainder.\n\nWe are also removing the fixed block size of 128.\n\nLet's write the entire new code block.\n\nNote: We must not change anything outside the EVOLVE-BLOCK markers.\n\nWe are only changing the kernel function and the forward method's call to the kernel (setting BLOCK_SIZE to triton.next_power_of_2(N)).\n\nWe output the entire code block as required.\n</think>\nBased on the performance analysis and top-performing examples, I'll optimize the current Triton kernel by switching to a two-pass logsumexp computation and using a dynamic block size. This approach significantly improved performance in the top-scoring programs:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    y_ptr,\n    stride_x,\n    M, N,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    row_start = pid * stride_x\n    \n    # First pass: compute row max\n    row_max = float('-inf')\n    for col_offset in range(0, N, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < N\n        ptr = x_ptr + row_start + col_idx\n        vals = tl.load(ptr, mask=mask, other=float('-inf'))\n        row_max = tl.maximum(row_max, tl.max(vals, 0))\n    \n    # Second pass: compute exp sum\n    exp_sum = 0.0\n    for col_offset in range(0, N, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < N\n        ptr = x_ptr + row_start + col_idx\n        vals = tl.load(ptr, mask=mask, other=0.0)\n        exp_vals = tl.exp(vals - row_max)\n        exp_sum += tl.sum(exp_vals, 0)\n    \n    # Compute logsumexp\n    log_sum_exp = row_max + tl.log(exp_sum)\n    \n    # Optimized activations\n    x1 = tl.maximum(0.01 * log_sum_exp, log_sum_exp)\n    x1 = tl.maximum(0.01 * x1, x1)\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n    x1 = x1 * 0.5 * (1 + tl.erf(x1 * 0.7071067811865475))\n\n    tl.store(y_ptr + pid, x1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        x = self.linear(x)\n        M, N = x.shape\n        y = torch.empty(M, 1, device=x.device, dtype=x.dtype)\n        grid = (M,)\n        # Use dynamic block size based on column count\n        BLOCK_SIZE = triton.next_power_of_2(N)\n        _forward_kernel[grid](x, y, x.stride(0), M, N, BLOCK_SIZE=BLOCK_SIZE)\n        return y\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\nKey optimizations made:\n1. Replaced the single-pass online logsumexp with a more efficient two-pass approach\n   - First pass computes the row maximum\n   - Second pass calculates the exponentiated sum\n2. Changed from fixed 128 block size to dynamic sizing using `triton.next_power_of_2(N)`\n   - Better matches data dimensions\n   - Improves memory access patterns\n3. Simplified the activation sequence while maintaining functionality\n4. Maintained the same kernel interface and program structure\n\nThis approach matches the structure of the top-performing programs (score 101.6) while preserving numerical correctness and Triton best practices. The two-pass method reduces expensive exponential scaling operations, and the dynamic block size improves cache utilization."]}}}
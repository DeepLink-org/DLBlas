{"id": "50ee55e0-72dc-4cad-9d7d-37f921c9b1b8", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\nimport torch.nn as nn\n\n@triton.jit\ndef _reduction_kernel(\n    input1_ptr,\n    input2_ptr,\n    output_ptr,\n    out_features,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    offsets = pid * out_features + tl.arange(0, BLOCK_SIZE)\n    mask = tl.arange(0, BLOCK_SIZE) < out_features\n\n    row = tl.load(input1_ptr + offsets, mask=mask)\n    sub_vec = tl.load(input2_ptr + tl.arange(0, BLOCK_SIZE), mask=mask)\n    \n    diff = row - sub_vec\n    row_sum = tl.sum(diff, axis=0)\n    mean_val = row_sum / out_features\n    \n    erf_val = libdevice.erf(mean_val * 0.7071067811865475)\n    gelu_val = 0.5 * mean_val * (1.0 + erf_val)\n    \n    tl.store(output_ptr + pid, gelu_val)\n\n@triton.jit\ndef _add_kernel(\n    input3_ptr,\n    scalars_ptr,\n    output_ptr,\n    in_features,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    \n    col_offsets = pid1 * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < in_features\n    \n    scalar = tl.load(scalars_ptr + pid0)\n    x = tl.load(input3_ptr + pid0 * in_features + col_offsets, mask=mask)\n    result = x + scalar\n    \n    tl.store(output_ptr + pid0 * in_features + col_offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n        self.subtract = nn.Parameter(torch.randn(out_features))\n\n    def forward(self, x):\n        original_x = x\n        x_linear = self.gemm(x)\n        \n        batch_size = x_linear.shape[0]\n        out_features = self.subtract.shape[0]\n        in_features = original_x.shape[1]\n        \n        scalars = torch.empty(batch_size, device=x_linear.device, dtype=torch.float32)\n        P2 = triton.next_power_of_2(out_features)\n        grid1 = (batch_size,)\n        _reduction_kernel[grid1](x_linear, self.subtract, scalars, out_features, BLOCK_SIZE=P2)\n        \n        output = torch.empty_like(original_x)\n        P3 = 1024\n        grid2 = (batch_size, (in_features + P3 - 1) // P3)\n        _add_kernel[grid2](original_x, scalars, output, in_features, BLOCK_SIZE=P3)\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "8b307326-6ea2-461d-8524-3c8bbf0490a6", "generation": 1, "timestamp": 1754573525.0295699, "iteration_found": 2, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 59.599999999999994, "speed_up": 1.396, "runtime_ms": 0.101, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.141, "speed_up": 1.396, "custom_timing": 0.101}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.138, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.141, "speed_up": 1.022, "custom_timing": 0.138}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.1380\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.141, 'speed_up': 1.022, 'custom_timing': 0.138}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.1380, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.141, 'speed_up': 1.022, 'custom_timing': 0.138}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a series of operations: Gemm, Subtract, GlobalAvgPool, LogSumExp, GELU, and ResidualAdd.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n        self.subtract = nn.Parameter(torch.randn(out_features))\n\n    def forward(self, x):\n        original_x = x.clone().detach()\n        # Gemm\n        x = self.gemm(x)\n\n        # Subtract\n        x = x - self.subtract\n\n        # GlobalAvgPool\n        x = torch.mean(x, dim=1, keepdim=True)\n\n        # LogSumExp\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n\n        # GELU\n        x = torch.nn.functional.gelu(x)\n\n        # ResidualAdd\n        x = x + original_x\n\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.1380), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.141, 'speed_up': 1.022, 'custom_timing': 0.138})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a series of operations: Gemm, Subtract, GlobalAvgPool, LogSumExp, GELU, and ResidualAdd.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n        self.subtract = nn.Parameter(torch.randn(out_features))\n\n    def forward(self, x):\n        original_x = x.clone().detach()\n        # Gemm\n        x = self.gemm(x)\n\n        # Subtract\n        x = x - self.subtract\n\n        # GlobalAvgPool\n        x = torch.mean(x, dim=1, keepdim=True)\n\n        # LogSumExp\n        x = torch.logsumexp(x, dim=1, keepdim=True)\n\n        # GELU\n        x = torch.nn.functional.gelu(x)\n\n        # ResidualAdd\n        x = x + original_x\n\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear layer (GEMM)\n  2. Subtract a learnable parameter\n  3. Global average pooling (over the sequence dimension, which is dim=1)\n  4. LogSumExp over the same dimension (but note: after pooling the tensor is [batch, 1, out_features] and we do logsumexp over dim=1 -> becomes [batch, 1, out_features]? Actually, it will remove the dim=1 so we get [batch, out_features] but then we do keepdim=True -> [batch, 1, out_features])\n  5. GELU activation\n  6. Residual connection: add the original input (which is [batch, in_features]) -> but note: at this point, the tensor is [batch, 1, out_features] and the original input is [batch, in_features]. This will fail because of shape mismatch.\n\nHowever, note the current program has a bug: the residual add is between a tensor of shape [batch, 1, out_features] and the original input of shape [batch, in_features]. This will cause a runtime error because the shapes are incompatible.\n\nBut the problem states that we are to rewrite the Triton kernel for performance. However, looking at the code, it is entirely in PyTorch and there is no Triton kernel. Moreover, the current program is the reference code? \n\nThe task says: \"Rewrite the program to improve its performance on the specified metrics.\" and we are allowed to use Triton.\n\nBut note the constraints: We are only allowed to change the Triton kernel source within the EVOLVE-BLOCK. However, the current code does not have any Triton kernel. So we are to introduce Triton kernels for the operations.\n\nHowever, the problem states that we are to generate Triton implementations for the reference PyTorch code. The current code is the reference PyTorch code. We are to replace it with a Triton version.\n\nBut note: the program structure must remain the same. We have a class `ModelNew` that is an `nn.Module` and has a forward function. We cannot change the interface.\n\nHow can we use Triton? We can replace the operations (like the linear layer, the subtract, the pooling, etc.) with custom Triton kernels? But note that the entire forward pass is a sequence of operations. It might be more efficient to fuse some of these operations.\n\nHowever, the constraints say:\n  - NEVER change @triton.jit function signatures or parameter names\n  - NEVER modify grid configuration, output tensor shapes, or PID logic\n\nBut we are starting from scratch? Actually, the example provided in the context was a Triton kernel for per_row_quantize_int8.\n\nGiven that the current code is a reference PyTorch code and we are to rewrite it in Triton for performance, we have to design a Triton kernel that does the entire forward pass? Or break it down into multiple kernels?\n\nBut note: the problem says \"Each file contains PyTorch reference (reference_code)\" and we are to generate Triton implementations (custom_code). So we are to replace the entire forward pass with a Triton kernel? Or replace individual operations?\n\nLooking at the provided example, they replaced a single function (per_row_quantize_int8) with a Triton kernel. Similarly, we might replace the entire forward pass with one Triton kernel? But the forward pass has multiple steps.\n\nAlternatively, we can replace the most expensive parts. The GEMM operation is the most expensive. However, note that the GEMM in the linear layer is already highly optimized by cuBLAS. So we might not get a speedup by writing our own GEMM. But the rest of the operations (subtract, global average pool, logsumexp, gelu, residual add) are pointwise or reductions and might be fused.\n\nHowever, the residual add is currently broken because of shape mismatch. We must fix that.\n\nLet's analyze the shapes:\n\n  Input: [batch_size, in_features] (let batch_size=128, in_features=1024)\n\n  After linear (GEMM): [batch_size, out_features] (out_features=512)\n\n  Subtract: [batch_size, out_features] - [out_features] -> [batch_size, out_features] (broadcasted)\n\n  Global average pool: we do `torch.mean(x, dim=1, keepdim=True)` -> [batch_size, 1] (because we are averaging over the out_features dimension? Actually, note: the tensor after linear is [batch, out_features]. The dimensions are [0: batch, 1: out_features]. So dim=1 is the out_features dimension. So we are averaging the out_features? Then we get [batch, 1]. Then we do logsumexp over dim=1? But that dim=1 is of size 1? Then logsumexp of a single element is just that element? Then we do gelu and then add the original input? The original input is [batch, 1024] and the current tensor is [batch, 1]. This cannot be added.\n\nSo the intended design might be:\n\n  The original input: [batch, in_features] (1024)\n  Linear: [batch, out_features] (512)\n  Subtract: [batch, out_features] - [out_features] -> [batch, out_features]\n  Global average pool: we want to average over the sequence? But there is no sequence dimension. The tensor is 2D: [batch, out_features]. Then what does it mean to do global average pool? Typically, if it were an image, we average over H and W. Here, we have only one spatial dimension? The out_features dimension? Then averaging over that dimension would produce [batch, 1]. Then we do logsumexp over that same dimension? That doesn't make sense because we just averaged it and then we have one element.\n\nAlternatively, the intended design might be to have a sequence dimension? But the input is [batch, in_features]. So it's a 2D tensor.\n\nI suspect the reference code is flawed. However, the problem states that we are to generate functionally equivalent Triton code. So we must fix the residual add to be correct? Or we must understand the intended functionality.\n\nLooking at the residual add: the original input is [batch, in_features] and the current tensor is [batch, 1] (after all operations). How can we add them? We cannot. Therefore, the reference code is broken.\n\nBut note: the correctness passed? The meta says: 'correctness_trials': '(5 / 5)'. How? Maybe the shapes are set in a way that in_features equals 1? But no, in_features=1024 and out_features=512.\n\nAlternatively, the residual add is meant to be with the original input after the linear layer? But the code does:\n  original_x = x.clone().detach()   # at the beginning, x is [batch, in_features]\n  ... then we do linear (to [batch, out_features]), subtract, then reduce to [batch,1], then logsumexp (which doesn't change the shape because dim=1 and keepdim=True) -> [batch,1], then gelu -> [batch,1]\n  then we try to add original_x [batch, in_features] to [batch,1] -> this will broadcast the [batch,1] to [batch, in_features]? Then we get [batch, in_features]? But that is not the same as the linear output.\n\nBut the output of the model is then [batch, in_features]? But the linear layer output was [batch, out_features] and then we did a reduction. The residual connection from the input to the very end would require that the output has the same shape as the input? But the input is [batch, in_features] and the output of the linear layer is [batch, out_features] and then we reduce to [batch,1]. Then we add the input? Then the output would be [batch, in_features]? But the linear layer output is lost.\n\nThis is confusing.\n\nGiven the ambiguity, we must stick to the reference code functionality. But the reference code is broken. Therefore, we must fix the residual add? But note: the problem says \"functionally equivalent\". So we must replicate the broken behavior? But the broken behavior would throw an error? Then why did the correctness pass?\n\nAlternatively, the reference code might have been run with a different set of parameters? The problem states: \"Dataset: 250 kernelbench/level1-level5 *.py files\". So we are only concerned with the code as given.\n\nBut note: the provided code in the EVOLVE-BLOCK does not run because of the shape mismatch. Therefore, we must fix the residual connection.\n\nHow to fix? The residual connection should be done with the output of the linear layer? Or we should change the reduction to not reduce the feature dimension? Or we should project the residual?\n\nLooking at the code, the residual add is done with the original input. So we must change the operations so that the output of the GELU has the same shape as the original input? But that is not the case.\n\nAlternatively, we can change the residual connection to be with the output of the linear layer? But then we are bypassing the subtract, the pooling, etc. That changes the functionality.\n\nGiven the complexity, and since the problem states that we are to rewrite for Triton, we might focus on the operations that are expensive and that we can optimize. However, the entire model is small and the operations are:\n\n  - Linear: (128,1024) * (1024,512) -> (128,512)   -> about 128*1024*512 = 67M multiplications? \n  - Subtract: 128*512 = 65K operations\n  - Global average pool: 128*512 = 65K operations? (but it's a reduction over 512 elements for 128 rows -> 128*512 reads and 128 writes? Actually, it's 128*512 reads and 128 writes? Then logsumexp: same, then gelu: 128 operations? then residual add: 128*1024? \n\nThe linear layer is the most expensive. But we cannot beat cuBLAS for GEMM? So we leave that as is.\n\nAlternatively, we can fuse the subtract and the gelu and the residual? But the residual is with the original input and the current tensor is [batch,1] and the original input is [batch,1024]. So we cannot fuse.\n\nTherefore, I suspect the reference code has a mistake in the residual connection. The residual should be done at the same shape? But the code does:\n\n        x = x + original_x\n\nand x is [batch,1] and original_x is [batch,1024]. This operation in PyTorch would try to broadcast the [batch,1] to [batch,1024] and then add. So the output would be [batch,1024]. That is valid. Then the model returns [batch,1024]. \n\nBut that means we are adding a vector of shape [batch,1] (which is the result of the entire processing) to the original input? That is unusual but valid. So we don't change that.\n\nNow, how to optimize with Triton?\n\nWe can write a Triton kernel for the entire sequence after the linear layer? Because the linear layer is done by cuBLAS and we cannot change that. Then we do:\n\n  subtract, global average pool (which is a reduction over the out_features dimension), logsumexp (which is also a reduction? but note: after the average pool we have a tensor of size [batch,1] and then logsumexp over dim=1 doesn't change the shape? because it's a reduction over a dimension of size 1? so it's a no-op). Then gelu and then we have to add the original input.\n\nBut wait, the logsumexp is done on a tensor of shape [batch,1] over dim=1. The result is the same tensor? So we can skip it? \n\nBut let's see the reference code:\n\n        x = torch.mean(x, dim=1, keepdim=True)   # [batch, 1]\n        x = torch.logsumexp(x, dim=1, keepdim=True)   # [batch, 1] -> same as x? because for each row we have one element: log(exp(x)) = x.\n\nSo the logsumexp is an identity in this case? So we can remove it? But the problem says we must be functionally equivalent. So if the reference code does it, we must do it.\n\nTherefore, the operations after the linear layer are:\n\n  subtract: [batch, out_features] - [out_features] -> [batch, out_features]\n  mean: reduce over the out_features dimension -> [batch, 1]\n  logsumexp: over the same dimension? but now the dimension is 1 -> identity -> [batch,1]\n  gelu: [batch,1] -> [batch,1]\n  add the original input [batch, in_features] -> [batch, in_features] (by broadcasting)\n\nWe can fuse the subtract, the mean reduction, and the gelu? And then we do the logsumexp (which is identity) and then the add.\n\nBut the add is with the original input which is not passed through the kernel? We have to pass it as an argument.\n\nAlternatively, we can write a kernel that does:\n\n  Inputs: \n    - output of the linear layer: tensor A of shape [batch, out_features]\n    - the subtract parameter: vector B of shape [out_features]\n    - the original input: tensor C of shape [batch, in_features]\n\n  Output: \n    - tensor D of shape [batch, in_features] = gelu(mean(A - B, dim=1)) + C   [but note: the gelu is applied to a scalar per batch, and then broadcasted to the entire row of C]\n\nBut note: the mean reduction is over the out_features dimension. We can compute the mean by:\n\n    for each row i in batch:\n        s = 0\n        for j in range(out_features):\n            s += A[i,j] - B[j]\n        mean_val = s / out_features\n\n    then apply gelu(mean_val) -> then for each row i, we set D[i] = C[i] + gelu(mean_val)\n\nSo we can do:\n\n    We'll launch a grid of batch_size blocks? Each block handles one batch.\n\n    Each block: \n        loads a row of A (of length out_features) and the entire B (if we cache it in shared memory? but out_features=512, which is small) \n        then computes the mean of (A[i,:] - B)\n        then applies gelu\n        then loads the entire row of C (in_features=1024) and adds the gelu(mean_val) to every element.\n\nBut note: the Triton kernel must be efficient. We can do:\n\n    Program ID: pid = batch index.\n\n    We'll have:\n        BLOCK_SIZE = out_features   (or we can tile? but out_features is 512, which is acceptable for one block)\n\n    Steps for one batch:\n\n        Step 1: Load the row of A and the vector B. We can load B into shared memory? But note: each batch uses the same B. We can have B in global memory and each thread loads a part.\n\n        Step 2: Each thread in the block computes a partial sum of (A[i, j] - B[j]) for a set of j.\n\n        Step 3: Reduce within the block to get the sum for the row.\n\n        Step 4: Compute mean = sum / out_features.\n\n        Step 5: Compute gelu(mean) -> we can use an approximation.\n\n        Step 6: Then we load the row of C and add gelu(mean) to every element.\n\n    But note: the output is of shape [batch, in_features]. The kernel would write the entire row of C plus the scalar gelu(mean) for that row.\n\n    We can do step 6 in the same kernel? But note: the in_features (1024) might be larger than the block size? We can set the block size to 1024? Or we can have multiple blocks per batch? But we have batch_size=128 and we are using 128 blocks? Then each block handles one batch and the entire row of in_features? But the row of in_features is 1024. We can set the block size to 1024? Then we would have 1024 threads per block? That is acceptable.\n\n    Alternatively, we can have one block per batch and then within the block, we split the in_features dimension? But we don't need to: the in_features dimension is independent.\n\n    However, note that the subtract and reduction over out_features is independent of in_features. So we can do:\n\n        Block = 1D, with block size = 1024? But then we have 1024 threads per block and 128 blocks.\n\n        But the reduction over out_features (512) requires that we have enough threads? We can use 512 threads for the reduction? But we are going to do a reduction of 512 elements with 1024 threads? Then we can assign each thread to one element? But 512 elements, so we can use 512 threads? But we have 1024 threads? Then we can do:\n\n          We'll use the first 512 threads to load the row of A and the vector B? Then do the subtraction and reduction? Then the rest of the threads (512 to 1024) would be idle during the reduction? Then we use the entire block to do the reduction? Actually, we can use the entire block to do the reduction? We can have 1024 threads and 512 elements? Then each thread loads one element? But we have two elements per thread? Or we can have 512 threads and each thread loads one element? Then we do a reduction.\n\n        Alternatively, we can do the reduction with 512 threads? Then we have 512 threads per block? But then we have 128 blocks? Then the total number of threads would be 128*512 = 65536? That is acceptable.\n\n        But then we have to write the output for the entire row of C? Which is 1024 elements? We would need 1024 threads per block? But we only have 512? Then we would have to do two loads per thread? Or we can have a different kernel? Or we can do two phases: first reduction, then broadcast and write.\n\n    Alternatively, we can split the kernel into two:\n\n        Kernel1: for the reduction (over out_features) and compute the scalar gelu(mean) for each batch. Output a vector of scalars of length batch.\n\n        Kernel2: for the add: for each batch, we take the scalar and add it to every element of the original input? But that is a simple broadcast and we can do it in a separate kernel? And it would be fast.\n\n    But the problem is to write one Triton kernel? Or we can write multiple? The constraints do not say we can only write one kernel.\n\n    However, the problem says: \"Generate functionally equivalent but faster Triton implementations (custom_code)\". We can write multiple kernels.\n\n    But note: the entire model is an nn.Module. We can replace the forward pass with:\n\n        x_linear = self.gemm(x)   # done by cuBLAS\n        # then we do our custom operations in Triton: subtract, mean, gelu, and then add to the original input\n        x = custom_triton_ops(x_linear, self.subtract, x_original=x)\n\n    So we can write one Triton kernel that does the subtract, mean, gelu, and then adds the result to the original input? But note: the original input is the input to the module, which is x at the beginning. We saved it as original_x.\n\n    We can design:\n\n        custom_triton_ops(input1, input2, input3) = \n            step1: for each row i: \n                      s = mean( input1[i,:] - input2 )\n            step2: s = gelu(s)\n            step3: output[i,:] = input3[i,:] + s   [broadcasting s to the entire row]\n\n    We can do step1 and step2 in one kernel and step3 in another? Or we can do it in one kernel? \n\n    One kernel:\n\n        We have three pointers: \n          input1: [batch, out_features]  (the linear output)\n          input2: [out_features]         (the subtract vector)\n          input3: [batch, in_features]   (the original input)\n\n        We want to output: [batch, in_features] = input3 + gelu(mean(input1 - input2))\n\n        We can do:\n\n          We launch a grid of (batch, in_features) but that would be inefficient for the reduction? Because each output element would require reading an entire row of input1? That is 512 elements per output element? Then the total reads would be batch * in_features * out_features = 128 * 1024 * 512 = 67M reads? That is inefficient.\n\n        Alternatively, we can do:\n\n          First, we compute the scalar per batch in a separate reduction kernel? Then we have a vector of scalars of length batch. Then we launch a kernel that adds the scalar to each row of input3? That kernel would be very efficient: we launch batch * in_features threads? That is 128 * 1024 = 131072 threads? And each thread does one addition? That is efficient.\n\n        So we break into two kernels:\n\n          Kernel1: reduction kernel (over out_features) to compute the scalar for each batch.\n          Kernel2: addition kernel (over in_features) to add the scalar to the entire row.\n\n        But note: the reduction kernel will be:\n\n          Input: input1 [batch, out_features], input2 [out_features]\n          Output: a vector of scalars [batch] = gelu( mean( input1[i,:] - input2 ) )\n\n        Then Kernel2: \n          Input: input3 [batch, in_features], the vector of scalars [batch]\n          Output: output [batch, in_features] = input3 + scalar[batch_index]   (broadcasted)\n\n        Then we return output.\n\n        How to write Kernel1?\n\n          We can use a reduction similar to the example per_row_quantize_int8, but for mean.\n\n        We can write:\n\n          @triton.jit\n          def _reduction_kernel(\n              input1_ptr,   # [batch, out_features]\n              input2_ptr,   # [out_features]\n              output_ptr,   # [batch]  (float32)\n              out_features, \n              BLOCK_SIZE: tl.constexpr,\n          ):\n              pid = tl.program_id(axis=0)   # batch index\n              # We'll use a block of threads for one row\n              # Each thread loads a segment of the row and the corresponding segment of input2\n              offsets = pid * out_features + tl.arange(0, BLOCK_SIZE)\n              mask = tl.arange(0, BLOCK_SIZE) < out_features\n\n              row = tl.load(input1_ptr + offsets, mask=mask)\n              # Load the entire input2? We can do the same offset and mask? because input2 is of length out_features\n              sub_vec = tl.load(input2_ptr + tl.arange(0, BLOCK_SIZE), mask=mask)\n\n              # subtract\n              diff = row - sub_vec\n              # reduce sum over the row\n              row_sum = tl.sum(diff, axis=0)\n              mean_val = row_sum / out_features\n              # GELU: we can use an approximation? or use tl.gelu? \n              # Check: Triton has gelu? \n              # According to triton.language, there is no built-in gelu. We have to approximate.\n              # We can use: gelu(x) = x * 0.5 * (1 + erf(x / sqrt(2)))\n              # But erf might be expensive. Alternatively, we can use a fast approximation: \n              #   gelu(x) = 0.5 * x * (1 + tl.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * x*x*x)))\n              # But note: we are in a reduction kernel and we are doing one gelu per batch? So it's cheap.\n\n              # Let's use the approximation:\n              #   x * 0.5 * (1.0 + tl.tanh(0.7978845608028654 * (x + 0.044715 * x * x * x)))\n              # But note: Triton has libdevice? We can use erf from libdevice? \n              #   erf = libdevice.erf(x * 0.7071067811865475)   # because sqrt(2) is about 1.414, and 1/sqrt(2) is 0.707\n              #   gelu = 0.5 * x * (1 + erf)\n              # We'll use the libdevice.erf version for accuracy.\n\n              # However, the problem says the reference uses torch.nn.functional.gelu, which is the exact gelu? \n              #   But in practice, PyTorch uses an approximation? Actually, the exact gelu is expensive.\n\n              # Since we are in Triton, we can use the approximation that is in the FlashAttention code? \n              #   They use: \n              #       tanh_approx = False\n              #       if tanh_approx:\n              #           return 0.5 * x * (1.0 + tl.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x * x * x)))\n              #       else:\n              #           return x * 0.5 * (1.0 + tl.erf(x / math.sqrt(2)))\n\n              # We'll use the erf version for accuracy.\n\n              # But note: Triton has math module? We can do:\n              #   sqrt2 = 1.41421356237\n              #   erf_arg = mean_val / sqrt2\n              #   erf_val = tl.erf(erf_arg)   # using libdevice.erf?\n              #   gelu_val = 0.5 * mean_val * (1 + erf_val)\n\n              # However, the Triton language extra is imported as `libdevice`? In the example, they used `libdevice.llrint`. So we can do:\n\n              erf_val = libdevice.erf(mean_val * 0.7071067811865475)\n              gelu_val = 0.5 * mean_val * (1.0 + erf_val)\n\n              tl.store(output_ptr + pid, gelu_val)\n\n          But note: we are not using the BLOCK_SIZE correctly. We are loading BLOCK_SIZE elements per row? But the row has out_features elements. We might have to loop if out_features is larger than BLOCK_SIZE? But out_features=512. We can set BLOCK_SIZE to 512? But the maximum block size in Triton is 1024? So 512 is acceptable.\n\n          However, we might want to make it general? We can set BLOCK_SIZE to the next power of two of out_features? But the problem says we are on a fixed hardware.\n\n        Then Kernel2: \n\n          @triton.jit\n          def _add_kernel(\n              input3_ptr,   # [batch, in_features]\n              scalars_ptr,  # [batch]\n              output_ptr,   # [batch, in_features]\n              in_features,\n              BLOCK_SIZE: tl.constexpr,\n          ):\n              pid0 = tl.program_id(axis=0)   # batch index\n              pid1 = tl.program_id(axis=1)   # column index in the row? But we are in 1D grid? \n              # Alternatively, we can use a 2D grid? \n              #   pid0 for batch, pid1 for the column block? \n              #   But we can do a 1D grid with size batch * (ceil(in_features / BLOCK_SIZE))? \n\n              # Let's do 1D grid by batch and then within the batch, we split the in_features dimension by blocks.\n\n              pid = tl.program_id(axis=0)\n              batch_idx = pid // ( (in_features + BLOCK_SIZE - 1) // BLOCK_SIZE )\n              col_idx = pid % ( (in_features + BLOCK_SIZE - 1) // BLOCK_SIZE ) \n              # Alternatively, we can use:\n              #   batch_idx = pid // num_blocks_per_batch\n              #   block_start = col_idx * BLOCK_SIZE\n\n              # But we can also use a 2D grid? Triton supports 2D grid? \n              #   grid = (batch, (in_features + BLOCK_SIZE -1)//BLOCK_SIZE)\n\n              # We'll design for 2D grid: (batch, num_blocks_per_batch)\n\n              # However, the problem says: NEVER modify grid configuration. But we are writing the kernel from scratch? And the grid is defined by the caller.\n\n              # In the function that calls the kernel, we can define the grid as (batch, num_blocks_per_batch) for the add kernel.\n\n              # But the problem says we cannot change the grid configuration? Actually, the grid configuration is defined in the host code. We are allowed to change the host code? \n\n              # The constraints say: \"NEVER modify grid configuration\" meaning for the same kernel? But we are writing a new kernel.\n\n              # Actually, the entire code is within the EVOLVE-BLOCK. We are replacing the forward function. We are allowed to change the host code? Yes, because we are replacing the entire ModelNew.\n\n          Alternatively, we can do a 1D grid with batch * ceil(in_features / BLOCK_SIZE) and then:\n\n              batch_idx = pid // ( (in_features + BLOCK_SIZE - 1) // BLOCK_SIZE )   -> but that is integer division? Actually, we want:\n              num_blocks = (in_features + BLOCK_SIZE - 1) // BLOCK_SIZE\n              batch_idx = pid // num_blocks\n              block_start = (pid % num_blocks) * BLOCK_SIZE\n\n          Then we load a block of input3 and add the scalar for batch_idx.\n\n          Steps:\n\n              scalar = tl.load(scalars_ptr + batch_idx)\n              col_offsets = block_start + tl.arange(0, BLOCK_SIZE)\n              mask = col_offsets < in_features\n              x = tl.load(input3_ptr + batch_idx * in_features + col_offsets, mask=mask)\n              output = x + scalar\n              tl.store(output_ptr + batch_idx * in_features + col_offsets, output, mask=mask)\n\n        But note: the scalar is the same for the entire block of columns.\n\n    However, the problem says: \"small iterative steps: Modify \u226420 lines per revision\". We are writing two kernels and changing the forward pass? That is more than 20 lines.\n\n    Alternatively, we can do it in one kernel? But the one kernel would have to do the reduction and then the broadcast. And we would have to save the intermediate scalar per batch in shared memory? Then broadcast it to all threads? Then do the addition? But the addition requires the entire row of in_features? And the reduction is over out_features? So we would need two passes? \n\n    We can do:\n\n        We launch a grid of batch * ceil(in_features / BLOCK_SIZE) but then within the block, we also do the reduction for the row? But the reduction requires reading out_features elements? And we are only loading a block of in_features? We would have to read the entire row of out_features for every block? Then we would read the same row multiple times? That is inefficient.\n\n    So we break it into two kernels.\n\n    Now, the entire code for the model:\n\n        We will replace the forward function with:\n\n          def forward(self, x):\n              original_x = x.clone().detach()   # but note: we are going to use it in the kernel, so we need to make sure it's contiguous? \n              # Gemm: linear layer\n              x_linear = self.gemm(x)\n\n              # We are going to do the rest in Triton\n              # Step 1: reduction kernel to compute the scalar per batch\n              batch_size = x_linear.shape[0]\n              # Allocate output for the scalars\n              scalars = torch.empty(batch_size, device=x_linear.device, dtype=torch.float32)\n              # Launch kernel1: grid = (batch_size,)\n              #   We need to set BLOCK_SIZE to out_features? but we can set it to the next power of two? \n              P2 = triton.next_power_of_2(self.subtract.shape[0])   # out_features\n              grid1 = (batch_size,)\n              _reduction_kernel[grid1](x_linear, self.subtract, scalars, self.subtract.shape[0], BLOCK_SIZE=P2)\n\n              # Step 2: add kernel\n              output = torch.empty_like(original_x)\n              in_features = original_x.shape[1]\n              P3 = triton.next_power_of_2(1024)   # we set BLOCK_SIZE for the add kernel to 1024? but in_features is 1024, so we can set to 1024? \n              grid2 = (batch_size, (in_features + P3 - 1) // P3)\n              _add_kernel[grid2](original_x, scalars, output, in_features, BLOCK_SIZE=P3)\n\n              return output\n\n    But note: the original reference code also did the gelu? Yes, the gelu is included in the reduction kernel.\n\n    But the reference code also did the logsumexp? We determined it was an identity. So we skip it.\n\n    However, we must be cautious: what if out_features is not 512? We are given in_features and out_features in __init__. So we use self.subtract.shape[0] for out_features.\n\n    But note: the reference code did:\n\n        x = torch.mean(x, dim=1, keepdim=True)   # [batch, 1]\n        x = torch.logsumexp(x, dim=1, keepdim=True)   # [batch, 1] -> identity\n\n    So we are not doing the logsumexp? That is a deviation. But it is mathematically an identity. However, if the mean is -inf, then logsumexp might be different? But the mean of finite numbers is finite. And if the mean is -inf, then logsumexp of [-inf] is -inf? and mean is -inf? So it is the same.\n\n    Therefore, we can skip it.\n\n    But the reference code does it. To be safe, we should include it? But note: the logsumexp on a single element is that element. So we can do:\n\n        In the reduction kernel, after the mean, we do:\n          scalar = mean_val   # because logsumexp of a single element is the element.\n\n        But wait: logsumexp([x]) = log(exp(x)) = x. So we don't need to do anything.\n\n    Therefore, we are safe.\n\n    Now, let's write the two kernels and the new forward function.\n\n    However, note: the constraints say \"NEVER change @triton.jit function signatures or parameter names\" but we are defining new kernels? So we are free.\n\n    But the problem says: we are only allowed to change the Triton kernel source within the EVOLVE-BLOCK? And the entire code is within the EVOLVE-BLOCK. So we can replace the entire ModelNew and also define the kernels.\n\n    But the example provided in the context defined the kernel inside the EVOLVE-BLOCK.\n\n    We will do:\n\n        import math\n        import triton\n        import triton.language as tl\n        from triton.language.extra import libdevice\n\n        Then define the two kernels and the new ModelNew.\n\n    But note: the original ModelNew had an __init__ and forward. We are changing the forward.\n\n    Also, the original had batch_size, in_features, out_features as global variables? And get_inputs, get_init_inputs? We must keep them.\n\n    However, the problem says: \"Name the output structure class as ModelNew\".\n\n    So we will keep the class name as ModelNew.\n\n    Let's code accordingly.\n\n    Steps:\n\n        We will replace the forward method of ModelNew with the new one that uses the two Triton kernels.\n\n        We will define the two kernels.\n\n        We will keep the __init__ the same? Because it sets up the linear layer and the subtract parameter.\n\n    But note: the original code had:\n        self.subtract = nn.Parameter(torch.randn(out_features))\n\n    So out_features is the same as the output dimension of the linear layer.\n\n    We are good.\n\n    However, the reduction kernel uses out_features as a variable. We pass it as an argument.\n\n    Let's write the kernels:\n\n        Kernel1: _reduction_kernel\n\n        Kernel2: _add_kernel\n\n    We must be cautious of the data types: \n\n        The linear layer output and the subtract parameter: default float32? \n        The original input: float32.\n\n        The scalars: float32.\n\n    We'll assume float32.\n\n    But the reduction kernel: \n\n        We are doing: \n            row = tl.load(input1_ptr + offsets, mask=mask)   -> float32\n            sub_vec = tl.load(input2_ptr + tl.arange(0, BLOCK_SIZE), mask=mask)   -> float32\n\n        Then operations in float32.\n\n    The add kernel: \n        original input is float32, scalar is float32, output is float32.\n\n    Now, let's write the code.\n\n    But note: the reduction kernel must be autotuned? The problem says we can do adaptive tuning. We can autotune the BLOCK_SIZE? But we are setting BLOCK_SIZE to the next power of two of out_features? That might be the best.\n\n    Alternatively, we can autotune the reduction kernel? For example, the number of warps and stages? \n\n    We can do:\n\n        @triton.autotune(...) for the reduction kernel.\n\n    But the problem says: \"small iterative steps: Modify \u226420 lines per revision\". We are already over? \n\n    Given the complexity, and since this is the first Triton version, we will do a basic version without autotune and then in future revisions we can tune.\n\n    We'll set the block size for the reduction kernel to the next power of two of out_features? And for the add kernel, we set the block size to 1024? Since in_features is 1024.\n\n    But note: in_features might be different? But in the global variables, it is 1024. And out_features=512.\n\n    We can compute the next power of two for the reduction kernel's BLOCK_SIZE? \n\n        P2 = triton.next_power_of_2(out_features)   # for out_features=512 -> 512\n\n    And for the add kernel, we set BLOCK_SIZE=1024? But if in_features is 1024, then one block per row? \n\n    We'll do:\n\n        grid2 = (batch_size, (in_features + BLOCK_SIZE - 1) // BLOCK_SIZE)\n\n        But if BLOCK_SIZE=1024 and in_features=1024, then we have one block per row? So grid2 = (batch_size, 1)\n\n    We'll write the kernels accordingly.\n\n    Finally, note that the reduction kernel might have out_features that is not a multiple of BLOCK_SIZE? We are using a mask, so it's safe.\n\n    Let's code.\n\n    IMPORTANT: We are not allowed to remove boundary checks. We use masks.\n\n    We'll write the entire EVOLVE-BLOCK.\n\n    But note: the reduction kernel uses tl.sum? We have to be cautious: tl.sum does a reduction in the thread block? Yes.\n\n    We'll do.\n\n    However, the problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\n    We will write the entire code within the EVOLVE-BLOCK.\n\n    But note: the original code had batch_size, in_features, out_features as global variables? And get_inputs, get_init_inputs? We must keep them.\n\n    We'll keep them.\n\n    Let's write.\n</think>\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\nimport torch.nn as nn\n\n@triton.jit\ndef _reduction_kernel(\n    input1_ptr,\n    input2_ptr,\n    output_ptr,\n    out_features,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    offsets = pid * out_features + tl.arange(0, BLOCK_SIZE)\n    mask = tl.arange(0, BLOCK_SIZE) < out_features\n\n    row = tl.load(input1_ptr + offsets, mask=mask)\n    sub_vec = tl.load(input2_ptr + tl.arange(0, BLOCK_SIZE), mask=mask)\n    \n    diff = row - sub_vec\n    row_sum = tl.sum(diff, axis=0)\n    mean_val = row_sum / out_features\n    \n    erf_val = libdevice.erf(mean_val * 0.7071067811865475)\n    gelu_val = 0.5 * mean_val * (1.0 + erf_val)\n    \n    tl.store(output_ptr + pid, gelu_val)\n\n@triton.jit\ndef _add_kernel(\n    input3_ptr,\n    scalars_ptr,\n    output_ptr,\n    in_features,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    \n    col_offsets = pid1 * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < in_features\n    \n    scalar = tl.load(scalars_ptr + pid0)\n    x = tl.load(input3_ptr + pid0 * in_features + col_offsets, mask=mask)\n    result = x + scalar\n    \n    tl.store(output_ptr + pid0 * in_features + col_offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n        self.subtract = nn.Parameter(torch.randn(out_features))\n\n    def forward(self, x):\n        original_x = x\n        x_linear = self.gemm(x)\n        \n        batch_size = x_linear.shape[0]\n        out_features = self.subtract.shape[0]\n        in_features = original_x.shape[1]\n        \n        scalars = torch.empty(batch_size, device=x_linear.device, dtype=torch.float32)\n        P2 = triton.next_power_of_2(out_features)\n        grid1 = (batch_size,)\n        _reduction_kernel[grid1](x_linear, self.subtract, scalars, out_features, BLOCK_SIZE=P2)\n        \n        output = torch.empty_like(original_x)\n        P3 = 1024\n        grid2 = (batch_size, (in_features + P3 - 1) // P3)\n        _add_kernel[grid2](original_x, scalars, output, in_features, BLOCK_SIZE=P3)\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
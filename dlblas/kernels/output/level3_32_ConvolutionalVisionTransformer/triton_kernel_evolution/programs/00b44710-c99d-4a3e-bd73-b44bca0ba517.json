{"id": "00b44710-c99d-4a3e-bd73-b44bca0ba517", "code": "@triton.jit\ndef linear_proj_kernel(\n    # Pointers to matrices\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    # Matrix dimensions\n    B, D, embed_dim,\n    # Strides\n    stride_xb, stride_xd,\n    stride_wd, stride_we,\n    stride_bb,\n    stride_ob, stride_oe,\n    # Meta-parameters\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEATURE: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    # Multiple programs along the batch and output feature dimensions.\n    pid_batch = tl.program_id(0)\n    pid_feature = tl.program_id(1)\n\n    # Create ranges for the batch and feature blocks.\n    batch_offs = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feature_offs = pid_feature * BLOCK_SIZE_FEATURE + tl.arange(0, BLOCK_SIZE_FEATURE)\n\n    # Create masks for the batch and feature blocks to handle boundaries.\n    batch_mask = batch_offs < B\n    feature_mask = feature_offs < embed_dim\n\n    # Initialize accumulator to zero.\n    acc = tl.zeros((BLOCK_SIZE_BATCH, BLOCK_SIZE_FEATURE), dtype=tl.float32)\n\n    # Loop over the input features (D) in blocks of size BLOCK_SIZE_K.\n    for k in range(0, D, BLOCK_SIZE_K):\n        # Create offsets and mask for the K dimension.\n        k_offs = k + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offs < D\n\n        # Load a block of x: [BLOCK_SIZE_BATCH, BLOCK_SIZE_K]\n        x_vals = tl.load(\n            x_ptr + batch_offs[:, None] * stride_xb + k_offs[None, :] * stride_xd,\n            mask=batch_mask[:, None] & k_mask[None, :],\n            other=0.0\n        )\n\n        # Load a block of weights: [BLOCK_SIZE_FEATURE, BLOCK_SIZE_K]\n        w_vals = tl.load(\n            weight_ptr + feature_offs[:, None] * stride_we + k_offs[None, :] * stride_wd,\n            mask=feature_mask[:, None] & k_mask[None, :],\n            other=0.0\n        )\n\n        # Compute the partial matrix multiplication.\n        acc += tl.dot(x_vals, w_vals, allow_tf32=True)\n\n    # Add bias if bias_ptr is not null (i.e., if bias is provided).\n    if bias_ptr is not None:\n        b_vals = tl.load(\n            bias_ptr + feature_offs,\n            mask=feature_mask,\n            other=0.0\n        )\n        acc += b_vals[None, :]\n\n    # Write back the result.\n    output_offs = batch_offs[:, None] * stride_ob + feature_offs[None, :] * stride_oe\n    tl.store(\n        output_ptr + output_offs,\n        acc,\n        mask=batch_mask[:, None] & feature_mask[None, :]\n    )", "language": "python", "parent_id": "694d2ece-c660-4511-a1e5-25b7ceabcfa1", "generation": 1, "timestamp": 1754596649.390165, "iteration_found": 29, "metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 2.19, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 2.14, "speed_up": 0.977, "custom_timing": 2.19}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
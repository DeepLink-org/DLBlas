{"id": "fe0bc778-790e-41ad-902a-b568fbe3b784", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_activation_softmax_reduce(\n    input_ptr,\n    output_ptr,\n    batch_size,\n    D, H, W,\n    total_spatial,\n    input_batch_stride, input_channel_stride, input_d_stride, input_h_stride, input_w_stride,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    total_pid = batch_size * total_spatial\n    if pid < total_pid:\n        batch_id = pid // total_spatial\n        spatial_id = pid % total_spatial\n        d = spatial_id // (H*W)\n        hw = spatial_id % (H*W)\n        h = hw // W\n        w = hw % W\n\n        base = batch_id * input_batch_stride + d * input_d_stride + h * input_h_stride + w * input_w_stride\n\n        max_val = -float('inf')\n        values = tl.zeros((16,), dtype=tl.float32)\n        for c in tl.static_range(16):\n            off = base + c * input_channel_stride\n            v = tl.load(input_ptr + off)\n            v_act = v * (tl.minimum(tl.maximum(v+3, 0), 6) / 6\n            v_act = tl.maximum(v_act, 0)\n            values[c] = v_act\n            max_val = tl.maximum(max_val, v_act)\n\n        exp_sum = 0.0\n        for c in tl.static_range(16):\n            exp_val = tl.exp(values[c] - max_val)\n            values[c] = exp_val\n            exp_sum += exp_val\n\n        for c in tl.static_range(16):\n            softmax_val = values[c] / exp_sum\n            out_off = batch_id * 16 + c\n            tl.atomic_add(output_ptr + out_off, softmax_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, bias=True):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias)\n\n    def forward(self, x):\n        x = self.conv(x)\n        batch, channels, D, H, W = x.shape\n        total_spatial = D * H * W\n        \n        if channels != 16:\n            x = torch.nn.functional.hardswish(x)\n            x = torch.relu(x)\n            x = torch.softmax(x, dim=1)\n            return torch.mean(x, dim=[2, 3, 4])\n        \n        x = x.contiguous()\n        output_accum = torch.zeros(batch, channels, device=x.device, dtype=torch.float32)\n        \n        grid = (batch * total_spatial,)\n        _fused_activation_softmax_reduce[grid](\n            x, output_accum,\n            batch, D, H, W, total_spatial,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),\n            BLOCK_SIZE=1,\n        )\n        return output_accum / total_spatial\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "055dec83-61f3-42d7-8daa-9f394b7f5368", "generation": 1, "timestamp": 1754566189.6755576, "iteration_found": 1, "metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'tl.store' is not found."}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.929, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0784, "speed_up": 0.084, "custom_timing": 0.929}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.9290\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0784, 'speed_up': 0.084, 'custom_timing': 0.929}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.9290, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0784, 'speed_up': 0.084, 'custom_timing': 0.929}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a 3D convolution, applies HardSwish, ReLU, Softmax, and then calculates the mean.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias=True):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.hardswish(x)\n        x = torch.relu(x)\n        x = torch.softmax(x, dim=1)\n        x = torch.mean(x, dim=[2, 3, 4])\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.9290), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0784, 'speed_up': 0.084, 'custom_timing': 0.929})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a 3D convolution, applies HardSwish, ReLU, Softmax, and then calculates the mean.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias=True):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.hardswish(x)\n        x = torch.relu(x)\n        x = torch.softmax(x, dim=1)\n        x = torch.mean(x, dim=[2, 3, 4])\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. 3D convolution\n  2. HardSwish\n  3. ReLU\n  4. Softmax (along dim=1)\n  5. Mean over spatial dimensions (dim=[2,3,4])\n\nThe current implementation uses standard PyTorch layers and is running on the GPU.\n\nHowever, the performance metrics show:\n  - reference_timing_ms: 0.0784\n  - custom_timing: 0.929 (which is the time for the custom implementation, but note that the speed_up is 0.084, meaning the custom is slower)\n\nWe are in the context of Triton, but note that the current code is pure PyTorch.\n\nThe goal is to rewrite the forward pass using Triton to speed it up.\n\nBut note: the entire model is a sequence of operations. We cannot rewrite the convolution in Triton easily (and it's already a built-in optimized op). However, the sequence after the convolution might be a candidate for fusion.\n\nLet's break down the operations after the convolution:\n\n  x = self.conv(x)   -> 3D convolution, output shape [batch, out_channels, depth, height, width]\n  Then we apply:\n    1. HardSwish: x * relu6(x+3) / 6\n    2. ReLU: max(0, x)   [Note: but note that HardSwish already outputs non-negative? Actually, it can be negative, but then ReLU will clip negatives to 0. However, note that HardSwish is defined as:\n        hardswish(x) = 0 if x<=-3, x if x>=3, and x*(x+3)/6 in between.\n        Then ReLU will set negative values to 0? But note: after HardSwish, the output is non-negative? Because for x in [-3,0] it's positive? Actually, for x in [-3,0]: (x+3) is non-negative and x is negative? So the product is negative? Then ReLU will set those to 0.\n\n  But note: the ReLU after HardSwish might be redundant? Because the HardSwish already produces non-negative for x>=-3? Actually, for x in [-3,0]: x*(x+3)/6 is negative? Then ReLU will set negative values to 0. So we can combine these two.\n\n  However, let's look at the sequence: \n      HardSwish: \n          x = x * (relu6(x+3) / 6) \n      Then ReLU: \n          x = relu(x)\n\n  This is equivalent to:\n      x = relu( x * (relu6(x+3)/6) )\n\n  But note that the ReLU will set negative outputs of HardSwish to 0. However, the original HardSwish is defined to be 0 for x<=-3, and for x in [-3,0] it is negative? Actually, the standard HardSwish is non-negative. Wait, let me check:\n\n      For x <= -3: hardswish(x) = 0\n      For -3 < x < 3: hardswish(x) = x*(x+3)/6 -> for x=-2: (-2)*(1)/6 = -2/6 = -1/3 -> negative?\n      For x>=3: hardswish(x)=x\n\n  So yes, it can be negative. Then ReLU will set those negative values to 0.\n\n  But note: the original model does HardSwish then ReLU. This is a custom sequence? It is not standard.\n\n  Then we do softmax along the channel dimension (dim=1) and then take the mean over the spatial dimensions.\n\n  The softmax is done per spatial location independently? Then we average the probabilities over space.\n\n  So the entire forward pass after convolution is:\n\n      x = self.conv(x)\n      x = torch.relu( torch.nn.functional.hardswish(x) )\n      x = torch.softmax(x, dim=1)\n      x = torch.mean(x, dim=[2,3,4])\n\n  We cannot easily fuse the convolution with the rest because it's a 3D convolution and the rest are pointwise and reductions.\n\n  However, the operations after the convolution (HardSwish, ReLU, Softmax, and then mean) can be fused into a single kernel to avoid multiple passes over the data.\n\n  Steps for fusion:\n\n    Step 1: We do the convolution (we leave that as is because it's efficient).\n\n    Step 2: We write a Triton kernel that:\n        - Takes the output of the convolution (let's call it `conv_out`) of shape [batch, channels, depth, height, width]\n        - For each spatial location (depth, height, width) and for each batch, we do:\n            a. Apply HardSwish: \n                  y = conv_out[b, c, d, h, w]\n                  temp = y * (min(max(y+3, 0), 6) / 6   # This is the relu6 version? But note: the original hardswish uses min(max(x+3,0),6) then divided by 6? Actually, it's: x * (relu6(x+3)) / 6.\n            b. Then apply ReLU: temp = max(temp, 0)\n            c. Then we want to compute the softmax along the channel dimension for the spatial location (d, h, w). But note: the softmax is over the channel dimension for each spatial location. So we cannot do it independently per element? We must do a reduction over channels.\n\n        However, note: the softmax requires:\n            For a given spatial location (d, h, w) and batch b, we have a vector of length `channels` (say `v`). Then:\n                v_softmax = exp(v) / sum(exp(v))\n\n        Then we take the mean over the spatial dimensions? Actually, the mean is done after the softmax and over the spatial dimensions. So we would:\n\n            For each channel c, we average the softmax probabilities over the spatial dimensions.\n\n        But note: the output of the model is of shape [batch, out_channels] because we take the mean over [2,3,4].\n\n    Therefore, we can break the kernel into two parts?\n\n        Part A: Compute the HardSwish and ReLU for every element.\n\n        Part B: Compute the softmax and then the mean.\n\n    However, note: the softmax requires a reduction over channels and then we do a reduction over space? Actually, we do:\n\n        Step 1: For each spatial location, we compute the softmax over the channels -> output of shape [batch, channels, depth, height, width]\n        Step 2: Then we average over the spatial dimensions -> [batch, channels]\n\n    We can fuse the entire sequence (from the convolution output to the final [batch, channels] output) in one kernel? \n\n    But note: the softmax reduction over channels requires a pass over the entire channel dimension for each spatial location. Then the mean over space requires a pass over the entire spatial dimensions for each channel.\n\n    Alternatively, we can do:\n\n        We can compute the softmax and then the spatial mean in a single kernel by:\n\n          For each batch and channel, we want to compute:\n             result[b, c] = (1/(D*H*W)) * sum_{d,h,w} [ softmax_c( conv_out[b, :, d, h, w] )[c] ]\n\n          But note: the softmax for a fixed (b, d, h, w) is: \n             s(c) = exp( x[b,c,d,h,w] ) / sum_{c'} exp( x[b,c',d,h,w] )\n\n          Then the result for (b, c) is the average of s(c) over d, h, w.\n\n        We can compute this without materializing the full softmax tensor? Yes, by:\n\n          Step 1: For a fixed (b, d, h, w), we compute:\n               M = max_{c} (x[b, c, d, h, w])\n               S = sum_{c} exp(x[b, c, d, h, w] - M)\n\n          Then the softmax for channel c at (d, h, w) is: exp(x[b,c,d,h,w]-M) / S.\n\n          Then the average for channel c is the sum over d, h, w of [ exp(x[b,c,d,h,w]-M) / S ] divided by (D*H*W).\n\n        But note: we cannot avoid the fact that we need to traverse the entire channel dimension for each spatial location to compute M and S.\n\n        And then we have to traverse the entire spatial dimension for each channel to accumulate the average.\n\n    This is a complex kernel. However, note that the spatial dimensions are 16x32x32 = 16384, and the channel dimension is 16. So the channel dimension is small.\n\n    We can design a kernel that:\n\n        - Each program handles one batch and one spatial location (so total work: batch * D * H * W)\n        - For each (b, d, h, w), we load the entire channel vector (16 elements) and compute:\n             M = max(channel_vector)\n             S = sum(exp(channel_vector - M))\n          Then we compute the softmax for each channel: exp(x - M) / S.\n\n        - Then we accumulate the softmax values for each channel into a global output tensor of shape [batch, channels]. We do an atomic add per channel? But note: we are accumulating over spatial locations.\n\n        But note: the accumulation is independent per batch and channel. We can use a reduction kernel? Or we can do:\n\n          We allocate an output tensor of zeros for [batch, channels]. Then each program (for one spatial location) will add the softmax vector (for that spatial location) to the output tensor at the corresponding batch and all channels.\n\n        However, the number of spatial locations is 16384 per batch, and batch is 128 -> total 2e6. And we have 16 channels -> 32e6 additions? That might be acceptable but we need to do atomic adds? We can avoid atomics by using a reduction? Alternatively, we can use a 2D grid where each block handles one batch and one channel? Then we would have to traverse all spatial locations for that batch and channel. But that would be 16384 per block, and we have 128*16=2048 blocks. That might be acceptable.\n\n    Alternatively, we can do:\n\n        Kernel 1: Apply HardSwish and ReLU to the entire tensor? That is a simple pointwise operation and can be done in a separate kernel (or even in the same kernel as the softmax and mean?).\n\n    However, note: we are allowed to write multiple kernels? The problem says we can only change the Triton kernel source within the EVOLVE-BLOCK. And the current code is a PyTorch model. We are going to replace the forward function with a Triton kernel call.\n\n    But note: the convolution must remain because it's a learnable layer. We are not going to rewrite the convolution.\n\n    Plan:\n\n        We leave the convolution as is.\n\n        Then we write a Triton kernel that takes the output of the convolution and does:\n\n            Step 1: Pointwise HardSwish and ReLU? Actually, we can fuse the pointwise operations with the reduction for softmax and mean? Or we can do:\n\n            We break the process:\n\n                Option 1: \n                  - First, do a pointwise kernel for HardSwish and ReLU. This will write to a temporary tensor? But we want to avoid that.\n\n                Option 2: \n                  - In the same kernel that computes the softmax and mean, we also do the pointwise operations? \n\n            So in the kernel that computes the softmax and mean for one spatial location, we first apply HardSwish and ReLU to each element of the channel vector? \n\n            But note: the softmax is applied after the activation. So yes, we can do:\n\n                For a fixed spatial location (d, h, w) and batch b, we load the channel vector of the convolution output.\n\n                For each channel c in the vector:\n                    value = conv_out[b, c, d, h, w]\n                    value = hardswish(value)   # and then ReLU? Actually, the sequence is HardSwish then ReLU.\n\n                But note: the original code does:\n\n                    x = torch.nn.functional.hardswish(x)   # this returns a tensor\n                    x = torch.relu(x)\n\n                So we do: value = max( hardswish(value), 0 )? But note: hardswish can be negative? Then we do max(0, hardswish(value)).\n\n            Then we compute the softmax on the vector of these activated values.\n\n        Then we accumulate the softmax vector for the spatial location into an output tensor of shape [batch, channels] (and then at the end we divide by the number of spatial locations?).\n\n        However, note: the accumulation must be done across spatial locations. We can do:\n\n            We allocate an output tensor `out` of zeros [batch, channels].\n\n            Then we launch a kernel with grid = (batch, D, H, W) and each program handles one spatial location and one batch.\n\n            For each program:\n                - Load the entire channel vector (16 elements) for (b, d, h, w).\n                - For each channel, apply: \n                     temp = max(0, hardswish( value ))\n                - Then compute the softmax of the entire channel vector (16 elements) at this spatial location.\n                - Then for each channel c, do:\n                     tl.atomic_add(out + b * channels + c, softmax_value[c] / (D*H*W))\n\n            But wait: the mean is the sum divided by (D*H*W). So we can accumulate the softmax_value[c] and then at the end we don't need to divide? Actually, we are accumulating the softmax_value[c] for each spatial location. Then we have the sum over the spatial locations. Then to get the mean we would divide by (D*H*W). However, we can do:\n\n                We accumulate the softmax_value[c] (without dividing by (D*H*W)) and then at the end of the kernel we do nothing? Then after the kernel, we do:\n\n                    out = out / (D*H*W)\n\n            But note: we cannot do that because we are in the kernel? We can do the division in the kernel? Actually, we can do:\n\n                tl.atomic_add(out + b * channels + c, softmax_value[c])\n\n            Then after the kernel, we do:\n\n                out = out / (D*H*W)\n\n            But that would be an extra kernel? We can avoid it by:\n\n                We precompute the total number of spatial locations: total_spatial = D*H*W.\n\n                Then in the atomic add we do:\n\n                    tl.atomic_add(out + b * channels + c, softmax_value[c] / total_spatial)\n\n            However, note: total_spatial is a constant. But we must be cautious: because we are doing floating point, and the division by total_spatial might be done for each atomic add? That would be 16384 * 16 * 128 atomic adds? That's 32e6 atomic operations? That might be slow.\n\n        Alternatively, we can do the division at the end in a separate kernel? Or even on the host? But we want to avoid extra passes.\n\n        Actually, we can do the division at the end by a single PyTorch division? That would be efficient.\n\n        So:\n\n            Step 1: Launch a kernel that accumulates the softmax results (without averaging) into a tensor `accum` of shape [batch, channels].\n            Step 2: Then do: accum = accum / (D*H*W)\n\n        But note: the kernel would do:\n\n            accum[b, c] = sum_{d,h,w} softmax( at (b, d, h, w) )[c]\n\n        Then we divide by (D*H*W) to get the mean.\n\n    However, note: the kernel must do the HardSwish and ReLU, then the softmax for each spatial location, and then accumulate the softmax vector.\n\n    But the channel dimension is only 16, so we can do:\n\n        We can have a 1D grid of size (batch * D * H * W). Each program handles one spatial location and one batch.\n\n        The program:\n            pid = tl.program_id(0)\n            total_spatial = D * H * W\n            batch_id = pid // total_spatial\n            spatial_id = pid % total_spatial\n            d = spatial_id // (H*W)\n            hw = spatial_id % (H*W)\n            h = hw // W\n            w = hw % W\n\n            Then we load the channel vector for (batch_id, d, h, w) from the input tensor.\n\n            We compute:\n                max_val = -infinity\n                for each channel in the vector:\n                    # apply HardSwish and ReLU\n                    v = tl.load(input_ptr + [index for the channel])\n                    v = v * (min(max(v+3, 0), 6)) / 6   # HardSwish\n                    v = max(v, 0)   # ReLU\n\n                    # then update max_val for the softmax\n                    max_val = max(max_val, v)\n\n                Then we compute the exponentials and the sum of exponentials.\n\n                Then we compute the softmax for each channel: exp(v - max_val) / sum(exp(v - max_val))\n\n                Then for each channel, we do an atomic add to the output tensor at (batch_id, channel) of the value (softmax_value).\n\n            We do not divide by total_spatial in the kernel.\n\n        Then after the kernel, we do:\n\n            output = output / (D*H*W)\n\n    But note: the atomic adds might be a bottleneck? We have 128*16*32*32 = 2e6 programs, each doing 16 atomic adds? That's 32e6 atomic operations? That might be acceptable on a GPU? But we must be cautious.\n\n    Alternatively, we can do a reduction per batch and channel? We can have a 2D grid: batch and channel. Then each program would iterate over all spatial locations? Then we would do:\n\n        grid = (batch, channels)\n        for batch_id in range(batch):\n            for channel_id in range(channels):\n                total = 0\n                for d in range(D):\n                    for h in range(H):\n                        for w in range(W):\n                            # load the value at (batch_id, channel_id, d, h, w)\n                            # apply HardSwish and ReLU to the value? But wait, the softmax requires the entire channel vector at (d, h, w) to compute the softmax for that location.\n\n                This approach is not efficient because we would load the entire channel vector multiple times? Actually, we would have to load the entire channel vector for each spatial location to compute the softmax for that location? And we are doing that for each channel? That would be very inefficient.\n\n    Therefore, the first approach (each program per spatial location) is better.\n\n    Implementation details:\n\n        We'll write a Triton kernel that takes:\n            input: pointer to the convolution output tensor of shape [batch, channels, D, H, W]\n            output: pointer to the accumulation tensor of shape [batch, channels] (we will zero it first)\n\n        The kernel will have a grid of (batch * D * H * W,)\n\n        We'll use:\n            total_spatial = D * H * W\n            batch_size = batch\n            n_channels = channels\n\n        Steps in the kernel:\n\n            pid = tl.program_id(0)\n            total_pid = batch_size * total_spatial   # total number of programs\n\n            if pid < total_pid:\n                batch_id = pid // total_spatial\n                spatial_id = pid % total_spatial\n                d = spatial_id // (H*W)\n                hw = spatial_id % (H*W)\n                h = hw // W\n                w = hw % W\n\n                # Now, we want to load the channel vector at (batch_id, :, d, h, w)\n                # We assume the tensor is contiguous? We can make it contiguous in the PyTorch code.\n\n                # We'll create an array to hold the channel vector and the activated values.\n                # We'll use a loop over channels? Since channels is small (16).\n\n                # We need to compute the base pointer for the spatial location (d, h, w) in the input tensor.\n\n                # The input tensor: [batch, channels, D, H, W]\n                # The offset for (batch_id, 0, d, h, w) is: \n                #   base = batch_id * (channels * D * H * W) + d * (H * W) + h * W + w\n                # Then for channel c, the offset is: base + c * (D * H * W)\n\n                # Actually, the stride for channel is D*H*W, for D is H*W, for H is W, for W is 1.\n\n                # So the pointer for channel c is: input_ptr + batch_id * (channels * D * H * W) + c * (D * H * W) + d * (H * W) + h * W + w\n\n                # We'll precompute the base for the spatial location:\n                base = batch_id * (channels * D * H * W) + d * (H * W) + h * W + w\n\n                # Now, we load the vector for c in [0, channels-1]:\n                max_val = -float('inf')\n                s = 0.0\n                # We'll store the activated values and the exponentials?\n                # We can do two passes: first pass to compute max, second to compute the sum of exponentials? But since channels is small, we can do:\n\n                # First, we do a loop to compute the activated values and the max.\n                values = tl.zeros((channels,), dtype=tl.float32)\n                for c in range(channels):\n                    off = base + c * (D * H * W)\n                    v = tl.load(input_ptr + off)\n                    # Apply HardSwish: \n                    #   hardswish = v * (min(max(v+3,0),6) / 6\n                    # Then ReLU: max(0, ...)\n                    # We break it down:\n                    v_hardswish = v * (tl.minimum(tl.maximum(v+3, 0), 6) / 6\n                    v_act = tl.maximum(v_hardswish, 0)\n                    values[c] = v_act\n                    if v_act > max_val:\n                        max_val = v_act\n\n                # Now compute the exponentials and the sum.\n                exp_values = tl.zeros((channels,), dtype=tl.float32)\n                sum_exp = 0.0\n                for c in range(channels):\n                    exp_val = tl.exp(values[c] - max_val)\n                    exp_values[c] = exp_val\n                    sum_exp += exp_val\n\n                # Now compute the softmax for each channel: exp_val / sum_exp\n                for c in range(channels):\n                    softmax_val = exp_values[c] / sum_exp\n                    # Accumulate this softmax_val into output at [batch_id, c]\n                    # The output tensor: [batch, channels] is contiguous? Then the offset for (batch_id, c) is: batch_id * channels + c\n                    output_off = batch_id * channels + c\n                    tl.atomic_add(output_ptr + output_off, softmax_val)\n\n        Then in the PyTorch code, after the kernel, we do:\n\n            output = output / (D * H * W)\n\n    But note: the input tensor might be in float32? The convolution output is float32? We assume so.\n\n    However, the atomic_add for float32 is supported? Yes.\n\n    But note: the number of programs is 128 * (16*32*32) = 128*16384 = 2097152. And each program does 3*16 = 48 operations? That should be acceptable.\n\n    But we must be cautious: the kernel might be slow because of the inner loops? But the channel dimension is only 16.\n\n    Alternatively, we can unroll the loops? Triton will unroll the loops if the range is constant? We can use `tl.static_range` for the channel loop.\n\n    Steps for the Triton kernel:\n\n        We must pass: \n            input_ptr, output_ptr, \n            batch_size, channels, D, H, W, \n            stride_batch, stride_channel, stride_d, stride_h, stride_w? \n            But we assumed contiguous? We can make the tensor contiguous.\n\n    Let's design the kernel:\n\n        @triton.jit\n        def _fused_activation_softmax_reduce(\n            input_ptr,\n            output_ptr,\n            batch_size,\n            channels,\n            D, H, W,\n            total_spatial,   # = D*H*W\n            input_batch_stride, input_channel_stride, input_d_stride, input_h_stride, input_w_stride,\n            BLOCK_SIZE: tl.constexpr,   # we can set BLOCK_SIZE to the total_spatial? Actually, we don't need a block for reduction because we are doing per spatial location.\n        ):\n            pid = tl.program_id(0)\n            total_pid = batch_size * total_spatial\n            if pid < total_pid:\n                batch_id = pid // total_spatial\n                spatial_id = pid % total_spatial\n                d = spatial_id // (H*W)\n                hw = spatial_id % (H*W)\n                h = hw // W\n                w = hw % W\n\n                # Compute the base pointer for the spatial location (d, h, w) in the input tensor for batch_id\n                base = batch_id * input_batch_stride + d * input_d_stride + h * input_h_stride + w * input_w_stride\n\n                # We'll use static unrolling for channels? Since channels is known at compile time (16) we can unroll.\n                max_val = -float('inf')\n                values = tl.zeros((channels,), dtype=tl.float32)\n                for c in tl.static_range(channels):\n                    off = base + c * input_channel_stride\n                    v = tl.load(input_ptr + off)\n                    # HardSwish and ReLU\n                    v_act = tl.maximum(v * (tl.minimum(tl.maximum(v+3, 0), 6) / 6, 0)\n                    values[c] = v_act\n                    if v_act > max_val:\n                        max_val = v_act\n\n                # Now compute the exponentials and the sum.\n                exp_sum = 0.0\n                for c in tl.static_range(channels):\n                    exp_val = tl.exp(values[c] - max_val)\n                    values[c] = exp_val\n                    exp_sum += exp_val\n\n                # Now compute the softmax and accumulate\n                for c in tl.static_range(channels):\n                    softmax_val = values[c] / exp_sum\n                    # output offset for (batch_id, c)\n                    out_off = batch_id * channels + c\n                    tl.atomic_add(output_ptr + out_off, softmax_val)\n\n    Then in the forward function:\n\n        def forward(self, x):\n            x = self.conv(x)   # [batch, channels, D, H, W]\n\n            # We are going to use the kernel\n            batch, channels, D, H, W = x.shape\n            total_spatial = D * H * W\n            # Make sure the tensor is contiguous\n            x = x.contiguous()\n            # Create an output tensor of zeros for accumulation: [batch, channels]\n            output_accum = torch.zeros(batch, channels, device=x.device, dtype=torch.float32)\n\n            # We need to pass the strides for the input tensor? \n            # We can get the strides: \n            #   x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4)\n            # But note: the convolution output might be in channels_last? We made it contiguous so it should be in standard format.\n\n            # Grid: (batch * total_spatial,)\n            grid = (batch * total_spatial,)\n\n            # Launch the kernel\n            _fused_activation_softmax_reduce[grid](\n                x, output_accum,\n                batch, channels, D, H, W, total_spatial,\n                x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),\n                BLOCK_SIZE=1,   # we don't have a block dimension? Actually, each program is independent.\n            )\n\n            # Then divide by the number of spatial locations\n            output_accum = output_accum / (D * H * W)\n\n            return output_accum\n\n    But note: the kernel uses static_range for channels. We must set the channels as a constant? Actually, we can pass `channels` as a variable and use `tl.static_range`? But `tl.static_range` requires a constant. So we must compile the kernel for a specific channel size? However, the model is fixed: out_channels=16.\n\n    Alternatively, we can unroll the loop by the constant 16? We can write:\n\n        for c in range(16):\n            ...\n\n    But then if we change the model, it breaks? But the model is fixed for the benchmark. So we can do:\n\n        channels = 16   # fixed\n\n    But note: the model is defined with out_channels=16. So we can hardcode 16? Or we can pass it as a constant and use `tl.static_range` with a constant? Actually, we can do:\n\n        in the kernel: if we know that channels is 16, we can unroll the loop.\n\n        But we are passing `channels` as an argument? Then we can do:\n\n            for c in tl.static_range(16):   # but what if channels is not 16? We must have the kernel work for any channel size? But the benchmark model uses 16.\n\n        Alternatively, we can note that the kernel is specific to the model. We can hardcode 16? But then the model becomes inflexible.\n\n    However, the problem says: we must maintain the same inputs and outputs. The model is defined with out_channels=16. So we can hardcode.\n\n    But note: the function get_init_inputs returns [in_channels, out_channels, kernel_size] and the model uses these. So we can use:\n\n        channels = self.conv.out_channels   # which is the out_channels passed in __init__\n\n    In the forward, we can check that channels==16? If not, we use a fallback? But the benchmark is fixed. We assume the model is created with out_channels=16.\n\n    So we can hardcode 16 in the kernel? Or we can pass the channel size and use a loop that is not unrolled? But then performance might suffer? But 16 is small.\n\n    Alternatively, we can use:\n\n        for c in range(channels):\n            ...   # and hope that Triton unrolls the loop if channels is a constant passed at compile time? \n\n    But note: Triton does not automatically unroll loops. We can use:\n\n        for c in tl.static_range(channels):\n            ...\n\n    But then we must pass `channels` as a constant expression? We can do:\n\n        CHANNELS: tl.constexpr\n\n    Then we can do:\n\n        @triton.jit\n        def _fused_activation_softmax_reduce(\n            ...,\n            CHANNELS: tl.constexpr,\n            ...):\n\n        and then use tl.static_range(CHANNELS)\n\n    But note: when we launch the kernel, we pass `channels` as a variable? We can pass it as a constant expression? \n\n        We can set it as a keyword argument in the kernel launch? But the Triton kernel launch doesn't allow passing constexpr from Python? We must pass it as a config? \n\n    Actually, we can pass it as a scalar and then use:\n\n        CHANNELS = channels   # and then use it in tl.static_range? But the static_range requires a compile-time constant.\n\n    This is tricky. Alternatively, we can note that the channel size is fixed at 16 and hardcode it.\n\n    Given the time, we will hardcode the channel size to 16.\n\n    We change the kernel to:\n\n        for c in tl.static_range(16):\n            ...\n\n    And we remove the `channels` argument? Or we leave it and don't use it? We can remove it.\n\n    But note: the output tensor accumulation uses `channels` (which is 16). So we can hardcode 16 in the kernel.\n\n    However, the kernel uses the stride for the channel dimension? We still need to pass the input_channel_stride.\n\n    So we keep the `channels` argument? Actually, we can remove it because we hardcode 16.\n\n    But note: the input_channel_stride is the stride in elements. We need it.\n\n    So we don't remove the stride.\n\n    We'll write:\n\n        @triton.jit\n        def _fused_activation_softmax_reduce(\n            input_ptr,\n            output_ptr,\n            batch_size,\n            D, H, W,\n            total_spatial,   # = D*H*W\n            input_batch_stride, input_channel_stride, input_d_stride, input_h_stride, input_w_stride,\n            BLOCK_SIZE: tl.constexpr,   # unused\n        ):\n            pid = tl.program_id(0)\n            total_pid = batch_size * total_spatial\n            if pid < total_pid:\n                batch_id = pid // total_spatial\n                spatial_id = pid % total_spatial\n                d = spatial_id // (H*W)\n                hw = spatial_id % (H*W)\n                h = hw // W\n                w = hw % W\n\n                base = batch_id * input_batch_stride + d * input_d_stride + h * input_h_stride + w * input_w_stride\n\n                max_val = -float('inf')\n                values = tl.zeros((16,), dtype=tl.float32)   # hardcoded 16\n                for c in tl.static_range(16):\n                    off = base + c * input_channel_stride\n                    v = tl.load(input_ptr + off)\n                    # HardSwish and ReLU\n                    v_act = tl.maximum(v * (tl.minimum(tl.maximum(v+3, 0), 6) / 6, 0)\n                    values[c] = v_act\n                    if v_act > max_val:\n                        max_val = v_act\n\n                exp_sum = 0.0\n                for c in tl.static_range(16):\n                    exp_val = tl.exp(values[c] - max_val)\n                    values[c] = exp_val\n                    exp_sum += exp_val\n\n                for c in tl.static_range(16):\n                    softmax_val = values[c] / exp_sum\n                    out_off = batch_id * 16 + c   # hardcoded 16\n                    tl.atomic_add(output_ptr + out_off, softmax_val)\n\n    Then in the forward function, we note that the output_accum is of shape [batch, 16]? We create it as `torch.zeros(batch, 16, ...)`.\n\n    But note: the model has out_channels=16, so it's fixed.\n\n    Now, let's compare with the original:\n\n        Original: \n            x = self.conv(x)\n            x = torch.nn.functional.hardswish(x)\n            x = torch.relu(x)\n            x = torch.softmax(x, dim=1)\n            x = torch.mean(x, dim=[2,3,4])\n\n        New:\n            x = self.conv(x)\n            # then we do the kernel and the division\n\n        We remove the intermediate steps.\n\n    We hope that this fusion avoids writing the intermediate tensors and thus saves memory bandwidth.\n\n    But note: the original might be optimized by PyTorch's JIT? But we are not using JIT.\n\n    We must compare the performance.\n\n    However, note: the current custom_timing is 0.929 ms and the reference_timing is 0.0784 ms? That doesn't make sense because the custom_timing is the time for the custom implementation? But the custom implementation is the same as the reference? Actually, the current program is the reference.\n\n    The problem says: we are in the reference code. We are going to replace it.\n\n    We must be cautious: the atomic_add might be slow? But we are doing 16 atomic adds per spatial location, and there are 2e6 spatial locations -> 32e6 atomic adds. This might be heavy.\n\n    Alternatively, we can do a reduction using shared memory? But note: the reduction is per batch and channel, and we have 128*16=2048 independent accumulation variables. We can have a 2D grid of 128*16=2048 blocks? Then each block accumulates over the spatial locations? Then we would load the entire channel vector for each spatial location? That would be 16384 per block? And we would have to compute the softmax for each spatial location? That would be 2048 blocks each doing 16384 iterations? That would be 2048 * 16384 = 33e6 iterations? And each iteration loads 16 floats? That's 528e6 loads? That might be worse.\n\n    Therefore, we stick with the first approach.\n\n    We'll try the kernel and see if it improves.\n\n    Steps for the new ModelNew:\n\n        We will keep the convolution.\n\n        We will replace the rest with the Triton kernel.\n\n    We must also note: the original model returns a tensor of shape [batch, out_channels]. Our new model does the same.\n\n    Let's code accordingly.\n\n    However, note: the Triton kernel must be compiled. We'll include it in the same file.\n\n    We'll write the kernel inside the ModelNew class? Or outside? We'll put it outside the class.\n\n    We'll do:\n\n        import triton\n        import triton.language as tl\n\n        @triton.jit\n        def _fused_activation_softmax_reduce(...):\n            ...\n\n    Then in the forward function of ModelNew, we call this kernel.\n\n    We must also note: the kernel uses the absolute value of the tensor? No, it uses the values.\n\n    We must also note: the original model uses float32? We are using float32.\n\n    Let's write the code accordingly.\n\n    But note: the kernel uses `tl.exp` which is for float32? Yes.\n\n    We must also note: the kernel uses `tl.atomic_add` for float32? Yes, supported.\n\n    Finally, note: we are not allowed to change the grid configuration? But the problem says:\n\n        \u274c NEVER change @triton.jit function signatures or parameter names  \n        \u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\n        But we are writing a new kernel? The original code didn't have a kernel. \n\n        We are replacing the PyTorch operations with a kernel. We are allowed.\n\n    We'll do:\n\n        class ModelNew(nn.Module):\n            def __init__(self, in_channels, out_channels, kernel_size, bias=True):\n                super(ModelNew, self).__init__()\n                self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias)\n\n            def forward(self, x):\n                x = self.conv(x)\n                batch, channels, D, H, W = x.shape\n                total_spatial = D * H * W\n                x = x.contiguous()\n                output_accum = torch.zeros(batch, channels, device=x.device, dtype=torch.float32)\n\n                grid = (batch * total_spatial,)\n                # Note: we hardcode the kernel to 16 channels? But we have channels=out_channels, which is 16.\n                if channels != 16:\n                    # Fallback to the original if not 16? Or we can use a generic kernel? But for the benchmark it's 16.\n                    # We'll use the original for now, but the benchmark is fixed to 16.\n                    x = torch.nn.functional.hardswish(x)\n                    x = torch.relu(x)\n                    x = torch.softmax(x, dim=1)\n                    x = torch.mean(x, dim=[2,3,4])\n                    return x\n\n                # Launch kernel\n                _fused_activation_softmax_reduce[grid](\n                    x, output_accum,\n                    batch, D, H, W, total_spatial,\n                    x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),\n                    BLOCK_SIZE=1,\n                )\n\n                output_accum = output_accum / total_spatial\n                return output_accum\n\n    We must define the kernel.\n\n    But note: the kernel uses the absolute value of the tensor? No, it uses the value.\n\n    However, note: the HardSwish formula: \n        v * (min(max(v+3,0),6) / 6\n\n    We can write it as:\n\n        v_act = v * (tl.minimum(tl.maximum(v+3, 0), 6) / 6\n        then do max(0, v_act)\n\n    But note: the original HardSwish in PyTorch is defined as:\n\n        x * (torch.nn.functional.relu6(x+3)) / 6\n\n    And then we do ReLU: max(0, x)\n\n    We do exactly that.\n\n    We'll write the kernel.\n\n    However, note: the kernel might be long? And we are only allowed to change the EVOLVE-BLOCK? We must put everything in the block.\n\n    We are going to replace the entire EVOLVE-BLOCK with:\n\n        import ... (including triton)\n\n        @triton.jit\n        def ... (the kernel)\n\n        class ModelNew ... (with the new forward)\n\n        batch_size = ... (unchanged)\n\n        get_inputs and get_init_inputs unchanged.\n\n    But note: the original EVOLVE-BLOCK does not include triton. We must import triton.\n\n    We'll add:\n\n        import triton\n        import triton.language as tl\n\n    But note: the environment has triton.\n\n    We'll write the code accordingly.\n\n    We hope this speeds up the model.\n\n    If it doesn't, we might have to reconsider.\n\n    Given the complexity, we might have to do multiple iterations.\n\n    But note: the current reference time is 0.0784 ms. Our kernel must be faster than the original sequence.\n\n    The original sequence does:\n\n        hardswish: one pass\n        relu: one pass? Actually, hardswish and relu might be fused by PyTorch? \n        softmax: one pass (with reductions) and then exponentials and normalization? This is expensive.\n        mean: another reduction.\n\n    We are doing one kernel that does all the pointwise activations and then the softmax and accumulation. We hope to save memory bandwidth.\n\n    Let's code accordingly.\n\n    We'll write the new code in the EVOLVE-BLOCK.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_activation_softmax_reduce(\n    input_ptr,\n    output_ptr,\n    batch_size,\n    D, H, W,\n    total_spatial,\n    input_batch_stride, input_channel_stride, input_d_stride, input_h_stride, input_w_stride,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    total_pid = batch_size * total_spatial\n    if pid < total_pid:\n        batch_id = pid // total_spatial\n        spatial_id = pid % total_spatial\n        d = spatial_id // (H*W)\n        hw = spatial_id % (H*W)\n        h = hw // W\n        w = hw % W\n\n        base = batch_id * input_batch_stride + d * input_d_stride + h * input_h_stride + w * input_w_stride\n\n        max_val = -float('inf')\n        values = tl.zeros((16,), dtype=tl.float32)\n        for c in tl.static_range(16):\n            off = base + c * input_channel_stride\n            v = tl.load(input_ptr + off)\n            v_act = v * (tl.minimum(tl.maximum(v+3, 0), 6) / 6\n            v_act = tl.maximum(v_act, 0)\n            values[c] = v_act\n            max_val = tl.maximum(max_val, v_act)\n\n        exp_sum = 0.0\n        for c in tl.static_range(16):\n            exp_val = tl.exp(values[c] - max_val)\n            values[c] = exp_val\n            exp_sum += exp_val\n\n        for c in tl.static_range(16):\n            softmax_val = values[c] / exp_sum\n            out_off = batch_id * 16 + c\n            tl.atomic_add(output_ptr + out_off, softmax_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, bias=True):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias)\n\n    def forward(self, x):\n        x = self.conv(x)\n        batch, channels, D, H, W = x.shape\n        total_spatial = D * H * W\n        \n        if channels != 16:\n            x = torch.nn.functional.hardswish(x)\n            x = torch.relu(x)\n            x = torch.softmax(x, dim=1)\n            return torch.mean(x, dim=[2, 3, 4])\n        \n        x = x.contiguous()\n        output_accum = torch.zeros(batch, channels, device=x.device, dtype=torch.float32)\n        \n        grid = (batch * total_spatial,)\n        _fused_activation_softmax_reduce[grid](\n            x, output_accum,\n            batch, D, H, W, total_spatial,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),\n            BLOCK_SIZE=1,\n        )\n        return output_accum / total_spatial\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "8e288483-da95-48ce-b64d-b624a42089ae", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n@triton.jit\ndef _forward_kernel(\n    input_ptr, weight_ptr, bias_ptr, constant, output_ptr,\n    stride_in0, stride_in1,\n    stride_w0, stride_w1,\n    stride_out0, stride_out1,\n    IN_FEATURES: tl.constexpr,\n    OUT_FEATURES: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)  # Batch index\n    tid = tl.arange(0, BLOCK_SIZE)  # Thread indices within block\n    \n    # Mask for output computation\n    mask_out = tid < OUT_FEATURES\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Loop over input features\n    for k in range(0, IN_FEATURES):\n        # Load input element\n        mask_in = k < IN_FEATURES\n        x_val = tl.load(\n            input_ptr + pid * stride_in0 + k * stride_in1, \n            mask=mask_in, \n            other=0.0\n        )\n        # Load weight element with output feature mask\n        w_val = tl.load(\n            weight_ptr + tid * stride_w0 + k * stride_w1, \n            mask=mask_out, \n            other=0.0\n        )\n        acc += w_val * x_val\n    \n    # Load bias\n    bias_val = tl.load(\n        bias_ptr + tid, \n        mask=mask_out, \n        other=0.0\n    )\n    acc += bias_val\n    \n    # Apply min and subtract operations\n    out_val = tl.minimum(acc, constant) - constant\n    \n    # Store result\n    tl.store(\n        output_ptr + pid * stride_out0 + tid * stride_out1, \n        out_val, \n        mask=mask_out\n    )\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, constant):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.constant = nn.Parameter(torch.tensor(constant))\n    \n    def forward(self, x):\n        x = x.contiguous()\n        batch_size, in_features = x.shape\n        out_features = self.linear.out_features\n        \n        # Get parameters as contiguous tensors\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        # Preallocate output\n        output = torch.empty((batch_size, out_features), \n                            device=x.device, dtype=x.dtype)\n        \n        # Compute block size as next power-of-two\n        max_features = max(in_features, out_features)\n        block_size = 1 if max_features == 0 else 2**math.ceil(math.log2(max_features))\n        \n        # Launch kernel - CORRECTED: removed (block_size,) from launch config\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, weight, bias, self.constant.item(), output,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            output.stride(0), output.stride(1),\n            IN_FEATURES=in_features,\n            OUT_FEATURES=out_features,\n            BLOCK_SIZE=block_size\n        )\n        return output\n\nbatch_size = 128\nin_features = 10\nout_features = 5\nconstant = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, constant]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "46980302-012d-4683-bbe6-6a4842d12d6b", "generation": 3, "timestamp": 1754584225.286661, "iteration_found": 28, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.815, "runtime_ms": 0.0688, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0561, "speed_up": 0.815, "custom_timing": 0.0688}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_68_Matmul_Min_Subtract", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpg67u0d_n.py\", line 87, in forward\n    _forward_kernel[grid, (block_size,)](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 617, in run\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/backends/nvidia/driver.py\", line 708, in __call__\n    self.launch(gridX, gridY, gridZ, stream, function, self.launch_cooperative_grid, self.launch_pdl,\nTypeError: 'tuple' object cannot be interpreted as an integer\n", "runtime_error_name": "builtins.TypeError", "reference_timing_ms": 0.0561, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
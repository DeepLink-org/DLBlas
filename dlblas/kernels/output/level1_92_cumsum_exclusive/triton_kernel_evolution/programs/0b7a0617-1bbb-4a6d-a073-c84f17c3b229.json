{"id": "0b7a0617-1bbb-4a6d-a073-c84f17c3b229", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef exclusive_cumsum_kernel(\n    x_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    base = pid * n_elements\n    current = 0.0\n    for i in range(0, n_elements):\n        xi = tl.load(x_ptr + base + i)\n        tl.store(output_ptr + base + i, current)\n        current += xi\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        original_shape = x.shape\n        if x.dim() == 0:\n            return x.clone()\n        \n        x = x.contiguous()\n        n_dim = x.dim()\n        dim = self.dim if self.dim >= 0 else n_dim + self.dim\n        \n        if dim == n_dim - 1:\n            total_slices = x.numel() // x.shape[-1]\n            n_elements = x.shape[-1]\n            x_2d = x.view(total_slices, n_elements)\n        else:\n            perm = list(range(n_dim))\n            perm.remove(dim)\n            perm.append(dim)\n            x_perm = x.permute(perm).contiguous()\n            total_slices = x_perm.numel() // x_perm.shape[-1]\n            n_elements = x_perm.shape[-1]\n            x_2d = x_perm.view(total_slices, n_elements)\n        \n        output_2d = torch.empty_like(x_2d)\n        BLOCK_SIZE = triton.next_power_of_2(n_elements)\n        grid = (total_slices,)\n        \n        exclusive_cumsum_kernel[grid](x_2d, output_2d, n_elements, BLOCK_SIZE)\n        \n        if dim == n_dim - 1:\n            output = output_2d.view(original_shape)\n        else:\n            output_3d = output_2d.view(x_perm.shape)\n            inv_perm = list(range(n_dim))\n            inv_perm = inv_perm[:dim] + [n_dim-1] + inv_perm[dim:-1]\n            output = output_3d.permute(inv_perm).contiguous()\n        \n        return output\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    return [torch.randn(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [dim]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "a2e3a690-7cab-4517-ae68-fec731038682", "generation": 3, "timestamp": 1754994341.2026448, "iteration_found": 21, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_92_cumsum_exclusive", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_issue": "Output shape mismatch: Expected torch.Size([127, 4001]), got torch.Size([128, 4000])", "correctness_issue_name": "correctness_issue", "reference_timing_ms": 0.0377, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_92_cumsum_exclusive", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_issue": "Output shape mismatch: Expected torch.Size([127, 4001]), got torch.Size([128, 4000])", "correctness_issue_name": "correctness_issue", "reference_timing_ms": 0.0377, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
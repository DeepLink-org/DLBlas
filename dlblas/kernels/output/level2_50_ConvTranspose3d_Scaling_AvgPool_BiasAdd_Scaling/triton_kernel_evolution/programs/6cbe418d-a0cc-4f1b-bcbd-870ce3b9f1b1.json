{"id": "6cbe418d-a0cc-4f1b-bcbd-870ce3b9f1b1", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_ops_kernel(\n    input_ptr,\n    output_ptr,\n    bias_ptr,\n    scale1,\n    scale2,\n    input_d, input_h, input_w,\n    pooled_d, pooled_h, pooled_w,\n    stride_in_b, stride_in_c, stride_in_d, stride_in_h, stride_in_w,\n    stride_out_b, stride_out_c, stride_out_d, stride_out_h, stride_out_w,\n    batch, channels,\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Calculate total output elements\n    num_output_elements = batch * channels * pooled_d * pooled_h * pooled_w\n    pid = tl.program_id(0)\n    start_idx = pid * BLOCK_SIZE\n    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < num_output_elements\n    \n    # Compute dimensions per element\n    ch_pool = channels * pooled_d * pooled_h * pooled_w\n    b_idx = offsets // ch_pool\n    remainder = offsets % ch_pool\n    \n    pool3d = pooled_d * pooled_h * pooled_w\n    c_idx = remainder // pool3d\n    remainder = remainder % pool3d\n    \n    pool2d = pooled_h * pooled_w\n    d_idx = remainder // pool2d\n    remainder = remainder % pool2d\n    \n    h_idx = remainder // pooled_w\n    w_idx = remainder % pooled_w\n    \n    # Load scalar parameters\n    scale1_val = tl.load(scale1)\n    scale2_val = tl.load(scale2)\n    \n    # Initialize accumulators\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    cnt = tl.zeros((BLOCK_SIZE,), dtype=tl.int32)\n    \n    # Process 2x2x2 pooling window\n    for kd in range(2):\n        d_in = d_idx * 2 + kd\n        for kh in range(2):\n            h_in = h_idx * 2 + kh\n            for kw in range(2):\n                w_in = w_idx * 2 + kw\n                \n                # Check input boundaries\n                in_bounds = (d_in < input_d) & (h_in < input_h) & (w_in < input_w)\n                valid_mask = in_bounds & mask\n                \n                # Compute input offset\n                input_offset = b_idx * stride_in_b + c_idx * stride_in_c + \\\n                               d_in * stride_in_d + h_in * stride_in_h + w_in * stride_in_w\n                \n                # Load input value\n                val = tl.load(input_ptr + input_offset, mask=valid_mask, other=0.0)\n                \n                # Scale and accumulate\n                scaled_val = val * scale1_val\n                acc = tl.where(valid_mask, acc + scaled_val, acc)\n                cnt = tl.where(valid_mask, cnt + 1, cnt)\n    \n    # Compute average pooling\n    avg_val = acc / tl.maximum(cnt, 1)\n    \n    # Load bias (per channel)\n    bias_val = tl.load(bias_ptr + c_idx, mask=mask, other=0.0)\n    \n    # Apply bias and scaling\n    result = (avg_val + bias_val) * scale2_val\n    \n    # Compute output offset\n    output_offset = b_idx * stride_out_b + c_idx * stride_out_c + \\\n                    d_idx * stride_out_d + h_idx * stride_out_h + w_idx\n    \n    # Store result\n    tl.store(output_ptr + output_offset, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(\n            in_channels, out_channels, kernel_size, \n            stride=stride, padding=padding\n        )\n        self.scale1 = nn.Parameter(torch.tensor(scale1, dtype=torch.float32))\n        self.scale2 = nn.Parameter(torch.tensor(scale2, dtype=torch.float32))\n        self.bias = nn.Parameter(torch.randn(bias_shape, dtype=torch.float32))\n        \n    def forward(self, x):\n        # Compute conv transpose output\n        x = self.conv_transpose(x)\n        \n        # Get input dimensions\n        B, C, D, H, W = x.shape\n        \n        # Calculate pooled dimensions\n        pooled_D = (D - 2) // 2 + 1\n        pooled_H = (H - 2) // 2 + 1\n        pooled_W = (W - 2) // 2 + 1\n        \n        # Create output tensor\n        output = torch.empty(\n            (B, C, pooled_D, pooled_H, pooled_W),\n            device=x.device, dtype=x.dtype\n        )\n        \n        # Total output elements\n        total_output_elements = B * C * pooled_D * pooled_H * pooled_W\n        \n        # Launch kernel configuration\n        BLOCK_SIZE = 128\n        grid = (triton.cdiv(total_output_elements, BLOCK_SIZE),)\n        \n        # Launch fused kernel\n        fused_ops_kernel[grid](\n            x, output, self.bias,\n            self.scale1, self.scale2,\n            D, H, W,           # Input dimensions\n            pooled_D, pooled_H, pooled_W,  # Pooled dimensions\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4),\n            B, C,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\nscale1 = 0.5\nscale2 = 1.0\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "f6bd4bd1-b1bd-4cca-a516-0639f0480ae5", "generation": 1, "timestamp": 1754580255.0904462, "iteration_found": 23, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.021, "runtime_ms": 4.49, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.093, "speed_up": 0.021, "custom_timing": 4.49}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 5.72, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.093, "speed_up": 0.016, "custom_timing": 5.72}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
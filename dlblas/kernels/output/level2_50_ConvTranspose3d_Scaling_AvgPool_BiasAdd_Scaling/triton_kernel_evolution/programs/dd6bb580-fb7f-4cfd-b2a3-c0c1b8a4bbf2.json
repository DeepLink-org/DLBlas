{"id": "dd6bb580-fb7f-4cfd-b2a3-c0c1b8a4bbf2", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\nimport math\n\nclass ModelNew(torch.nn.Module):\n    \"\"\"\n    Optimized model using Triton for fused 3D transposed convolution, scaling, \n    average pooling, bias addition, and scaling operations.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape):\n        super(ModelNew, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.scale1 = torch.nn.Parameter(torch.tensor(scale1))\n        self.scale2 = torch.nn.Parameter(torch.tensor(scale2))\n        self.bias = torch.nn.Parameter(torch.randn(bias_shape))\n        \n        # Weight initialization\n        self.weight = torch.nn.Parameter(\n            torch.empty(in_channels, out_channels, kernel_size, kernel_size, kernel_size)\n        )\n        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n\n    def forward(self, x):\n        B, C, D, H, W = x.shape\n        # Calculate output dimensions after transposed convolution\n        D_out = (D - 1) * self.stride - 2 * self.padding + self.kernel_size\n        H_out = (H - 1) * self.stride - 2 * self.padding + self.kernel_size\n        W_out = (W - 1) * self.stride - 2 * self.padding + self.kernel_size\n        \n        # Output after pooling\n        pool_out = torch.empty(\n            B, self.out_channels, \n            D_out // 2, H_out // 2, W_out // 2,\n            device=x.device, dtype=x.dtype\n        )\n        \n        # Grid configuration\n        total_elements = pool_out.numel()\n        grid = lambda _: (triton.cdiv(total_elements, 256),)\n        \n        # Launch fused kernel\n        self.triton_fused_kernel[grid](\n            x, self.weight, self.bias,\n            pool_out,\n            self.scale1, self.scale2,\n            B, C, D, H, W,\n            D_out, H_out, W_out,\n            self.stride, self.padding,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),\n            pool_out.stride(0), pool_out.stride(1), pool_out.stride(2), pool_out.stride(3), pool_out.stride(4),\n            total_elements,\n        )\n        return pool_out\n\n    @triton.jit\n    def triton_fused_kernel(\n        x_ptr, weight_ptr, bias_ptr, out_ptr,\n        scale1, scale2,\n        B, C, D, H, W,\n        D_out, H_out, W_out,\n        stride, padding,\n        x_bs, x_cs, x_ds, x_hs, x_ws,\n        out_bs, out_cs, out_ds, out_hs, out_ws,\n        total_elements,\n        BLOCK_SIZE: tl.constexpr,\n    ):\n        pid = tl.program_id(0)\n        idx = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        mask = idx < total_elements\n        \n        # Decompose output index\n        b_idx = idx // ( (D_out//2) * (H_out//2) * (W_out//2) * C ) \n        remainder = idx % ( (D_out//2) * (H_out//2) * (W_out//2) * C )\n        c_idx = remainder // ( (D_out//2) * (H_out//2) * (W_out//2) )\n        remainder = remainder % ( (D_out//2) * (H_out//2) * (W_out//2) )\n        d_pool = remainder // ( (H_out//2) * (W_out//2) )\n        remainder = remainder % ( (H_out//2) * (W_out//2) )\n        h_pool = remainder // (W_out//2)\n        w_pool = remainder % (W_out//2)\n        \n        # Convert to transposed conv coordinates\n        d_out = d_pool * 2\n        h_out = h_pool * 2\n        w_out = w_pool * 2\n        \n        # Initialize accumulator\n        acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        scale1_val = tl.load(scale1)\n        scale2_val = tl.load(scale2)\n        \n        # Compute transposed convolution\n        for kd in range(3):\n            for kh in range(3):\n                for kw in range(3):\n                    # Calculate input indices\n                    d_in = (d_out + kd - padding) // stride\n                    h_in = (h_out + kh - padding) // stride\n                    w_in = (w_out + kw - padding) // stride\n                    \n                    # Check divisibility and bounds\n                    valid_d = (d_out + kd - padding) % stride == 0\n                    valid_h = (h_out + kh - padding) % stride == 0\n                    valid_w = (w_out + kw - padding) % stride == 0\n                    in_bounds = (d_in >= 0) & (d_in < D) & (h_in >= 0) & (h_in < H) & (w_in >= 0) & (w_in < W)\n                    valid_mask = valid_d & valid_h & valid_w & in_bounds\n                    \n                    if tl.sum(valid_mask) > 0:\n                        # Load input\n                        x_offset = b_idx*x_bs + d_in*x_ds + h_in*x_hs + w_in*x_ws\n                        x_val = tl.load(x_ptr + x_offset, mask=valid_mask, other=0.0)\n                        \n                        # Load weight\n                        w_offset = c_idx*C*27 + kd*9 + kh*3 + kw\n                        w_val = tl.load(weight_ptr + w_offset, mask=valid_mask, other=0.0)\n                        \n                        acc += x_val * w_val\n        \n        # Apply first scaling\n        acc = acc * scale1_val\n        \n        # Average pooling\n        pool_val = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        count = tl.zeros((BLOCK_SIZE,), dtype=tl.int32)\n        \n        for pd in range(2):\n            for ph in range(2):\n                for pw in range(2):\n                    d = d_out + pd\n                    h = h_out + ph\n                    w = w_out + pw\n                    in_bounds = (d < D_out) & (h < H_out) & (w < W_out)\n                    if tl.sum(in_bounds) > 0:\n                        pool_val += tl.where(in_bounds, acc, 0.0)\n                        count += tl.where(in_bounds, 1, 0)\n        \n        pool_val = pool_val / tl.maximum(count, 1)\n        \n        # Add bias and apply second scaling\n        bias_val = tl.load(bias_ptr + c_idx)\n        result = (pool_val + bias_val) * scale2_val\n        \n        # Store final result\n        out_offset = b_idx*out_bs + c_idx*out_cs + d_pool*out_ds + h_pool*out_hs + w_pool*out_ws\n        tl.store(out_ptr + out_offset, result, mask=mask)\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\nscale1 = 0.5\nscale2 = 1.0\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "f6bd4bd1-b1bd-4cca-a516-0639f0480ae5", "generation": 1, "timestamp": 1754574966.195701, "iteration_found": 7, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_50_ConvTranspose3d_Scaling_AvgPool_BiasAdd_Scaling", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpig5qjy1_.py\", line 48, in forward\n    self.triton_fused_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 561, in run\n    bound_args, specialization, options = binder(*args, **kwargs)\nTypeError: dynamic_func() missing 1 required positional argument: 'BLOCK_SIZE'\n", "runtime_error_name": "builtins.TypeError", "reference_timing_ms": 0.093, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 5.72, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.093, "speed_up": 0.016, "custom_timing": 5.72}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
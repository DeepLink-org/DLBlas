{"id": "a716ad48-6a5e-4c37-8d7d-c9888832313f", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit(num_warps=8)\ndef conv1x1_kernel(\n    x_ptr, w_ptr, output_ptr,\n    in_channels, out_channels, H, W,\n    stride_xb, stride_xc, stride_xh, stride_xw,\n    stride_woc, stride_wic,\n    stride_ob, stride_oc, stride_oh, stride_ow,\n    BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid_hw = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    n_offset = pid_n * BLOCK_N\n    n_mask = n_offset + tl.arange(0, BLOCK_N) < out_channels\n    \n    b = pid_hw // (H * W)\n    hw = pid_hw % (H * W)\n    h = hw // W\n    w = hw % W\n    \n    acc = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    for k in range(0, tl.cdiv(in_channels, BLOCK_K)):\n        k_offset = k * BLOCK_K\n        k_mask = k_offset + tl.arange(0, BLOCK_K) < in_channels\n        \n        # Vectorized load for input with cache optimization\n        x_ptr_offset = b*stride_xb + h*stride_xh + w*stride_xw + (k_offset + tl.arange(0, BLOCK_K))*stride_xc\n        x_vals = tl.load(x_ptr + x_ptr_offset, mask=k_mask, other=0.0, cache_modifier=\".cg\")\n        \n        # Optimized weight loading with vectorization\n        w_ptr_offset = (n_offset + tl.arange(0, BLOCK_N)[:, None])*stride_woc + (k_offset + tl.arange(0, BLOCK_K)[None, :])*stride_wic\n        w_vals = tl.load(w_ptr + w_ptr_offset, mask=n_mask[:, None] & k_mask[None, :], other=0.0, cache_modifier=\".cg\")\n        \n        # Tensor core-optimized matrix multiplication\n        acc += tl.dot(w_vals, x_vals, allow_tf32=True)\n    \n    out_ptr_offset = b*stride_ob + h*stride_oh + w*stride_ow + (n_offset + tl.arange(0, BLOCK_N))*stride_oc\n    tl.store(output_ptr + out_ptr_offset, acc, mask=n_mask)\n\nclass TritonConv1x1(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n    \n    def forward(self, x):\n        batch_size, _, H, W = x.shape\n        output = torch.empty((batch_size, self.out_channels, H, W), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size * H * W, triton.cdiv(self.out_channels, 128))\n        conv1x1_kernel[grid](\n            x, self.weight, output,\n            self.in_channels, self.out_channels, H, W,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            BLOCK_N=128, BLOCK_K=64\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio):\n        super(ModelNew, self).__init__()\n        self.use_residual = (stride == 1 and in_channels == out_channels)\n        hidden_dim = in_channels * expand_ratio\n        \n        if expand_ratio != 1:\n            self.expand_conv = nn.Sequential(\n                TritonConv1x1(in_channels, hidden_dim),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True)\n            )\n        \n        self.depthwise_conv = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, stride=stride, \n                      padding=(kernel_size-1)//2, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True)\n        )\n        \n        self.project_conv = nn.Sequential(\n            TritonConv1x1(hidden_dim, out_channels),\n            nn.BatchNorm2d(out_channels)\n        )\n    \n    def forward(self, x):\n        identity = x\n        \n        if hasattr(self, 'expand_conv'):\n            x = self.expand_conv(x)\n        \n        x = self.depthwise_conv(x)\n        x = self.project_conv(x)\n        \n        if self.use_residual:\n            x += identity\n        \n        return x\n\nimport math\nbatch_size = 10\nin_channels = 112\nout_channels = 192\nkernel_size = 5\nstride = 2\nexpand_ratio = 6\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, 224, 224)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, expand_ratio]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "a797e2a0-08f2-4421-96ce-fcbb3d75c5d3", "generation": 3, "timestamp": 1754591613.8620849, "iteration_found": 27, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "", "exec_log": "[Child] \u7ed1\u5b9a GPU 7, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 7, in <module>\nTypeError: jit() got an unexpected keyword argument 'num_warps'\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.005, "runtime_ms": 47.4, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.223, "speed_up": 0.005, "custom_timing": 47.4}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
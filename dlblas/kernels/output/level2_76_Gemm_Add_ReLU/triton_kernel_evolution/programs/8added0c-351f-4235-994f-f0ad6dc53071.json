{"id": "8added0c-351f-4235-994f-f0ad6dc53071", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_gemm_bias_relu_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    out_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_bn,\n    stride_om, stride_on,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_n[None, :] * stride_wn + offs_k[:, None] * stride_wk\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_valid = tl.minimum(BLOCK_K, k_remaining)\n        k_mask = offs_k < k_valid\n        \n        x_chunk = tl.load(\n            x_ptrs, \n            mask=k_mask[None, :] & (offs_m[:, None] < M), \n            other=0.0\n        )\n        w_chunk = tl.load(\n            w_ptrs, \n            mask=k_mask[:, None] & (offs_n[None, :] < N), \n            other=0.0\n        )\n        \n        acc += tl.dot(x_chunk, w_chunk, allow_tf32=True)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    \n    b_ptrs = b_ptr + offs_n\n    bias = tl.load(b_ptrs, mask=offs_n < N, other=0.0)\n    acc += bias[None, :]\n    acc = tl.where(acc > 0, acc, 0.0)\n    \n    out_ptrs = out_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=False)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n    \n    def forward(self, x):\n        M, K = x.shape\n        N = self.bias.shape[0]\n        \n        out = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        BLOCK_M, BLOCK_N, BLOCK_K = 64, 64, 32\n        grid = (\n            triton.cdiv(M, BLOCK_M),\n            triton.cdiv(N, BLOCK_N),\n        )\n        \n        fused_gemm_bias_relu_kernel[grid](\n            x,\n            self.gemm.weight,\n            self.bias,\n            out,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.gemm.weight.stride(0), self.gemm.weight.stride(1),\n            self.bias.stride(0),\n            out.stride(0), out.stride(1),\n            BLOCK_M=BLOCK_M,\n            BLOCK_N=BLOCK_N,\n            BLOCK_K=BLOCK_K\n        )\n        \n        return out\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "5fe87b5d-9b80-492d-8f25-3512981f2cb8", "generation": 3, "timestamp": 1754582247.3933144, "iteration_found": 16, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.887, "runtime_ms": 0.0619, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0549, "speed_up": 0.887, "custom_timing": 0.0619}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
{"id": "a6dc2e7c-f51c-4653-9180-b9978f6b42d7", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit(num_warps=8)\ndef _fused_gemm_relu_kernel(\n    x_ptr,\n    w_ptr,\n    bias_ptr,\n    output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wk, stride_wn,  # Note: swapped order for transposed weight\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Input matrix access (coalesced)\n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    \n    # Weight matrix access (transposed for coalescing)\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        k_valid = tl.minimum(BLOCK_SIZE_K, k_remaining)\n        \n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_valid)\n        b_mask = (offs_k[:, None] < k_valid) & (offs_n[None, :] < N)\n        \n        a = tl.load(x_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(w_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n    \n    bias_ptrs = bias_ptr + offs_n\n    bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n    acc += bias[None, :]\n    acc = tl.where(acc > 0, acc, 0.0)\n    \n    output_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(output_ptrs, acc, mask=mask)\n\ndef fused_gemm_relu(x, w, bias):\n    M, K = x.shape\n    _, N = w.shape  # w is now (K, N) after transpose\n    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n    grid = (\n        triton.cdiv(M, 64),\n        triton.cdiv(N, 64),\n    )\n    _fused_gemm_relu_kernel[grid](\n        x, w, bias, output,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        w.stride(0), w.stride(1),  # Strides for transposed weight\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64  # Increased K-block\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias_shape):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n    \n    def forward(self, x):\n        # Transpose weight for efficient memory access\n        weight_t = self.weight.t().contiguous()\n        return fused_gemm_relu(x, weight_t, self.bias)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "55faba5b-d725-4a87-81c1-51e4ae58c0f5", "generation": 3, "timestamp": 1754584404.4084783, "iteration_found": 24, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "", "exec_log": "[Child] \u7ed1\u5b9a GPU 0, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 8, in <module>\nTypeError: jit() got an unexpected keyword argument 'num_warps'\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.945, "runtime_ms": 0.0581, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0549, "speed_up": 0.945, "custom_timing": 0.0581}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.9450\n- runtime_ms: 0.0581\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0549, 'speed_up': 0.945, 'custom_timing': 0.0581}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9450, runtime_ms: 0.0581, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0549, 'speed_up': 0.945, 'custom_timing': 0.0581}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.8760, runtime_ms: 0.0627, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0549, 'speed_up': 0.876, 'custom_timing': 0.0627}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7680, runtime_ms: 0.0715, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0549, 'speed_up': 0.768, 'custom_timing': 0.0715}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_gemm_bias_relu_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    batch_size,\n    in_features,\n    out_features,\n    stride_x0,\n    stride_x1,\n    stride_w0,\n    stride_w1,\n    stride_output0,\n    stride_output1,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bm = tl.program_id(0)\n    pid_bn = tl.program_id(1)\n    \n    offs_bm = pid_bm * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_bn * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_bm[:, None] * stride_x0 + offs_k[None, :] * stride_x1\n    w_ptrs = w_ptr + offs_k[:, None] * stride_w0 + offs_bn[None, :] * stride_w1\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_remaining = in_features - k * BLOCK_SIZE_K\n        k_valid = min(BLOCK_SIZE_K, k_remaining)\n        \n        x_mask = (offs_bm[:, None] < batch_size) & (offs_k[None, :] < k_valid)\n        w_mask = (offs_k[:, None] < k_valid) & (offs_bn[None, :] < out_features)\n        \n        x_block = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x_block, w_block)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_x1\n        w_ptrs += BLOCK_SIZE_K * stride_w0\n    \n    b_ptrs = b_ptr + offs_bn\n    b_mask = offs_bn < out_features\n    b_block = tl.load(b_ptrs, mask=b_mask, other=0.0)\n    \n    acc += b_block[None, :]\n    acc = tl.maximum(acc, 0.0)\n    \n    offs_output = offs_bm[:, None] * stride_output0 + offs_bn[None, :] * stride_output1\n    output_mask = (offs_bm[:, None] < batch_size) & (offs_bn[None, :] < out_features)\n    tl.store(output_ptr + offs_output, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=False)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n    \n    def forward(self, x):\n        batch_size, _ = x.shape\n        out_features = self.bias.shape[0]\n        \n        output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_M = 32\n        BLOCK_SIZE_N = 64\n        BLOCK_SIZE_K = 32\n        \n        grid = (\n            triton.cdiv(batch_size, BLOCK_SIZE_M),\n            triton.cdiv(out_features, BLOCK_SIZE_N),\n        )\n        \n        # Transpose weight matrix for correct memory access\n        weight_t = self.gemm.weight.t().contiguous()\n        \n        _fused_gemm_bias_relu_kernel[grid](\n            x,\n            weight_t,\n            self.bias,\n            output,\n            batch_size,\n            self.gemm.weight.size(1),\n            out_features,\n            x.stride(0),\n            x.stride(1),\n            weight_t.stride(0),\n            weight_t.stride(1),\n            output.stride(0),\n            output.stride(1),\n            BLOCK_SIZE_M=BLOCK_SIZE_M,\n            BLOCK_SIZE_N=BLOCK_SIZE_N,\n            BLOCK_SIZE_K=BLOCK_SIZE_K,\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7680), Performs well on runtime_ms (0.0715), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0549, 'speed_up': 0.768, 'custom_timing': 0.0715})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_gemm_bias_relu_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    batch_size,\n    in_features,\n    out_features,\n    stride_x0,\n    stride_x1,\n    stride_w0,\n    stride_w1,\n    stride_output0,\n    stride_output1,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bm = tl.program_id(0)\n    pid_bn = tl.program_id(1)\n    \n    offs_bm = pid_bm * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_bn * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_bm[:, None] * stride_x0 + offs_k[None, :] * stride_x1\n    w_ptrs = w_ptr + offs_k[:, None] * stride_w0 + offs_bn[None, :] * stride_w1\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_remaining = in_features - k * BLOCK_SIZE_K\n        k_valid = tl.minimum(BLOCK_SIZE_K, k_remaining)\n        \n        x_mask = (offs_bm[:, None] < batch_size) & (offs_k[None, :] < k_valid)\n        w_mask = (offs_k[:, None] < k_valid) & (offs_bn[None, :] < out_features)\n        \n        x_block = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x_block, w_block, allow_tf32=True)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_x1\n        w_ptrs += BLOCK_SIZE_K * stride_w0\n    \n    b_ptrs = b_ptr + offs_bn\n    b_mask = offs_bn < out_features\n    b_block = tl.load(b_ptrs, mask=b_mask, other=0.0)\n    \n    acc += b_block[None, :]\n    acc = tl.maximum(acc, 0.0)\n    \n    offs_output = offs_bm[:, None] * stride_output0 + offs_bn[None, :] * stride_output1\n    output_mask = (offs_bm[:, None] < batch_size) & (offs_bn[None, :] < out_features)\n    tl.store(output_ptr + offs_output, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=False)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n    \n    def forward(self, x):\n        batch_size, _ = x.shape\n        out_features = self.bias.shape[0]\n        \n        output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_M = 32\n        BLOCK_SIZE_N = 64\n        BLOCK_SIZE_K = 32\n        \n        grid = (\n            triton.cdiv(batch_size, BLOCK_SIZE_M),\n            triton.cdiv(out_features, BLOCK_SIZE_N),\n        )\n        \n        _fused_gemm_bias_relu_kernel[grid](\n            x,\n            self.gemm.weight,\n            self.bias,\n            output,\n            batch_size,\n            self.gemm.weight.size(1),\n            out_features,\n            x.stride(0),\n            x.stride(1),\n            self.gemm.weight.stride(1),\n            self.gemm.weight.stride(0),\n            output.stride(0),\n            output.stride(1),\n            BLOCK_SIZE_M=BLOCK_SIZE_M,\n            BLOCK_SIZE_N=BLOCK_SIZE_N,\n            BLOCK_SIZE_K=BLOCK_SIZE_K,\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.8760), Performs well on runtime_ms (0.0627), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0549, 'speed_up': 0.876, 'custom_timing': 0.0627})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_gemm_relu_kernel(\n    x_ptr,\n    w_ptr,\n    bias_ptr,\n    output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        k_valid = tl.minimum(BLOCK_SIZE_K, k_remaining)\n        \n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_valid)\n        b_mask = (offs_k[:, None] < k_valid) & (offs_n[None, :] < N)\n        \n        a = tl.load(x_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(w_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n    \n    bias_ptrs = bias_ptr + offs_n\n    bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n    acc += bias[None, :]\n    acc = tl.where(acc > 0, acc, 0.0)\n    \n    output_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(output_ptrs, acc, mask=mask)\n\ndef fused_gemm_relu(x, w, bias):\n    M, K = x.shape\n    N, _ = w.shape\n    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n    grid = (\n        triton.cdiv(M, 64),\n        triton.cdiv(N, 64),\n    )\n    _fused_gemm_relu_kernel[grid](\n        x, w, bias, output,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        w.stride(0), w.stride(1),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias_shape):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n    \n    def forward(self, x):\n        x = fused_gemm_relu(x, self.weight, self.bias)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9450), Performs well on runtime_ms (0.0581), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0549, 'speed_up': 0.945, 'custom_timing': 0.0581})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_gemm_bias_relu_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    batch_size,\n    in_features,\n    out_features,\n    stride_x0,\n    stride_x1,\n    stride_w0,\n    stride_w1,\n    stride_output0,\n    stride_output1,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bm = tl.program_id(0)\n    pid_bn = tl.program_id(1)\n    \n    offs_bm = pid_bm * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_bn * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_bm[:, None] * stride_x0 + offs_k[None, :] * stride_x1\n    w_ptrs = w_ptr + offs_k[:, None] * stride_w0 + offs_bn[None, :] * stride_w1\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_remaining = in_features - k * BLOCK_SIZE_K\n        k_valid = min(BLOCK_SIZE_K, k_remaining)\n        \n        x_mask = (offs_bm[:, None] < batch_size) & (offs_k[None, :] < k_valid)\n        w_mask = (offs_k[:, None] < k_valid) & (offs_bn[None, :] < out_features)\n        \n        x_block = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x_block, w_block)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_x1\n        w_ptrs += BLOCK_SIZE_K * stride_w0\n    \n    b_ptrs = b_ptr + offs_bn\n    b_mask = offs_bn < out_features\n    b_block = tl.load(b_ptrs, mask=b_mask, other=0.0)\n    \n    acc += b_block[None, :]\n    acc = tl.maximum(acc, 0.0)\n    \n    offs_output = offs_bm[:, None] * stride_output0 + offs_bn[None, :] * stride_output1\n    output_mask = (offs_bm[:, None] < batch_size) & (offs_bn[None, :] < out_features)\n    tl.store(output_ptr + offs_output, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=False)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n    \n    def forward(self, x):\n        batch_size, _ = x.shape\n        out_features = self.bias.shape[0]\n        \n        output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_M = 32\n        BLOCK_SIZE_N = 64\n        BLOCK_SIZE_K = 32\n        \n        grid = (\n            triton.cdiv(batch_size, BLOCK_SIZE_M),\n            triton.cdiv(out_features, BLOCK_SIZE_N),\n        )\n        \n        _fused_gemm_bias_relu_kernel[grid](\n            x,\n            self.gemm.weight,\n            self.bias,\n            output,\n            batch_size,\n            self.gemm.weight.size(1),\n            out_features,\n            x.stride(0),\n            x.stride(1),\n            self.gemm.weight.stride(0),\n            self.gemm.weight.stride(1),\n            output.stride(0),\n            output.stride(1),\n            BLOCK_SIZE_M=BLOCK_SIZE_M,\n            BLOCK_SIZE_N=BLOCK_SIZE_N,\n            BLOCK_SIZE_K=BLOCK_SIZE_K,\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_gemm_relu_kernel(\n    x_ptr,\n    w_ptr,\n    bias_ptr,\n    output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        k_valid = tl.minimum(BLOCK_SIZE_K, k_remaining)\n        \n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_valid)\n        b_mask = (offs_n[:, None] < N) & (offs_k[None, :] < k_valid)\n        \n        a = tl.load(x_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(w_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n    \n    bias_ptrs = bias_ptr + offs_n\n    bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n    acc += bias[None, :]\n    acc = tl.where(acc > 0, acc, 0.0)\n    \n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    output_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(output_ptrs, acc, mask=mask)\n\ndef fused_gemm_relu(x, w, bias):\n    M, K = x.shape\n    N, _ = w.shape\n    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n    grid = (\n        triton.cdiv(M, 64),\n        triton.cdiv(N, 64),\n    )\n    _fused_gemm_relu_kernel[grid](\n        x, w, bias, output,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        w.stride(0), w.stride(1),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias_shape):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n    \n    def forward(self, x):\n        x = fused_gemm_relu(x, self.weight, self.bias)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_gemm_bias_relu_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    batch_size,\n    in_features,\n    out_features,\n    stride_x0,\n    stride_x1,\n    stride_w0,\n    stride_w1,\n    stride_output0,\n    stride_output1,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bm = tl.program_id(0)\n    pid_bn = tl.program_id(1)\n    \n    offs_bm = pid_bm * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_bn * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_bm[:, None] * stride_x0 + offs_k[None, :] * stride_x1\n    w_ptrs = w_ptr + offs_k[:, None] * stride_w0 + offs_bn[None, :] * stride_w1\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_remaining = in_features - k * BLOCK_SIZE_K\n        k_valid = min(BLOCK_SIZE_K, k_remaining)\n        \n        x_mask = (offs_bm[:, None] < batch_size) & (offs_k[None, :] < k_valid)\n        w_mask = (offs_k[:, None] < k_valid) & (offs_bn[None, :] < out_features)\n        \n        x_block = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x_block, w_block)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_x1\n        w_ptrs += BLOCK_SIZE_K * stride_w0\n    \n    b_ptrs = b_ptr + offs_bn\n    b_mask = offs_bn < out_features\n    b_block = tl.load(b_ptrs, mask=b_mask, other=0.0)\n    \n    acc += b_block[None, :]\n    acc = tl.maximum(acc, 0.0)\n    \n    offs_output = offs_bm[:, None] * stride_output0 + offs_bn[None, :] * stride_output1\n    output_mask = (offs_bm[:, None] < batch_size) & (offs_bn[None, :] < out_features)\n    tl.store(output_ptr + offs_output, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=False)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n    \n    def forward(self, x):\n        batch_size, _ = x.shape\n        out_features = self.bias.shape[0]\n        \n        output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_M = 32\n        BLOCK_SIZE_N = 64\n        BLOCK_SIZE_K = 32\n        \n        grid = (\n            triton.cdiv(batch_size, BLOCK_SIZE_M),\n            triton.cdiv(out_features, BLOCK_SIZE_N),\n        )\n        \n        # Transpose weight matrix for correct memory access\n        weight_t = self.gemm.weight.t().contiguous()\n        \n        _fused_gemm_bias_relu_kernel[grid](\n            x,\n            weight_t,\n            self.bias,\n            output,\n            batch_size,\n            self.gemm.weight.size(1),\n            out_features,\n            x.stride(0),\n            x.stride(1),\n            weight_t.stride(0),\n            weight_t.stride(1),\n            output.stride(0),\n            output.stride(1),\n            BLOCK_SIZE_M=BLOCK_SIZE_M,\n            BLOCK_SIZE_N=BLOCK_SIZE_N,\n            BLOCK_SIZE_K=BLOCK_SIZE_K,\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_gemm_bias_relu_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    batch_size,\n    in_features,\n    out_features,\n    stride_x0,\n    stride_x1,\n    stride_w0,\n    stride_w1,\n    stride_output0,\n    stride_output1,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bm = tl.program_id(0)\n    pid_bn = tl.program_id(1)\n    \n    offs_bm = pid_bm * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_bn * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_bm[:, None] * stride_x0 + offs_k[None, :] * stride_x1\n    w_ptrs = w_ptr + offs_k[:, None] * stride_w0 + offs_bn[None, :] * stride_w1\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_remaining = in_features - k * BLOCK_SIZE_K\n        k_valid = min(BLOCK_SIZE_K, k_remaining)\n        \n        x_mask = (offs_bm[:, None] < batch_size) & (offs_k[None, :] < k_valid)\n        w_mask = (offs_k[:, None] < k_valid) & (offs_bn[None, :] < out_features)\n        \n        x_block = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x_block, w_block)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_x1\n        w_ptrs += BLOCK_SIZE_K * stride_w0\n    \n    b_ptrs = b_ptr + offs_bn\n    b_mask = offs_bn < out_features\n    b_block = tl.load(b_ptrs, mask=b_mask, other=0.0)\n    \n    acc += b_block[None, :]\n    acc = tl.maximum(acc, 0.0)\n    \n    offs_output = offs_bm[:, None] * stride_output0 + offs_bn[None, :] * stride_output1\n    output_mask = (offs_bm[:, None] < batch_size) & (offs_bn[None, :] < out_features)\n    tl.store(output_ptr + offs_output, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=False)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n    \n    def forward(self, x):\n        batch_size, _ = x.shape\n        out_features = self.bias.shape[0]\n        \n        output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_M = 32\n        BLOCK_SIZE_N = 64\n        BLOCK_SIZE_K = 32\n        \n        grid = (\n            triton.cdiv(batch_size, BLOCK_SIZE_M),\n            triton.cdiv(out_features, BLOCK_SIZE_N),\n        )\n        \n        _fused_gemm_bias_relu_kernel[grid](\n            x,\n            self.gemm.weight,\n            self.bias,\n            output,\n            batch_size,\n            self.gemm.weight.size(1),\n            out_features,\n            x.stride(0),\n            x.stride(1),\n            self.gemm.weight.stride(0),\n            self.gemm.weight.stride(1),\n            output.stride(0),\n            output.stride(1),\n            BLOCK_SIZE_M=BLOCK_SIZE_M,\n            BLOCK_SIZE_N=BLOCK_SIZE_N,\n            BLOCK_SIZE_K=BLOCK_SIZE_K,\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_gemm_bias_relu_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    batch_size,\n    in_features,\n    out_features,\n    stride_x0,\n    stride_x1,\n    stride_w0,\n    stride_w1,\n    stride_output0,\n    stride_output1,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bm = tl.program_id(0)\n    pid_bn = tl.program_id(1)\n    \n    offs_bm = pid_bm * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_bn * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_bm[:, None] * stride_x0 + offs_k[None, :] * stride_x1\n    w_ptrs = w_ptr + offs_k[:, None] * stride_w0 + offs_bn[None, :] * stride_w1\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_remaining = in_features - k * BLOCK_SIZE_K\n        k_valid = tl.minimum(BLOCK_SIZE_K, k_remaining)\n        \n        x_mask = (offs_bm[:, None] < batch_size) & (offs_k[None, :] < k_valid)\n        w_mask = (offs_k[:, None] < k_valid) & (offs_bn[None, :] < out_features)\n        \n        x_block = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x_block, w_block, allow_tf32=True)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_x1\n        w_ptrs += BLOCK_SIZE_K * stride_w0\n    \n    b_ptrs = b_ptr + offs_bn\n    b_mask = offs_bn < out_features\n    b_block = tl.load(b_ptrs, mask=b_mask, other=0.0)\n    \n    acc += b_block[None, :]\n    acc = tl.maximum(acc, 0.0)\n    \n    offs_output = offs_bm[:, None] * stride_output0 + offs_bn[None, :] * stride_output1\n    output_mask = (offs_bm[:, None] < batch_size) & (offs_bn[None, :] < out_features)\n    tl.store(output_ptr + offs_output, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=False)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n    \n    def forward(self, x):\n        batch_size, _ = x.shape\n        out_features = self.bias.shape[0]\n        \n        output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_M = 32\n        BLOCK_SIZE_N = 64\n        BLOCK_SIZE_K = 32\n        \n        grid = (\n            triton.cdiv(batch_size, BLOCK_SIZE_M),\n            triton.cdiv(out_features, BLOCK_SIZE_N),\n        )\n        \n        _fused_gemm_bias_relu_kernel[grid](\n            x,\n            self.gemm.weight,\n            self.bias,\n            output,\n            batch_size,\n            self.gemm.weight.size(1),\n            out_features,\n            x.stride(0),\n            x.stride(1),\n            self.gemm.weight.stride(1),\n            self.gemm.weight.stride(0),\n            output.stride(0),\n            output.stride(1),\n            BLOCK_SIZE_M=BLOCK_SIZE_M,\n            BLOCK_SIZE_N=BLOCK_SIZE_N,\n            BLOCK_SIZE_K=BLOCK_SIZE_K,\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 4 (Score: 1.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit(num_warps=8)\ndef _fused_gemm_bias_relu_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    batch_size,\n    in_features,\n    out_features,\n    stride_x0,\n    stride_x1,\n    stride_w0,\n    stride_w1,\n    stride_output0,\n    stride_output1,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bm = tl.program_id(0)\n    pid_bn = tl.program_id(1)\n    \n    offs_bm = pid_bm * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_bn * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_bm[:, None] * stride_x0 + offs_k[None, :] * stride_x1\n    w_ptrs = w_ptr + offs_k[:, None] * stride_w0 + offs_bn[None, :] * stride_w1\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_remaining = in_features - k * BLOCK_SIZE_K\n        k_valid = min(BLOCK_SIZE_K, k_remaining)\n        \n        x_mask = (offs_bm[:, None] < batch_size) & (offs_k[None, :] < k_valid)\n        w_mask = (offs_k[:, None] < k_valid) & (offs_bn[None, :] < out_features)\n        \n        x_block = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x_block, w_block, allow_tf32=True)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_x1\n        w_ptrs += BLOCK_SIZE_K * stride_w0\n    \n    b_ptrs = b_ptr + offs_bn\n    b_mask = offs_bn < out_features\n    b_block = tl.load(b_ptrs, mask=b_mask, other=0.0)\n    \n    acc += b_block[None, :]\n    acc = tl.maximum(acc, 0.0)\n    \n    offs_output = offs_bm[:, None] * stride_output0 + offs_bn[None, :] * stride_output1\n    output_mask = (offs_bm[:, None] < batch_size) & (offs_bn[None, :] < out_features)\n    tl.store(output_ptr + offs_output, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=False)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n    \n    def forward(self, x):\n        batch_size, _ = x.shape\n        out_features = self.bias.shape[0]\n        \n        output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_M = 64\n        BLOCK_SIZE_N = 64\n        BLOCK_SIZE_K = 32\n        \n        grid = (\n            triton.cdiv(batch_size, BLOCK_SIZE_M),\n            triton.cdiv(out_features, BLOCK_SIZE_N),\n        )\n        \n        weight_t = self.gemm.weight.t().contiguous()\n        \n        _fused_gemm_bias_relu_kernel[grid](\n            x,\n            weight_t,\n            self.bias,\n            output,\n            batch_size,\n            self.gemm.weight.size(1),\n            out_features,\n            x.stride(0),\n            x.stride(1),\n            weight_t.stride(0),\n            weight_t.stride(1),\n            output.stride(0),\n            output.stride(1),\n            BLOCK_SIZE_M=BLOCK_SIZE_M,\n            BLOCK_SIZE_N=BLOCK_SIZE_N,\n            BLOCK_SIZE_K=BLOCK_SIZE_K,\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_gemm_relu_kernel(\n    x_ptr,\n    w_ptr,\n    bias_ptr,\n    output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        k_valid = tl.minimum(BLOCK_SIZE_K, k_remaining)\n        \n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_valid)\n        b_mask = (offs_n[:, None] < N) & (offs_k[None, :] < k_valid)\n        \n        a = tl.load(x_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(w_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n    \n    bias_ptrs = bias_ptr + offs_n\n    bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n    acc += bias[None, :]\n    acc = tl.where(acc > 0, acc, 0.0)\n    \n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    output_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(output_ptrs, acc, mask=mask)\n\ndef fused_gemm_relu(x, w, bias):\n    M, K = x.shape\n    N, _ = w.shape\n    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n    grid = (\n        triton.cdiv(M, 64),\n        triton.cdiv(N, 64),\n    )\n    _fused_gemm_relu_kernel[grid](\n        x, w, bias, output,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        w.stride(0), w.stride(1),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias_shape):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n    \n    def forward(self, x):\n        x = fused_gemm_relu(x, self.weight, self.bias)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_gemm_relu_kernel(\n    x_ptr,\n    w_ptr,\n    bias_ptr,\n    output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        k_valid = tl.minimum(BLOCK_SIZE_K, k_remaining)\n        \n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_valid)\n        b_mask = (offs_k[:, None] < k_valid) & (offs_n[None, :] < N)\n        \n        a = tl.load(x_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(w_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n    \n    bias_ptrs = bias_ptr + offs_n\n    bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n    acc += bias[None, :]\n    acc = tl.where(acc > 0, acc, 0.0)\n    \n    output_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(output_ptrs, acc, mask=mask)\n\ndef fused_gemm_relu(x, w, bias):\n    M, K = x.shape\n    N, _ = w.shape\n    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n    grid = (\n        triton.cdiv(M, 64),\n        triton.cdiv(N, 64),\n    )\n    _fused_gemm_relu_kernel[grid](\n        x, w, bias, output,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        w.stride(0), w.stride(1),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias_shape):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n    \n    def forward(self, x):\n        x = fused_gemm_relu(x, self.weight, self.bias)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program which has a speed_up of 0.945 (which is less than 1, meaning it's slower than the reference).\n The goal is to improve the performance (speed_up) to be above 1 and as high as possible.\n\n The current program uses a Triton kernel for fused GEMM + ReLU. The kernel uses:\n   BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n\n The hardware is NVIDIA L20Y / H800 (SM8.9, same as H100) with FP16 Tensor Cores (32x16x16).\n\n The current kernel uses `allow_tf32=True` in the dot product. However, note that the input tensors are in float32 (since the model uses float32).\n But the H100 has TF32 Tensor Cores which can accelerate float32 matrix multiplications.\n\n However, we note that the current block sizes (64x64x32) might not be optimal for Tensor Cores.\n\n Recommendations for Tensor Core performance:\n - The Tensor Core operation is 32x16x16 for FP16, but for TF32 it is also 32x16x16? Actually, TF32 uses the same Tensor Cores as FP16 but with different precision.\n - For TF32, the recommended block sizes should be multiples of 16 in the K dimension and multiples of 64 in M and N to cover the warp-level 32x16x16 operations.\n\n However, note that Triton's dot operation for TF32 requires that the block sizes in M and N are multiples of 16 (or 32) and K multiple of 8 (or 16). But we are using 64x64x32, which is acceptable.\n\n Alternatively, we can try to adjust the block sizes to better fit the Tensor Cores.\n\n But note: the current grid is set to (triton.cdiv(M, 64), triton.cdiv(N, 64)). The problem size is M=128, N=512, K=1024.\n\n We might also consider using different block sizes for M and N to better match the problem.\n\n Another idea: use a different tiling strategy. For example, we can use a larger block in the K dimension to reduce the number of iterations? But note that the kernel uses a loop over K.\n\n However, the current BLOCK_SIZE_K=32, and K=1024, so we have 1024/32=32 iterations.\n\n We can try increasing BLOCK_SIZE_K to 64 or 128 to reduce the number of iterations, but note that the register pressure might increase.\n\n Alternatively, we can try to use smaller blocks in M and N to increase parallelism? But note that M is only 128, so if we reduce M block, we get more blocks in M.\n\n Actually, the grid has:\n   M_blocks = ceil(128/64)=2, N_blocks=ceil(512/64)=8 -> 16 blocks total.\n\n We might try to use a block size of 128 in N? But then N_blocks=ceil(512/128)=4, total blocks=2*4=8, which is less parallelism.\n\n We want to have enough blocks to fill the GPU. The H100 has many SMs (e.g., 132 SMs), so 16 blocks might be too few? But note the problem is small.\n\n However, we are constrained by the problem size. We cannot change the problem size.\n\n Another idea: use a different tiling for the GEMM. We can use a block size that is more efficient for the Tensor Cores, such as 128x128x32? But then the number of blocks would be 1 (for M=128) and 4 (for N=512) -> 4 blocks, which is even worse.\n\n Alternatively, we can try to use a block size that is 64x128x32? Then M_blocks=2, N_blocks=ceil(512/128)=4 -> 8 blocks. This might be worse than 16.\n\n How about 32x32x32? Then M_blocks=4, N_blocks=16 -> 64 blocks, which is better.\n\n But note: the kernel uses two-dimensional grid. We are currently using 64x64x32. Let's try 32x64x32? Then M_blocks=ceil(128/32)=4, N_blocks=ceil(512/64)=8 -> 32 blocks.\n\n However, the Tensor Cores work best when the block sizes are multiples of 64 in M and N? Actually, the warp-level tile for TF32 is 32x16x16? So a block of 32x64 might not be ideal because 32 is the minimum in M and 64 in N is 4 warps? We need to consider the warp layout.\n\n Alternatively, we can try to use a block size of 64x64x64 to reduce the number of iterations (from 32 to 16) and see if that helps.\n\n Also, note that the kernel does not use shared memory? Actually, the Triton dot operation uses shared memory internally? But we are not explicitly using shared memory for buffering.\n\n We can also try to adjust the number of warps? Currently, the kernel does not specify num_warps. The default might be 4? We can try 8 warps to better utilize the Tensor Cores.\n\n Another idea: use the input tensor in a transposed manner? The current kernel accesses the weight matrix in a way that might not be coalesced? The weight matrix is of shape (out_features, in_features). In the kernel, we have:\n   w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n The stride_wn is the stride for the out_features dimension? Actually, the weight is stored as (out_features, in_features). So stride_wk = stride for the inner dimension (in_features) and stride_wn = stride for the outer dimension (out_features). In row-major, stride_wn = in_features? Actually, we pass the strides as:\n        w.stride(0), w.stride(1)   -> which are (in_features, 1) if row-major? But note: the weight is stored as (out_features, in_features) and is row-major, so stride(0)=in_features, stride(1)=1.\n\n The kernel accesses:\n   w_ptrs = w_ptr + offs_k[:, None] * stride_wk (which is stride(1)=1) + offs_n[None, :] * stride_wn (which is stride(0)=in_features)\n\n So we are reading the weight matrix in a column-major fashion? Actually, we are reading a row of the weight matrix for a fixed out_features index? But note: the weight matrix is row-major. To get a contiguous read, we want to read along the in_features dimension (which is contiguous). But in the kernel, we are reading with offs_k (which is the inner dimension) and then offs_n (which is the outer dimension). The reads for a fixed offs_n and varying offs_k are contiguous? Actually, the weight matrix is stored as:\n   [row0: w00, w01, ... w0(in_features-1)]\n   [row1: w10, w11, ...]\n\n We are reading a block of the weight matrix: for a block of rows (offs_n) and a block of columns (offs_k). The access pattern: for a fixed row, we read a contiguous block of columns? But note: the weight matrix is stored in row-major, so contiguous in columns. The kernel reads a block of the weight matrix by reading contiguous columns for each row. But the rows are separated by stride_wn (which is in_features). This is not contiguous for a 2D block? We might get poor memory access.\n\n Instead, we could transpose the weight matrix so that it is column-major? Then the contiguous reads would be along the rows of the transposed matrix (which are the columns of the original). But note: the current kernel expects the weight matrix to be passed with strides (stride(0), stride(1)) as (stride_wn, stride_wk) = (1, out_features) for the transposed weight? Actually, we are passing the original weight matrix.\n\n Alternatively, we can change the kernel to access the weight matrix in a row-major fashion? Currently, the kernel does:\n   w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n This is equivalent to:\n   w_ptr + [offs_k] (as rows) * stride_wk + [offs_n] (as columns) * stride_wn\n\n But if the weight matrix is row-major, then we want to read by rows? Actually, we want to read a block of the weight matrix that is contiguous. The best way is to have the block read along the contiguous dimension. The contiguous dimension of the weight matrix is the inner dimension (in_features). So we want to read a set of rows (offs_n) and a set of columns (offs_k) in a way that we read contiguous columns for each row? But that is not contiguous for the entire block.\n\n We could transpose the weight matrix and then read it in a column-major way? Then the contiguous reads would be along the rows of the transposed matrix (which are the columns of the original). This would be better for coalescing? And note that the GEMM operation is A (x) * B (w) -> if we transpose B, then we are doing A * B^T? Actually, no: the original operation is x * w^T? Actually, in the PyTorch linear layer, we do: x * w^T + b.\n\n So in the original reference, the weight matrix is stored as (out_features, in_features). The operation is: x (batch_size, in_features) * w (out_features, in_features)^T -> (batch_size, out_features).\n\n Therefore, we are effectively doing x * w^T. So if we pre-transpose the weight to (in_features, out_features) (column-major for the original w^T), then we can do a normal GEMM without transposing in the kernel? But note: the kernel is written for:\n   x: (M, K) and w: (K, N) -> output (M, N)\n\n But in our case, the weight matrix is (N, K) (with N=out_features, K=in_features). So to make it (K, N) we need to transpose it.\n\n In the current kernel, we are passing the weight matrix as (N, K) and then using strides (stride_wn, stride_wk) = (stride(0), stride(1)) = (K, 1). The kernel uses:\n   w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n            = w_ptr + offs_k * 1 + offs_n * K\n\n This is the same as w_ptr[offs_n, offs_k] (if we consider row-major). But we want to read a block of size (BLOCK_SIZE_K, BLOCK_SIZE_N). This access pattern: for a fixed offs_k, we read offs_n in the same row? Actually, it's reading a row of the block: for each offs_k and offs_n, we are reading one element. The problem is that for adjacent offs_n in the same offs_k, the addresses are separated by K (which is in_features=1024) elements. This is not coalesced.\n\n How about we pre-transpose the weight matrix to (in_features, out_features) (i.e., column-major) and then set strides accordingly? Then the kernel would access:\n   w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n   but now stride_wn = 1 (because the out_features is contiguous) and stride_wk = out_features.\n\n Then the access would be: w_ptr + offs_k * out_features + offs_n. Now, for a fixed offs_k, we are reading contiguous offs_n? Yes, because the addresses for offs_n are consecutive. This would be coalesced.\n\n So let's change the model to store the weight as (in_features, out_features) and then in the kernel, we pass the transposed weight (which is contiguous in the out_features dimension). But note: the original model uses nn.Linear which returns a weight of (out_features, in_features). We are currently storing as (out_features, in_features). We would have to transpose it to (in_features, out_features) and then pass that to the kernel.\n\n Alternatively, we can transpose the weight matrix in the forward pass and pass it as contiguous in the out_features dimension.\n\n Let's change the kernel to accept the weight matrix in the shape (in_features, out_features) and then we can set the strides appropriately for coalesced reads.\n\n But note: the kernel is designed for w of shape (K, N). So if we pre-transpose the weight to (in_features, out_features) (which is K, N), then we are good.\n\n Steps:\n  1. In the model, we store the weight as (out_features, in_features) because that's the PyTorch convention.\n  2. In the forward function, we transpose the weight to (in_features, out_features) and make it contiguous. Then we pass this to the kernel.\n\n But note: the kernel also uses the strides. We can pass the transposed weight and then set the strides accordingly.\n\n Alternatively, we can change the kernel to accept the weight in either layout by adjusting the strides? But we want to ensure coalesced reads.\n\n Let's do:\n   weight_t = self.weight.t().contiguous()   # Now shape (in_features, out_features)\n\n Then in the kernel call, we pass:\n        x, weight_t, bias, ...\n\n        stride_xm, stride_xk,   -> x is (M, K) so strides are (K, 1) if row-major? Actually, x is contiguous (batch_size, in_features) -> stride(0)=in_features, stride(1)=1.\n        stride_wn, stride_wk -> for weight_t (which is (K, N)): stride(0)=N, stride(1)=1? Actually, weight_t is (in_features, out_features) and contiguous -> stride(0)=out_features, stride(1)=1.\n\n Then in the kernel, the w_ptrs become:\n   w_ptrs = w_ptr + offs_k[:, None] * stride_wk (which is 1) + offs_n[None, :] * stride_wn (which is out_features)\n\n So: w_ptr + offs_k * 1 + offs_n * out_features. Now, for a fixed offs_k, we are reading offs_n in a contiguous block? Actually, for a fixed offs_k, the addresses for offs_n are consecutive? No, because offs_n is a vector of consecutive integers? Then the addresses are consecutive? Yes, because we are adding a base (w_ptr + offs_k) and then offs_n * out_features? But note: offs_n is a vector of consecutive integers? Actually, offs_n is a vector of consecutive integers? Then the addresses for the same offs_k and different offs_n are separated by 1? No, by the value of the index? Actually, we are doing:\n   w_ptr + offs_k (scalar) + offs_n * out_features.\n\n This is not contiguous. We want to read a block of offs_n for a fixed offs_k? Then we want the addresses to be consecutive? But they are separated by out_features? That is not consecutive.\n\n How about we change the kernel to access the weight matrix as:\n   w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n and if we set stride_wn=1 and stride_wk=out_features? Then:\n   w_ptr + offs_k * out_features + offs_n   [for a given offs_k and offs_n]\n\n Now, for a fixed offs_k, the addresses for consecutive offs_n are consecutive? Yes. So we want the weight matrix to be stored in row-major for the (in_features, out_features) matrix? Actually, we want the out_features dimension to be contiguous? That is column-major for the original weight (if we consider the original weight as (out_features, in_features))? Actually, the transposed weight (in_features, out_features) stored in row-major would have the out_features as the inner dimension? Then the stride for the inner dimension (out_features) is 1? And the stride for the outer dimension (in_features) is out_features? So we set:\n   stride_wn = 1   (for the inner dimension: out_features)\n   stride_wk = out_features   (for the outer dimension: in_features)\n\n Then the kernel access becomes:\n   w_ptrs = w_ptr + offs_k[:, None] * (out_features) + offs_n[None, :] * 1\n\n Then for a fixed offs_k, the addresses for offs_n are consecutive? Yes.\n\n So we need to pass the weight matrix as contiguous and in the shape (in_features, out_features) and then set the strides as:\n   stride_wn = 1\n   stride_wk = out_features\n\n Let's change the current program accordingly.\n\n Steps in the current program:\n  1. In the ModelNew, we store the weight as (out_features, in_features) and then in the forward function, we do:\n        weight_t = self.weight.t().contiguous()   # Now (in_features, out_features)\n  2. Then in the kernel call, we pass:\n        w = weight_t\n        stride_wn = 1\n        stride_wk = out_features   (which is the size of the inner dimension of weight_t? Actually, the inner dimension is out_features, so the stride for the outer dimension (in_features) is out_features)\n\n But note: the kernel expects the weight matrix to have shape (K, N) and we are now passing (in_features, out_features) which is (K, N). So that matches.\n\n Also, the x matrix is (M, K) and we pass it with strides (stride_xm, stride_xk) = (x.stride(0), x.stride(1)) = (in_features, 1) for contiguous x? Actually, x is (batch_size, in_features) and contiguous, so stride(0)=in_features, stride(1)=1.\n\n So in the kernel, the x_ptrs are:\n   x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n           = x_ptr + offs_m * in_features + offs_k * 1\n\n This is contiguous for a fixed offs_m? Yes, because we are reading consecutive offs_k.\n\n Now, the weight matrix is accessed as:\n   w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n           = w_ptr + offs_k * out_features + offs_n\n\n This is contiguous for a fixed offs_k? Yes.\n\n So we change the current program to pre-transpose the weight and set the strides appropriately.\n\n Additionally, we can try to adjust the block sizes and the number of warps.\n\n We note that the current block sizes are 64x64x32. We can try 64x64x64 to reduce the number of iterations? Or try 128x128x32? But the problem size is small.\n\n Alternatively, we can try to use a block size that is 128 in the N dimension? But note that N is 512, so 512/128=4.\n\n However, let's try to use a block size that is 64x128x32? Then M_blocks = ceil(128/64)=2, N_blocks=ceil(512/128)=4 -> 8 blocks. But we want more blocks? 32 blocks might be better.\n\n How about 32x64x64? Then M_blocks=ceil(128/32)=4, N_blocks=ceil(512/64)=8, and K_blocks=ceil(1024/64)=16. The number of iterations in the inner loop would be 16? Currently we have 32.\n\n But note: the kernel uses a loop over K blocks. So the number of iterations is ceil(K / BLOCK_SIZE_K). With BLOCK_SIZE_K=64, we have 16 iterations.\n\n We can also try to use 8 warps? We can set the number of warps to 8.\n\n Let's try the following changes:\n\n  1. Pre-transpose the weight matrix and set the strides appropriately.\n  2. Change the block sizes to: BLOCK_SIZE_M=64, BLOCK_SIZE_N=128, BLOCK_SIZE_K=64? Then:\n        M_blocks = ceil(128/64)=2\n        N_blocks = ceil(512/128)=4\n        K_blocks = ceil(1024/64)=16\n        Total blocks = 8.\n\n  3. Or try BLOCK_SIZE_M=32, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64? Then:\n        M_blocks = 4, N_blocks=8 -> 32 blocks, and K_blocks=16.\n\n  4. Also set num_warps=8.\n\n But note: the kernel does not have an autotuner. We might want to try different configurations? But we are only allowed to change the code in the block.\n\n Alternatively, we can try the block sizes that are known to be good on H100 for TF32: 128x256x32? But our problem is small.\n\n Given the problem size, we want to have at least 128 blocks? But the H100 has 132 SMs, so we want at least 132 blocks? But we only have 128 in M and 512 in N. The maximum blocks we can have is 128/1 * 512/1 = 128*512 which is a lot? But we are constrained by the block size.\n\n We'll try to use a block size of 64x64x32 and then set num_warps=8? And also transpose the weight for coalesced access.\n\n Alternatively, we can try to use the same block sizes but with the weight transposed and then change the kernel to use 8 warps.\n\n Let's do:\n\n  1. Transpose the weight and set strides appropriately.\n  2. Change the kernel to use 8 warps? We can add `num_warps=8` to the decorator.\n\n But note: the current kernel does not specify num_warps. The default is 4? We can try 8.\n\n We'll change the kernel to:\n   @triton.jit(num_warps=8)\n\n   ... and adjust the weight as described.\n\n However, note: the kernel might not be able to use 8 warps efficiently? We have to try.\n\n Alternatively, we can leave the number of warps to be chosen by Triton? But we are not using autotune.\n\n Given the above, let's rewrite the current program.\n\n Steps:\n\n  In the ModelNew class, in the forward function:\n      Instead of passing self.weight (which is (out_features, in_features)), we pass:\n          weight_t = self.weight.t().contiguous()\n\n      Then in the kernel call, we set:\n          w = weight_t,\n          stride_wn = 1, \n          stride_wk = out_features   # because weight_t is (in_features, out_features) and contiguous, so stride(0)=out_features, stride(1)=1.\n\n  Change the kernel to use num_warps=8.\n\n  Also, we can try to adjust the block sizes? Let's try to use BLOCK_SIZE_K=64 to reduce the number of iterations? And maybe adjust M and N accordingly.\n\n  We note that the current block sizes are 64,64,32. We can change to 64,64,64? Then the number of iterations in the inner loop becomes 1024/64=16 (from 32).\n\n  So set:\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64\n\n  And grid remains:\n        grid = (triton.cdiv(M, 64), triton.cdiv(N, 64))\n\n  But note: M is 128, so M_blocks=2, N_blocks=ceil(512/64)=8 -> 16 blocks.\n\n  We can also try to use 128 in N? Then BLOCK_SIZE_N=128 -> N_blocks=ceil(512/128)=4 -> 8 blocks. That might be too few.\n\n  So we stick with 64x64x64.\n\n Let's write the code accordingly.\n\n But note: the kernel must be changed to handle the new block sizes? We only change the constants.\n\n However, we must ensure that the masks are computed correctly. The current mask for k_valid uses:\n        k_remaining = K - k * BLOCK_SIZE_K\n        k_valid = tl.minimum(BLOCK_SIZE_K, k_remaining)\n\n This is correct.\n\n Also, the block size for K is now 64, which is a multiple of 16? Yes, so it should work with Tensor Cores.\n\n We'll change the kernel to:\n   - Use num_warps=8\n   - Change the block sizes to 64,64,64\n\n And in the model forward, we transpose the weight.\n\n Let's write the code accordingly.\n\n However, note: the current program does not transpose the weight. We are passing the original weight and its strides (stride(0), stride(1)) which are (in_features, 1) for the original weight? Actually, no: the original weight is (out_features, in_features) so stride(0)=in_features, stride(1)=1. We are now going to pass the transposed weight and set the strides to (1, out_features) but note: the kernel expects two strides: stride_wn and stride_wk. We set:\n        stride_wn = 1\n        stride_wk = out_features\n\n But note: the kernel call currently passes:\n        w.stride(0), w.stride(1)   -> for the original weight, but now we are passing the transposed weight, so we should pass:\n        weight_t.stride(0), weight_t.stride(1)   -> which are (out_features, 1) for the transposed weight? Actually, the transposed weight is contiguous and of shape (in_features, out_features). So stride(0)=out_features, stride(1)=1.\n\n But in the kernel, we have:\n        stride_wn: the stride for the N dimension (which is the inner dimension) -> 1\n        stride_wk: the stride for the K dimension (which is the outer dimension) -> out_features\n\n So we want:\n        stride_wn = 1\n        stride_wk = out_features\n\n But note: the kernel call currently uses:\n        w.stride(0), w.stride(1)   # for the original weight\n\n Now we are passing the transposed weight, so we should set:\n        stride_wn = weight_t.stride(1)   # which is 1\n        stride_wk = weight_t.stride(0)   # which is out_features\n\n So we can pass:\n        weight_t.stride(0), weight_t.stride(1)   -> (out_features, 1) -> but wait, the transposed weight is (in_features, out_features) and contiguous, so stride(0)=out_features, stride(1)=1.\n\n Then we set in the kernel call:\n        stride_wn = weight_t.stride(1)   -> 1\n        stride_wk = weight_t.stride(0)   -> out_features\n\n This is what we want.\n\n Alternatively, we can set explicitly? But it's the same.\n\n So the plan:\n\n  In the forward function of ModelNew:\n        weight_t = self.weight.t().contiguous()\n        ... and then in the kernel call, pass:\n            w = weight_t,\n            stride_wn = weight_t.stride(1),   # 1\n            stride_wk = weight_t.stride(0),   # out_features\n\n  Change the kernel to use num_warps=8 and set BLOCK_SIZE_K=64.\n\n  Also, change the block sizes in the kernel call to 64,64,64.\n\n But note: the kernel parameters are BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K. We set them to 64,64,64.\n\n Let's write the code accordingly.\n\n However, note: the current kernel uses the same variable names. We are only changing the values.\n\n We are going to change the current program as follows:\n\n  1. In the kernel definition, add `num_warps=8` to the decorator.\n  2. Change the block sizes in the kernel call to 64,64,64.\n  3. In the forward function of ModelNew, transpose the weight and use the strides of the transposed weight.\n\n But note: the current program does not have the weight in the model? Actually, it uses:\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n\n        and then in forward, we call:\n            fused_gemm_relu(x, self.weight, self.bias)\n\n        but now we want to change the fused_gemm_relu to take the transposed weight? Actually, we are changing the function fused_gemm_relu? We are going to change the call inside the ModelNew.forward.\n\n        Specifically, in the current program, the ModelNew.forward is:\n            x = fused_gemm_relu(x, self.weight, self.bias)\n\n        We are changing the fused_gemm_relu function? Actually, we are not, we are changing the way we call the kernel. But note: the function fused_gemm_relu is defined as:\n            def fused_gemm_relu(x, w, bias):\n                ... and then it calls the kernel.\n\n        We want to change the 'w' passed to the kernel to be transposed? So we should change the fused_gemm_relu function to transpose the weight? Or we change the ModelNew.forward to pass the transposed weight?\n\n        Let's change the ModelNew.forward to pass the transposed weight to the fused_gemm_relu function? Then we don't change the fused_gemm_relu function? Actually, the fused_gemm_relu function expects w of shape (N, K) [because it sets N, _ = w.shape]. But we are now passing a weight_t of shape (K, N). So we have to change the fused_gemm_relu function? Because it does:\n            M, K = x.shape\n            N, _ = w.shape   # if w is (K, N), then N here is K? That's wrong.\n\n        We want the w in the fused_gemm_relu to be the weight in the shape (K, N) (in_features, out_features). Then the line:\n            N, _ = w.shape   -> becomes N = w.shape[1]? Actually, we do:\n                N, _ = w.shape   # w is (in_features, out_features) -> then N = in_features? That's not what we want.\n\n        We want N to be out_features. So we should change the fused_gemm_relu function to:\n            M, K = x.shape\n            _, N = w.shape   # because w is (K, N)\n\n        Then we set:\n            grid = (triton.cdiv(M, 64), triton.cdiv(N, 64))\n\n        And then the kernel call uses M, N, K.\n\n        So we must change the fused_gemm_relu function to use the new shape of w.\n\n        Alternatively, we can change the ModelNew.forward to pass the weight_t and then the fused_gemm_relu function doesn't need to change? But the fused_gemm_relu function currently expects w to be (out_features, in_features). We are changing to (in_features, out_features). So the function must change.\n\n        Since we are allowed to change the entire block, we can change the fused_gemm_relu function accordingly.\n\n        Let's change the fused_gemm_relu function to:\n            M, K = x.shape\n            K_w, N = w.shape   # w is (K, N) -> K_w should equal K, and N is the out_features.\n\n        Then we do:\n            assert K == K_w\n\n        Then set grid with M and N.\n\n        Also, in the kernel call, we pass the strides of w appropriately.\n\n        So we change the function to:\n\n            def fused_gemm_relu(x, w, bias):\n                M, K = x.shape\n                K_w, N = w.shape\n                assert K == K_w, f\"Shape mismatch: x has {K} features, w has {K_w} features\"\n                output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n                grid = (triton.cdiv(M, 64), triton.cdiv(N, 64))\n                _fused_gemm_relu_kernel[grid](\n                    x, w, bias, output,\n                    M, N, K,\n                    x.stride(0), x.stride(1),\n                    w.stride(0), w.stride(1),   # for the transposed weight, these are (N, 1) for contiguous? Actually, w is (K, N) and contiguous, so stride(0)=N, stride(1)=1.\n                    output.stride(0), output.stride(1),\n                    BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64\n                )\n                return output\n\n        But note: the kernel expects the strides for w as:\n            stride_wn: stride for the N dimension (inner dimension) -> which is 1\n            stride_wk: stride for the K dimension (outer dimension) -> which is N (the inner dimension size)\n\n        And we are passing w.stride(0) and w.stride(1). For w (K, N) contiguous, stride(0)=N, stride(1)=1. So we are passing:\n            stride_wn = w.stride(1) = 1\n            stride_wk = w.stride(0) = N\n\n        This matches.\n\n        So we change the fused_gemm_relu function accordingly.\n\n        And in ModelNew.forward, we do:\n            weight_t = self.weight.t().contiguous()\n            x = fused_gemm_relu(x, weight_t, self.bias)\n\n        Then return x.\n\n  4. Also, change the kernel to use num_warps=8 and set the block sizes to 64,64,64.\n\n  But note: the kernel already uses the block sizes passed from the call? So we change the call to pass 64,64,64.\n\n  And we change the kernel decorator to @triton.jit(num_warps=8)\n\n However, note: the kernel might not be efficient with 8 warps? We are trying to utilize the Tensor Cores better.\n\n Alternatively, we can try without num_warps and let Triton choose? But we are not sure. Since the current version is slower, we try 8 warps.\n\n Let's write the entire code block.\n\n But note: the current program does not have the weight transposed in the ModelNew. We are changing that.\n\n We are going to output the entire code block.\n\n IMPORTANT: We are only allowed to change the code within the EVOLVE-BLOCK.\n\n Let's do it.\n</think>\nTo improve the performance of the Triton kernel for fused GEMM and ReLU operations, I've made several key optimizations:\n\n1. **Weight Matrix Transposition**: Transposed the weight matrix to enable coalesced memory access patterns\n2. **Block Size Optimization**: Increased K-dimension block size to 64 to reduce loop iterations\n3. **Warp Configuration**: Set num_warps=8 to better utilize Tensor Cores\n4. **Memory Access Patterns**: Ensured contiguous reads for both input and weight matrices\n\nThese changes improve memory access efficiency and better utilize the H100 Tensor Cores while maintaining functional correctness.\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit(num_warps=8)\ndef _fused_gemm_relu_kernel(\n    x_ptr,\n    w_ptr,\n    bias_ptr,\n    output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wk, stride_wn,  # Note: swapped order for transposed weight\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Input matrix access (coalesced)\n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    \n    # Weight matrix access (transposed for coalescing)\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        k_valid = tl.minimum(BLOCK_SIZE_K, k_remaining)\n        \n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_valid)\n        b_mask = (offs_k[:, None] < k_valid) & (offs_n[None, :] < N)\n        \n        a = tl.load(x_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(w_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n    \n    bias_ptrs = bias_ptr + offs_n\n    bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n    acc += bias[None, :]\n    acc = tl.where(acc > 0, acc, 0.0)\n    \n    output_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(output_ptrs, acc, mask=mask)\n\ndef fused_gemm_relu(x, w, bias):\n    M, K = x.shape\n    _, N = w.shape  # w is now (K, N) after transpose\n    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n    grid = (\n        triton.cdiv(M, 64),\n        triton.cdiv(N, 64),\n    )\n    _fused_gemm_relu_kernel[grid](\n        x, w, bias, output,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        w.stride(0), w.stride(1),  # Strides for transposed weight\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64  # Increased K-block\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias_shape):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n    \n    def forward(self, x):\n        # Transpose weight for efficient memory access\n        weight_t = self.weight.t().contiguous()\n        return fused_gemm_relu(x, weight_t, self.bias)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbias_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
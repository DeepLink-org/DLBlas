{"id": "57b952ad-dab0-4960-911f-e12f15fd368c", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n    \n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        a = tl.load(a_ptrs, mask=k_mask[None, :], other=0.0)\n        b = tl.load(b_ptrs, mask=k_mask[:, None], other=0.0)\n        a = a.to(tl.float16)\n        b = b.to(tl.float16)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    offs_cm = offs_m[:, None]\n    offs_cn = offs_n[None, :]\n    c_ptrs = c_ptr + stride_cm * offs_cm + stride_cn * offs_cn\n    c_mask = (offs_cm < M) & (offs_cn < N)\n    \n    if bias_ptr is not None:\n        bias_ptrs = bias_ptr + offs_n\n        bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\ndef triton_linear(x, weight, bias=None):\n    M, K = x.shape\n    K, N = weight.shape\n    x = x.contiguous()\n    weight = weight.contiguous().t()\n    output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n    \n    grid = lambda opt: (triton.cdiv(M, opt['BLOCK_M']) * triton.cdiv(N, opt['BLOCK_N']),)\n    matmul_kernel[grid](\n        x, weight, output, bias,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        output.stride(0), output.stride(1),\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.randn(out_features))\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x):\n        x = triton_linear(x, self.weight, self.bias)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "d21da7c2-24dd-4228-a7cb-d6da371ea2b9", "generation": 1, "timestamp": 1754572598.4427533, "iteration_found": 0, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_39_Gemm_Scale_BatchNorm", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp_5_i8mdj.py\", line 90, in forward\n    x = x * self.scale\nRuntimeError: The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1\n", "runtime_error_name": "builtins.RuntimeError", "reference_timing_ms": 0.0935, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0829, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0935, "speed_up": 1.128, "custom_timing": 0.0829}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0829\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0935, 'speed_up': 1.128, 'custom_timing': 0.0829}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0829, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0935, 'speed_up': 1.128, 'custom_timing': 0.0829}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, scales the result, and applies batch normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0829), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0935, 'speed_up': 1.128, 'custom_timing': 0.0829})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, scales the result, and applies batch normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear layer (gemm)\n  2. Element-wise scaling by a parameter vector (scale)\n  3. BatchNorm1d\n\nThe current program is a pure PyTorch implementation. We are to rewrite it using Triton for performance.\n\nHowever, note that the entire model is a sequence of operations. The most computationally expensive part is the linear layer (gemm). \nBut the task does not specify which part to optimize. Since the problem mentions Triton, we assume we are to rewrite the entire forward pass in Triton? \nBut wait: the original model uses nn.Linear and nn.BatchNorm1d. We cannot replace the batchnorm with Triton easily because it has state (running mean/var) and the backward is complex.\n\nLooking at the context: the dataset is kernelbench. This suggests that we are to write a kernel for a specific operation. \nBut the example given was a quantization kernel. Now we have a model with multiple operations.\n\nGiven the constraints of the problem, we must note:\n\n- We cannot change the function signatures (the forward method) or the parameters of the model.\n- The model must be functionally equivalent.\n\nBut the current code is a PyTorch module. How do we use Triton? We can replace the forward pass with a custom Triton kernel? \nBut the forward pass has multiple operations (gemm, scaling, batchnorm). \n\nAlternatively, we might be expected to write a kernel for the entire forward pass? That is complex and might not be necessary.\n\nAnother possibility: the problem expects us to replace the most expensive part (the gemm) with a Triton kernel. However, note that the problem says:\n\n\"Generate functionally equivalent but faster Triton implementations (custom_code)\"\n\nAnd the example was a function (per_row_quantize_int8) that was replaced by a Triton kernel.\n\nIn the current program, the forward function is:\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nWe cannot replace the entire model with one kernel because the model has parameters (weight, bias, scale, bn parameters) and the batchnorm has state.\n\nBut note: the model is defined as a class. We are allowed to change the internal implementation? The constraints say:\n\n\u274c NEVER change @triton.jit function signatures or parameter names\n\nBut we are not given a Triton kernel in the current code. So we are to introduce one?\n\nLet me reexamine the problem: the dataset contains 250 kernelbench/level1-level5 *.py files. Each file contains PyTorch reference (reference_code) and we are to generate Triton implementations (custom_code). \n\nTherefore, I suspect that the current program is actually a kernel in disguise? \n\nLooking at the code: \n  - It defines a class ModelNew that is an nn.Module.\n  - It also defines batch_size, in_features, out_features, scale_shape, and functions get_inputs and get_init_inputs.\n\nThis setup is typical for a kernel benchmark: the model is the kernel, and the get_inputs provides the input tensor.\n\nBut wait: the example quantization function did not use a class. It was a function. So why is this a class?\n\nI think the problem is that the benchmark expects a model that can be run. We are allowed to change the forward pass to use Triton.\n\nPlan:\n\nWe will keep the class and the parameters, but we will replace the forward pass with a Triton kernel that does:\n  - The matrix multiplication (gemm) of the linear layer.\n  - The element-wise scaling.\n  - The batch normalization.\n\nBut note: the batchnorm has learnable parameters (weight and bias) and running statistics. We cannot ignore the running statistics in training. However, the problem does not specify if we are in training or evaluation mode.\n\nBut the benchmark might be running in evaluation mode? We are not told. To be safe, we must support both.\n\nHowever, the complexity of writing a fused kernel for gemm + scaling + batchnorm (with training and inference) is very high.\n\nAlternatively, we can break the problem into two parts:\n\n1. Replace the gemm with a Triton kernel. Then leave the scaling and batchnorm as PyTorch.\n2. Or, we can write a kernel that fuses the gemm and scaling, and then use the PyTorch batchnorm.\n\nBut note: the batchnorm is applied along the out_features dimension. The output of the gemm is [batch_size, out_features]. The scaling is by a vector of size [out_features]. So the scaling is a row-wise operation.\n\nWe can write a Triton kernel that does the gemm and the scaling together? That would avoid one pass over the data.\n\nBut the batchnorm is a separate operation that also does a row-wise operation (normalizing each feature across the batch). Fusing the gemm, scaling and batchnorm would be even better.\n\nHowever, the batchnorm in training mode requires computing the mean and variance over the batch, which requires a reduction. This is non-trivial.\n\nGiven the complexity and the fact that the problem does not specify the mode, I propose we do:\n\n  - For now, we only replace the gemm (linear layer) with a Triton kernel. We leave the scaling and batchnorm as they are.\n\nWhy?\n  - The gemm is the most expensive operation.\n  - We are constrained by the fact that we must not change the signatures and we must maintain correctness.\n\nWe can write a Triton kernel for the linear layer (matrix multiplication). The linear layer in PyTorch is: x @ weight.t() + bias.\n\nWe'll write a Triton kernel for the matrix multiplication (without the bias) and then add the bias in a separate step? But note: the Triton kernel can also add the bias.\n\nAlternatively, we can write a kernel that does:\n  output = x @ weight.t() + bias\n\nThen we do:\n  output = output * self.scale   # in PyTorch\n  output = self.bn(output)       # in PyTorch\n\nThis would be a two-step: Triton for the linear, then PyTorch for the rest.\n\nBut note: the current model uses nn.Linear, which already does x @ weight.t() + bias.\n\nSo we are replacing the nn.Linear with a custom Triton kernel. The nn.Linear is already optimized (using cuBLAS), so we must be careful to beat it.\n\nWe are to write a Triton kernel for the linear layer. We can use the example from Triton documentation:\n\n  https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html\n\nBut note: the problem says we are using H100/H800 with Tensor Cores. We should use Tensor Core operations.\n\nWe can write a kernel that uses tensor cores for FP16? But the input is FP32? \n\nLooking at the get_inputs: \n    return [torch.randn(batch_size, in_features)]\n\nThis is FP32 by default.\n\nWe have two choices:\n  1. Write the kernel in FP32 and use Tensor Cores? (H100 supports FP32 Tensor Cores with TF32, but Triton might not expose that easily)\n  2. Convert the input and weight to FP16, do the kernel in FP16, then convert back? But that would change the numerics and might break the batchnorm.\n\nAlternatively, we can use the kernel in FP32 without Tensor Cores? That might not be faster than cuBLAS.\n\nBut note: the problem says we can use Tensor Core alignment (FP16/bf16/tf32). We can use tf32 for the matrix multiplication.\n\nHowever, Triton currently (as of 2.1) does not support tf32. It supports FP16 and BF16 for tensor cores.\n\nSo we must convert to FP16? Let's check the hardware: H100 has FP16 Tensor Core 32\u00d716\u00d716.\n\nWe can do:\n\n  Convert x and weight to FP16 -> do the matrix multiplication in FP16 with tensor cores -> convert result to FP32 -> add bias (in FP32) -> then do scaling and batchnorm in FP32.\n\nBut the conversion might add overhead. And we must be cautious of the accuracy.\n\nAlternatively, we can do the entire computation in FP32 without tensor cores? That might not be faster.\n\nGiven the complexity, and the fact that the current implementation is already using cuBLAS (which is highly optimized), we must be cautious.\n\nBut the problem asks for a Triton implementation. So we are expected to write a kernel.\n\nLet's write a kernel for the linear layer (gemm) that uses FP16 tensor cores. We'll convert the input and weight to FP16, do the multiplication, then convert the result back to FP32 and add the bias.\n\nWe'll use the matrix multiplication kernel from Triton that uses tensor cores. We can use the example from the Triton documentation:\n\n  https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html#matrix-multiplication-with-tensor-cores\n\nBut note: that example uses FP16.\n\nSteps:\n\n  1. In the forward function, we will replace:\n        x = self.gemm(x)\n\n     with:\n        x = custom_linear(x, self.gemm.weight, self.gemm.bias)\n\n  2. We define a function `custom_linear` that uses a Triton kernel.\n\n  3. We must handle the bias: if bias is present, we add it.\n\n  4. We must handle the data type: convert input and weight to FP16, then convert the result to FP32.\n\n  5. We must handle the shape: the linear layer does: (batch, in_features) @ (out_features, in_features).t() -> (batch, out_features)\n\n     So the matrix multiplication is: \n        x (shape [B, M]) @ weight (shape [N, M]).t() -> [B, N]\n     Actually, weight is stored as [out_features, in_features] so we are multiplying by weight.t() which is [in_features, out_features]? \n     But note: the linear layer does: x @ weight.t().\n\n     So we are doing: \n        x (B, M) @ weight (N, M).t() -> (B, N)   [because (B, M) @ (M, N) = (B, N)]\n\n     Therefore, we are multiplying two matrices: A (B, M) and B (M, N) to get C (B, N).\n\n  6. We'll write a kernel for A (B, M) and B (M, N) -> C (B, N). We can use the kernel from the tutorial.\n\n  7. We must set the grid appropriately.\n\nHowever, note the constraints:\n\n\u274c NEVER change grid configuration\n\nBut the grid configuration in the current code is not defined for Triton. We are introducing a kernel. We are free to choose the grid? \n\nBut the problem says: \"NEVER change grid configuration\" meaning the grid that is already set? Since there isn't one, we set it appropriately.\n\nLet me write the kernel:\n\nWe'll define:\n\n    @triton.autotune(...)\n    @triton.jit\n    def _matmul_kernel( ... ):\n\n    def custom_linear(x, weight, bias=None):\n        # x: [B, M], weight: [N, M] -> we want to compute x @ weight.t()\n        # which is equivalent to: (B, M) @ (M, N) -> (B, N)\n\n        # We need to reshape weight to [M, N]? Actually, we can use weight.t() which is [M, N]?\n        # But note: the kernel expects two matrices: A (B, M) and B (M, N) -> C (B, N)\n\n        # We will not explicitly transpose weight because we can treat it as already transposed? \n        # Actually, we can pass the weight as is and then use the kernel that expects B in column-major? \n        # Alternatively, we can transpose the weight to be [M, N] and then the kernel would read it in row-major as [M, N].\n\n        # But note: the linear layer's weight is stored as [N, M]. We want to multiply by weight.t() which is [M, N]. \n        # So we can pass weight.t() to the kernel? \n\n        # Let me do: \n        #   A = x   (B, M)\n        #   B = weight.t()   (M, N)   [because weight is (N, M) -> transpose is (M, N)]\n        # Then C = A @ B = (B, N)\n\n        # So we set:\n        #   a_ptr = x\n        #   b_ptr = weight.t().contiguous()\n\n        # But note: the kernel might require contiguous memory.\n\n        # Steps:\n        #   Check that x and weight are contiguous? If not, make contiguous.\n        #   Allocate output: (B, N)\n\n        # Then call the kernel.\n\n        # Then add bias if present.\n\n        pass\n\nBut note: the kernel from the tutorial is for A (M, K) and B (K, N) -> C (M, N). Our A is (B, M) and B is (M, N). So we can set:\n   M = B, K = M, N = N.\n\nWait: the tutorial kernel expects:\n   A: (M, K) in row-major\n   B: (K, N) in row-major\n   C: (M, N) in row-major\n\nBut our A is (B, M) -> we want to set M=B, K=M, N=N? Then the output would be (B, N) which is correct.\n\nBut note: the dimensions must be powers of two? Not necessarily, but the kernel uses blocking.\n\nWe can use the kernel from the tutorial.\n\nWe'll adapt the kernel to use FP16 and tensor cores.\n\nBut note: the tutorial kernel uses tensor cores. We can copy it.\n\nHowever, the tutorial kernel does not support bias. We will add the bias after.\n\nBut the bias addition is cheap (O(B*N)), so we can do it in a separate kernel or in PyTorch.\n\nGiven the complexity, let's do:\n\n  Step 1: Write the matrix multiplication kernel for FP16 with tensor cores.\n  Step 2: Convert the input and weight to FP16, compute the output in FP32 (by accumulating in FP32 and then converting) or in FP16 then convert to FP32? \n          The tutorial kernel does accumulation in FP32 and then writes FP32? Actually, it uses FP16 for the tensor cores and accumulates in FP32.\n\n  Step 3: Convert the output to FP32 and then add the bias (if present) in FP32.\n\n  Step 4: Then do the scaling and batchnorm in PyTorch.\n\nBut note: the batchnorm expects FP32? Yes, because the model is in FP32.\n\nSo the plan:\n\n    def forward(self, x):\n        # Instead of: x = self.gemm(x)\n        x = custom_linear(x, self.gemm.weight, self.gemm.bias)   # returns FP32 tensor of shape [B, out_features]\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nWe must write `custom_linear` as described.\n\nWe'll take the kernel from the tutorial and adjust it to our dimensions.\n\nBut note: the tutorial kernel is for square matrices? It works for any M, K, N.\n\nWe'll use:\n\n    https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html#matrix-multiplication-with-tensor-cores\n\nWe copy the kernel and the function.\n\nWe must set the grid: \n    grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n\nBut wait: the kernel uses two-dimensional grid? Actually, the tutorial uses:\n\n    grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n\nBut in the example above, the quantization kernel used a 1D grid. \n\nThe constraints say: never change grid configuration? But we are introducing a kernel. We are free to set the grid as needed.\n\nHowever, the problem says: \"NEVER change grid configuration, output tensor shapes, or PID logic\"\n\nBut we are setting the grid for the first time? We are not changing an existing one. So we set it appropriately.\n\nBut note: the constraint might refer to the grid that is passed to the kernel? We are setting it.\n\nWe'll set the grid as in the tutorial: (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n\nBut note: the tutorial kernel uses a 2D grid. We must define the pid for the 2D grid.\n\nWe'll write the kernel and the function.\n\nHowever, note the constraints: \n  \u274c NEVER remove boundary checks or out-of-bound masks\n\nThe tutorial kernel has boundary checks. We keep them.\n\nWe must also note: the input x is of shape [B, M] and weight is [N, M]. We are using weight.t() which is [M, N]. So:\n\n   A = x: [B, M] -> we set M = B (number of rows), K = M (number of columns) for A? \n   But wait: the kernel expects A to be of shape (M, K) and B of shape (K, N). \n\n   We have:\n        A: (B, M) -> we want to consider it as (B, M) -> so M = B, K = M.\n        B: (M, N) -> so K = M, N = N.\n\n   Then the output C: (B, N) -> (M, N) in the kernel? \n\n   But note: the kernel expects the matrix A to be of shape (M, K). We have A of shape (B, M) which we can view as (B, M) -> so M=B, K=M.\n\n   So the kernel will compute C of shape (B, N) which is what we want.\n\nBut the kernel in the tutorial expects the matrices to be in row-major. Our tensors are in row-major.\n\nSo we can use the kernel.\n\nWe'll write the custom_linear function.\n\nSteps in custom_linear:\n\n    x = x.contiguous()\n    weight = weight.contiguous()\n    B, M = x.shape\n    N = weight.shape[0]   # weight is [N, M] -> so we need to transpose to get [M, N]\n    weight_t = weight.t().contiguous()   # [M, N]\n\n    # Allocate output: [B, N]\n    output = torch.empty((B, N), device=x.device, dtype=torch.float32)\n\n    # We'll convert x and weight_t to FP16?\n    # But the kernel in the tutorial uses FP16 for A and B, and FP32 for C.\n    # So we do:\n    x_fp16 = x.to(torch.float16)\n    weight_fp16 = weight_t.to(torch.float16)\n\n    # Now call the kernel with:\n    #   A = x_fp16, B = weight_fp16, C = output (which is FP32)\n    #   M = B, N = N, K = M\n    #   stride_am = M, stride_ak = 1   [for A: row-major, so row stride is K (M) and column stride is 1?]\n    #   Actually, for A: [B, M] -> row stride = M, column stride = 1 -> so stride_am = M, stride_ak = 1? \n    #   But note: the kernel expects A to be of shape (M, K) -> so the row stride is K, column stride is 1? \n    #   But we have A as (B, M) -> which is (M, K) in the kernel? We set M=B, K=M. Then the row stride (for the first dimension) should be K (which is M). \n    #   Similarly, for B: [M, N] -> row stride = N, column stride = 1? \n    #   But the kernel expects B of shape (K, N) -> so row stride = N, column stride = 1.\n\n    # So we set:\n    #   stride_am = M   # because to move to next row in A (which is of length M) we step by M? \n    #   Actually, the stride for the first dimension of A (which is B) is M? \n    #   In memory, A (B, M) is stored row-major: the first row has M elements, then the second row ... so the stride between rows is M.\n    #   Similarly, for B (M, N): the stride between rows (for the first dimension) is N.\n\n    # The kernel uses:\n    #   a_ptr + off_am * stride_am + off_ak * stride_ak\n    #   off_am: index in M dimension (0..M-1) -> row index in A\n    #   off_ak: index in K dimension (0..K-1) -> column index in A\n\n    # So for a row-major A (M, K) the element at (i, j) is at a_ptr + i * stride_am + j * stride_ak.\n    # We set stride_am = K? Because the row has K elements? But note: our A is (B, M) and we set M=B, K=M -> so the row has K=M elements -> so stride_am = K = M.\n\n    # Actually, the tutorial sets:\n    #   stride_ak = 1   (because contiguous in the K dimension within a row)\n    #   stride_am = K   (because the next row is offset by K elements)\n\n    # So we set:\n    #   stride_am = M   # because K = M? But wait: K is the inner dimension? Actually, in our A (B, M): \n    #       element [i, j] is at i * M + j -> so the stride for the first dimension (am) is M, and for the second (ak) is 1.\n\n    # Similarly, for B (M, N): \n    #   element [i, j] is at i * N + j -> so stride_bk = 1, stride_bn = N? \n    #   But the kernel expects B (K, N): \n    #       off_bk = ... (index in K) and off_bn = ... (index in N)\n    #       so element at (k, n) is at b_ptr + k * stride_bk + n * stride_bn? \n    #       But the kernel uses: \n    #           b_ptrs = b_ptr + (offs_bk[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    #       and they set stride_bk = N, stride_bn = 1? \n\n    #   Actually, the tutorial sets for B (which is of shape (K, N) and row-major):\n    #       stride_bk = N, stride_bn = 1.\n\n    #   But our B is (M, N) and we set K = M -> so the row stride is N? and column stride is 1.\n\n    #   So we set:\n    #       stride_bk = N, stride_bn = 1.\n\n    #   But note: the kernel expects the B matrix to be passed in row-major. We have B as (M, N) in row-major -> so yes.\n\n    # Therefore, we set:\n    #   stride_am = M   # because the row dimension of A is B (which is M in the kernel) and the row stride is M (the number of columns in A, which is K in the kernel? but note: in A the row has length K -> so the stride is K? but we set K=M -> so M is the same as K?).\n\n    #   Actually, in the kernel, the matrix A is of shape (M, K). The stride for the first dimension (M) should be the number of elements in a row of A -> which is K. \n    #   But we set K = M (the inner dimension of A). So we set stride_am = K = M.\n\n    #   Similarly, for B: the stride for the first dimension (K) is the number of elements in a row of B -> which is N. So stride_bk = N.\n\n    #   And for A, the stride for the second dimension (K) is 1? and for B, the stride for the second dimension (N) is 1? \n\n    #   But note: the kernel uses:\n    #       stride_ak: the stride for the K dimension in A -> which is the inner dimension? and in row-major, it's 1.\n    #       stride_am: the stride for the M dimension in A -> which is the outer dimension? and it's the entire row -> so K.\n    #   So we set:\n    #       stride_am = K   # which is M (the inner dimension of the original A)\n    #       stride_ak = 1\n    #       stride_bk = N   # because the kernel expects the row stride of B (which is the first dimension, of size K) to be N (the inner dimension of B)\n    #       stride_bn = 1\n\n    # But note: our A (B, M) is stored as [B, M] -> the stride for the first dimension (B) is M, and the second dimension (M) is 1.\n    #   So in terms of the kernel's A (which is (M, K) = (B, M)): \n    #       the first dimension (M) has stride M (which is the same as K, because K=M) -> so stride_am = M = K.\n    #       the second dimension (K) has stride 1 -> so stride_ak = 1.\n\n    #   Similarly, for B (M, N) stored as [M, N]:\n    #       the first dimension (M) has stride N -> so stride_bk = N (because the kernel's B is (K, N) and the first dimension is K, which we set to M -> so the stride for the first dimension of B is N) and stride_bn = 1.\n\n    #   So we set:\n    #       stride_am = M   # because K = M, so K is M -> but note: the kernel expects stride_am to be the stride for the M dimension (which is the number of elements per row) -> which is K. And K=M -> so we set stride_am = M.\n    #       stride_ak = 1\n    #       stride_bk = N\n    #       stride_bn = 1\n\n    # Now call the kernel.\n\n    # Then after the kernel, we add the bias.\n\n    # The kernel writes to output (which is FP32) of shape (B, N).\n\n    # Then we add the bias (if present) which is of shape (N,). We can do:\n    #   if bias is not None:\n    #       output += bias   # broadcasting\n\n    # But note: the bias in the linear layer is of shape (out_features,) = (N,).\n\n    # Return output.\n\nWe'll write the kernel as in the tutorial, but note the tutorial kernel has:\n\n    # pid for the row of C\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\nWe set the grid as (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n\nWe must also set the tuning configurations.\n\nBut note: the tutorial kernel uses:\n\n    BLOCK_SIZE_M = 128\n    BLOCK_SIZE_N = 64\n    BLOCK_SIZE_K = 32\n    GROUP_SIZE_M = 8\n\nWe can use autotune.\n\nWe'll copy the autotune configs from the tutorial.\n\nBut note: the tutorial kernel has:\n\n    configs = [\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n    ]\n\nWe'll use that.\n\nWe must be cautious: the kernel uses tensor cores, so it requires the block sizes to be multiples of 16? \n\nLet's code accordingly.\n\nWe'll define the custom_linear function and the kernel.\n\nBut note: the kernel requires the dimensions to be powers of two? Not necessarily, but the block sizes must be multiples of 16 for tensor cores.\n\nWe must handle boundaries: the kernel already has boundary checks.\n\nWe'll write the kernel and the function.\n\nHowever, note: the current model is defined with:\n    self.gemm = nn.Linear(in_features, out_features)\n\nWe are replacing the call to self.gemm with custom_linear. But note: the linear layer has a bias? Yes, unless we set bias=False. \n\nThe current model does have bias? Yes, because we did not set bias=False. So we must pass the bias.\n\nWe'll write the function to handle bias.\n\nWe must also note: the kernel is for a fixed set of block sizes? We use autotune.\n\nWe must also note: the kernel uses tensor cores, which require the input to be in FP16. We convert the input and weight to FP16. But the output is FP32.\n\nNow, let's write the code.\n\nBut note: the entire code block must be within the EVOLVE-BLOCK. We are replacing the current ModelNew class.\n\nWe are going to:\n\n  1. Import triton and triton.language.\n  2. Define the kernel and the custom_linear function.\n  3. Then define the ModelNew class, which uses custom_linear in the forward.\n\nWe must be cautious: the kernel definition must be inside the EVOLVE-BLOCK? The problem says to return the complete updated code containing the EVOLVE-BLOCK comment marker.\n\nWe'll put everything inside the EVOLVE-BLOCK.\n\nBut note: the current code has:\n\n    import torch\n    import torch.nn as nn\n\nWe must add:\n    import triton\n    import triton.language as tl\n\nWe'll do that.\n\nWe'll write the kernel and the custom_linear function, then the ModelNew class.\n\nBut note: the current ModelNew class has parameters (gemm, scale, bn). We keep them.\n\nWe only change the forward function.\n\nWe must also note: the custom_linear function must be defined outside the class.\n\nSo the structure:\n\n    import torch\n    import torch.nn as nn\n    import triton\n    import triton.language as tl\n\n    # Define the kernel and custom_linear\n\n    class ModelNew(nn.Module):\n        ... # same __init__\n        def forward(self, x):\n            # use custom_linear\n\n    ... # batch_size, etc.\n\nBut note: the kernel uses tl.extra.libdevice? Not in this kernel.\n\nWe'll write the kernel.\n\nDue to the length, we might exceed the token limit? We must be concise.\n\nWe'll write the kernel and hope it fits.\n\nBut note: the problem says to change only the Triton kernel source within the block. But we are also changing the forward and adding imports.\n\nThe entire block is replaceable? Yes, because the problem says \"return the complete updated code containing the EVOLVE-BLOCK\".\n\nSo we do:\n\n    # ================== EVOLVE-BLOCK-START ==================\n    ... our code ...\n    # =================== EVOLVE-BLOCK-END ===================\n\nLet's write the code.\n\nNote: we must be cautious of the dimensions and the strides.\n\nWe'll write the kernel as in the tutorial, but adjust the parameters.\n\nBut note: the tutorial kernel uses:\n\n    a_ptr, b_ptr, c_ptr,  # pointers to matrices\n    M, N, K: tl.constexpr,  # matrix dimensions\n    stride_am, stride_ak,  # strides for A\n    stride_bk, stride_bn,  # strides for B\n    stride_cm, stride_cn,  # strides for C\n\nWe must set stride_cm = N, stride_cn = 1? because the output is (B, N) -> row-major -> so the row stride is N and column stride is 1.\n\nSo we set:\n    stride_cm = N\n    stride_cn = 1\n\nBut note: our output is of shape (B, N) -> the kernel's C is (M, N) -> so M=B, N=N -> the row stride (for M) should be N.\n\nSo we set stride_cm = N, stride_cn = 1.\n\nWe are ready.\n\nWe'll write the kernel and then the custom_linear function.\n\nBut note: the kernel from the tutorial uses:\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n\nWe don't need the modulo? But it's for handling boundaries. We keep it.\n\nWe'll now write the code.\n\nNote: We are making an iterative change. The problem says to change <=20 lines? This is more, but we are introducing a kernel.\n\nWe hope it's acceptable.\n\nIf it's too long, we might have to skip the autotune configs? But the tutorial has 8 configs.\n\nWe'll include them.\n\nLet's code.\n</think>\nTo improve performance, we'll replace the PyTorch linear layer with an optimized Triton kernel that leverages tensor cores for faster matrix multiplication. The Triton kernel uses FP16 computation with tensor cores while maintaining FP32 accumulation for numerical accuracy. This implementation also includes autotuning for optimal block sizes and efficient memory access patterns.\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n    \n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        a = tl.load(a_ptrs, mask=k_mask[None, :], other=0.0)\n        b = tl.load(b_ptrs, mask=k_mask[:, None], other=0.0)\n        a = a.to(tl.float16)\n        b = b.to(tl.float16)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    offs_cm = offs_m[:, None]\n    offs_cn = offs_n[None, :]\n    c_ptrs = c_ptr + stride_cm * offs_cm + stride_cn * offs_cn\n    c_mask = (offs_cm < M) & (offs_cn < N)\n    \n    if bias_ptr is not None:\n        bias_ptrs = bias_ptr + offs_n\n        bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\ndef triton_linear(x, weight, bias=None):\n    M, K = x.shape\n    K, N = weight.shape\n    x = x.contiguous()\n    weight = weight.contiguous().t()\n    output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n    \n    grid = lambda opt: (triton.cdiv(M, opt['BLOCK_M']) * triton.cdiv(N, opt['BLOCK_N']),)\n    matmul_kernel[grid](\n        x, weight, output, bias,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        output.stride(0), output.stride(1),\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.randn(out_features))\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x):\n        x = triton_linear(x, self.weight, self.bias)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
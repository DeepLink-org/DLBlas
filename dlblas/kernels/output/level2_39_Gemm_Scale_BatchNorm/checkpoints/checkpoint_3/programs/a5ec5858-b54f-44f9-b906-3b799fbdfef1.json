{"id": "a5ec5858-b54f-44f9-b906-3b799fbdfef1", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 128, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'BLOCK_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 128, 'BLOCK_K': 128}, num_warps=8),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef fused_linear_scale_kernel(\n    x_ptr, weight_ptr, bias_ptr, scale_ptr, output_ptr,\n    in_features, out_features,\n    stride_x_batch, stride_x_feature,\n    stride_weight_out, stride_weight_in,\n    stride_output_batch, stride_output_feature,\n    BLOCK_SIZE: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_block = tl.program_id(1)\n    \n    feature_offset = pid_block * BLOCK_SIZE\n    feature_idx = feature_offset + tl.arange(0, BLOCK_SIZE)\n    feature_mask = feature_idx < out_features\n    \n    x_row_ptr = x_ptr + pid_batch * stride_x_batch\n    output_row_ptr = output_ptr + pid_batch * stride_output_batch + feature_offset\n    \n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_K):\n        k_idx = k + tl.arange(0, BLOCK_K)\n        k_mask = k_idx < in_features\n        \n        x_val = tl.load(x_row_ptr + k_idx, mask=k_mask, other=0.0)\n        \n        weight_ptr_block = weight_ptr + feature_idx[:, None] * stride_weight_out + k_idx[None, :]\n        weight_vals = tl.load(weight_ptr_block, mask=feature_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        acc += tl.sum(weight_vals * x_val[None, :], axis=1)\n    \n    bias_vals = tl.load(bias_ptr + feature_idx, mask=feature_mask, other=0.0)\n    scale_vals = tl.load(scale_ptr + feature_idx, mask=feature_mask, other=0.0)\n    \n    result = (acc + bias_vals) * scale_vals\n    tl.store(output_row_ptr + tl.arange(0, BLOCK_SIZE), result, mask=feature_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.zeros(out_features))\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = x.contiguous()\n        batch_size, _ = x.shape\n        output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size, triton.cdiv(out_features, 128))\n        fused_linear_scale_kernel[grid](\n            x, self.weight, self.bias, self.scale, output,\n            in_features, out_features,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE=128\n        )\n        \n        output = self.bn(output)\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "d21da7c2-24dd-4228-a7cb-d6da371ea2b9", "generation": 1, "timestamp": 1754573429.9324841, "iteration_found": 3, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_39_Gemm_Scale_BatchNorm", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpeijgyhyf.py\", line 68, in forward\n    fused_linear_scale_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 135, in _bench\n    raise ValueError(f\"Conflicting meta-parameters: {', '.join(conflicts)}.\"\nValueError: Conflicting meta-parameters: BLOCK_SIZE. Make sure that you don't re-define auto-tuned symbols.\n", "runtime_error_name": "builtins.ValueError", "reference_timing_ms": 0.0935, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0829, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0935, "speed_up": 1.128, "custom_timing": 0.0829}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0829\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0935, 'speed_up': 1.128, 'custom_timing': 0.0829}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0829, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0935, 'speed_up': 1.128, 'custom_timing': 0.0829}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, scales the result, and applies batch normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0829), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0935, 'speed_up': 1.128, 'custom_timing': 0.0829})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, scales the result, and applies batch normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear layer (gemm)\n  2. Element-wise scaling by a parameter vector\n  3. BatchNorm1d\n\nThe current implementation is entirely in PyTorch and runs on the GPU. However, we are asked to rewrite it using Triton for performance.\n\nBut note: the current model uses high-level PyTorch operations (nn.Linear and nn.BatchNorm1d) which are already optimized by PyTorch. However, the element-wise scaling is a simple operation.\n\nHowever, the task is to rewrite the entire forward pass (or at least the key operations) in Triton to get a speedup.\n\nBut note: the provided example shows a Triton kernel for quantization. We are not doing quantization here.\n\nWe must be cautious: the instructions say to only modify the Triton kernel source within the EVOLVE-BLOCK. However, the current code does not have a Triton kernel.\n\nTherefore, we must design a Triton kernel that fuses the operations: linear (matrix multiplication) + scaling + batch normalization.\n\nWhy fuse? Because we can avoid writing intermediate results to global memory and do everything in the kernel.\n\nHowever, note that batch normalization is a stateful operation (it has running mean and variance during training) and during inference it uses fixed statistics.\n\nBut the problem does not specify whether we are in training or inference. The current model uses nn.BatchNorm1d which behaves differently in training and eval.\n\nGiven the complexity, we might break down:\n\nOption 1: Fuse the linear and scaling, and leave batch norm as a separate step? But then we don't get the full benefit.\n\nOption 2: Fuse all three. But note that batch norm has two stages: training and inference. We must support both? The model is an nn.Module.\n\nHowever, the instructions say: \"Generate functionally equivalent but faster Triton implementations\".\n\nWe are allowed to change the internal implementation, but the forward function must behave the same.\n\nWe cannot change the signature of the model.\n\nPlan:\n\n1. We will write a Triton kernel that does the linear (matrix multiplication) and the scaling in one step. Then we leave the batch norm as is? But that might not give enough speedup.\n\n2. Alternatively, we can also fuse the batch norm? But note: batch norm requires statistics over the batch dimension. Our kernel would have to compute mean and variance for the entire batch? That would require communication across threads? And we have to update the running statistics during training.\n\nThis is very complex.\n\nBut note: the current model has batch_size=128, in_features=1024, out_features=512.\n\nWe can design a kernel that:\n\n  - Each program ID (pid) handles one row (one sample in the batch) and one output feature block? Or one row and multiple output features?\n\nBut the batch norm is applied per feature across the batch. So to compute the mean for a feature, we need to see all the batch elements for that feature.\n\nWe can do:\n\n  Step 1: Fused linear + scaling for the entire batch. We can write a kernel that for each row (batch element) and for a block of output features, computes the linear and scaling.\n\n  Step 2: Then we do batch norm. But note: during training, the batch norm layer will compute the mean and variance for the current batch and update the running statistics. During inference, it uses the running statistics.\n\nWe cannot easily fuse the batch norm without a kernel that does reduction across the batch.\n\nAlternatively, we can write a kernel that does:\n\n  - For the linear and scaling: this is straightforward.\n\n  - For the batch norm: we can precompute the mean and variance in a separate kernel? But that would be two passes.\n\nGiven the complexity and the fact that the batch norm is a separate operation that PyTorch already optimizes, we might focus on fusing the linear and scaling.\n\nBut note: the scaling is an element-wise operation and is very cheap. The linear layer (matrix multiplication) is the heavy part.\n\nTherefore, the main opportunity is to write an efficient matrix multiplication kernel in Triton.\n\nHowever, the problem says that we are on H100/H800 with Tensor Cores. We should use them.\n\nWe can write a Triton kernel for the linear layer that uses Tensor Cores? And then we also do the scaling in the same kernel? The scaling is per output feature, so we can load the scale vector once per block.\n\nSteps for the fused linear+scaling kernel:\n\n  Let x: [batch_size, in_features]\n  Let weight: [out_features, in_features]\n  Let bias: [out_features]  (note: the current model uses nn.Linear which has bias by default? But we see the model does not have bias? Actually, the current model does not show the Linear layer having bias? We must check: nn.Linear has bias by default. But in the model, we don't see bias being used in the forward? Actually, the current model does not use the bias? It does: self.gemm = nn.Linear(...). Then in forward: x = self.gemm(x). So it uses both weight and bias.\n\nBut then we do: x * self.scale. And then batch norm.\n\nSo the operations are:\n\n  x = x @ weight.T + bias   [batch_size, out_features]\n  x = x * scale   [batch_size, out_features]  (scale is [out_features] and broadcasted)\n\nWe can fuse:\n\n  output = (x @ weight.T + bias) * scale\n\nBut note: we can also fuse the batch norm? The batch norm does:\n\n  y = (x - mean) / sqrt(var + eps) * weight + bias\n\nBut the batch norm has its own weight and bias (affine parameters) and also the running mean and running var.\n\nHowever, during training, we have to update the running mean and var. That is not trivial to fuse.\n\nGiven the complexity, we might start by fusing the linear and scaling.\n\nWe can write a kernel that does:\n\n  For each batch element (row) and for a block of output features, we compute the matrix multiplication and then scale the result.\n\nBut note: the bias is added and then scaled? So:\n\n  output[i, j] = ( (x[i, :] @ weight[j, :] + bias[j]) ) * scale[j]\n\nWe can do:\n\n  out_block = (x_block @ weight_block.T)  [this computes the dot product for a block of output features?] but we also need to add bias and then multiply by scale.\n\nHowever, the entire weight matrix is [out_features, in_features]. We can break the output features into blocks.\n\nWe design:\n\n  - The grid: (batch_size, num_blocks) where num_blocks = ceil(out_features / BLOCK_SIZE)\n\n  - Each program will compute a block of BLOCK_SIZE output features for one batch element.\n\n  - We load a block of the input row (x[i, :]) and the corresponding weight block (weight[j_block_start:j_block_end, :]).\n\n  - We compute the dot product for the block of output features.\n\n  - Then we add the bias for the block and then multiply by the scale for the block.\n\nBut note: the bias and scale are vectors of length out_features. We can load a block of bias and scale.\n\nHowever, we must be cautious: the current model uses nn.Linear which has a bias. We are including that.\n\nBut what about the batch norm? We are not fusing the batch norm. We will leave that as a separate step.\n\nSo the model would become:\n\n  def forward(self, x):\n      # Instead of: x = self.gemm(x); x = x * self.scale\n      # We do: x = fused_linear_scale(x, self.gemm.weight, self.gemm.bias, self.scale)\n      x = self.bn(x)\n\nWe are replacing the two operations (gemm and scaling) with one fused kernel.\n\nNow, we write the Triton kernel for fused_linear_scale.\n\nSteps for the kernel:\n\n  Inputs:\n    x: input tensor [batch_size, in_features]\n    weight: [out_features, in_features]\n    bias: [out_features]\n    scale: [out_features]\n\n  Output: [batch_size, out_features]\n\n  We will have:\n    grid = (batch_size, triton.cdiv(out_features, BLOCK_SIZE))\n\n  Each program (i, block_id) handles:\n    row i of x, and a block of output features: [block_id*BLOCK_SIZE : (block_id+1)*BLOCK_SIZE]\n\n  Steps in the kernel:\n    pid_batch = tl.program_id(0)\n    pid_block = tl.program_id(1)\n\n    # Create pointers for the row in x\n    x_ptr = x + pid_batch * in_features_stride\n\n    # We'll iterate over the input features in blocks to compute the dot product.\n\n    # We need to precompute the pointer for the weight block: weight + pid_block * BLOCK_SIZE * in_features_stride\n\n    # But note: the weight matrix is stored in row-major: each row is an output feature.\n\n    # We'll use BLOCK_SIZE for the output features and BLOCK_K for the input features.\n\n    # We'll accumulate the result in a variable for the block.\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Loop over the input features in blocks of size BLOCK_K\n    for k in range(0, in_features, BLOCK_K):\n        # Load a block of x: [BLOCK_K]\n        x_block = tl.load(x_ptr + k + tl.arange(0, BLOCK_K), mask=(tl.arange(0, BLOCK_K) < in_features - k), other=0.0)\n\n        # Load a block of weight: [BLOCK_SIZE, BLOCK_K]\n        weight_block_ptr = weight + pid_block * BLOCK_SIZE * in_features_stride + k\n        # We need to broadcast the mask for weight: [BLOCK_SIZE, BLOCK_K]\n        mask_w = (tl.arange(0, BLOCK_SIZE)[:, None] < out_features - pid_block * BLOCK_SIZE) & (tl.arange(0, BLOCK_K)[None, :] < in_features - k)\n        weight_block = tl.load(weight_block_ptr + tl.arange(0, BLOCK_SIZE)[:, None] * in_features_stride + tl.arange(0, BLOCK_K)[None, :], mask=mask_w, other=0.0)\n\n        # Compute the dot product: [BLOCK_SIZE] = [BLOCK_SIZE, BLOCK_K] @ [BLOCK_K]\n        # We do: for each row in weight_block: dot with x_block\n        acc += tl.sum(weight_block * x_block[None, :], axis=1)\n\n    # Now add bias: we load a block of bias [BLOCK_SIZE]\n    bias_block = tl.load(bias + pid_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE), mask=tl.arange(0, BLOCK_SIZE) < out_features - pid_block * BLOCK_SIZE, other=0.0)\n    acc += bias_block\n\n    # Multiply by scale: we load a block of scale [BLOCK_SIZE]\n    scale_block = tl.load(scale + pid_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE), mask=tl.arange(0, BLOCK_SIZE) < out_features - pid_block * BLOCK_SIZE, other=0.0)\n    acc = acc * scale_block\n\n    # Write the result: [BLOCK_SIZE] for this batch and output block\n    output_ptr = output + pid_batch * out_features_stride + pid_block * BLOCK_SIZE\n    tl.store(output_ptr + tl.arange(0, BLOCK_SIZE), acc, mask=tl.arange(0, BLOCK_SIZE) < out_features - pid_block * BLOCK_SIZE)\n\nBut note: we must be cautious about the strides.\n\nAlternatively, we can use the Triton matrix multiplication tutorial as a base.\n\nHowever, the problem says we must not change the grid configuration? Actually, we are defining the grid ourselves.\n\nBut note: the constraints say: \"NEVER change @triton.jit function signatures or parameter names\" \u2014 but we are writing a new kernel? Actually, we are replacing the entire model.\n\nWait, the current code block is:\n\n  class ModelNew(nn.Module):\n      ... and then the batch_size, etc.\n\nWe are going to change the ModelNew class to use a custom kernel.\n\nBut note: the instructions say: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" \u2014 but the current block does not contain a Triton kernel? It's a PyTorch model.\n\nThis is confusing.\n\nWe must read: \"The Evolution system will pass the EVOLVE-BLOCK from the previous version. Modify ONLY the Triton kernel source within this block\".\n\nBut the current block is a PyTorch model. We are allowed to change the entire block? The output format says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\nSo we can replace the entire block with a new one that uses Triton.\n\nBut note: the model must have the same inputs and outputs. The get_inputs and get_init_inputs must also be the same? Because they are used for testing.\n\nTherefore, we will:\n\n  - Keep the class name: ModelNew\n  - Keep the same __init__ parameters: in_features, out_features, scale_shape, eps, momentum\n  - Keep the same batch_size, in_features, out_features, scale_shape in the global scope.\n\nBut we change the forward method to use a Triton kernel for the linear and scaling.\n\nWe also need to write the Triton kernel and the helper function.\n\nWe note:\n\n  The current model uses:\n      self.gemm = nn.Linear(in_features, out_features)\n      self.scale = nn.Parameter(torch.randn(scale_shape))\n\n  We will not use nn.Linear, but we will store the weight and bias as parameters? And then also the scale.\n\n  We also keep the batch norm.\n\n  So in __init__:\n\n      self.weight = nn.Parameter(torch.randn(out_features, in_features))\n      self.bias = nn.Parameter(torch.zeros(out_features))\n      self.scale = nn.Parameter(torch.randn(scale_shape))\n      self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n  And then in forward, we call our fused kernel.\n\nBut note: the original nn.Linear uses a weight matrix of shape [out_features, in_features] and bias [out_features]. And the scale is also [out_features]. So we are good.\n\nWe will write:\n\n  def forward(self, x):\n      # x: [batch_size, in_features]\n      output = torch.empty((x.size(0), out_features), device=x.device, dtype=x.dtype)\n      # Call the Triton kernel with grid (batch_size, ceil(out_features/BLOCK_SIZE))\n      fused_linear_scale_kernel[(x.size(0), triton.cdiv(out_features, BLOCK_SIZE))]( ... )\n      output = self.bn(output)\n      return output\n\nWe must write the kernel and the helper function.\n\nBut note: the kernel must be compiled and we must handle the strides.\n\nWe are going to write the kernel as described above.\n\nHowever, note that the input x might not be contiguous? We can require contiguous.\n\nWe must also consider the data types. The model uses the default (float32). We can use float32 in the kernel.\n\nBut for Tensor Cores, we might want to use FP16? The problem says the hardware has FP16 Tensor Cores. However, the model is in float32. We can do the computation in float32? Or we can change the model to use FP16? But we must be functionally equivalent.\n\nWe are not allowed to change the dtype. So we do everything in float32.\n\nBut note: the Tensor Cores on H100 can do FP32? Actually, they can do FP32, but the throughput for FP32 is lower than FP16. But we are constrained by the problem.\n\nAlternatively, we can use FP16 if the input is FP16? But the problem does not specify. The get_inputs uses torch.randn which is float32.\n\nWe will do everything in float32.\n\nWe must also note: the kernel must be autotuned? We can provide multiple configurations.\n\nWe will write the kernel and then wrap it in a function.\n\nGiven the complexity, we will write the kernel in the same file.\n\nBut note: the EVOLVE-BLOCK must contain the entire code? Yes.\n\nWe are going to write a long code block.\n\nWe also note: the original model had batch_size, in_features, out_features, scale_shape as global variables? And also get_inputs and get_init_inputs. We must keep them.\n\nWe are going to restructure the code block.\n\nLet's write:\n\n  import torch\n  import torch.nn as nn\n  import triton\n  import triton.language as tl\n\n  # Define the fused linear scale kernel\n\n  @triton.autotune(...)\n  @triton.jit\n  def fused_linear_scale_kernel(...):\n      ...\n\n  class ModelNew(nn.Module):\n      ... (with the new parameters and the kernel call)\n\n  batch_size = ...\n  ... (the rest)\n\nBut note: the kernel must be defined before the model.\n\nWe must be cautious about the BLOCK_SIZE and BLOCK_K. We will set them as triton.constexpr.\n\nWe will choose a BLOCK_SIZE for output features (e.g., 128) and BLOCK_K for input features (e.g., 64). But we will autotune.\n\nHowever, due to the complexity, we might start with a fixed configuration and then autotune.\n\nWe must also note: the kernel must be generic for any batch_size, in_features, out_features.\n\nWe will write the kernel and then the model.\n\nWe are going to replace the entire EVOLVE-BLOCK.\n\nBut note: the previous program was a simple PyTorch model. We are replacing it with a Triton version.\n\nWe must be cautious: the current program is in the EVOLVE-BLOCK. We are going to rewrite the entire block.\n\nLet's write the new code block.\n\nImportant: we must not change the function signatures of the model (__init__ and forward) and the global variables (batch_size, in_features, ...) and the functions get_inputs and get_init_inputs.\n\nWe change the internal implementation of the model.\n\nWe are going to write:\n\n  import torch\n  import torch.nn as nn\n  import triton\n  import triton.language as tl\n\n  # Triton kernel for fused linear + scaling\n  @triton.jit\n  def fused_linear_scale_kernel(\n      x_ptr, weight_ptr, bias_ptr, scale_ptr, output_ptr,\n      in_features, out_features,\n      stride_x_batch, stride_x_feature,\n      stride_weight_feature_out, stride_weight_feature_in,\n      stride_output_batch, stride_output_feature,\n      BLOCK_SIZE: tl.constexpr, BLOCK_K: tl.constexpr,\n  ):\n      # We have two-dimensional grid: batch and output feature block\n      pid_batch = tl.program_id(0)\n      pid_block = tl.program_id(1)\n\n      # The block of output features we are processing\n      feature_offset = pid_block * BLOCK_SIZE\n      feature_index = feature_offset + tl.arange(0, BLOCK_SIZE)\n      mask_feature = feature_index < out_features\n\n      # Pointers to the current batch row in x\n      x_row_ptr = x_ptr + pid_batch * stride_x_batch\n\n      # Accumulator for the output block\n      acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n      # Loop over input features by blocks of BLOCK_K\n      for k in range(0, in_features, BLOCK_K):\n          k_index = k + tl.arange(0, BLOCK_K)\n          mask_k = k_index < in_features\n\n          # Load a block of x\n          x_val = tl.load(x_row_ptr + k_index, mask=mask_k, other=0.0)\n\n          # Load a block of weight: [BLOCK_SIZE, BLOCK_K]\n          weight_block_ptr = weight_ptr + feature_offset * stride_weight_feature_out + k_index\n          # We need to load a matrix: [BLOCK_SIZE, BLOCK_K]\n          # The weight matrix is stored as [out_features, in_features] -> stride: stride_weight_feature_out for the output feature dimension, stride_weight_feature_in for the input feature? Actually, we have:\n          #   weight_ptr + i*stride_weight_feature_out + j*stride_weight_feature_in\n          # But in our case, the weight matrix is stored in row-major: so stride_weight_feature_out = in_features? and stride_weight_feature_in = 1?\n          # We are given the strides. We assume the weight matrix is contiguous? But we have the strides.\n\n          # We'll load with:\n          #   weight_block_ptr + tl.arange(0, BLOCK_SIZE)[:, None] * stride_weight_feature_out + tl.arange(0, BLOCK_K)[None, :] * stride_weight_feature_in\n          # But note: we are at a fixed block of output features and a fixed block of input features.\n\n          # Create a mask for the weight block: [BLOCK_SIZE, BLOCK_K]\n          mask_weight = mask_feature[:, None] & mask_k[None, :]\n\n          weight_block = tl.load(\n              weight_block_ptr + tl.arange(0, BLOCK_SIZE)[:, None] * stride_weight_feature_out + tl.arange(0, BLOCK_K)[None, :] * stride_weight_feature_in,\n              mask=mask_weight, other=0.0\n          )\n\n          # Compute the dot product: [BLOCK_SIZE] = sum over k: weight_block * x_val[None, :]\n          acc += tl.sum(weight_block * x_val[None, :], axis=1)\n\n      # Load bias and scale for the block\n      bias_val = tl.load(bias_ptr + feature_index, mask=mask_feature, other=0.0)\n      scale_val = tl.load(scale_ptr + feature_index, mask=mask_feature, other=0.0)\n\n      # Compute: (acc + bias_val) * scale_val\n      result = (acc + bias_val) * scale_val\n\n      # Write the result\n      output_row_ptr = output_ptr + pid_batch * stride_output_batch + feature_offset * stride_output_feature\n      tl.store(output_row_ptr + tl.arange(0, BLOCK_SIZE), result, mask=mask_feature)\n\n  class ModelNew(nn.Module):\n      def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n          super(ModelNew, self).__init__()\n          # Instead of using nn.Linear, we store the weight and bias as parameters\n          self.weight = nn.Parameter(torch.randn(out_features, in_features))\n          self.bias = nn.Parameter(torch.zeros(out_features))\n          self.scale = nn.Parameter(torch.randn(scale_shape))\n          self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n      def forward(self, x):\n          # Ensure contiguous\n          x = x.contiguous()\n          batch_size, _ = x.shape\n          output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n\n          # Check the strides\n          stride_x_batch = x.stride(0)\n          stride_x_feature = x.stride(1)\n          # For weight: [out_features, in_features] -> strides: (in_features, 1) if contiguous\n          stride_weight_feature_out = self.weight.stride(0)\n          stride_weight_feature_in = self.weight.stride(1)\n          stride_output_batch = output.stride(0)\n          stride_output_feature = output.stride(1)\n\n          # Grid: (batch_size, ceil(out_features / BLOCK_SIZE))\n          grid = (batch_size, triton.cdiv(out_features, BLOCK_SIZE))\n\n          # We choose BLOCK_SIZE and BLOCK_K. We set BLOCK_SIZE to 128 and BLOCK_K to 64 for now.\n          # We can make them configurable with autotune in the future.\n          BLOCK_SIZE = 128\n          BLOCK_K = 64\n\n          fused_linear_scale_kernel[grid](\n              x, self.weight, self.bias, self.scale, output,\n              in_features, out_features,\n              stride_x_batch, stride_x_feature,\n              stride_weight_feature_out, stride_weight_feature_in,\n              stride_output_batch, stride_output_feature,\n              BLOCK_SIZE, BLOCK_K\n          )\n\n          output = self.bn(output)\n          return output\n\n  batch_size = 128\n  in_features = 1024\n  out_features = 512\n  scale_shape = (out_features,)\n\n  def get_inputs():\n      return [torch.randn(batch_size, in_features)]\n\n  def get_init_inputs():\n      return [in_features, out_features, scale_shape]\n\nBut note: we must not change the get_inputs and get_init_inputs.\n\nHowever, we have a problem: the model now has two parameters (weight and bias) instead of one (gemm). But the original model had the gemm as a submodule. The parameters are now stored directly in the model.\n\nThis is acceptable because the overall parameters are the same: weight, bias, scale, and the batch norm parameters.\n\nWe are replacing the nn.Linear with our own parameters.\n\nBut note: the original model had the Linear module, which has its own weight and bias. The state_dict would change? But the problem does not specify that we must preserve the state_dict structure, only the functionality.\n\nWe are maintaining the same parameters (with the same names? not exactly). But the problem says: \"functionally equivalent\".\n\nWe must also note: the original model had the Linear module, and now we are not using it. But the behavior should be the same.\n\nWe will hope that this is acceptable.\n\nWe must also note: the kernel might not be faster than the highly optimized cuBLAS for the linear layer? But we are fusing the scaling. And we are using a kernel that might be optimized for the specific sizes? We are using a simple kernel without Tensor Cores (since we are in float32).\n\nBut the problem says we can use Tensor Cores for FP16. But we are in float32. So we cannot.\n\nAlternatively, we can use FP16 if the input is FP16? But the input is FP32.\n\nWe might need to change the model to use FP16? But we are not allowed to change the interface. The get_inputs returns FP32.\n\nTherefore, we stick to FP32.\n\nWe hope that the fusion of the scaling saves memory bandwidth.\n\nBut note: the scaling is a cheap operation. The main gain might be from the custom matrix multiplication? But it might be slower than cuBLAS.\n\nGiven the risk, we might consider only fusing the scaling with the linear without writing the entire matrix multiplication? But that would require writing the intermediate result and then reading it again for scaling.\n\nWe are fusing so we avoid writing and then reading the intermediate.\n\nBut the current kernel is a naive implementation. We must optimize it.\n\nWe can use tiling and shared memory? We are already tiling the input features.\n\nBut we are not using shared memory? We are loading a block of x and a block of weight and then doing the dot product.\n\nWe must note: the input block (x_val) is reused for all the output features in the block? But we are only computing one output block at a time.\n\nWe are not using shared memory for x? Each program loads its own x_block and weight_block.\n\nThis is standard.\n\nBut we are not using Tensor Cores, so we are limited by the computational throughput.\n\nWe might need to autotune the block sizes and the number of warps.\n\nWe will add autotuning.\n\nGiven the complexity and the fact that we are limited by the instruction to change <=20 lines per revision, we might start with a simple kernel and then optimize in the next steps.\n\nBut note: the problem says to return the complete new code block.\n\nWe are going to write the entire block as above, but we will add autotuning.\n\nWe will define multiple configurations for BLOCK_SIZE and BLOCK_K.\n\nBut note: the kernel must be compiled for each configuration.\n\nWe will write:\n\n  @triton.autotune(\n      configs=[\n          triton.Config({'BLOCK_SIZE': 128, 'BLOCK_K': 64}, num_warps=4),\n          triton.Config({'BLOCK_SIZE': 64, 'BLOCK_K': 32}, num_warps=4),\n          triton.Config({'BLOCK_SIZE': 128, 'BLOCK_K': 32}, num_warps=4),\n          triton.Config({'BLOCK_SIZE': 64, 'BLOCK_K': 64}, num_warps=4),\n          triton.Config({'BLOCK_SIZE': 256, 'BLOCK_K': 64}, num_warps=8),\n      ],\n      key=['in_features', 'out_features'],\n  )\n  @triton.jit\n  def fused_linear_scale_kernel(...):\n      ...\n\n  And then in the call, we don't specify BLOCK_SIZE and BLOCK_K? They will be passed by the autotuner.\n\n  So we remove the BLOCK_SIZE and BLOCK_K from the kernel call? Actually, they are passed as constants by the autotuner.\n\n  We remove the BLOCK_SIZE and BLOCK_K from the kernel call in the model.\n\n  We change the model's forward:\n\n      # Instead of specifying BLOCK_SIZE and BLOCK_K, we let autotuner choose.\n      fused_linear_scale_kernel[grid](\n          x, self.weight, self.bias, self.scale, output,\n          in_features, out_features,\n          stride_x_batch, stride_x_feature,\n          stride_weight_feature_out, stride_weight_feature_in,\n          stride_output_batch, stride_output_feature,\n      )\n\n  And in the kernel, we have BLOCK_SIZE and BLOCK_K as tl.constexpr.\n\nBut note: the kernel signature must match. We remove the explicit passing of BLOCK_SIZE and BLOCK_K? But the autotuner will pass them as constants.\n\n  The kernel call in Triton will pass the constants from the chosen config.\n\n  So we remove them from the argument list in the call? But in the kernel definition, they are there as constexpr.\n\n  How does the autotuner work? It will compile a version of the kernel for each config. Then at runtime, it will choose the best one based on the key (in_features, out_features). And then call that kernel with the constants set.\n\n  Therefore, in the call site, we do not pass BLOCK_SIZE and BLOCK_K.\n\n  We must remove them from the call.\n\nLet's adjust.\n\nWe are going to write a complete new EVOLVE-BLOCK.\n\nBut note: the kernel might be long. We hope it is correct.\n\nWe must also note: the kernel must have boundary checks for the input features and output features.\n\nWe have mask_feature and mask_k.\n\nWe are going to write the code.\n\nGiven the complexity, we hope it compiles.\n\nIf it doesn't, we will have to fix in the next iteration.\n\nWe output the entire code block.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 128, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'BLOCK_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 128, 'BLOCK_K': 128}, num_warps=8),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef fused_linear_scale_kernel(\n    x_ptr, weight_ptr, bias_ptr, scale_ptr, output_ptr,\n    in_features, out_features,\n    stride_x_batch, stride_x_feature,\n    stride_weight_out, stride_weight_in,\n    stride_output_batch, stride_output_feature,\n    BLOCK_SIZE: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_block = tl.program_id(1)\n    \n    feature_offset = pid_block * BLOCK_SIZE\n    feature_idx = feature_offset + tl.arange(0, BLOCK_SIZE)\n    feature_mask = feature_idx < out_features\n    \n    x_row_ptr = x_ptr + pid_batch * stride_x_batch\n    output_row_ptr = output_ptr + pid_batch * stride_output_batch + feature_offset\n    \n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_K):\n        k_idx = k + tl.arange(0, BLOCK_K)\n        k_mask = k_idx < in_features\n        \n        x_val = tl.load(x_row_ptr + k_idx, mask=k_mask, other=0.0)\n        \n        weight_ptr_block = weight_ptr + feature_idx[:, None] * stride_weight_out + k_idx[None, :]\n        weight_vals = tl.load(weight_ptr_block, mask=feature_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        acc += tl.sum(weight_vals * x_val[None, :], axis=1)\n    \n    bias_vals = tl.load(bias_ptr + feature_idx, mask=feature_mask, other=0.0)\n    scale_vals = tl.load(scale_ptr + feature_idx, mask=feature_mask, other=0.0)\n    \n    result = (acc + bias_vals) * scale_vals\n    tl.store(output_row_ptr + tl.arange(0, BLOCK_SIZE), result, mask=feature_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.zeros(out_features))\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = x.contiguous()\n        batch_size, _ = x.shape\n        output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size, triton.cdiv(out_features, 128))\n        fused_linear_scale_kernel[grid](\n            x, self.weight, self.bias, self.scale, output,\n            in_features, out_features,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE=128\n        )\n        \n        output = self.bn(output)\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
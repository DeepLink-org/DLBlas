{"id": "7ab2762a-7acb-441a-b4af-f54e4000202f", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_linear_scale_kernel(\n    x_ptr, w_ptr, b_ptr, s_ptr, out_ptr,\n    in_features, out_features,\n    stride_x, stride_w, stride_b, stride_s, stride_out,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # Batch index\n    pid1 = tl.program_id(1)  # Output feature block index\n    \n    # Create ranges for current block\n    offs_n = pid1 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Create masks for boundaries\n    mask_n = offs_n < out_features\n    mask_k = offs_k < in_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    # Loop over input features\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        # Load block of input data\n        x_vals = tl.load(\n            x_ptr + pid0 * stride_x + k + offs_k, \n            mask=mask_k, other=0.0\n        )\n        \n        # Load block of weights\n        w_vals = tl.load(\n            w_ptr + (offs_n[:, None]) * stride_w + (k + offs_k[None, :]), \n            mask=mask_n[:, None] & mask_k[None, :], other=0.0\n        )\n        \n        # Compute partial dot product\n        x_vals_f16 = x_vals.to(tl.float16)\n        w_vals_f16 = w_vals.to(tl.float16)\n        partial = tl.sum(x_vals_f16[None, :] * w_vals_f16, axis=1)\n        acc += partial.to(tl.float32)\n    \n    # Load bias and scale\n    b_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)\n    s_vals = tl.load(s_ptr + offs_n, mask=mask_n, other=0.0)\n    \n    # Apply bias and scale\n    output = (acc + b_vals) * s_vals\n    \n    # Store result\n    tl.store(\n        out_ptr + pid0 * stride_out + offs_n, \n        output, mask=mask_n\n    )\n\ndef fused_linear_scale(x, weight, bias, scale):\n    batch_size, in_features = x.shape\n    out_features = weight.shape[0]\n    \n    # Preallocate output tensor\n    output = torch.empty((batch_size, out_features), \n                         device=x.device, dtype=torch.float32)\n    \n    # Calculate grid dimensions\n    grid = lambda meta: (\n        batch_size, \n        triton.cdiv(out_features, meta['BLOCK_SIZE_N'])\n    )\n    \n    # Launch kernel with optimized block sizes\n    fused_linear_scale_kernel[grid](\n        x, weight, bias, scale, output,\n        in_features, out_features,\n        x.stride(0), weight.stride(0), \n        bias.stride(0), scale.stride(0), output.stride(0),\n        BLOCK_SIZE_N=128, BLOCK_SIZE_K=64\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x):\n        # Fused linear layer + scaling\n        x = fused_linear_scale(x, self.gemm.weight, self.gemm.bias, self.scale)\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "d21da7c2-24dd-4228-a7cb-d6da371ea2b9", "generation": 1, "timestamp": 1754572716.9867496, "iteration_found": 2, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.926, "runtime_ms": 0.101, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0935, "speed_up": 0.926, "custom_timing": 0.101}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0829, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0935, "speed_up": 1.128, "custom_timing": 0.0829}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0829\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0935, 'speed_up': 1.128, 'custom_timing': 0.0829}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0829, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0935, 'speed_up': 1.128, 'custom_timing': 0.0829}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, scales the result, and applies batch normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0829), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0935, 'speed_up': 1.128, 'custom_timing': 0.0829})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, scales the result, and applies batch normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear layer (gemm)\n  2. Element-wise scaling by a parameter vector (scale)\n  3. BatchNorm1d\n\nThe current implementation is in pure PyTorch. We are to rewrite the forward pass using Triton to speed up the operations.\n\nHowever, note that the entire model is composed of standard layers (Linear, element-wise multiplication, BatchNorm). \nThe Linear layer is a matrix multiplication that can be optimized, but the element-wise scaling and BatchNorm are also present.\n\nBut note: the problem states that we are to rewrite the forward pass. The current forward pass is:\n\n    x = self.gemm(x)\n    x = x * self.scale\n    x = self.bn(x)\n\nWe cannot change the signature of the forward method, and we cannot change the parameters (like replacing the BatchNorm with a custom one) because the model is already defined.\n\nHowever, we are allowed to replace the implementation of the forward pass with a Triton kernel that does the entire sequence of operations? But note the constraints:\n\n  \u274c NEVER change @triton.jit function signatures or parameter names\n  \u274c NEVER modify grid configuration, output tensor shapes, or PID logic\n\nBut in the current code, there is no Triton kernel. We are going to introduce one.\n\nBut note: the model has parameters (weight and bias in the linear layer, scale, and parameters in the BatchNorm). We cannot change the parameters.\n\nPlan:\n\nWe can write a Triton kernel that fuses the three operations? However, the BatchNorm operation is:\n\n    y = (x - running_mean) * weight / sqrt(running_var + eps) + bias\n\nBut note: during training, the running_mean and running_var are updated. However, during inference, they are fixed.\n\nBut the problem does not specify training or inference. We must be cautious.\n\nAlternatively, we can break the operations:\n\n  Step 1: Replace the linear layer with a Triton GEMM.\n  Step 2: Do the scaling and then the BatchNorm.\n\nBut note: the BatchNorm is applied per channel, so it is also a per-channel scaling and shift.\n\nWe can fuse the scaling (by self.scale) and the BatchNorm into a single per-channel operation?\n\n  Let S = self.scale (vector of length out_features)\n  BatchNorm: \n      weight (gamma) and bias (beta) are parameters of the BatchNorm.\n      running_mean and running_var are state.\n\n  After the linear layer, we have a tensor x of shape (batch_size, out_features).\n\n  Then: \n      x = x * S   [element-wise per feature]\n\n  Then BatchNorm does:\n      x = (x - running_mean) * gamma / sqrt(running_var + eps) + beta\n\n  This can be rewritten as:\n      x = x * (gamma * S) / sqrt(running_var + eps) + (beta - gamma * running_mean / sqrt(running_var + eps))\n\n  But note: the running_mean and running_var are computed from the data during training and fixed during eval.\n\n  However, the model might be in training mode or eval mode. We cannot precompute the fused parameters if we are training because the running_mean and running_var are updated after each forward pass.\n\n  Therefore, we cannot fuse the scaling and BatchNorm during training? Because the running statistics are updated based on the output after scaling.\n\n  But note: the BatchNorm update uses the current minibatch. The scaling operation is part of the model, so the input to BatchNorm is the scaled tensor.\n\n  Therefore, we cannot precompute a fixed fusion for the scaling and BatchNorm during training. However, during inference we can.\n\n  The problem does not specify mode. We must design for both? But the evaluation pipeline is for correctness and speed. We must pass correctness.\n\n  Alternatively, we can write the forward pass in Triton without fusing the BatchNorm? Or we can do:\n\n      Option 1: Write a Triton kernel for the GEMM and leave the scaling and BatchNorm in PyTorch.\n      Option 2: Write a kernel that does the GEMM and the scaling, then call BatchNorm.\n      Option 3: Write a kernel that does the entire chain.\n\n  However, note that the BatchNorm operation is relatively cheap (it's O(n)) compared to the GEMM (O(n^2)). The main gain would be from optimizing the GEMM.\n\n  Therefore, let's focus on the linear layer.\n\n  The current linear layer is implemented by PyTorch's `nn.Linear`. We can replace it with a Triton GEMM kernel.\n\n  Steps:\n\n    1. We will write a Triton kernel for matrix multiplication. The kernel should be able to handle the input of shape (batch_size, in_features) and weight of shape (out_features, in_features).\n\n    2. We will replace the `self.gemm(x)` with a call to our Triton GEMM.\n\n    3. We must also handle the bias? The current `nn.Linear` has a bias. We see that in the model, we have `self.gemm = nn.Linear(...)`. By default, `nn.Linear` has bias=True? But we don't see the initialization. We must check the original model.\n\n    However, in the provided code, the `__init__` of `ModelNew` does not specify bias. The default is True.\n\n    So we have to include the bias.\n\n  How to structure:\n\n    We will create a Triton kernel for the linear layer (with bias) and then do the scaling and BatchNorm as before? Or we can also include the scaling in the same kernel? \n\n    Since the scaling is a per-channel operation, we can do:\n\n        output = (x @ weight.t() + bias) * scale\n\n    in one kernel? That would save one read and write of the entire output matrix.\n\n    Then we can do the BatchNorm in PyTorch.\n\n    Alternatively, we can also fuse the BatchNorm if we are in inference mode? But again, we don't know.\n\n  Given the constraints of the problem, let's do:\n\n    We write a Triton kernel for the linear layer (with bias) and then do the scaling and BatchNorm in PyTorch. This is safe.\n\n  However, the problem says: \"Rewrite the program to improve its performance\". We are allowed to change the forward pass.\n\n  We can also consider writing the entire chain (GEMM + scaling + BatchNorm) in one kernel? That would be more efficient.\n\n  But note: the BatchNorm requires computing the mean and variance over the batch for each channel? Actually, no: the BatchNorm layer in inference mode uses the precomputed running_mean and running_var. In training mode, it computes the mean and variance for the current minibatch and updates the running statistics.\n\n  Fusing the entire chain in training mode is complex because we would have to update the running statistics. We are not allowed to change the model's parameters or state update logic.\n\n  Therefore, we will do:\n\n    - For inference: we can precompute the fused parameters and do a simple affine transform per channel.\n    - For training: we cannot precompute, so we would have to compute the mean and variance of the scaled output and then update the running statistics? And then normalize.\n\n  This is beyond the scope of a simple Triton kernel.\n\n  Considering the complexity and the fact that the problem does not specify the mode, we will assume that the model is in eval mode for performance testing? Or we must handle both.\n\n  But note: the evaluation pipeline for correctness: it runs the model and checks the output. We don't know if it sets the model to eval or train.\n\n  Let's look at the original model: it uses `nn.BatchNorm1d` which by default uses the running statistics in eval mode and the minibatch statistics in train mode.\n\n  We are not allowed to change the behavior.\n\n  Therefore, we decide:\n\n    We will write a Triton kernel for the linear layer (with bias) and the scaling (element-wise per channel). Then we will call the BatchNorm as before.\n\n    This way, the BatchNorm layer will behave exactly as before.\n\n    We will fuse the linear and the scaling.\n\n  The new forward pass:\n\n      x = fused_linear_scale(x, self.gemm.weight, self.gemm.bias, self.scale)\n      x = self.bn(x)\n\n  Why fuse linear and scaling? Because it avoids writing the entire output and then reading it again for scaling.\n\n  The kernel for fused linear and scaling:\n\n      Inputs: \n          x: (batch_size, in_features)\n          weight: (out_features, in_features)\n          bias: (out_features,)\n          scale: (out_features,)\n\n      Output: (batch_size, out_features) where each element (i, j) is:\n          out[i, j] = (x[i] @ weight[j] + bias[j]) * scale[j]\n\n  We can write a Triton kernel for this.\n\n  How to write the kernel:\n\n      We can use a block for each row of the output? Because the output has shape (batch_size, out_features). \n\n      Alternatively, we can use a 2D grid: one dimension for batch, one for output features? But note: the scaling is per output feature.\n\n      We can do:\n\n          pid_batch = tl.program_id(0)\n          pid_feature = tl.program_id(1)\n\n      Then we compute one element of the output: \n          out[pid_batch, pid_feature] = (x[pid_batch, :] @ weight[pid_feature, :] + bias[pid_feature]) * scale[pid_feature]\n\n      But this would be inefficient because we are loading the entire row of x and weight for each output element.\n\n      Alternatively, we can do a tiled matrix multiplication.\n\n  However, note that the problem says: the current speedup is 1.128 (so about 12.8% faster than reference) but that was for the pure PyTorch version? Actually, the current program is the reference code? \n\n  The current program is:\n\n      class ModelNew ... \n\n      and the metrics say: \n          stage: reference code\n\n      and the speed_up is 1.0000? But the meta says: \n          'reference_timing_ms': 0.0935, 'speed_up': 1.128, 'custom_timing': 0.0829\n\n      This is confusing.\n\n  But note: the task is to rewrite the program. The current program is the reference. We are to write a Triton version.\n\n  We are going to write a Triton kernel for the fused linear and scaling. We leave the BatchNorm as is.\n\n  Steps for the kernel:\n\n      We will use a 1D grid for the batch dimension? And then within the kernel, loop over the output features? Or use a 2D grid.\n\n      Given that the batch size (128) and out_features (512) are not too large, we can use a 2D grid: \n          grid = (batch_size, out_features)\n\n      But that would be 128 * 512 = 65536 blocks. That might be acceptable.\n\n      Alternatively, we can use a 1D grid for the batch and have each block compute a row of the output (i.e., all features for one batch). Then we would have 128 blocks. But then we need to load the entire weight matrix? That might not fit in shared memory.\n\n      We can do:\n\n          pid = tl.program_id(0)   # over batch_size\n          We will compute the entire row of the output for this batch.\n\n      How to compute one row?\n\n          x_row = x[pid]   # (in_features,)\n          Then for each j in range(out_features):\n              w_row = weight[j]   # (in_features,)\n              dot = tl.sum(x_row * w_row)\n              out[pid, j] = (dot + bias[j]) * scale[j]\n\n      But we cannot loop over out_features in a kernel? We can, but it would be inefficient.\n\n      We can use tiling for the inner product? But note: the inner product is over in_features (1024). We can tile that.\n\n      However, note: we are already going to have one program per batch. We can then have the block size be the number of features we compute in parallel? But we are computing one row of the output, so we need to compute all out_features (512) for one batch. We can have a block of 512 threads? But 512 is the maximum block size? Actually, we can have 1024 threads per block.\n\n      So we can do:\n\n          pid = tl.program_id(0)   # batch index\n\n          We will load the row of x for this batch into shared memory? Or we can load it once per thread? Actually, each thread will need the entire row? That would be 1024 elements. We can load it in chunks.\n\n          Alternatively, we can have each thread compute one output feature? Then we would have 512 threads per block. We can do:\n\n              block of 512 threads.\n\n          Steps:\n\n              pid = tl.program_id(0)   # batch index\n              j = tl.program_id(1)      # output feature index? But we don't have a second dimension in the grid? We can use a 2D grid: (batch_size, out_features) and then each block has 1 thread? That would be inefficient.\n\n          Instead, we can use a 1D grid of size (batch_size) and then a block of size (out_features). Then:\n\n              j = tl.program_id(1) becomes j = tl.arange(0, out_features) within the block? Actually, we can have:\n\n                  block of size = min(BLOCK_SIZE, out_features) and then loop? But 512 is not too big, we can set BLOCK_SIZE=512.\n\n          But note: the number of threads per block is limited to 1024, and 512 is acceptable.\n\n      So:\n\n          pid = tl.program_id(0)   # batch index, from 0 to batch_size-1\n          j = tl.arange(0, BLOCK_SIZE)   # BLOCK_SIZE = out_features? But we set BLOCK_SIZE to 512, which is exactly the out_features.\n\n          But note: we must handle cases where out_features is not a power of two? Here it is 512, which is a power of two.\n\n      Steps in the kernel:\n\n          # Load the row of x for this batch\n          x_ptr + pid * in_features + ... \n          We can load the entire row of x in chunks? Or we can have each thread load one element of x? But we have 512 threads and in_features=1024, so we can assign two elements per thread.\n\n          Alternatively, we can use a loop to compute the inner product. We can tile the inner product by chunks of the input features.\n\n      We'll do:\n\n          Let BLOCK_K = 128 (for example) and we tile the in_features dimension.\n\n          Initialize accumulator to zero.\n\n          for k in range(0, in_features, BLOCK_K):\n              # Load a block of x and weight for the current feature and the current tile\n              x_block = tl.load(x_ptr + pid * in_features + k + tl.arange(0, BLOCK_K), mask = ...)\n              # For weight, we want to load a block for all output features? But we are computing one batch and all output features at once? \n\n              We have a 2D weight matrix: we want to load a block of weight for all output features (512) and the current tile (BLOCK_K) of features.\n\n          But note: we are in a block of 512 threads. We want to load a block of weight of size (BLOCK_K, out_features) and then do a dot product? That would require (BLOCK_K * out_features) elements. That is 128*512=65536 elements per tile, which is too big.\n\n      Alternatively, we can have each thread compute one output feature. Then each thread only needs the row of weight for that feature. So:\n\n          pid = tl.program_id(0)   # batch index\n          j = tl.arange(0, out_features)   # but we are in a block of 512 threads, so each thread j will compute one output feature.\n\n          We can have:\n\n              j_idx = tl.arange(0, out_features)   # but we are in a 1D block of size 512, so we can use j_idx = tl.program_id(1) doesn't exist because we are 1D grid and 1D block? \n\n          Actually, we are using a 1D grid (batch_size) and 1D block (out_features). Then:\n\n              j = tl.program_id(1)   -> no, we don't have 2D grid.\n\n          We can use:\n\n              j = tl.arange(0, out_features)   # but then we have 512 threads, each with an index in [0,511]. We can do:\n\n              j = tl.arange(0, out_features)   # but this is a vector of 512 elements. We want each thread to have a scalar j.\n\n          How about:\n\n              j = tl.arange(0, out_features)   # in the block, we have 512 threads, so we can do:\n\n              j_index = tl.program_id(0)   # no, that's the batch.\n\n          We need a 2D grid? \n\n      Let me restructure:\n\n          We will use a 2D grid: \n              grid = (batch_size, triton.cdiv(out_features, BLOCK_SIZE))\n\n          Then each block computes a contiguous chunk of out_features for one batch.\n\n          Within the block, we have:\n\n              j = tl.arange(0, BLOCK_SIZE)   # indices in the chunk\n              j_global = pid1 * BLOCK_SIZE + j\n\n          Then we load the entire row of x? And then for each j in the chunk, we load the corresponding row of weight and compute the dot product.\n\n      But note: we are going to load the row of x multiple times (once per block in the same batch)? That is inefficient.\n\n      We can load the row of x once per block and store it in shared memory? Then each thread in the block can use it.\n\n      Steps:\n\n          pid0 = batch index\n          pid1 = chunk index for the output features\n\n          # Load the row of x into shared memory (for the entire block to use)\n          We load the entire row? But the row has in_features=1024 elements. We can load it in chunks? Actually, we can load the entire row if we use shared memory and we have enough shared memory? 1024 * 4 = 4096 bytes per block? That is acceptable.\n\n          But note: the maximum shared memory per block is 48KB. 4096 bytes is fine.\n\n          Then:\n\n              for j in the chunk (BLOCK_SIZE elements):\n                  w_ptr = weight + j_global * in_features   # the row of weight for feature j_global\n                  Then compute the dot product between x_row and w_row.\n\n          How to compute the dot product? We can use a loop over the in_features? But that would be 1024 iterations. We can tile it? Or we can use vector instructions.\n\n      Alternatively, we can do:\n\n          We load the entire row of x into shared memory (x_shared).\n\n          Then we load a block of weight rows for the chunk? That would be BLOCK_SIZE rows of in_features? That would be BLOCK_SIZE * in_features, which is 512*1024=524288 elements? That is too big.\n\n      Instead, we can compute the dot product by tiling the in_features dimension. We do:\n\n          acc = 0\n          for k in range(0, in_features, BLOCK_K):\n              x_block = tl.load(x_shared + k + tl.arange(0, BLOCK_K))   # load from shared memory\n              w_block = tl.load(weight + j_global * in_features + k + tl.arange(0, BLOCK_K), mask = ...)   # but note: j_global is different for each thread.\n\n          Then each thread computes its own dot product.\n\n      This is feasible.\n\n  Considering the complexity and the fact that the problem is level1-level5, and this is likely a level1 or level2, we might start with a simpler kernel.\n\n  Alternatively, we can use the Triton matrix multiplication tutorial and modify it to include the bias and scaling? But note: we are doing:\n\n      out = (x @ weight.t() + bias) * scale\n\n  We can break it into:\n\n      out = (x @ weight.t()) * scale + (bias * scale)\n\n  But note: that is not the same as the original. The original does:\n\n      out = (x @ weight.t() + bias) * scale\n\n  So we must do:\n\n      out = (x @ weight.t()) * scale + bias * scale\n\n  But that is two operations. We can do:\n\n      First: matrix multiplication without bias.\n      Then: add bias and then scale? Or we can fuse the scaling into the matrix multiplication? \n\n  Actually, we can do:\n\n      out = (x @ (weight * scale[:, None]).t() + (bias * scale)\n\n  But note: that would be:\n\n          (x @ (weight * scale[:, None]).t() = (x * (weight * scale[:, None]))? No, matrix multiplication: \n          (x @ (weight * scale[:, None]).t() = x @ (weight.t() * scale[:, None])? \n\n      Actually, we have:\n\n          (weight * scale[:, None]) is of shape (out_features, in_features) -> then we transpose to (in_features, out_features)? \n\n      The matrix multiplication: x (batch_size, in_features) @ (in_features, out_features) -> (batch_size, out_features)\n\n      But note: we don't need to transpose if we change the kernel? The kernel we write can take the weight in the original layout.\n\n  However, to avoid confusion, let's stick to the direct computation.\n\n  Given the time, we decide to write a kernel that does:\n\n      out[i, j] = (sum_k x[i, k] * weight[j, k] + bias[j]) * scale[j]\n\n  with tiling over the input features.\n\n  We'll use:\n\n      grid = (batch_size, triton.cdiv(out_features, BLOCK_SIZE_N))\n      BLOCK_SIZE_N: the number of output features per block.\n\n      And we tile the input features by BLOCK_SIZE_K.\n\n  Steps:\n\n      pid0 = batch index\n      pid1 = block index for output features\n\n      # Load a block of the row of x into shared memory? Actually, we can load a block of x for the current tile in input features and then load a block of weight for the current tile and the current block of output features.\n\n      But note: we are only processing one row of x? So we can load a chunk of x for the current tile in input features.\n\n      We have:\n\n          range_n = pid1 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n          mask_n = range_n < out_features\n\n          range_k = tl.arange(0, BLOCK_SIZE_K)\n          offs_k = range_k\n\n      Then for the current tile in input features:\n\n          x_ptr = x + pid0 * in_features + offs_m * BLOCK_SIZE_K   ... wait, we are not tiling the batch? \n\n      Actually, we are only processing one batch index (pid0). We are going to compute a block of out_features for this batch.\n\n      We need to load a block of x for the current tile in input features (of size BLOCK_SIZE_K) at offset offs_k_start.\n\n      Then we load a block of weight: for the current block of output features (range_n) and the current tile in input features (offs_k_start to offs_k_start+BLOCK_SIZE_K). The weight block has shape (BLOCK_SIZE_N, BLOCK_SIZE_K).\n\n      Then we do:\n\n          x_block = tl.load(x_ptrs, mask=mask_k, ...)\n          w_block = tl.load(w_ptrs, mask=mask_n[:, None] & mask_k[None, :], ...)\n\n          acc = tl.dot(x_block, w_block)   # but wait, we want to do a matrix multiplication? Actually, we are computing a block of the row vector times a block of the weight matrix? \n\n          Specifically, we are computing the contribution of the current tile to the entire block of output features.\n\n      Then we accumulate.\n\n      Finally, after the loop over k, we have the dot product for the block of output features. Then we add the bias for the block and multiply by the scale.\n\n  But note: the dot function in Triton is for two vectors? Or for matrix multiplication? Actually, we can use tl.sum(x_block * w_block, axis=1) for each row in the weight block? But that is not efficient.\n\n  Alternatively, we can use the matrix multiplication operation if we make x_block a row vector and w_block a matrix? But the tl.dot operation in Triton expects two 2D tensors? \n\n  We can do:\n\n      acc = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n      for k in range(0, in_features, BLOCK_SIZE_K):\n          # Load x block: a vector of size BLOCK_SIZE_K\n          x_block = tl.load(x_ptr, mask=mask_k, ...)\n          # Load weight block: a matrix of size (BLOCK_SIZE_N, BLOCK_SIZE_K)\n          w_block = tl.load(w_ptr, mask=mask_n[:, None] & mask_k[None, :], ...)\n\n          # Now, we want to compute: acc += x_block * w_block   (matrix-vector product) -> but w_block is a matrix, so we do:\n          #   acc += tl.sum(x_block[None, :] * w_block, axis=1)\n          # But that is not efficient.\n\n      Instead, we can do:\n\n          w_block = w_block.to(x_block.dtype)   # in case they are different\n          acc += tl.sum(x_block[None, :] * w_block, axis=1)\n\n      But note: we are broadcasting x_block to (BLOCK_SIZE_N, BLOCK_SIZE_K) and then doing an element-wise multiplication and then summing along the last axis.\n\n      This is O(BLOCK_SIZE_N * BLOCK_SIZE_K) operations and we are doing it in a loop. It might be slow.\n\n  Alternatively, we can use the tl.dot operation? But tl.dot expects two 2D tensors. We can:\n\n          x_block_2d = x_block[None, :]   # (1, BLOCK_SIZE_K)\n          w_block_2d = w_block             # (BLOCK_SIZE_N, BLOCK_SIZE_K)\n          product = tl.dot(x_block_2d, w_block_2d, trans_b=True)   # (1, BLOCK_SIZE_N)\n\n          Then we add to acc.\n\n      But note: trans_b=True means we do x_block_2d (1, K) * w_block_2d (K, N) -> (1, N) -> then we can add to acc (which is (BLOCK_SIZE_N,)) by broadcasting.\n\n      Actually, we can do:\n\n          product = tl.dot(x_block_2d, w_block_2d.T)   # which is the same.\n\n      But tl.dot supports this? And it uses the tensor cores if available? We are using FP16? The input x is float32? The model uses the default dtype.\n\n  Looking at the original: \n      get_inputs returns torch.randn(batch_size, in_features) -> float32.\n\n  The weight and bias in the linear layer are float32 by default.\n\n  We can do the computation in float32.\n\n  However, we can convert to FP16 for tensor cores? But we must be cautious about precision.\n\n  The problem does not specify, but we are allowed to use FP16 if we can maintain correctness? The tolerance is float32.\n\n  We'll do the accumulation in float32 and use FP16 for the matrix multiplication? We can use mixed precision.\n\n  But note: the Triton dot operation for FP16 requires the inputs to be FP16. We can convert x_block and w_block to FP16.\n\n  Steps for mixed precision:\n\n      x_block_f16 = x_block.to(tl.float16)\n      w_block_f16 = w_block.to(tl.float16)\n      product_f32 = tl.dot(x_block_f16, w_block_f16, trans_b=True).to(tl.float32)\n\n  But note: the tl.dot operation returns FP32 for FP16 inputs? Or we have to convert back? The Triton dot product for FP16 returns FP32.\n\n  Actually, the documentation says: \"the input tensors must have the same dtype and it must be in {float16, bfloat16} and the accumulator must have dtype float32\".\n\n  So we can do:\n\n      x_block_f16 = x_block.to(tl.float16)\n      w_block_f16 = w_block.to(tl.float16)\n      product = tl.dot(x_block_f16, w_block_f16)   # returns a 1xBLOCK_SIZE_N tensor of float32.\n\n  But note: the dimensions: \n        x_block_2d: (1, BLOCK_SIZE_K)\n        w_block_2d: (BLOCK_SIZE_N, BLOCK_SIZE_K) -> we want to multiply as: (1, K) @ (K, N) -> (1, N) but tl.dot(a, b) does a @ b, so if a is (1, K) and b is (N, K) then we need b.T? \n\n      We have two options:\n          Option A: store the weight in row-major without transposing. Then we have the weight matrix in layout (out_features, in_features). So for one block, we are loading a block of (BLOCK_SIZE_N, BLOCK_SIZE_K) from the weight matrix. Then we want to compute: x_block (1, BLOCK_SIZE_K) @ weight_block (BLOCK_SIZE_N, BLOCK_SIZE_K).T = (1, BLOCK_SIZE_N)\n\n          But that is (1, K) @ (K, N) = (1, N). \n\n          So we can do:\n\n              product = tl.dot(x_block_f16, w_block_f16, trans_b=True)\n\n      But note: the weight matrix is stored row-major. The row for output feature j is stored at weight + j * in_features + k. So to get the weight block for the current tile in k, we load a block of size (BLOCK_SIZE_N, BLOCK_SIZE_K) in row-major order. Then we want to multiply by x_block (row vector) on the right? \n\n          We want: for each output feature j in the block: dot = sum_k x_block[k] * weight_block[j, k]\n\n          This is: x_block (1, K) * weight_block (N, K) -> then reduce over k? Actually, it is (1, K) @ (K, N) -> (1, N) if we transpose the weight_block? \n\n          But note: weight_block is (N, K) and we want to use it as (K, N) for the multiplication? That's what trans_b=True does.\n\n      Alternatively, we can store the weight in column-major? But we are given the weight in row-major.\n\n  So we do:\n\n      product = tl.dot(x_block_f16, w_block_f16, trans_b=True)\n\n  Then we accumulate to acc (which is a vector of size BLOCK_SIZE_N).\n\n  After the loop, we add the bias and multiply by scale.\n\n  Then store.\n\n  We must be cautious with the mask: for the last tile in k and the last block in n.\n\n  Let's write the kernel.\n\n  We'll set:\n\n      BLOCK_SIZE_N = 128   # number of output features per block\n      BLOCK_SIZE_K = 32    # tile size for input features\n\n  And we use a 2D grid.\n\n  Steps in the kernel:\n\n      pid0 = tl.program_id(0)   # batch index\n      pid1 = tl.program_id(1)   # block index for output features\n\n      # Offsets for the output features in this block:\n      offs_n = pid1 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n      mask_n = offs_n < out_features\n\n      # Pointers for the weight and bias for this block of output features:\n      w_ptr = weight + offs_n[:, None] * in_features   # pointer to the rows of weight for the current block of output features, shape (BLOCK_SIZE_N, in_features)\n      b_ptr = bias + offs_n   # (BLOCK_SIZE_N,)\n\n      # Pointer for x: for the current batch\n      x_ptr = x + pid0 * in_features   # (in_features,)\n\n      # Initialize accumulator to zero.\n      acc = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n\n      # Loop over k in steps of BLOCK_SIZE_K\n      for k in range(0, in_features, BLOCK_SIZE_K):\n          # Load a block of x for the current tile in input features\n          offs_k = k + tl.arange(0, BLOCK_SIZE_K)\n          mask_k = offs_k < in_features\n          x_block = tl.load(x_ptr + offs_k, mask=mask_k)   # (BLOCK_SIZE_K,)\n\n          # Load a block of weight: for the current block of output features and the current tile in input features.\n          w_block = tl.load(w_ptr + offs_k[None, :], mask=mask_n[:, None] & mask_k[None, :])   # (BLOCK_SIZE_N, BLOCK_SIZE_K)\n\n          # Convert to FP16 for tensor core\n          x_block_f16 = x_block.to(tl.float16)\n          w_block_f16 = w_block.to(tl.float16)\n\n          # Compute the dot product for this tile: (1, K) @ (K, N) -> (1, N) but note: we are doing (1, K) @ (N, K)^T -> (1, N)\n          # But tl.dot expects two 2D tensors. We have:\n          #   x_block_f16: shape (1, BLOCK_SIZE_K)   [if we add a dimension]\n          #   w_block_f16: shape (BLOCK_SIZE_N, BLOCK_SIZE_K)\n          # We want: (1, K) @ (N, K)^T = (1, N) -> but we can do without explicitly transposing by using trans_b=True.\n          # However, tl.dot(a, b, trans_b=True) means b is transposed -> (1, K) @ (K, N) = (1, N)\n          # So we need to set trans_b=True.\n          product = tl.dot(x_block_f16[None, :], w_block_f16, trans_b=True)   # (1, BLOCK_SIZE_N)\n\n          # Accumulate in float32\n          acc += product[0, :]   # product[0] is a vector of size BLOCK_SIZE_N\n\n      # After the loop, add bias and scale\n      bias_block = tl.load(b_ptr, mask=mask_n)   # (BLOCK_SIZE_N,)\n      # Then: out = (acc + bias_block) * scale_block\n      scale_block = tl.load(scale + offs_n, mask=mask_n)   # (BLOCK_SIZE_N,)\n      out = (acc + bias_block) * scale_block\n\n      # Now, store the output\n      output_ptr = output + pid0 * out_features + offs_n\n      tl.store(output_ptr, out, mask=mask_n)\n\n  But note: the scale is applied per output feature, so we load scale_block for the current block of output features.\n\n  Then we must create an output tensor.\n\n  We'll allocate the output tensor in the forward function.\n\n  However, note: the kernel is for the fused linear and scaling. We then pass the output to the BatchNorm.\n\n  We must be cautious: the BatchNorm expects a float32 tensor? The output of the kernel is float32.\n\n  The BatchNorm is then applied.\n\n  Now, let's integrate this into the model.\n\n  We will write the kernel and then in the forward function:\n\n      def forward(self, x):\n          # Instead of: x = self.gemm(x)\n          #            x = x * self.scale\n          # we do:\n          out = fused_linear_scale(x, self.gemm.weight, self.gemm.bias, self.scale)\n          out = self.bn(out)\n          return out\n\n  We must handle the fact that the model might be on GPU.\n\n  We'll write the kernel and the helper function.\n\n  Given the constraints of the problem, we are only allowed to change the EVOLVE-BLOCK. We must output the entire code block.\n\n  We are going to replace the entire class ModelNew and the helper functions? But note the problem says: \"Provide the complete new program code\" within the EVOLVE-BLOCK.\n\n  We will write:\n\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def _fused_linear_scale_kernel(...):\n          ...\n\n      def fused_linear_scale(x, weight, bias, scale):\n          ...   # allocates output and calls the kernel\n\n      class ModelNew(nn.Module):\n          ...   # same __init__\n          def forward(self, x):\n              out = fused_linear_scale(x, self.gemm.weight, self.gemm.bias, self.scale)\n              out = self.bn(out)\n              return out\n\n  And the rest of the code (batch_size, etc.) remains.\n\n  But note: the original model had:\n        self.gemm = nn.Linear(in_features, out_features)\n\n      We are still using it to hold the weight and bias? Yes, so we can use self.gemm.weight and self.gemm.bias.\n\n  However, the original model also had a scaling by self.scale and then BatchNorm.\n\n  We are fusing the linear and the scale.\n\n  Let's write the helper function.\n\n  We assume the tensors are contiguous.\n\n  Steps for fused_linear_scale:\n\n      batch_size, in_features = x.shape\n      out_features = weight.shape[0]\n\n      output = torch.empty((batch_size, out_features), device=x.device, dtype=torch.float32)\n\n      # Grid and block:\n      grid = (batch_size, triton.cdiv(out_features, BLOCK_SIZE_N))\n      kernel = _fused_linear_scale_kernel[grid](x, weight, bias, scale, output, ... other constants ...)\n\n      return output\n\n  We must pass the constants: in_features, out_features.\n\n  We'll pass them as tl.constexpr.\n\n  We'll define the kernel with:\n\n      x_ptr, weight_ptr, bias_ptr, scale_ptr, output_ptr,\n      in_features: tl.constexpr,\n      out_features: tl.constexpr,\n      BLOCK_SIZE_N: tl.constexpr,\n      BLOCK_SIZE_K: tl.constexpr,\n\n  But note: we are using a 2D grid. The grid for the kernel is (batch_size, num_blocks_n)\n\n  We set BLOCK_SIZE_N and BLOCK_SIZE_K as compile-time constants.\n\n  We'll choose BLOCK_SIZE_N=128, BLOCK_SIZE_K=64.\n\n  We also note: the problem says that the hardware has FP16 tensor cores. We are using them.\n\n  Let's write the kernel.\n\n  But note: the kernel might be long. We are allowed to output the entire code block.\n\n  However, the problem says: \"Small iterative steps: Modify \u226420 lines per revision\". But we are starting from scratch? \n\n  But note: the current code is 20 lines? We are replacing the forward pass and adding the kernel and helper function.\n\n  We'll do it.\n\n  We hope for a speedup.\n\n  If we encounter compilation errors, we can fix in the next iteration.\n\n  We'll output the entire EVOLVE-BLOCK.\n\n  IMPORTANT: We must not change the class name (ModelNew) or the functions get_inputs and get_init_inputs.\n\n  Let's write the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_linear_scale_kernel(\n    x_ptr, w_ptr, b_ptr, s_ptr, out_ptr,\n    in_features, out_features,\n    stride_x, stride_w, stride_b, stride_s, stride_out,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # Batch index\n    pid1 = tl.program_id(1)  # Output feature block index\n    \n    # Create ranges for current block\n    offs_n = pid1 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Create masks for boundaries\n    mask_n = offs_n < out_features\n    mask_k = offs_k < in_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    # Loop over input features\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        # Load block of input data\n        x_vals = tl.load(\n            x_ptr + pid0 * stride_x + k + offs_k, \n            mask=mask_k, other=0.0\n        )\n        \n        # Load block of weights\n        w_vals = tl.load(\n            w_ptr + (offs_n[:, None]) * stride_w + (k + offs_k[None, :]), \n            mask=mask_n[:, None] & mask_k[None, :], other=0.0\n        )\n        \n        # Compute partial dot product\n        x_vals_f16 = x_vals.to(tl.float16)\n        w_vals_f16 = w_vals.to(tl.float16)\n        partial = tl.sum(x_vals_f16[None, :] * w_vals_f16, axis=1)\n        acc += partial.to(tl.float32)\n    \n    # Load bias and scale\n    b_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)\n    s_vals = tl.load(s_ptr + offs_n, mask=mask_n, other=0.0)\n    \n    # Apply bias and scale\n    output = (acc + b_vals) * s_vals\n    \n    # Store result\n    tl.store(\n        out_ptr + pid0 * stride_out + offs_n, \n        output, mask=mask_n\n    )\n\ndef fused_linear_scale(x, weight, bias, scale):\n    batch_size, in_features = x.shape\n    out_features = weight.shape[0]\n    \n    # Preallocate output tensor\n    output = torch.empty((batch_size, out_features), \n                         device=x.device, dtype=torch.float32)\n    \n    # Calculate grid dimensions\n    grid = lambda meta: (\n        batch_size, \n        triton.cdiv(out_features, meta['BLOCK_SIZE_N'])\n    )\n    \n    # Launch kernel with optimized block sizes\n    fused_linear_scale_kernel[grid](\n        x, weight, bias, scale, output,\n        in_features, out_features,\n        x.stride(0), weight.stride(0), \n        bias.stride(0), scale.stride(0), output.stride(0),\n        BLOCK_SIZE_N=128, BLOCK_SIZE_K=64\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x):\n        # Fused linear layer + scaling\n        x = fused_linear_scale(x, self.gemm.weight, self.gemm.bias, self.scale)\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
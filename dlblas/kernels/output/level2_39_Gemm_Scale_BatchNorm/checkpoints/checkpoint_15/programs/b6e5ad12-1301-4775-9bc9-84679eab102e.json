{"id": "b6e5ad12-1301-4775-9bc9-84679eab102e", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_linear_scale_kernel(\n    x_ptr, w_ptr, b_ptr, s_ptr, out_ptr,\n    in_features, out_features,\n    stride_x, stride_w, stride_b, stride_s, stride_out,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # Batch index\n    pid1 = tl.program_id(1)  # Output feature block index\n    \n    # Create ranges for current block\n    offs_n = pid1 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Create masks for boundaries\n    mask_n = offs_n < out_features\n    mask_k = offs_k < in_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    # Loop over input features\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        # Load block of input data\n        x_vals = tl.load(\n            x_ptr + pid0 * stride_x + k + offs_k, \n            mask=mask_k, other=0.0\n        )\n        \n        # Load block of weights\n        w_vals = tl.load(\n            w_ptr + (offs_n[:, None]) * stride_w + (k + offs_k[None, :]), \n            mask=mask_n[:, None] & mask_k[None, :], other=0.0\n        )\n        \n        # Compute partial dot product\n        partial = tl.dot(w_vals, x_vals, allow_tf32=True)\n        acc += partial\n    \n    # Load bias and scale\n    b_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)\n    s_vals = tl.load(s_ptr + offs_n, mask=mask_n, other=0.0)\n    \n    # Apply bias and scale\n    output = (acc + b_vals) * s_vals\n    \n    # Store result\n    tl.store(\n        out_ptr + pid0 * stride_out + offs_n, \n        output, mask=mask_n\n    )\n\ndef fused_linear_scale(x, weight, bias, scale):\n    batch_size, in_features = x.shape\n    out_features = weight.shape[0]\n    \n    # Preallocate output tensor\n    output = torch.empty((batch_size, out_features), \n                         device=x.device, dtype=torch.float32)\n    \n    # Calculate grid dimensions\n    grid = lambda meta: (\n        batch_size, \n        triton.cdiv(out_features, meta['BLOCK_SIZE_N'])\n    )\n    \n    # Launch kernel with optimized block sizes\n    fused_linear_scale_kernel[grid](\n        x, weight, bias, scale, output,\n        in_features, out_features,\n        x.stride(0), weight.stride(0), \n        bias.stride(0), scale.stride(0), output.stride(0),\n        BLOCK_SIZE_N=128, BLOCK_SIZE_K=64\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x):\n        # Fused linear layer + scaling\n        x = fused_linear_scale(x, self.gemm.weight, self.gemm.bias, self.scale)\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "0967c5a0-60b1-46a6-8a0f-c44e33de99ab", "generation": 3, "timestamp": 1754574534.5742521, "iteration_found": 9, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_39_Gemm_Scale_BatchNorm", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2045, in dot\n    return _semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1497, in dot\n    assert lhs_rank == rhs_rank == 2 or lhs_rank == rhs_rank == 3, f\"Both inputs must be either 2D or 3D; (lhs: {lhs.shape} vs rhs: {rhs.shape})\"\nAssertionError: Both inputs must be either 2D or 3D; (lhs: ['constexpr[128]', 'constexpr[64]'] vs rhs: ['constexpr[64]'])\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp6uadlfcl.py\", line 92, in forward\n    x = fused_linear_scale(x, self.gemm.weight, self.gemm.bias, self.scale)\n  File \"/tmp/tmp6uadlfcl.py\", line 74, in fused_linear_scale\n    fused_linear_scale_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 36:18:\n            x_ptr + pid0 * stride_x + k + offs_k, \n            mask=mask_k, other=0.0\n        )\n\n        # Load block of weights\n        w_vals = tl.load(\n            w_ptr + (offs_n[:, None]) * stride_w + (k + offs_k[None, :]), \n            mask=mask_n[:, None] & mask_k[None, :], other=0.0\n        )\n\n        # Compute partial dot product\n        partial = tl.dot(w_vals, x_vals, allow_tf32=True)\n                  ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2045, in dot\n    return _semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1497, in dot\n    assert lhs_rank == rhs_rank == 2 or lhs_rank == rhs_rank == 3, f\"Both inputs must be either 2D or 3D; (lhs: {lhs.shape} vs rhs: {rhs.shape})\"\nAssertionError: Both inputs must be either 2D or 3D; (lhs: ['constexpr[128]', 'constexpr[64]'] vs rhs: ['constexpr[64]'])\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0935, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 0.0000\n- correctness: 0.0000\n- score: -100.0000\n- stage: no_valid_triton_code\n- feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 0.0000, correctness: 0.0000, score: -100.0000, stage: no_valid_triton_code, feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9260, runtime_ms: 0.1010, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0935, 'speed_up': 0.926, 'custom_timing': 0.101}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_linear_scale_kernel(\n    x_ptr, w_ptr, b_ptr, s_ptr, out_ptr,\n    in_features, out_features,\n    stride_x, stride_w, stride_b, stride_s, stride_out,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # Batch index\n    pid1 = tl.program_id(1)  # Output feature block index\n    \n    # Create ranges for current block\n    offs_n = pid1 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Create masks for boundaries\n    mask_n = offs_n < out_features\n    mask_k = offs_k < in_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    # Loop over input features\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        # Load block of input data\n        x_vals = tl.load(\n            x_ptr + pid0 * stride_x + k + offs_k, \n            mask=mask_k, other=0.0\n        )\n        \n        # Load block of weights\n        w_vals = tl.load(\n            w_ptr + (offs_n[:, None]) * stride_w + (k + offs_k[None, :]), \n            mask=mask_n[:, None] & mask_k[None, :], other=0.0\n        )\n        \n        # Compute partial dot product\n        x_vals_f16 = x_vals.to(tl.float16)\n        w_vals_f16 = w_vals.to(tl.float16)\n        partial = tl.sum(x_vals_f16[None, :] * w_vals_f16, axis=1)\n        acc += partial.to(tl.float32)\n    \n    # Load bias and scale\n    b_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)\n    s_vals = tl.load(s_ptr + offs_n, mask=mask_n, other=0.0)\n    \n    # Apply bias and scale\n    output = (acc + b_vals) * s_vals\n    \n    # Store result\n    tl.store(\n        out_ptr + pid0 * stride_out + offs_n, \n        output, mask=mask_n\n    )\n\ndef fused_linear_scale(x, weight, bias, scale):\n    batch_size, in_features = x.shape\n    out_features = weight.shape[0]\n    \n    # Preallocate output tensor\n    output = torch.empty((batch_size, out_features), \n                         device=x.device, dtype=torch.float32)\n    \n    # Calculate grid dimensions\n    grid = lambda meta: (\n        batch_size, \n        triton.cdiv(out_features, meta['BLOCK_SIZE_N'])\n    )\n    \n    # Launch kernel with optimized block sizes\n    fused_linear_scale_kernel[grid](\n        x, weight, bias, scale, output,\n        in_features, out_features,\n        x.stride(0), weight.stride(0), \n        bias.stride(0), scale.stride(0), output.stride(0),\n        BLOCK_SIZE_N=128, BLOCK_SIZE_K=64\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x):\n        # Fused linear layer + scaling\n        x = fused_linear_scale(x, self.gemm.weight, self.gemm.bias, self.scale)\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9260), Performs well on runtime_ms (0.1010), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0935, 'speed_up': 0.926, 'custom_timing': 0.101})\n\n\n### Program 2 (Score: -100.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (0.0000), Performs well on correctness (0.0000), Performs well on score (-100.0000), Performs well on stage (no_valid_triton_code), Performs well on feedback (You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_linear_scale_kernel(\n    x_ptr, w_ptr, b_ptr, s_ptr, out_ptr,\n    in_features, out_features,\n    stride_x, stride_w, stride_b, stride_s, stride_out,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # Batch index\n    pid1 = tl.program_id(1)  # Output feature block index\n    \n    # Create ranges for current block\n    offs_n = pid1 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Create masks for boundaries\n    mask_n = offs_n < out_features\n    mask_k = offs_k < in_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    # Loop over input features\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        # Load block of input data\n        x_vals = tl.load(\n            x_ptr + pid0 * stride_x + k + offs_k, \n            mask=mask_k, other=0.0\n        )\n        \n        # Load block of weights\n        w_vals = tl.load(\n            w_ptr + (offs_n[:, None]) * stride_w + (k + offs_k[None, :]), \n            mask=mask_n[:, None] & mask_k[None, :], other=0.0\n        )\n        \n        # Compute partial dot product\n        x_vals_f16 = x_vals.to(tl.float16)\n        w_vals_f16 = w_vals.to(tl.float16)\n        partial = tl.sum(x_vals_f16[None, :] * w_vals_f16, axis=1)\n        acc += partial.to(tl.float32)\n    \n    # Load bias and scale\n    b_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)\n    s_vals = tl.load(s_ptr + offs_n, mask=mask_n, other=0.0)\n    \n    # Apply bias and scale\n    output = (acc + b_vals) * s_vals\n    \n    # Store result\n    tl.store(\n        out_ptr + pid0 * stride_out + offs_n, \n        output, mask=mask_n\n    )\n\ndef fused_linear_scale(x, weight, bias, scale):\n    batch_size, in_features = x.shape\n    out_features = weight.shape[0]\n    \n    # Preallocate output tensor\n    output = torch.empty((batch_size, out_features), \n                         device=x.device, dtype=torch.float32)\n    \n    # Calculate grid dimensions\n    grid = lambda meta: (\n        batch_size, \n        triton.cdiv(out_features, meta['BLOCK_SIZE_N'])\n    )\n    \n    # Launch kernel with optimized block sizes\n    fused_linear_scale_kernel[grid](\n        x, weight, bias, scale, output,\n        in_features, out_features,\n        x.stride(0), weight.stride(0), \n        bias.stride(0), scale.stride(0), output.stride(0),\n        BLOCK_SIZE_N=128, BLOCK_SIZE_K=64\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x):\n        # Fused linear layer + scaling\n        x = fused_linear_scale(x, self.gemm.weight, self.gemm.bias, self.scale)\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program which is a pure PyTorch model without Triton. The feedback says: \"You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\"\n\nThe task is to rewrite the program to include a Triton kernel for the fused linear and scale operation.\n\nLooking at the top-performing program (Program 1) and the inspiration program, we see that they use a Triton kernel to fuse the linear layer (matrix multiplication with weight and bias) and the scaling by a per-feature scale.\n\nThe current program (ModelNew) does:\n    x = self.gemm(x)   # linear layer: x @ weight.T + bias\n    x = x * self.scale  # element-wise multiplication by a vector of shape (out_features,)\n    x = self.bn(x)      # batch normalization\n\nWe are going to focus on fusing the linear layer and the scaling (the first two operations) into one Triton kernel.\n\nThe plan:\n1. We will write a Triton kernel `fused_linear_scale_kernel` that does:\n   - Inputs: x (batch_size, in_features), weight (out_features, in_features), bias (out_features), scale (out_features)\n   - Output: (x @ weight.T + bias) * scale   [shape: batch_size, out_features]\n\n2. We replace the two operations in the forward pass with a call to a function `fused_linear_scale` that uses the Triton kernel.\n\n3. We must be cautious about the strides and the block sizes.\n\nWe note from the top-performing program that they use:\n   - Two-dimensional grid: (batch_size, triton.cdiv(out_features, BLOCK_SIZE_N))\n   - The kernel loads blocks of the input and weight, accumulates in float32, then adds bias and multiplies by scale.\n\nWe will adapt the kernel from the top-performing program.\n\nImportant: The current program does not have Triton imports. We must add them.\n\nLet's rewrite the code accordingly.\n\nNote: We are only allowed to change the EVOLVE-BLOCK. We must output the entire block with the new code.\n\nWe are going to:\n   - Import triton and triton.language as tl.\n   - Define the kernel function with the same signature as in the top-performing program.\n   - Define the helper function `fused_linear_scale` that launches the kernel.\n   - Modify the forward method of ModelNew to use `fused_linear_scale` instead of the separate operations.\n\nWe also note that the top-performing program used fixed block sizes (128 for N and 64 for K). We can start with that and then consider autotuning in the future if needed.\n\nLet's write the code accordingly.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_linear_scale_kernel(\n    x_ptr, w_ptr, b_ptr, s_ptr, out_ptr,\n    in_features, out_features,\n    stride_x, stride_w, stride_b, stride_s, stride_out,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # Batch index\n    pid1 = tl.program_id(1)  # Output feature block index\n    \n    # Create ranges for current block\n    offs_n = pid1 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Create masks for boundaries\n    mask_n = offs_n < out_features\n    mask_k = offs_k < in_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    # Loop over input features\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        # Load block of input data\n        x_vals = tl.load(\n            x_ptr + pid0 * stride_x + k + offs_k, \n            mask=mask_k, other=0.0\n        )\n        \n        # Load block of weights\n        w_vals = tl.load(\n            w_ptr + (offs_n[:, None]) * stride_w + (k + offs_k[None, :]), \n            mask=mask_n[:, None] & mask_k[None, :], other=0.0\n        )\n        \n        # Compute partial dot product\n        partial = tl.dot(w_vals, x_vals, allow_tf32=True)\n        acc += partial\n    \n    # Load bias and scale\n    b_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)\n    s_vals = tl.load(s_ptr + offs_n, mask=mask_n, other=0.0)\n    \n    # Apply bias and scale\n    output = (acc + b_vals) * s_vals\n    \n    # Store result\n    tl.store(\n        out_ptr + pid0 * stride_out + offs_n, \n        output, mask=mask_n\n    )\n\ndef fused_linear_scale(x, weight, bias, scale):\n    batch_size, in_features = x.shape\n    out_features = weight.shape[0]\n    \n    # Preallocate output tensor\n    output = torch.empty((batch_size, out_features), \n                         device=x.device, dtype=torch.float32)\n    \n    # Calculate grid dimensions\n    grid = lambda meta: (\n        batch_size, \n        triton.cdiv(out_features, meta['BLOCK_SIZE_N'])\n    )\n    \n    # Launch kernel with optimized block sizes\n    fused_linear_scale_kernel[grid](\n        x, weight, bias, scale, output,\n        in_features, out_features,\n        x.stride(0), weight.stride(0), \n        bias.stride(0), scale.stride(0), output.stride(0),\n        BLOCK_SIZE_N=128, BLOCK_SIZE_K=64\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x):\n        # Fused linear layer + scaling\n        x = fused_linear_scale(x, self.gemm.weight, self.gemm.bias, self.scale)\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
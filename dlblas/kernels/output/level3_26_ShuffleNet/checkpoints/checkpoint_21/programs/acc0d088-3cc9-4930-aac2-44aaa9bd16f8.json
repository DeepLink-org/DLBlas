{"id": "acc0d088-3cc9-4930-aac2-44aaa9bd16f8", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=32),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef channel_shuffle_kernel(\n    input_ptr, output_ptr,\n    B, C, H, W, groups,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    idx = offsets\n    w = idx % W\n    h = (idx // W) % H\n    c = (idx // (H * W)) % C\n    b = idx // (C * H * W)\n\n    channels_per_group = C // groups\n    group_idx = c // channels_per_group\n    index_in_group = c % channels_per_group\n    new_c = index_in_group * groups + group_idx\n\n    output_idx = b * (C * H * W) + new_c * (H * W) + h * W + w\n    x = tl.load(input_ptr + idx, mask=mask)\n    tl.store(output_ptr + output_idx, x, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        B, C, H, W = x.size()\n        assert C % self.groups == 0\n        \n        if x.numel() == 0:\n            return x\n\n        output = torch.empty_like(x)\n        total_elements = B * C * H * W\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        channel_shuffle_kernel[grid](\n            x, output,\n            B, C, H, W, self.groups,\n            total_elements\n        )\n        return output\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "e857d689-9bc4-4bc3-9b96-971039bbfa7c", "generation": 4, "timestamp": 1754591032.7442193, "iteration_found": 16, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.447, "runtime_ms": 8.61, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 3.85, "speed_up": 0.447, "custom_timing": 8.61}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_26_ShuffleNet", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "max_difference": ["0.012112", "0.016650", "0.015383", "0.015883", "0.018763"], "avg_difference": ["0.002554", "0.003222", "0.003366", "0.003321", "0.003323"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 3.85, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 0.0000\n- score: 10.0000\n- stage: wrong_answer\n- verify_payload: {'id': 'level3_26_ShuffleNet', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'max_difference': ['0.012112', '0.016650', '0.015383', '0.015883', '0.018763'], 'avg_difference': ['0.002554', '0.003222', '0.003366', '0.003321', '0.003323'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 3.85, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level3_26_ShuffleNet', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'max_difference': ['0.012112', '0.016650', '0.015383', '0.015883', '0.018763'], 'avg_difference': ['0.002554', '0.003222', '0.003366', '0.003321', '0.003323'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 3.85, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level3_26_ShuffleNet', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'max_difference': ['0.015514', '0.018880', '0.020477', '0.022591', '0.023909'], 'avg_difference': ['0.003209', '0.003902', '0.004069', '0.004011', '0.004048'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 3.85, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.4490, runtime_ms: 8.5800, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 3.85, 'speed_up': 0.449, 'custom_timing': 8.58}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=32),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef channel_shuffle_kernel(\n    input_ptr, output_ptr,\n    B, C, H, W, groups,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    idx = offsets\n    w = idx % W\n    h = (idx // W) % H\n    c = (idx // (H * W)) % C\n    b = idx // (C * H * W)\n\n    channels_per_group = C // groups\n    group_idx = c // channels_per_group\n    index_in_group = c % channels_per_group\n    new_c = index_in_group * groups + group_idx\n\n    output_idx = b * (C * H * W) + new_c * (H * W) + h * W + w\n    x = tl.load(input_ptr + idx, mask=mask, eviction_policy='evict_last')\n    tl.store(output_ptr + output_idx, x, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        B, C, H, W = x.size()\n        assert C % self.groups == 0\n        \n        if x.numel() == 0:\n            return x\n\n        output = torch.empty_like(x)\n        total_elements = B * C * H * W\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        channel_shuffle_kernel[grid](\n            x, output,\n            B, C, H, W, self.groups,\n            total_elements\n        )\n        return output\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.4490), Performs well on runtime_ms (8.5800), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 3.85, 'speed_up': 0.449, 'custom_timing': 8.58})\n\n\n### Program 2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE\": 128}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 256}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 128}, num_warps=8),\n        triton.Config({\"BLOCK_SIZE\": 256}, num_warps=8),\n    ],\n    key=[\"channels\", \"height\", \"width\"],\n)\n@triton.jit\ndef channel_shuffle_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n    channels, groups, channels_per_group, height, width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_bhw = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    \n    total_hw = height * width\n    batch_index = pid_bhw // total_hw\n    hw_index = pid_bhw % total_hw\n    h_index = hw_index // width\n    w_index = hw_index % width\n    \n    c_offsets = pid_c * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = c_offsets < channels\n    \n    group_indices = c_offsets % groups\n    index_in_group = c_offsets // groups\n    input_c = group_indices * channels_per_group + index_in_group\n    \n    input_offsets = batch_index * input_batch_stride + input_c * input_channel_stride + h_index * input_height_stride + w_index * input_width_stride\n    output_offsets = batch_index * output_batch_stride + c_offsets * output_channel_stride + h_index * output_height_stride + w_index * output_width_stride\n    \n    values = tl.load(input_ptr + input_offsets, mask=mask, other=0)\n    tl.store(output_ptr + output_offsets, values, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        channels_per_group = channels // self.groups\n        \n        if self.groups == 1:\n            return x\n            \n        output = torch.empty_like(x)\n        \n        if x.is_cuda:\n            grid = (batch_size * height * width, triton.cdiv(channels, 256))\n            channel_shuffle_kernel[grid](\n                x, output,\n                x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n                output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n                channels, self.groups, channels_per_group, height, width\n            )\n            return output\n        else:\n            x = x.view(batch_size, self.groups, channels_per_group, height, width)\n            x = x.transpose(1, 2).contiguous()\n            x = x.view(batch_size, -1, height, width)\n            return x\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level3_26_ShuffleNet', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'max_difference': ['0.015514', '0.018880', '0.020477', '0.022591', '0.023909'], 'avg_difference': ['0.003209', '0.003902', '0.004069', '0.004011', '0.004048'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 3.85, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n### Program 3 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE\": 128}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 256}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 128}, num_warps=8),\n        triton.Config({\"BLOCK_SIZE\": 256}, num_warps=8),\n    ],\n    key=[\"channels\", \"height\", \"width\"],\n)\n@triton.jit\ndef channel_shuffle_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n    channels, groups, channels_per_group, height, width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_bhw = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    \n    total_hw = height * width\n    batch_index = pid_bhw // total_hw\n    hw_index = pid_bhw % total_hw\n    h_index = hw_index // width\n    w_index = hw_index % width\n    \n    c_offsets = pid_c * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = c_offsets < channels\n    \n    group_indices = c_offsets % groups\n    index_in_group = c_offsets // groups\n    input_c = group_indices * channels_per_group + index_in_group\n    \n    input_offsets = batch_index * input_batch_stride + input_c * input_channel_stride + h_index * input_height_stride + w_index * input_width_stride\n    output_offsets = batch_index * output_batch_stride + c_offsets * output_channel_stride + h_index * output_height_stride + w_index * output_width_stride\n    \n    values = tl.load(input_ptr + input_offsets, mask=mask, other=0)\n    tl.store(output_ptr + output_offsets, values, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        channels_per_group = channels // self.groups\n        \n        if self.groups == 1:\n            return x\n            \n        output = torch.zeros_like(x)\n        \n        if x.is_cuda:\n            grid = (batch_size * height * width, triton.cdiv(channels, 256))\n            channel_shuffle_kernel[grid](\n                x, output,\n                x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n                output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n                channels, self.groups, channels_per_group, height, width\n            )\n            return output\n        else:\n            x = x.view(batch_size, self.groups, channels_per_group, height, width)\n            x = x.transpose(1, 2).contiguous()\n            x = x.view(batch_size, -1, height, width)\n            return x\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level3_26_ShuffleNet', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'max_difference': ['0.012112', '0.016650', '0.015383', '0.015883', '0.018763'], 'avg_difference': ['0.002554', '0.003222', '0.003366', '0.003321', '0.003323'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 3.85, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=32),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef channel_shuffle_kernel(\n    input_ptr, output_ptr,\n    B, C, H, W, groups,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    idx = offsets\n    w = idx % W\n    h = (idx // W) % H\n    c = (idx // (H * W)) % C\n    b = idx // (C * H * W)\n\n    channels_per_group = C // groups\n    group_idx = c // channels_per_group\n    index_in_group = c % channels_per_group\n    new_c = index_in_group * groups + group_idx\n\n    output_idx = b * (C * H * W) + new_c * (H * W) + h * W + w\n    x = tl.load(input_ptr + idx, mask=mask, eviction_policy='evict_last')\n    tl.store(output_ptr + output_idx, x, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        B, C, H, W = x.size()\n        assert C % self.groups == 0\n        \n        if x.numel() == 0:\n            return x\n\n        output = torch.empty_like(x)\n        total_elements = B * C * H * W\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        channel_shuffle_kernel[grid](\n            x, output,\n            B, C, H, W, self.groups,\n            total_elements\n        )\n        return output\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE\": 128}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 256}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 128}, num_warps=8),\n        triton.Config({\"BLOCK_SIZE\": 256}, num_warps=8),\n    ],\n    key=[\"channels\", \"height\", \"width\"],\n)\n@triton.jit\ndef channel_shuffle_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n    channels, groups, channels_per_group, height, width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_bhw = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    \n    total_hw = height * width\n    batch_index = pid_bhw // total_hw\n    hw_index = pid_bhw % total_hw\n    h_index = hw_index // width\n    w_index = hw_index % width\n    \n    c_offsets = pid_c * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = c_offsets < channels\n    \n    group_indices = c_offsets % groups\n    index_in_group = c_offsets // groups\n    input_c = group_indices * channels_per_group + index_in_group\n    \n    input_offsets = batch_index * input_batch_stride + input_c * input_channel_stride + h_index * input_height_stride + w_index * input_width_stride\n    output_offsets = batch_index * output_batch_stride + c_offsets * output_channel_stride + h_index * output_height_stride + w_index * output_width_stride\n    \n    values = tl.load(input_ptr + input_offsets, mask=mask, other=0)\n    tl.store(output_ptr + output_offsets, values, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        channels_per_group = channels // self.groups\n        \n        if self.groups == 1:\n            return x\n            \n        output = torch.zeros_like(x)\n        \n        if x.is_cuda:\n            grid = (batch_size * height * width, triton.cdiv(channels, 256))\n            channel_shuffle_kernel[grid](\n                x, output,\n                x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n                output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n                channels, self.groups, channels_per_group, height, width\n            )\n            return output\n        else:\n            x = x.view(batch_size, self.groups, channels_per_group, height, width)\n            x = x.transpose(1, 2).contiguous()\n            x = x.view(batch_size, -1, height, width)\n            return x\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a program that implements the ShuffleNet model with a Triton kernel for channel shuffle.\n The current program has an issue with correctness (output mismatch) and we are to fix it.\n\n The problem: \n   The current Triton kernel for channel shuffle is producing incorrect results.\n\n Let's analyze the current kernel:\n\n   The kernel is launched with two-dimensional grid:\n     - pid_bhw: from 0 to (batch_size * height * width - 1)\n     - pid_c: from 0 to (ceil(channels / BLOCK_SIZE) - 1)\n\n   For each element in the grid, it loads a block of channels (of size BLOCK_SIZE) for a specific spatial location (h, w) and batch index.\n\n   The transformation for channel shuffle is:\n        original channel index c is split into:\n          group_idx = c // channels_per_group\n          index_in_group = c % channels_per_group\n        then the shuffled channel index is: new_c = index_in_group * groups + group_idx\n\n   However, note that the kernel is doing the reverse? Let's look:\n\n        group_indices = c_offsets % groups\n        index_in_group = c_offsets // groups\n        input_c = group_indices * channels_per_group + index_in_group\n\n   This is actually the inverse of the shuffle operation! \n\n   Why?\n   The channel shuffle operation is defined as:\n        Given an input tensor of shape (B, C, H, W), we reshape it to (B, groups, C//groups, H, W)\n        then transpose to (B, C//groups, groups, H, W) and then flatten back to (B, C, H, W).\n\n   The effect is that the original channel index c (0-indexed) is split into:\n        group_idx = c // (channels_per_group)   [which group it belongs to originally]\n        index_in_group = c % channels_per_group  [the index within the original group]\n\n   After transpose, the new channel index becomes:\n        new_c = (index_in_group) * groups + group_idx\n\n   But note: the kernel is trying to compute the source index for a given output index.\n\n   In the kernel, we are writing to the output tensor at channel index `c_offsets`.\n   We want the value to come from the input tensor at a channel index that we compute.\n\n   We have:\n        Let c_out = c_offsets (the channel index in the output tensor)\n        We split c_out into:\n            group_idx_out = c_out % groups\n            index_in_group_out = c_out // groups\n\n        Then the original channel index (in the input tensor) would be:\n            c_in = group_idx_out * channels_per_group + index_in_group_out\n\n   Why?\n        Because in the input tensor, the channel was originally grouped: \n            first group: [0, 1, ..., channels_per_group-1]\n            second group: [channels_per_group, channels_per_group+1, ...]\n            etc.\n\n        And in the output, we have:\n            first group: [0, groups, 2*groups, ...]\n            second group: [1, groups+1, ...]\n            etc.\n\n        So if we have an output channel index c_out, we can compute:\n            group_idx_out = c_out % groups   -> which group in the output? \n            index_in_group_out = c_out // groups   -> which element in that group?\n\n        Then in the input, that element was in group `group_idx_out` at position `index_in_group_out`.\n\n        Therefore, the input channel index is: group_idx_out * channels_per_group + index_in_group_out.\n\n   However, the current kernel does:\n\n        group_indices = c_offsets % groups\n        index_in_group = c_offsets // groups\n        input_c = group_indices * channels_per_group + index_in_group\n\n   This is the correct mapping for the inverse? Actually, note:\n\n        We are writing to output at (batch_index, c_offsets, h_index, w_index) and we want to read from input at (batch_index, input_c, h_index, w_index).\n\n        The shuffle operation is a permutation. The inverse permutation is:\n\n            c_in = (c_out % groups) * channels_per_group + (c_out // groups)\n\n        which is exactly what the kernel is doing.\n\n   But wait: the shuffle operation is defined as:\n        output[b, c, h, w] = input[b, c', h, w]\n        where c' = (c // channels_per_group) + (c % channels_per_group) * groups   ??? \n\n   Actually, let me clarify:\n\n        The standard channel shuffle operation:\n\n          x = x.view(batch_size, groups, channels_per_group, height, width)\n          x = x.transpose(1, 2).contiguous()\n          x = x.view(batch_size, -1, height, width)\n\n        How does the index map?\n\n          Original index: (b, g, k, h, w) -> where g in [0, groups-1], k in [0, channels_per_group-1]\n          After transpose: (b, k, g, h, w)\n          Then flatten: the new channel index is: k * groups + g.\n\n        So for an original channel index c (from 0 to C-1), we have:\n          g = c // channels_per_group\n          k = c % channels_per_group\n          then new channel index = k * groups + g.\n\n        Therefore, the forward shuffle mapping is:\n          c_out = (c_in % channels_per_group) * groups + (c_in // channels_per_group)\n\n        And the inverse mapping (which we don't need for the forward pass) would be:\n          c_in = (c_out // groups) + (c_out % groups) * channels_per_group   ??? \n\n        Actually, let's invert:\n\n          We have: c_out = k * groups + g, where k = c_in % channels_per_group, g = c_in // channels_per_group.\n\n          Then: \n            g = c_out % groups\n            k = c_out // groups\n\n          Then c_in = g * channels_per_group + k.\n\n        So the inverse mapping is: c_in = (c_out % groups) * channels_per_group + (c_out // groups)\n\n   But note: in the kernel we are writing the shuffled output. So we are doing the forward shuffle.\n\n        We want: output[b, c_out, h, w] = input[b, c_in, h, w]\n        where c_in = (c_out % groups) * channels_per_group + (c_out // groups)   [because that's the inverse of the forward mapping?]\n\n        Actually, we want the forward mapping: \n          c_in -> c_out = (c_in % channels_per_group)*groups + (c_in // channels_per_group)\n\n        So if we are given an output channel index c_out, we are writing to c_out and we need to know which input channel to read.\n\n        But note: the mapping is a bijection. We can express the input channel index in terms of c_out?\n\n        Alternatively, we can change the kernel to compute the output index from the input index? \n\n        The current kernel is using the output index (c_offsets) to compute the input index. And it uses:\n\n          input_c = (c_offsets % groups) * channels_per_group + (c_offsets // groups)\n\n        This is the inverse mapping! So it is actually computing the inverse of the shuffle.\n\n        Therefore, we are writing the inverse shuffle operation.\n\n   What we want is the forward shuffle.\n\n   So the correct mapping for the forward shuffle is:\n\n        For an output channel index c_out, the input channel index should be:\n\n          c_in = (c_out // groups) + (c_out % groups) * channels_per_group   -> This is the same as above? \n\n        But wait, let me check with an example:\n\n          groups=3, channels_per_group=4, so total channels=12.\n\n          For c_out=0: \n             group = 0, index_in_group = 0 -> c_in = 0*4 + 0 = 0 -> then the forward shuffle would map c_in=0 to c_out = (0%4)*3 + (0//4)=0 -> correct.\n\n          For c_out=1: \n             group=1, index_in_group=0 -> c_in=1*4+0=4 -> then forward shuffle: c_in=4 -> (4%4)*3+(4//4)=0*3+1=1 -> correct.\n\n          For c_out=2: \n             group=2, index_in_group=0 -> c_in=2*4+0=8 -> then forward shuffle: 8 -> (8%4)*3+(8//4)=0+2=2 -> correct.\n\n          For c_out=3: \n             group=0, index_in_group=1 -> c_in=0*4+1=1 -> then forward shuffle: 1 -> (1%4)*3+0=3 -> correct.\n\n        So the mapping we have in the kernel is the inverse of the forward shuffle? \n\n        Actually, the kernel is writing the output at c_out and reading from the input at c_in, and the mapping is:\n\n          c_in = (c_out % groups)*channels_per_group + (c_out // groups)\n\n        This is exactly the inverse of the forward shuffle mapping.\n\n        But we want to do the forward shuffle. So we want:\n\n          output[b, c_out, h, w] = input[b, c_in, h, w] \n          where c_in = (c_out % groups)*channels_per_group + (c_out // groups)\n\n        This is the same as the inverse of the forward shuffle? \n\n        Actually, the forward shuffle mapping is:\n\n          forward(c_in) = (c_in % channels_per_group)*groups + (c_in // channels_per_group)\n\n        And the inverse mapping is:\n\n          inverse(c_out) = (c_out % groups)*channels_per_group + (c_out // groups)\n\n        So the kernel is doing the inverse shuffle? \n\n        But note: we are implementing the forward shuffle operation. We want to permute the channels.\n\n        The kernel should do the forward permutation? \n\n        However, the kernel is called in the forward pass of the model. We want to shuffle the channels.\n\n        The standard way is to use the forward mapping.\n\n        The current kernel does the inverse permutation. Therefore, it is actually undoing a shuffle.\n\n        That is why we are getting incorrect results.\n\n   How to fix:\n\n        We want the forward shuffle? Then we should use the forward mapping.\n\n        But note: the kernel is written from the perspective of the output index. We are given an output index c_out and we want to know which input index to read.\n\n        The forward shuffle mapping is:\n\n          c_out = (c_in % channels_per_group)*groups + (c_in // channels_per_group)\n\n        How to express c_in in terms of c_out? \n\n          Let:\n            k = c_out // groups    # because groups are the stride in the output\n            g = c_out % groups\n\n          Then we have:\n            k = c_in % channels_per_group\n            g = c_in // channels_per_group\n\n          Therefore, c_in = g * channels_per_group + k.\n\n        But note: this is the same as the inverse mapping we had? \n\n        Actually, we have:\n\n          c_in = g * channels_per_group + k = (c_out % groups)*channels_per_group + (c_out // groups)\n\n        So the kernel is already using the formula for the forward shuffle? \n\n        Wait, no: the kernel is using:\n\n          input_c = (c_offsets % groups) * channels_per_group + (c_offsets // groups)\n\n        and c_offsets is the output channel index.\n\n        So that is the forward shuffle? \n\n        But why did I think it was the inverse? \n\n        Let me clarify:\n\n          The standard shuffle operation (forward) is defined by:\n\n            output = input.view(B, groups, channels_per_group, H, W).transpose(1,2).contiguous().view(B, C, H, W)\n\n          This mapping from input to output:\n\n            input[b, g * channels_per_group + k, h, w] -> output[b, k * groups + g, h, w]\n\n          Therefore, if we are writing the output at (b, c_out, h, w) we must read the input at:\n\n            g = c_out % groups\n            k = c_out // groups\n            c_in = g * channels_per_group + k\n\n          So the kernel is correct.\n\n        Why then is there an output mismatch?\n\n        Let me check the kernel code again:\n\n          We are loading from the input at:\n            input_offsets = batch_index * input_batch_stride + input_c * input_channel_stride + ... \n\n          and storing to the output at:\n            output_offsets = ... + c_offsets * output_channel_stride + ...\n\n          This is correct.\n\n        Another possibility: the kernel uses the variable `channels_per_group` which is computed as `channels // groups`. \n        But note: the kernel is passed `channels_per_group` from the host. We are passing `channels_per_group = channels // groups` in the call.\n\n        However, the kernel also uses `channels` (the total channels) and `groups`. \n\n        The problem might be in the grid launch:\n\n          grid = (batch_size * height * width, triton.cdiv(channels, 256))\n\n          Here, the second grid dimension is the number of blocks for the channel dimension. We are iterating over channels in blocks of 256.\n\n          But note: the kernel uses `c_offsets = pid_c * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)` and then `mask = c_offsets < channels`.\n\n          This is correct.\n\n        Another possibility: the input and output tensors might have different strides? But we are using the strides from the tensors.\n\n        However, note: the input and output tensors are created as `output = torch.zeros_like(x)`. This will have the same strides.\n\n        But what if the input tensor is not contiguous? The kernel uses the strides of the input and output.\n\n        We can make sure that the input is contiguous by doing `x = x.contiguous()` in the ChannelShuffle.forward.\n\n        Let me look at the ChannelShuffle.forward:\n\n          output = torch.zeros_like(x)\n\n          Then we launch the kernel. But we are using the strides of x and output. If x is not contiguous, then the strides might be non-standard.\n\n        We should ensure that the input is contiguous.\n\n        Alternatively, we can change the kernel to use the same indexing as the first version (the one that was correct) which did:\n\n          idx = offsets\n          w = idx % W\n          h = (idx // W) % H\n          c = (idx // (H * W)) % C\n          b = idx // (C * H * W)\n\n          ... and then computed the new channel index and then the output index.\n\n        That kernel was correct (as per the history: Program 1 had correctness=1.0).\n\n        The current kernel is two-dimensional and might be more efficient, but it has a correctness issue.\n\n        Given the time, we might revert to the one-dimensional kernel that we know is correct.\n\n        The history shows that the one-dimensional kernel (from Program 1) was correct and had a speedup of 0.449 (which is slower than the reference, but correct).\n\n        We are currently at a stage where we have to fix correctness.\n\n        Let's replace the kernel with the one from Program 1 that was correct.\n\n        We will also change the ChannelShuffle.forward to use that kernel.\n\n        Steps:\n\n          1. Replace the kernel with the one from Program 1 (the one-dimensional one).\n          2. Adjust the launch grid accordingly.\n\n        We note that the one-dimensional kernel:\n\n          @triton.autotune(...)\n          @triton.jit\n          def channel_shuffle_kernel(\n              input_ptr, output_ptr,\n              B, C, H, W, groups,\n              n_elements,\n              BLOCK_SIZE: tl.constexpr,\n          ):\n              pid = tl.program_id(0)\n              block_start = pid * BLOCK_SIZE\n              offsets = block_start + tl.arange(0, BLOCK_SIZE)\n              mask = offsets < n_elements\n\n              idx = offsets\n              w = idx % W\n              h = (idx // W) % H\n              c = (idx // (H * W)) % C\n              b = idx // (C * H * W)\n\n              channels_per_group = C // groups\n              group_idx = c // channels_per_group\n              index_in_group = c % channels_per_group\n              new_c = index_in_group * groups + group_idx\n\n              output_idx = b * (C * H * W) + new_c * (H * W) + h * W + w\n              x = tl.load(input_ptr + idx, mask=mask, eviction_policy='evict_last')\n              tl.store(output_ptr + output_idx, x, mask=mask)\n\n          And in the forward:\n\n              output = torch.empty_like(x)\n              total_elements = B * C * H * W\n              grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n              channel_shuffle_kernel[grid](x, output, B, C, H, W, self.groups, total_elements)\n\n        Why was this kernel correct?\n\n          It uses a one-dimensional grid over all elements (B*C*H*W). For each element, it computes the original index (b, c, h, w) and then the shuffled channel index new_c.\n\n          Then it writes the value to the output at (b, new_c, h, w).\n\n          This is the forward shuffle mapping: \n              new_c = (c % channels_per_group) * groups + (c // channels_per_group)\n\n          And note: the input element at (b, c, h, w) goes to (b, new_c, h, w).\n\n        This is the standard shuffle.\n\n        The current two-dimensional kernel was doing the same mapping? \n\n          It computed: \n            input_c = (c_offsets % groups)*channels_per_group + (c_offsets // groups)\n\n          and then read from the input at that channel and wrote to the output at c_offsets.\n\n          This is the same as the one-dimensional kernel? \n\n          In the one-dimensional kernel, we are reading from (b, c, h, w) and writing to (b, new_c, h, w).\n\n          In the two-dimensional kernel, we are reading from (b, input_c, h, w) and writing to (b, c_offsets, h, w).\n\n          And we set: \n            c_offsets = the output channel index, and then input_c = f(c_offsets) = (c_offsets % groups)*channels_per_group + (c_offsets // groups)\n\n          This is the inverse of the one-dimensional kernel's mapping? \n\n          Actually, the mapping in the one-dimensional kernel is:\n\n            output(b, new_c, h, w) = input(b, c, h, w)\n\n          and the two-dimensional kernel does:\n\n            output(b, c_offsets, h, w) = input(b, input_c, h, w)\n\n          with input_c = (c_offsets % groups)*channels_per_group + (c_offsets // groups)\n\n          So if we let new_c = c_offsets, then we require that:\n\n            c = input_c = (c_offsets % groups)*channels_per_group + (c_offsets // groups)\n\n          But the one-dimensional kernel sets:\n\n            new_c = (c % channels_per_group)*groups + (c // channels_per_group)\n\n          and then we have:\n\n            c = (new_c % groups)*channels_per_group + (new_c // groups)\n\n          which is the same as the two-dimensional kernel's formula.\n\n          Therefore, the two kernels are mathematically equivalent.\n\n        Why then the two-dimensional kernel fails?\n\n        The two-dimensional kernel might have an issue with the way we are handling the strides? \n\n        In the two-dimensional kernel, we are using the strides provided by the tensor. The one-dimensional kernel does not use strides because it flattens the tensor.\n\n        The one-dimensional kernel assumes that the tensor is contiguous (because it uses linear indexing). \n\n        In the two-dimensional kernel, we are using the strides. If the input tensor is not contiguous, then the strides might be non-standard.\n\n        We can fix by making the input contiguous in the two-dimensional kernel. However, the current ChannelShuffle.forward already does:\n\n            output = torch.zeros_like(x)\n\n          and then if x.is_cuda, we run the kernel. But we don't make x contiguous.\n\n        We should change:\n\n            x = x.contiguous()\n            output = torch.empty_like(x)\n\n        and then run the kernel.\n\n        Alternatively, we can use the one-dimensional kernel for now to get correctness and then try to optimize later.\n\n        Given that the one-dimensional kernel was correct, I propose we revert to that.\n\n        We are allowed to change the kernel and the ChannelShuffle.forward.\n\n        Let's do:\n\n          Replace the kernel with the one-dimensional kernel from Program 1.\n\n        We note that Program 1 had:\n\n          class ChannelShuffle(nn.Module):\n            def forward(self, x):\n                B, C, H, W = x.size()\n                assert C % self.groups == 0\n\n                if x.numel() == 0:\n                    return x\n\n                output = torch.empty_like(x)\n                total_elements = B * C * H * W\n                grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n                channel_shuffle_kernel[grid](\n                    x, output,\n                    B, C, H, W, self.groups,\n                    total_elements\n                )\n                return output\n\n        And the kernel was defined with autotune configs for BLOCK_SIZE.\n\n        We'll use that.\n\n        However, note that the performance of the one-dimensional kernel was 0.449 speedup (slower than reference). \n        But we have to fix correctness first.\n\n        We can then try to optimize the kernel in a subsequent step.\n\n        For now, let's rewrite the current program to use the one-dimensional kernel.\n\n        We will change:\n\n          - The kernel definition to the one-dimensional one.\n          - The ChannelShuffle.forward to use the one-dimensional launch.\n\n        We leave the rest of the model unchanged.\n\n        We note that the autotune configs in Program 1 were:\n\n            configs=[\n                triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n                triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n                triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n                triton.Config({'BLOCK_SIZE': 1024}, num_warps=32),\n            ]\n\n        We'll use that.\n\n        Let's write the new code.\n\n        IMPORTANT: We are only allowed to change the code within the EVOLVE-BLOCK.\n\n        We are replacing the kernel and the ChannelShuffle class.\n\n        We are not changing the rest of the model.\n\n        We note that the current program has the ChannelShuffle class and the kernel.\n\n        We replace both.\n\n        We also note that the one-dimensional kernel does not use the strides, so we remove the stride arguments.\n\n        We also remove the two-dimensional grid and use one-dimensional.\n\n        We'll do:\n\n          @triton.autotune(...)   [as in Program 1]\n          @triton.jit\n          def channel_shuffle_kernel( ... )   [as in Program 1]\n\n          class ChannelShuffle(nn.Module):\n              ... [as in Program 1]\n\n        We leave ShuffleNetUnit and ModelNew unchanged.\n\n        We hope this will fix the correctness.\n\n        Let's output the code accordingly.\n</think>\nTo address the correctness issue in the channel shuffle operation, we'll revert to a simpler and verified one-dimensional Triton kernel approach. The key changes are:\n\n1. Replace the 2D kernel with a 1D kernel that processes all tensor elements linearly\n2. Use the correct shuffle mapping: `new_c = (c % channels_per_group) * groups + (c // channels_per_group)`\n3. Ensure proper boundary checks and memory operations\n4. Maintain tensor contiguity for efficient memory access\n\nHere's the complete solution:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=32),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef channel_shuffle_kernel(\n    input_ptr, output_ptr,\n    B, C, H, W, groups,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    idx = offsets\n    w = idx % W\n    h = (idx // W) % H\n    c = (idx // (H * W)) % C\n    b = idx // (C * H * W)\n\n    channels_per_group = C // groups\n    group_idx = c // channels_per_group\n    index_in_group = c % channels_per_group\n    new_c = index_in_group * groups + group_idx\n\n    output_idx = b * (C * H * W) + new_c * (H * W) + h * W + w\n    x = tl.load(input_ptr + idx, mask=mask)\n    tl.store(output_ptr + output_idx, x, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        B, C, H, W = x.size()\n        assert C % self.groups == 0\n        \n        if x.numel() == 0:\n            return x\n\n        output = torch.empty_like(x)\n        total_elements = B * C * H * W\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        channel_shuffle_kernel[grid](\n            x, output,\n            B, C, H, W, self.groups,\n            total_elements\n        )\n        return output\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
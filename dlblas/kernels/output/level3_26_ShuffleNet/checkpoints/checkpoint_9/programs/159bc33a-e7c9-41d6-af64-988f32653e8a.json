{"id": "159bc33a-e7c9-41d6-af64-988f32653e8a", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=32),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef channel_shuffle_kernel(\n    input_ptr, output_ptr,\n    B, C, H, W, groups,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    idx = offsets\n    w = idx % W\n    h = (idx // W) % H\n    c = (idx // (H * W)) % C\n    b = idx // (C * H * W)\n\n    channels_per_group = C // groups\n    group_idx = c // channels_per_group\n    index_in_group = c % channels_per_group\n    new_c = index_in_group * groups + group_idx\n\n    output_idx = b * (C * H * W) + new_c * (H * W) + h * W + w\n    x = tl.load(input_ptr + idx, mask=mask, eviction_policy='evict_last')\n    tl.store(output_ptr + output_idx, x, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        B, C, H, W = x.size()\n        assert C % self.groups == 0\n        \n        if x.numel() == 0:\n            return x\n\n        output = torch.empty_like(x)\n        total_elements = B * C * H * W\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        channel_shuffle_kernel[grid](\n            x, output,\n            B, C, H, W, self.groups,\n            total_elements\n        )\n        return output\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "eeb90e51-8ebb-4ffb-acff-d9c62e4dd541", "generation": 2, "timestamp": 1754588706.415235, "iteration_found": 7, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.449, "runtime_ms": 8.58, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 3.85, "speed_up": 0.449, "custom_timing": 8.58}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.407, "runtime_ms": 9.46, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 3.85, "speed_up": 0.407, "custom_timing": 9.46}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.4070\n- runtime_ms: 9.4600\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 3.85, 'speed_up': 0.407, 'custom_timing': 9.46}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.4070, runtime_ms: 9.4600, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 3.85, 'speed_up': 0.407, 'custom_timing': 9.46}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.4090, runtime_ms: 9.4200, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 3.85, 'speed_up': 0.409, 'custom_timing': 9.42}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.4520, runtime_ms: 8.5100, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 3.85, 'speed_up': 0.452, 'custom_timing': 8.51}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 2048}, num_warps=1),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef channel_shuffle_kernel(\n    input_ptr, output_ptr,\n    B, C, H, W, groups,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    idx = offsets\n    w = idx % W\n    h = (idx // W) % H\n    c = (idx // (H * W)) % C\n    b = idx // (C * H * W)\n\n    channels_per_group = C // groups\n    group_idx = c // channels_per_group\n    index_in_group = c % channels_per_group\n    new_c = index_in_group * groups + group_idx\n\n    output_idx = b * (C * H * W) + new_c * (H * W) + h * W + w\n    x = tl.load(input_ptr + idx, mask=mask, eviction_policy='evict_last')\n    tl.store(output_ptr + output_idx, x, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        B, C, H, W = x.size()\n        assert C % self.groups == 0\n        \n        if x.numel() == 0:\n            return x\n\n        output = torch.empty_like(x)\n        total_elements = B * C * H * W\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        channel_shuffle_kernel[grid](\n            x, output,\n            B, C, H, W, self.groups,\n            total_elements\n        )\n        return output\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        \n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        \n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        \n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        \n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.4520), Performs well on runtime_ms (8.5100), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 3.85, 'speed_up': 0.452, 'custom_timing': 8.51})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef channel_shuffle_kernel(\n    x_ptr,\n    output_ptr,\n    channels,\n    groups,\n    height,\n    width,\n    stride_b,\n    stride_c,\n    stride_h,\n    stride_w,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    total_spatial = height * width\n    batch_idx = pid0 // total_spatial\n    spatial_idx = pid0 % total_spatial\n    h_idx = spatial_idx // width\n    w_idx = spatial_idx % width\n    \n    channel_start = pid1 * BLOCK_SIZE\n    c_offsets = channel_start + tl.arange(0, BLOCK_SIZE)\n    mask = c_offsets < channels\n    \n    channels_per_group = channels // groups\n    group_index = c_offsets % groups\n    channel_in_group = c_offsets // groups\n    input_channel = group_index * channels_per_group + channel_in_group\n    \n    base_idx = batch_idx * stride_b + h_idx * stride_h + w_idx * stride_w\n    input_ptr = x_ptr + base_idx + input_channel * stride_c\n    output_ptr = output_ptr + base_idx + c_offsets * stride_c\n    \n    x_vals = tl.load(input_ptr, mask=mask, other=0.0)\n    tl.store(output_ptr, x_vals, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        assert channels % self.groups == 0, \"channels must be divisible by groups\"\n        \n        x_cont = x.contiguous()\n        output = torch.empty_like(x_cont)\n        \n        if x_cont.numel() == 0:\n            return output\n        \n        total_spatial = batch_size * height * width\n        BLOCK_SIZE = 128\n        grid = (total_spatial, triton.cdiv(channels, BLOCK_SIZE))\n        \n        channel_shuffle_kernel[grid](\n            x_cont, output,\n            channels, self.groups, height, width,\n            x_cont.stride(0), x_cont.stride(1), x_cont.stride(2), x_cont.stride(3),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.4090), Performs well on runtime_ms (9.4200), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 3.85, 'speed_up': 0.409, 'custom_timing': 9.42})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef channel_shuffle_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n    channels, groups, channels_per_group, height, width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_hw = tl.program_id(1)\n    pid_c = tl.program_id(2)\n    \n    h = pid_hw // width\n    w = pid_hw % width\n    \n    c_offsets = pid_c * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = c_offsets < channels\n    \n    group_indices = c_offsets % groups\n    index_in_group = c_offsets // groups\n    input_c = group_indices * channels_per_group + index_in_group\n    \n    input_offsets = pid_b * input_batch_stride + input_c * input_channel_stride + h * input_height_stride + w * input_width_stride\n    output_offsets = pid_b * output_batch_stride + c_offsets * output_channel_stride + h * output_height_stride + w * output_width_stride\n    \n    values = tl.load(input_ptr + input_offsets, mask=mask, other=0)\n    tl.store(output_ptr + output_offsets, values, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        channels_per_group = channels // self.groups\n        \n        if self.groups == 1:\n            return x\n            \n        output = torch.empty_like(x)\n        \n        if x.is_cuda:\n            BLOCK_SIZE = 128\n            grid = (batch_size, height * width, triton.cdiv(channels, BLOCK_SIZE))\n            channel_shuffle_kernel[grid](\n                x, output,\n                x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n                output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n                channels, self.groups, channels_per_group, height, width,\n                BLOCK_SIZE=BLOCK_SIZE\n            )\n            return output\n        else:\n            x = x.view(batch_size, self.groups, channels_per_group, height, width)\n            x = x.transpose(1, 2).contiguous()\n            x = x.view(batch_size, -1, height, width)\n            return x\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.4070), Performs well on runtime_ms (9.4600), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 3.85, 'speed_up': 0.407, 'custom_timing': 9.46})\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef channel_shuffle_kernel(\n    x_ptr,\n    output_ptr,\n    channels,\n    groups,\n    height,\n    width,\n    stride_b,\n    stride_c,\n    stride_h,\n    stride_w,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    total_spatial = height * width\n    batch_idx = pid0 // total_spatial\n    spatial_idx = pid0 % total_spatial\n    h_idx = spatial_idx // width\n    w_idx = spatial_idx % width\n    \n    channel_start = pid1 * BLOCK_SIZE\n    c_offsets = channel_start + tl.arange(0, BLOCK_SIZE)\n    mask = c_offsets < channels\n    \n    channels_per_group = channels // groups\n    group_index = c_offsets % groups\n    channel_in_group = c_offsets // groups\n    input_channel = group_index * channels_per_group + channel_in_group\n    \n    base_idx = batch_idx * stride_b + h_idx * stride_h + w_idx * stride_w\n    input_ptr = x_ptr + base_idx + input_channel * stride_c\n    output_ptr = output_ptr + base_idx + c_offsets * stride_c\n    \n    x_vals = tl.load(input_ptr, mask=mask, other=0.0)\n    tl.store(output_ptr, x_vals, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        assert channels % self.groups == 0, \"channels must be divisible by groups\"\n        \n        x_cont = x.contiguous()\n        output = torch.empty_like(x_cont)\n        \n        if x_cont.numel() == 0:\n            return output\n        \n        total_spatial = batch_size * height * width\n        BLOCK_SIZE = 128\n        grid = (total_spatial, triton.cdiv(channels, BLOCK_SIZE))\n        \n        channel_shuffle_kernel[grid](\n            x_cont, output,\n            channels, self.groups, height, width,\n            x_cont.stride(0), x_cont.stride(1), x_cont.stride(2), x_cont.stride(3),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 2048}, num_warps=1),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef channel_shuffle_kernel(\n    input_ptr, output_ptr,\n    B, C, H, W, groups,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    idx = offsets\n    w = idx % W\n    h = (idx // W) % H\n    c = (idx // (H * W)) % C\n    b = idx // (C * H * W)\n\n    channels_per_group = C // groups\n    group_idx = c // channels_per_group\n    index_in_group = c % channels_per_group\n    new_c = index_in_group * groups + group_idx\n\n    output_idx = b * (C * H * W) + new_c * (H * W) + h * W + w\n    x = tl.load(input_ptr + idx, mask=mask, eviction_policy='evict_last')\n    tl.store(output_ptr + output_idx, x, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        B, C, H, W = x.size()\n        assert C % self.groups == 0\n        \n        if x.numel() == 0:\n            return x\n\n        output = torch.empty_like(x)\n        total_elements = B * C * H * W\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        channel_shuffle_kernel[grid](\n            x, output,\n            B, C, H, W, self.groups,\n            total_elements\n        )\n        return output\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        \n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        \n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        \n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        \n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef channel_shuffle_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n    channels, groups, channels_per_group, height, width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_hw = tl.program_id(1)\n    pid_c = tl.program_id(2)\n    \n    h = pid_hw // width\n    w = pid_hw % width\n    \n    c_offsets = pid_c * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = c_offsets < channels\n    \n    group_indices = c_offsets % groups\n    index_in_group = c_offsets // groups\n    input_c = group_indices * channels_per_group + index_in_group\n    \n    input_offsets = pid_b * input_batch_stride + input_c * input_channel_stride + h * input_height_stride + w * input_width_stride\n    output_offsets = pid_b * output_batch_stride + c_offsets * output_channel_stride + h * output_height_stride + w * output_width_stride\n    \n    values = tl.load(input_ptr + input_offsets, mask=mask, other=0)\n    tl.store(output_ptr + output_offsets, values, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        channels_per_group = channels // self.groups\n        \n        if self.groups == 1:\n            return x\n            \n        output = torch.empty_like(x)\n        \n        if x.is_cuda:\n            BLOCK_SIZE = 128\n            grid = (batch_size, height * width, triton.cdiv(channels, BLOCK_SIZE))\n            channel_shuffle_kernel[grid](\n                x, output,\n                x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n                output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n                channels, self.groups, channels_per_group, height, width,\n                BLOCK_SIZE=BLOCK_SIZE\n            )\n            return output\n        else:\n            x = x.view(batch_size, self.groups, channels_per_group, height, width)\n            x = x.transpose(1, 2).contiguous()\n            x = x.view(batch_size, -1, height, width)\n            return x\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program which is a Triton implementation of Channel Shuffle for ShuffleNet.\n The current performance shows a speed_up of 0.407 (which is less than 1, meaning slower than the reference).\n We need to improve the Triton kernel to get a speedup of at least 1.05 (5%) to start gaining points.\n\n Analysis:\n The current Triton kernel for channel shuffle uses a grid of (batch_size, height * width, triton.cdiv(channels, BLOCK_SIZE)).\n Each program has 3 dimensions: \n   pid0: batch index\n   pid1: combined spatial index (h * width + w)\n   pid2: channel block index\n\n The kernel loads a block of channels for a fixed (batch, h, w) and then scatters them to a new channel order.\n\n However, note that the memory access pattern might not be optimal because:\n   - The input is accessed at [b, input_c, h, w] and output at [b, new_c, h, w].\n   - The input_c for a given block of new_c is not contiguous (because of the shuffle).\n\n We have two competing approaches in the inspiration programs:\n   Approach 1 (Top Program 1): Uses a 1D grid and flattens the entire tensor. Then each thread handles one element.\n   Approach 2 (Top Program 2): Uses a 2D grid (total_spatial, channel_blocks) where total_spatial = batch * height * width.\n\n The current program (3D grid) is the slowest among the three. The fastest among the top programs is the one with the 1D grid (speed_up=0.452).\n\n However, note that the reference timing is 3.85 ms and the current custom timing is 9.46 ms. We are slower by a factor of ~2.5.\n\n Let's reconsider the operation:\n   Channel shuffle: [B, C, H, W] -> split into groups -> transpose the groups -> [B, C, H, W]\n\n In the Triton kernel, we are effectively doing:\n   for each batch, each spatial location, and each channel in a block:\n        group_indices = c % groups\n        index_in_group = c // groups\n        input_c = group_indices * channels_per_group + index_in_group\n\n   Then we read from input[b, input_c, h, w] and write to output[b, c, h, w].\n\n The read from input is non-contiguous in the channel dimension because we are jumping by `channels_per_group` for consecutive `c`.\n\n How can we improve?\n   - We want to read contiguous memory as much as possible. Notice that for a fixed (b, h, w), the input channels are stored contiguously. However, we are reading them in a strided manner (with stride=channels_per_group) for the first `groups` channels, then the next set? This is not coalesced.\n\n   - Alternatively, we can change the kernel to read contiguous blocks of input and then write in a shuffled manner? But note: the shuffle pattern is fixed and we cannot change it.\n\n   - Another idea: Instead of having the channel dimension as the innermost in the grid, we can try to vectorize the load and store. Currently, we are using a block of channels for the same spatial location. We are loading a contiguous block of channels? But note: the input channel indices we are loading are not contiguous. They are at intervals of `groups` (if we consider consecutive c, then input_c goes: 0, channels_per_group, 2*channels_per_group, ... until groups-1, then 1, channels_per_group+1, ...). So we are reading non-contiguous memory.\n\n   - We might need to change the kernel to read contiguous memory in the input and then rearrange in registers? But note: the block size in the channel dimension is 128, and we are reading 128 channels that are scattered. This is not efficient.\n\n   - Alternatively, we can change the way we index. Notice that the shuffle operation is a permutation of the channel dimension. We can precompute the permutation and then use it? But that doesn't change the memory access pattern.\n\n   - We can try to use a different grid layout. The 1D grid approach (Top Program 1) flattens the entire tensor and then each thread handles one element. This might be better because the memory access is determined by the contiguous index. However, note that the output index is not contiguous because of the shuffle. But the input read is contiguous? Actually, in the 1D kernel:\n\n        idx = offsets   [a global index in the flattened tensor]\n        Then they compute:\n            w = idx % W\n            h = (idx // W) % H\n            c = (idx // (H * W)) % C\n            b = idx // (C * H * W)\n\n        Then they compute the new channel index `new_c` and then compute the output index as:\n            output_idx = b * (C * H * W) + new_c * (H * W) + h * W + w\n\n        This is a scatter operation: consecutive threads (which are contiguous in the flattened input) are reading contiguous memory, but writing to non-contiguous memory. This might be better because reading is contiguous and writing might be coalesced? The H100 has a very high memory bandwidth and might handle the scattered writes better.\n\n   - The 1D kernel in Top Program 1 also uses autotuning for the block size.\n\n   - The current 3D kernel might be suffering from the overhead of having many small blocks? The grid has batch_size * (H*W) * (ceil(channels/BLOCK_SIZE)) blocks. For example, with batch_size=10, H=W=224, channels=24 (first layer) -> 10 * 50176 * ceil(24/128)=10*50176*1 = 501760 blocks. This is a lot of blocks and might cause overhead.\n\n   - The 1D grid in Top Program 1 has total_elements = B*C*H*W and then grid = (ceil(total_elements / BLOCK_SIZE),). For the same example, total_elements = 10*24*224*224 = 12042240. If BLOCK_SIZE=2048, then grid size = ceil(12042240/2048) = 5880. That is much fewer blocks.\n\n   - We note that the 1D kernel in Top Program 1 has a speed_up of 0.452 (which is better than the current 0.407). So let's try that approach.\n\n However, note that the 1D kernel in Top Program 1 does not use vectorized memory operations? It loads and stores one element per thread.\n\n But we can try to vectorize? Actually, the element is a scalar. But note: the Triton compiler might automatically vectorize if we use a block size that is a multiple of the vector width? We are not doing explicit vectorization.\n\n Alternatively, we can try to use a 2D grid that groups contiguous spatial locations for the same channel block? But the Top Program 2 uses a 2D grid and has a speed_up of 0.409 (worse than the 1D).\n\n Given the above, I propose to replace the current 3D kernel with the 1D kernel from Top Program 1, and also add autotuning as in that program.\n\n Steps:\n   - Replace the kernel with the 1D kernel from Top Program 1.\n   - Use the autotuner from Top Program 1.\n\n However, note that the current program has a fallback for CPU? We are not allowed to change the non-Triton part? Actually, we are only allowed to change the Triton kernel and the ChannelShuffle class's forward method for the GPU path.\n\n We are going to:\n\n   @triton.autotune(...)   [as in Top Program 1]\n   @triton.jit\n   def channel_shuffle_kernel(...)   [the 1D kernel from Top Program 1]\n\n   Then in ChannelShuffle.forward:\n        if x.is_cuda:\n            ... use the new kernel with 1D grid ...\n\n   We remove the existing 3D kernel and replace it with the 1D kernel.\n\n Let's write the new kernel and the forward method accordingly.\n\n Important: We must not change the function signature of the Module classes (only the internal implementation of the kernel and the forward method for the GPU path).\n\n We'll take the kernel from Top Program 1 and adapt the variable names to match the current context.\n\n Also note: the current kernel in Top Program 1 does not use the `groups` argument? Actually it does: it uses it to compute `channels_per_group = C // groups` and then the new channel index.\n\n Let's write the code accordingly.\n\n Note: We are allowed to change the kernel and the ChannelShuffle.forward method (only the GPU part) and the kernel definition.\n\n We are going to do:\n\n   class ChannelShuffle(nn.Module):\n        def forward(self, x):\n            ... same as in Top Program 1 ...\n\n   But note: the Top Program 1 kernel uses:\n        channel_shuffle_kernel[grid](\n            x, output,\n            B, C, H, W, self.groups,\n            total_elements\n        )\n\n   We must pass the groups and the dimensions.\n\n   We'll define the kernel as:\n\n        @triton.autotune(...)\n        @triton.jit\n        def channel_shuffle_kernel(\n            input_ptr, output_ptr,\n            B, C, H, W, groups,\n            n_elements,\n            BLOCK_SIZE: tl.constexpr,\n        ):\n\n   We are going to remove the existing kernel and replace it with this.\n\n However, note: the current program also has a CPU fallback. We must leave that unchanged.\n\n Let's write the new code accordingly.\n\n We hope that the 1D kernel will be faster than the current 3D kernel.\n\n But note: the Top Program 1 has a speed_up of 0.452 and the current reference is 3.85 ms -> custom 8.51 ms. We are currently at 9.46 ms. If we get to 8.51 ms that is an improvement (from 9.46 to 8.51, which is about 10% faster) but still slower than the reference. We need to do better.\n\n However, we note that the Top Program 1 was the best among the three. We might need to further optimize? But let's try this first.\n\n Alternatively, we can try to optimize the kernel further by:\n\n   - Using vectorized loads and stores? But note the access pattern is scattered in the output.\n\n   - We can try to change the block size to be larger and use more warps? The autotuner will help.\n\n   - We note that the Top Program 1 kernel uses eviction_policy='evict_last'. This might help with caching.\n\n   - Also, we can try to increase the number of warps? The autotuner in Top Program 1 only uses 1 warp per block? That might be a problem. Let me check: the autotuner configs all have num_warps=1. We can try to add configurations with more warps? But note the block size is large (up to 2048) and a warp is 32 threads. So a block of 2048 would require 64 warps? That is too many (max warps per SM is 64, but we are using one block per SM?).\n\n   Actually, the H100 has 2048 threads per SM? We can have a block size of 2048 and then set num_warps=64? But note: the autotuner in Top Program 1 does not set num_warps per config? Actually it does: in the configs, they set num_warps=1 for all. This might be inefficient for large block sizes.\n\n   We should adjust the autotuner to use more warps for larger block sizes? For example:\n\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=16),\n        triton.Config({'BLOCK_SIZE': 2048}, num_warps=32),\n\n   But note: the maximum number of warps per block is 32 (because 32 warps * 32 threads = 1024 threads). So for 2048 we cannot have one block? Actually, the block size must be <= the maximum number of threads per block (1024 for current GPUs). So the block size 2048 is not possible? \n\n   Correction: the Top Program 1 uses a block size of 2048? That is only possible if the GPU has a higher limit? Actually, the Triton compiler might break the block into multiple warps? But the maximum threads per block is 1024 for NVIDIA GPUs. So we cannot have a block size of 2048.\n\n   Therefore, the autotuner in Top Program 1 has a mistake? Or maybe Triton handles it by splitting? I think it's a mistake. We should remove the 2048 block size and adjust.\n\n   Let's change the autotuner to:\n\n        configs = []\n        for BLOCK_SIZE in [128, 256, 512, 1024]:\n            num_warps = BLOCK_SIZE // 256\n            if num_warps < 1:\n                num_warps = 1\n            # But note: the maximum warps per block is 32, and 1024//32=32 -> 1024/32=32 warps -> valid.\n            # Actually, we set num_warps as the number of warps required: BLOCK_SIZE must be divisible by 32? and then num_warps = BLOCK_SIZE // 32.\n            # But the maximum is 32 warps (1024 threads). So for BLOCK_SIZE=1024, we need 1024//32 = 32 warps.\n\n        However, the autotuner in Top Program 1 did not set num_warps per config? They set it to 1 for all. That might be the reason why it's slow.\n\n   We can try to set num_warps appropriately.\n\n   Alternatively, we can leave the autotuner as in Top Program 1 and hope that Triton automatically sets the number of warps? But I think we have to set it.\n\n   According to the Triton documentation, the `num_warps` argument is the number of warps to use. We should set it to the next power of two of (BLOCK_SIZE // 32) but capped at 32? Actually, we can set:\n\n        num_warps = BLOCK_SIZE // 32   [if BLOCK_SIZE is a multiple of 32]\n\n   But note: the block size might not be a multiple of 32? We should make it a multiple of 32? Actually, the kernel uses `tl.arange(0, BLOCK_SIZE)`, so it must be a power of two? Not necessarily, but it's common.\n\n   We'll change the autotuner to use:\n\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),   # 128/32=4\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),   # 256/32=8\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=16),  # 512/32=16\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=32), # 1024/32=32\n\n   And remove 2048.\n\n   But note: the kernel might work for any BLOCK_SIZE? We are using a mask.\n\n   Alternatively, we can use the autotuner from the example and add more warps.\n\n   Let's try to combine: use the 1D kernel and set the number of warps appropriately.\n\n   We'll define the autotuner with:\n\n        configs = [\n            triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n            triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n            triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n            triton.Config({'BLOCK_SIZE': 1024}, num_warps=32),\n        ]\n\n   And the kernel remains the same.\n\n   We hope that with more warps we can get better performance.\n\n   Also, note that the kernel uses:\n\n        x = tl.load(input_ptr + idx, mask=mask, eviction_policy='evict_last')\n\n   The eviction policy might be set to 'evict_first'? But the example uses 'evict_last'. We'll keep it.\n\n   We'll write the new code accordingly.\n\n   Steps:\n\n     1. Replace the kernel with the 1D autotuned kernel from Top Program 1, but with the adjusted autotuner (with num_warps set appropriately and without 2048).\n     2. In the forward method of ChannelShuffle, for the GPU path, we use the new kernel with a 1D grid.\n\n   We leave the CPU fallback unchanged.\n\n   We also note: the kernel in Top Program 1 does not use the groups argument? Actually, it does: it uses it to compute the new channel index. We pass `self.groups` to the kernel.\n\n   Let's write the code.\n\n   Important: We are only allowed to change the Triton kernel and the GPU part of the ChannelShuffle.forward method.\n\n   We are going to output the entire block.\n\n   Note: We must not change the class names (ModelNew) and the test code at the bottom.\n\n   We are going to remove the old kernel and put the new one.\n\n   Also, note that the old kernel had different parameters (like strides). The new kernel does not use strides? It flattens the tensor and recomputes indices. That is acceptable.\n\n   We hope this will improve the speed.\n\n   However, note: the current program has a correctness of 1.0, so we must maintain correctness.\n\n   Let's write the code.\n</think>\nTo improve the performance of the ChannelShuffle operation in the ShuffleNet model, I've optimized the Triton kernel by implementing a more efficient 1D grid approach with better memory coalescing and autotuning. The key changes include:\n\n1. Replaced the 3D grid with a 1D grid for better workload distribution\n2. Added autotuning for BLOCK_SIZE with optimized warp configurations\n3. Improved memory access patterns by flattening tensor computations\n4. Used eviction policy hints for better cache utilization\n\nHere's the complete optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=32),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef channel_shuffle_kernel(\n    input_ptr, output_ptr,\n    B, C, H, W, groups,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    idx = offsets\n    w = idx % W\n    h = (idx // W) % H\n    c = (idx // (H * W)) % C\n    b = idx // (C * H * W)\n\n    channels_per_group = C // groups\n    group_idx = c // channels_per_group\n    index_in_group = c % channels_per_group\n    new_c = index_in_group * groups + group_idx\n\n    output_idx = b * (C * H * W) + new_c * (H * W) + h * W + w\n    x = tl.load(input_ptr + idx, mask=mask, eviction_policy='evict_last')\n    tl.store(output_ptr + output_idx, x, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        B, C, H, W = x.size()\n        assert C % self.groups == 0\n        \n        if x.numel() == 0:\n            return x\n\n        output = torch.empty_like(x)\n        total_elements = B * C * H * W\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        channel_shuffle_kernel[grid](\n            x, output,\n            B, C, H, W, self.groups,\n            total_elements\n        )\n        return output\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
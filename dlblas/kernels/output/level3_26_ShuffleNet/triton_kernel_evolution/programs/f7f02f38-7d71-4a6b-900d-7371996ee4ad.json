{"id": "f7f02f38-7d71-4a6b-900d-7371996ee4ad", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_C': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_C': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_C': 256}, num_warps=4),\n    ],\n    key=['channels']\n)\n@triton.jit\ndef channel_shuffle_kernel(\n    input_ptr, output_ptr,\n    channels, groups, height, width,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n    BLOCK_SIZE_C: tl.constexpr,\n):\n    pid_bhw = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    \n    # Separate batch and spatial dimensions\n    total_spatial = height * width\n    batch_idx = pid_bhw // total_spatial\n    spatial_idx = pid_bhw % total_spatial\n    h_idx = spatial_idx // width\n    w_idx = spatial_idx % width\n    \n    # Create channel block offsets\n    c_offsets = pid_c * BLOCK_SIZE_C + tl.arange(0, BLOCK_SIZE_C)\n    mask = c_offsets < channels\n    \n    # Compute shuffled channel indices\n    channels_per_group = channels // groups\n    group_index = c_offsets % groups\n    index_in_group = c_offsets // groups\n    input_c = group_index * channels_per_group + index_in_group\n    \n    # Calculate base pointer offsets\n    base_offset = (batch_idx * input_batch_stride + \n                   h_idx * input_height_stride + \n                   w_idx * input_width_stride)\n    \n    # Compute input offsets for shuffled channels\n    input_offsets = base_offset + input_c * input_channel_stride\n    x = tl.load(input_ptr + input_offsets, mask=mask, other=0.0)\n    \n    # Compute output offsets\n    output_base = (batch_idx * output_batch_stride + \n                   h_idx * output_height_stride + \n                   w_idx * output_width_stride)\n    output_offsets = output_base + c_offsets * output_channel_stride\n    \n    tl.store(output_ptr + output_offsets, x, mask=mask)\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        channels_per_group = channels // self.groups\n        \n        if self.groups == 1:\n            return x\n            \n        if x.is_cuda:\n            # Ensure contiguous memory layout\n            x_cont = x.contiguous()\n            output = torch.empty_like(x_cont)\n            \n            # Configure kernel grid\n            total_spatial = batch_size * height * width\n            grid = (total_spatial, triton.cdiv(channels, 128))  # 128 is default BLOCK_SIZE_C\n            \n            # Launch kernel with tensor stride information\n            channel_shuffle_kernel[grid](\n                x_cont, output,\n                channels, self.groups, height, width,\n                x_cont.stride(0), x_cont.stride(1), x_cont.stride(2), x_cont.stride(3),\n                output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            )\n            return output\n        else:\n            # Fallback to PyTorch for CPU\n            x = x.view(batch_size, self.groups, channels_per_group, height, width)\n            x = x.transpose(1, 2).contiguous()\n            return x.view(batch_size, -1, height, width)\n\nclass ShuffleNetUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3):\n        super(ShuffleNetUnit, self).__init__()\n        assert out_channels % 4 == 0\n        mid_channels = out_channels // 4\n        \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        \n        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        \n        self.shuffle = ChannelShuffle(groups)\n        \n        if in_channels == out_channels:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.shuffle(out)\n        out = F.relu(self.bn3(self.conv3(out)))\n        out += self.shortcut(x)\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):\n        super(ModelNew, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)\n        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)\n        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)\n        \n        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn5 = nn.BatchNorm2d(1024)\n        \n        self.fc = nn.Linear(1024, num_classes)\n    \n    def _make_stage(self, in_channels, out_channels, repeats, groups):\n        layers = []\n        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))\n        for _ in range(1, repeats):\n            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        \n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        \n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        \n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "b2038d61-a6c0-4f39-924d-ebc48ec1e245", "generation": 2, "timestamp": 1754592551.359902, "iteration_found": 27, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_26_ShuffleNet", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "max_difference": ["0.034725", "0.050403", "0.048278", "0.048068", "0.046290"], "avg_difference": ["0.005606", "0.009363", "0.009434", "0.009515", "0.009619"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 3.85, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.455, "runtime_ms": 8.46, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 3.85, "speed_up": 0.455, "custom_timing": 8.46}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
{"id": "cefbeb95-b50b-4e28-870e-26521aed66fd", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _attention_kernel(\n    Q, K, V, Out,\n    stride_qb, stride_qh, stride_qm,\n    stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn,\n    stride_ob, stride_oh, stride_om,\n    seq_len, \n    head_dim: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_h = tl.program_id(1)\n    pid_m = tl.program_id(2)\n    \n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, head_dim)\n    \n    # Compute pointers with proper masking\n    q_ptr = Q + pid_b * stride_qb + pid_h * stride_qh + (offs_m[:, None] * stride_qm + offs_d[None, :])\n    k_ptr_base = K + pid_b * stride_kb + pid_h * stride_kh\n    v_ptr_base = V + pid_b * stride_vb + pid_h * stride_vh\n    \n    # Initialize accumulators\n    acc = tl.zeros((BLOCK_M, head_dim), dtype=tl.float32)\n    m_i = tl.full((BLOCK_M,), float('-inf'), dtype=tl.float32)\n    l_i = tl.zeros((BLOCK_M,), dtype=tl.float32)\n    \n    scale = 1.0 / tl.sqrt(tl.constexpr(head_dim * 1.0))\n    \n    # Load Q once per block\n    q_mask = offs_m[:, None] < seq_len\n    q = tl.load(q_ptr, mask=q_mask, other=0.0).to(tl.float32)\n    \n    for block_n in range(0, tl.cdiv(seq_len, BLOCK_N)):\n        offs_n_current = block_n * BLOCK_N + offs_n\n        k_ptr = k_ptr_base + (offs_n_current[None, :] * stride_kn + offs_d[:, None])\n        v_ptr = v_ptr_base + (offs_n_current[:, None] * stride_vn + offs_d[None, :])\n        \n        # Load K and V with proper masking\n        k_mask = offs_n_current[None, :] < seq_len\n        k = tl.load(k_ptr, mask=k_mask, other=0.0).to(tl.float32)\n        \n        v_mask = offs_n_current[:, None] < seq_len\n        v = tl.load(v_ptr, mask=v_mask, other=0.0).to(tl.float32)\n        \n        # Compute attention scores\n        qk = tl.dot(q, k) * scale\n        \n        # Online softmax with numerical stability\n        m_ij = tl.max(qk, axis=1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha_old = tl.exp(m_i - m_i_new)\n        \n        p = tl.exp(qk - m_i_new[:, None])\n        l_ij = tl.sum(p, axis=1)\n        \n        # Update accumulators\n        pv = tl.dot(p, v)\n        acc = acc * alpha_old[:, None] + pv\n        l_i = l_i * alpha_old + l_ij\n        m_i = m_i_new\n    \n    # Normalize and convert back to original dtype\n    acc = acc / l_i[:, None]\n    acc = acc.to(Q.dtype.element_ty)\n    \n    # Store output\n    out_ptr = Out + pid_b * stride_ob + pid_h * stride_oh + (offs_m[:, None] * stride_om + offs_d[None, :])\n    out_mask = offs_m[:, None] < seq_len\n    tl.store(out_ptr, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super(ModelNew, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        L = H * W\n        x_reshaped = x.view(B, C, L).permute(2, 0, 1)\n        \n        q = self.q_proj(x_reshaped)\n        k = self.k_proj(x_reshaped)\n        v = self.v_proj(x_reshaped)\n        \n        q = q.view(L, B, self.num_heads, self.head_dim).permute(1, 2, 0, 3)\n        k = k.view(L, B, self.num_heads, self.head_dim).permute(1, 2, 0, 3)\n        v = v.view(L, B, self.num_heads, self.head_dim).permute(1, 2, 0, 3)\n        \n        out = torch.empty_like(q)\n        \n        BLOCK_M = 128\n        BLOCK_N = 64\n        grid = (B, self.num_heads, triton.cdiv(L, BLOCK_M))\n        \n        _attention_kernel[grid](\n            q, k, v, out,\n            q.stride(0), q.stride(1), q.stride(2),\n            k.stride(0), k.stride(1), k.stride(2),\n            v.stride(0), v.stride(1), v.stride(2),\n            out.stride(0), out.stride(1), out.stride(2),\n            L, self.head_dim,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N\n        )\n        \n        out = out.permute(2, 0, 1, 3).reshape(L, B, C)\n        out = self.out_proj(out)\n        out = self.norm(out + x_reshaped)\n        out = out.permute(1, 2, 0).view(B, C, H, W)\n        \n        return out\n\nembed_dim = 128\nnum_heads = 4\nbatch_size = 2\nnum_channels = embed_dim\nimage_height = 128\nimage_width = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, num_channels, image_height, image_width)]\n\ndef get_init_inputs():\n    return [embed_dim, num_heads]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "80731692-b305-4fe3-abfe-4b1f2c0a62f5", "generation": 4, "timestamp": 1754595966.5246532, "iteration_found": 18, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_31_VisionAttention", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "max_difference": ["0.238592", "0.247373", "0.242118", "0.270904", "0.239257"], "avg_difference": ["0.056394", "0.056733", "0.056066", "0.056213", "0.055577"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.335, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_31_VisionAttention", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "max_difference": ["0.239875", "0.248529", "0.243042", "0.270659", "0.238869"], "avg_difference": ["0.056436", "0.056714", "0.056102", "0.056261", "0.055625"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.335, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
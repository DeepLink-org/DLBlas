{"id": "430fb8db-090a-4404-a2f9-338dee58b5a5", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128}, num_warps=8),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef _forward_kernel(\n    input_ptr, weight_ptr, bias_ptr, add_value_ptr, output_ptr,\n    in_features, out_features, batch_size,\n    stride_input0, stride_input1,\n    stride_weight0, stride_weight1,\n    stride_output0, stride_output1,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    n_offsets = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = n_offsets < out_features\n    \n    input_row_start = pid_batch * stride_input0\n    output_row_start = pid_batch * stride_output0\n    \n    accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        mask_k = k_offsets < in_features\n        \n        input_block = tl.load(\n            input_ptr + input_row_start + k_offsets,\n            mask=mask_k, other=0.0\n        )\n        \n        weight_block_ptr = weight_ptr + n_offsets[:, None] * stride_weight0 + k_offsets[None, :]\n        weight_block = tl.load(\n            weight_block_ptr,\n            mask=mask_n[:, None] & mask_k[None, :], other=0.0\n        )\n        \n        accumulator += tl.sum(weight_block * input_block[None, :], axis=1)\n    \n    bias_block = tl.load(bias_ptr + n_offsets, mask=mask_n, other=0.0)\n    accumulator += bias_block\n    \n    add_value_block = tl.load(add_value_ptr + n_offsets, mask=mask_n, other=0.0)\n    accumulator += add_value_block\n    \n    swished = accumulator * tl.sigmoid(accumulator)\n    tanhed = tl.tanh(swished)\n    inner = tanhed * 0.7978845608028654 * (1 + 0.044715 * (tanhed * tanhed))\n    gelu_output = 0.5 * tanhed * (1 + tl.tanh(inner))\n    hardtanh_output = tl.minimum(tl.maximum(gelu_output, -1.0), 1.0)\n    \n    output_block_ptr = output_ptr + output_row_start + n_offsets\n    tl.store(output_block_ptr, hardtanh_output, mask=mask_n)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape))\n        self.in_features = in_features\n        self.out_features = out_features\n\n    def forward(self, x):\n        output = torch.empty(\n            (x.size(0), self.out_features), \n            device=x.device, dtype=x.dtype\n        )\n        \n        grid = (x.size(0), triton.cdiv(self.out_features, 128))\n        \n        _forward_kernel[grid](\n            x, self.matmul.weight, self.matmul.bias, self.add_value, output,\n            self.in_features, self.out_features, x.size(0),\n            x.stride(0), x.stride(1),\n            self.matmul.weight.stride(0), self.matmul.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_N=128, BLOCK_SIZE_K=64\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "ccb7a9d3-2d42-4435-b130-7193e31b53f9", "generation": 1, "timestamp": 1754585891.8759465, "iteration_found": 1, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_95_Matmul_Add_Swish_Tanh_GELU_Hardtanh", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmprfcn2f93.py\", line 90, in forward\n    _forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 135, in _bench\n    raise ValueError(f\"Conflicting meta-parameters: {', '.join(conflicts)}.\"\nValueError: Conflicting meta-parameters: BLOCK_SIZE_N, BLOCK_SIZE_K. Make sure that you don't re-define auto-tuned symbols.\n", "runtime_error_name": "builtins.ValueError", "reference_timing_ms": 0.0872, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0782, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0872, "speed_up": 1.115, "custom_timing": 0.0782}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0782\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.115, 'custom_timing': 0.0782}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0782, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.115, 'custom_timing': 0.0782}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, adds a value, applies Swish, Tanh, GELU, and Hardtanh activation functions.\n    \"\"\"\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x + self.add_value\n        x = torch.sigmoid(x) * x # Swish\n        x = torch.tanh(x)\n        x = torch.nn.functional.gelu(x) # GELU\n        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0782), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.115, 'custom_timing': 0.0782})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, adds a value, applies Swish, Tanh, GELU, and Hardtanh activation functions.\n    \"\"\"\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x + self.add_value\n        x = torch.sigmoid(x) * x # Swish\n        x = torch.tanh(x)\n        x = torch.nn.functional.gelu(x) # GELU\n        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear transformation (matmul + bias) -> but note: nn.Linear includes a bias, but we also have an extra add_value\n  2. Then applies: Swish (x * sigmoid(x)), Tanh, GELU, and Hardtanh.\n\nHowever, the current implementation is entirely in PyTorch and runs on the GPU. The goal is to rewrite the forward pass using Triton to speed it up.\n\nBut note: The model includes parameters (the linear layer and the add_value). We cannot change the parameters.\n\nThe current program structure:\n  - We have a class ModelNew that inherits from nn.Module.\n  - The forward pass is defined as above.\n\nWe are allowed to change only the Triton kernel source within the EVOLVE-BLOCK. However, note that the current code does not use Triton at all.\n\nTherefore, we must rewrite the forward function using Triton kernels.\n\nSteps to consider:\n\n1. The linear layer: We can fuse the linear transformation (matmul) with the subsequent element-wise operations (add, Swish, Tanh, GELU, Hardtanh) into one or more Triton kernels.\n\n2. However, note that the linear layer has a weight and a bias (from nn.Linear) and then we add another parameter (self.add_value). So the operations are:\n   x = input @ weight.T + bias   [from nn.Linear]\n   x = x + self.add_value\n\n   Then the activations.\n\n3. We can break the forward pass into two parts:\n   a) The matrix multiplication: This is the most expensive part. We can write a Triton kernel for the matmul and then fuse the bias addition and the add_value, and then the activations.\n\n4. Alternatively, we can write one kernel that does:\n   - Loads a block of input, the weight matrix, the bias, and the add_value.\n   - Computes the matrix multiplication for that block.\n   - Adds the bias (broadcasted over the batch) and then the add_value (also broadcasted? note: add_value_shape is (out_features,)).\n   - Then applies the sequence of activations.\n\n5. However, note that the activations are applied element-wise, so they can be fused without much overhead.\n\n6. The challenge: The input has shape (batch_size, in_features) and the weight has shape (out_features, in_features). The output is (batch_size, out_features).\n\n7. We can write a Triton kernel that processes the input in blocks. We can use the following approach:\n\n   - Each program will compute a block of output. We can have a 2D grid: \n        pid_batch = program_id(0)  # over batch\n        pid_feature = program_id(1) # over out_features\n\n   But note: we are adding two vectors (bias and add_value) that are of length out_features. So we need to broadcast them over the batch.\n\n   Alternatively, we can use a 1D grid and have each block compute a row of the output (a row being of length out_features). But note: the batch_size might be large and out_features might be 512, which is acceptable for a block.\n\n   However, we want to parallelize over both dimensions? \n\n   Considering the problem size: batch_size=128, in_features=1024, out_features=512.\n\n   We can have:\n     grid = (batch_size, )\n\n   and each block computes a row (of length out_features) of the output.\n\n   Steps for one row:\n\n     - Load the input row (of length in_features) for the current batch.\n     - Multiply by the weight matrix (which is of shape (out_features, in_features)). We can break the out_features into chunks? Actually, we can compute the entire row in one block? \n\n     But note: in_features is 1024, and out_features is 512. We can have the block size be 512? But 512 threads is acceptable (if we use multiple warps). \n\n   Alternatively, we can have two levels of blocking:\n\n     Blocking over the input features (to compute the dot product) and then over the output features? \n\n   Actually, we can do:\n\n     For a given batch element, we want to compute a vector of length out_features. We can have:\n\n        pid = program_id(0)  # over batch\n        and then we break the out_features into groups? \n\n   However, note that the bias and add_value are the same for the entire row. So we can compute the entire row in one block? \n\n   But 512 is the length of the row and we can have 512 threads. We can use:\n\n        BLOCK_SIZE = 512   # which is out_features\n\n   But wait: the weight matrix has shape (out_features, in_features). So each output element requires a dot product of length in_features (1024). \n\n   Therefore, we must reduce over the in_features dimension.\n\n   We can use:\n\n        BLOCK_SIZE_K = 32   # for reducing the inner dimension\n\n   We'll break the in_features dimension into chunks of 32, and each thread will compute a dot product of 32 elements? \n\n   Actually, we can use a 2D block:\n\n        BLOCK_SIZE_K = 32   # for the inner dimension (K)\n        BLOCK_SIZE_N = 32   # for the output dimension (N)\n\n   But note: we are computing a row of the output (for one batch). We can think of the output as a 1D vector of length out_features (N=512). We can break it into 16 blocks of 32? \n\n   Alternatively, we can use the following:\n\n        pid_batch = program_id(0)   # batch index\n        pid_n = program_id(1)        # which chunk of the output features? \n\n        and then we compute a block of BLOCK_SIZE_N output features for one batch.\n\n   This way, we have:\n\n        grid = (batch_size, triton.cdiv(out_features, BLOCK_SIZE_N))\n\n   Then in the kernel:\n\n        We load a block of input: of size (1, in_features) for the current batch? Actually, we need the entire input row? \n\n        But note: we are only computing a block of the output features. So we only need the weights for the block of output features and the entire input row? \n\n        We can do:\n\n            accumulator = 0\n            for k in range(0, in_features, BLOCK_SIZE_K):\n                # Load a block of input [BLOCK_SIZE_K]\n                # Load a block of weights [BLOCK_SIZE_N, BLOCK_SIZE_K] for the current output block\n                # Then do: accumulator += dot(weights_block, input_block)\n\n        Then add bias and add_value for the current output block.\n\n        Then apply the activations.\n\n   However, note: the bias and add_value are vectors of length out_features. We only need the slice for the current block.\n\n   This approach is standard for matrix multiplication.\n\n8. But note: the activations are element-wise and can be applied at the end.\n\n9. We must also consider the data types: the input is float32? The model uses randn which is float32.\n\n10. We can write the kernel for the entire forward pass? \n\n    Steps in the kernel:\n\n        input_ptr: pointer to the input tensor (batch_size, in_features)\n        weight_ptr: pointer to the weight tensor (out_features, in_features)\n        bias_ptr: pointer to the bias tensor (out_features)   [from the linear layer]\n        add_value_ptr: pointer to the add_value tensor (out_features)\n\n        output_ptr: pointer to the output tensor (batch_size, out_features)\n\n        We'll have two block dimensions: BLOCK_SIZE_K (for K dimension) and BLOCK_SIZE_N (for N dimension).\n\n        pid_batch = tl.program_id(0)\n        pid_n = tl.program_id(1)   # which block of the output features\n\n        # offsets for the current block in the output feature dimension\n        n_offsets = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        mask_n = n_offsets < out_features   # because out_features might not be divisible\n\n        # The current input row: for batch index pid_batch\n        input_row_start = pid_batch * in_features\n        # We'll iterate over the K dimension (in_features) in blocks of BLOCK_SIZE_K\n\n        accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n\n        for k in range(0, in_features, BLOCK_SIZE_K):\n            k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n            mask_k = k_offsets < in_features\n\n            # Load a block of input\n            input_block = tl.load(input_ptr + input_row_start + k_offsets, mask=mask_k, other=0.0)\n\n            # Load a block of weights: for the current n block and current k block\n            weight_block_ptr = weight_ptr + n_offsets[:, None] * in_features + k_offsets[None, :]\n            weight_block = tl.load(weight_block_ptr, mask=mask_n[:, None] & mask_k[None, :], other=0.0)\n\n            # Compute the dot product for this block\n            accumulator += tl.sum(weight_block * input_block[None, :], axis=1)\n\n        # Now, we have the accumulator for the linear part (without bias) for the current block.\n\n        # Load the bias for the current n block\n        bias_block = tl.load(bias_ptr + n_offsets, mask=mask_n, other=0.0)\n        accumulator += bias_block\n\n        # Load the add_value for the current n block\n        add_value_block = tl.load(add_value_ptr + n_offsets, mask=mask_n, other=0.0)\n        accumulator += add_value_block\n\n        # Now apply the activations:\n\n        # Swish: x * sigmoid(x)\n        swished = accumulator * tl.sigmoid(accumulator)\n\n        # Tanh\n        tanhed = tl.tanh(swished)\n\n        # GELU: we can use tl.gelu? Note: Triton has tl.gelu for fast approximation? But the original uses torch.nn.functional.gelu which by default is tanh approximation? \n        # We can use: gelu = 0.5 * accumulator * (1 + tl.tanh(tl.sqrt(2/math.pi) * (accumulator + 0.044715 * accumulator**3)))\n        # However, Triton has an intrinsic: tl.gelu. But note: the tl.gelu in Triton might be the exact version? We should check the documentation.\n\n        # But note: the original uses torch.nn.functional.gelu, which by default is the tanh approximation. We can use the same formula for consistency.\n\n        # However, the problem says: we must match the reference. So we should use the same GELU as the reference.\n\n        # Since the reference uses torch.nn.functional.gelu, which is the tanh approximation, we can implement that.\n\n        # But note: the Triton kernel does not have a built-in for the tanh approximation? We have to write it.\n\n        # Alternatively, we can use the fast gelu approximation that Triton provides? But we must match.\n\n        # The problem says: we must be numerically correct within float32 tolerance. The Triton gelu might be different.\n\n        # Let's use the same as the reference: \n        #   gelu(x) = x * 0.5 * (1.0 + torch.tanh(x * 0.7978845608028654 * (1.0 + 0.044715 * x * x)))\n\n        # However, note: we are applying GELU after tanh? Actually, the order is:\n        #   x -> Swish -> Tanh -> GELU -> Hardtanh\n\n        # So we are applying GELU on the tanh output.\n\n        # We'll implement the GELU with the tanh approximation.\n\n        # But note: we are in the middle of the kernel and we want speed. We might consider using the built-in tl.gelu if it is the same approximation? \n\n        # According to Triton documentation, tl.gelu uses the same approximation? I don't know. To be safe, we can write the exact formula.\n\n        # However, the problem says we are allowed to replace staged softmax with online/FlashAttention variants, but GELU is not that. \n\n        # We'll write the formula.\n\n        # But note: the input to GELU is the output of tanh, which is in the range [-1,1]. So the GELU will be applied to numbers in that range.\n\n        # We can write:\n\n            # gelu(x) = 0.5 * x * (1 + tanh( sqrt(2/pi) * (x + 0.044715 * x**3) ))\n\n        # However, the reference implementation in PyTorch uses: \n            # x * 0.5 * (1.0 + torch.tanh(x * 0.7978845608028654 * (1.0 + 0.044715 * x * x)))\n\n        # Let me check: \n            # sqrt(2/pi) = 0.7978845608028654\n            # and then they factor: x * (1 + 0.044715 * x*x) = x + 0.044715 * x**3\n\n        # So it is the same.\n\n        # Therefore, we can compute:\n\n            # x = tanhed   (from the previous step)\n            # inner = x * 0.7978845608028654 * (1 + 0.044715 * (x * x))\n            # gelu_output = 0.5 * x * (1 + tl.tanh(inner))\n\n        # But note: the constant 0.7978845608028654 and 0.044715 are fixed.\n\n        # Alternatively, we can precompute: \n            # inner = x * (0.7978845608028654 + 0.035677408136 * (x*x))\n        # because 0.7978845608028654 * 0.044715 = 0.035677408136 (approx)\n\n        # But let's do it step by step for clarity.\n\n        # However, note: we are applying GELU to the tanhed output, which is in [-1,1]. So the cubic term is small.\n\n        # Then Hardtanh: clamp between -1 and 1.\n\n        # But wait: the Hardtanh is applied to the GELU output? Yes.\n\n        # And note: the Hardtanh in the reference is: torch.nn.functional.hardtanh(x, min_val=-1, max_val=1)\n\n        # So we can do:\n\n            # hardtanh = tl.minimum(tl.maximum(gelu_output, -1.0), 1.0)\n\n        # Alternatively, we can use tl.where, but min and max are fine.\n\n    However, note: we have already applied Tanh and then GELU, and then Hardtanh. The Tanh already bounds the value to [-1,1]. Then GELU: \n        For x in [-1,1]:\n          GELU(x) = 0.5 * x * (1 + tanh(...)) \n        The tanh(...) is also in [-1,1] so the whole term is in [-1,1]? Actually, 0.5*x*(1+something in [-1,1]) -> then 0.5*x*(0 to 2) -> so the result is in [-1,1]. Then Hardtanh is redundant? \n\n    But we must follow the reference.\n\n11. We must also note: the kernel must handle boundaries (using masks) for the input and weight.\n\n12. We are going to write the kernel and then replace the forward function to use this kernel.\n\nBut note: the current model has parameters (weight, bias, add_value). We need to pass these as arguments to the kernel.\n\nWe must also initialize the output tensor.\n\nWe are going to change the forward function to:\n\n    def forward(self, x):\n        # x: (batch_size, in_features)\n        # Instead of using the linear layer, we call our kernel\n        output = torch.empty((x.size(0), self.matmul.out_features), device=x.device, dtype=x.dtype)\n        # We need the weight and bias from self.matmul, and self.add_value\n        # Call the triton kernel\n        # grid = (batch_size, number of blocks in the output feature dimension)\n        grid = (x.size(0), triton.cdiv(self.matmul.out_features, BLOCK_SIZE_N))\n        # We need to pass: input, weight, bias, add_value, output, and constants: in_features, out_features, etc.\n\n        _forward_kernel[grid](x, self.matmul.weight, self.matmul.bias, self.add_value, output, ...)\n\n        return output\n\n13. However, note: the kernel must be defined. We'll define it with the @triton.jit decorator.\n\n14. We must choose BLOCK_SIZE_N and BLOCK_SIZE_K. We can make them tunable? But for now, let's set them to 128 and 64? We can use autotune.\n\n15. But note: the problem says we can change the Triton kernel source. We are allowed to use autotune.\n\n16. We are going to write the kernel and the forward function inside the class? Actually, we cannot put the kernel inside the class because Triton kernels are static.\n\n    We'll define the kernel outside the class.\n\n17. We must be cautious: the kernel must be compiled for the specific device. We can define it at the top level.\n\n18. The current EVOLVE-BLOCK must contain the entire code? The problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker\".\n\n    So we must write the entire code block that includes the class and the kernel.\n\n19. We are going to restructure the code:\n\n    - We'll define the Triton kernel (with autotune) at the top of the block.\n    - Then define the ModelNew class, and in the forward function we call the kernel.\n\n20. We must also note: the original model had the linear layer as a module (self.matmul). We are going to use its weight and bias in the kernel.\n\n21. We must also note: the add_value is a parameter of the model.\n\n22. We'll write:\n\n    @triton.autotune(...)\n    @triton.jit\n    def _forward_kernel( ... )\n\n    class ModelNew(nn.Module):\n        ... same __init__ ...\n\n        def forward(self, x):\n            ... call the kernel ...\n\n23. But note: the kernel must handle the entire computation: matmul, add bias, add add_value, and then the four activations.\n\n24. We'll write the kernel in detail.\n\nHowever, note: the problem says we are allowed to change only the Triton kernel source within the block? But actually, we are replacing the entire forward pass with a kernel call. So we are changing the forward function.\n\nBut the constraints say: \"NEVER change @triton.jit function signatures or parameter names\" -> we are defining a new one.\n\nAlso, note: the grid configuration: we are allowed to change it? The constraints say: \"NEVER modify grid configuration\", but that is for the existing Triton kernel? Here we are creating a new one.\n\nWe are allowed to do so because the previous code did not have a Triton kernel.\n\nBut note: the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" -> but we are also modifying the forward function. \n\nHowever, the original code did not have a Triton kernel. We are introducing one. And the forward function is within the block? \n\nYes, the entire code we are to rewrite is inside the EVOLVE-BLOCK.\n\nTherefore, we can change the forward function to use the kernel.\n\nBut note: the original code had the linear layer as a module. We are going to keep the module because it holds the parameters. We are just replacing the computation.\n\n25. Steps for the kernel:\n\n    We'll define:\n\n        input_ptr, weight_ptr, bias_ptr, add_value_ptr, output_ptr\n        in_features, out_features,   (as integers, but we can pass them as constants or as arguments? We can pass as arguments and then use tl.constexpr)\n\n        BLOCK_SIZE_K: tl.constexpr, \n        BLOCK_SIZE_N: tl.constexpr\n\n    The kernel:\n\n        pid_batch = tl.program_id(0)\n        pid_n = tl.program_id(1)\n\n        # We are going to compute a block of output of size (1, BLOCK_SIZE_N) for the batch index pid_batch.\n\n        # The input row: \n        input_row_start = pid_batch * in_features\n        # The output row: \n        output_row_start = pid_batch * out_features\n\n        # The current block of output features: [n_offsets] of length BLOCK_SIZE_N\n        n_offsets = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        mask_n = n_offsets < out_features\n\n        accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n\n        # Loop over K (in_features) in blocks of BLOCK_SIZE_K\n        for k in range(0, in_features, BLOCK_SIZE_K):\n            k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n            mask_k = k_offsets < in_features\n\n            # Load input block: [BLOCK_SIZE_K]\n            input_block = tl.load(input_ptr + input_row_start + k_offsets, mask=mask_k, other=0.0)\n\n            # Load weight block: [BLOCK_SIZE_N, BLOCK_SIZE_K]\n            # The weight matrix is stored in row-major: each row is an output feature. So the weight for the current n block is at:\n            #   weight_ptr + n_offsets[:, None] * in_features + k_offsets[None, :]\n            weight_block_ptr = weight_ptr + (n_offsets[:, None] * in_features + k_offsets[None, :])\n            weight_block = tl.load(weight_block_ptr, mask=mask_n[:, None] & mask_k[None, :], other=0.0)\n\n            # Compute the product and accumulate\n            product = tl.dot(weight_block, input_block)   # This is a vector of length BLOCK_SIZE_N?\n            # But note: weight_block is [BLOCK_SIZE_N, BLOCK_SIZE_K] and input_block is [BLOCK_SIZE_K]. The dot product along the K dimension?\n            # Actually, we can do: \n            #   product = tl.sum(weight_block * input_block[None, :], axis=1)\n            # But tl.dot is more efficient? However, tl.dot expects two 2D arrays? \n\n            # Alternatively, we can use tl.sum with the product.\n\n            # We'll do the element-wise multiplication and then sum along axis 1.\n            # But note: we can use tl.dot only if the arrays are 2D? \n\n            # Let me check: we have weight_block of shape (BLOCK_SIZE_N, BLOCK_SIZE_K) and input_block of shape (BLOCK_SIZE_K,). We can reshape input_block to (BLOCK_SIZE_K, 1) and then use tl.dot? \n\n            # Actually, we can do:\n\n            #   product = tl.sum(weight_block * input_block, axis=1)\n\n            # This is efficient? \n\n            # We'll do it that way.\n\n            product = tl.sum(weight_block * input_block[None, :], axis=1)\n            accumulator += product\n\n        # Now add bias and add_value\n        bias_block = tl.load(bias_ptr + n_offsets, mask=mask_n, other=0.0)\n        accumulator += bias_block\n\n        add_value_block = tl.load(add_value_ptr + n_offsets, mask=mask_n, other=0.0)\n        accumulator += add_value_block\n\n        # Now apply Swish: accumulator * sigmoid(accumulator)\n        swished = accumulator * tl.sigmoid(accumulator)\n\n        # Tanh\n        tanhed = tl.tanh(swished)\n\n        # GELU: using tanh approximation\n        #   gelu(x) = 0.5 * x * (1 + tanh( sqrt(2/pi) * (x + 0.044715 * x**3) ))\n        #   where x is tanhed (from the previous step)\n        # But note: we are applying GELU to the tanhed output? Yes.\n        x = tanhed\n        # Compute the inner part: sqrt(2/pi) = 0.7978845608028654\n        inner = x * 0.7978845608028654 * (1 + 0.044715 * (x * x))\n        tanh_inner = tl.tanh(inner)\n        gelu_output = 0.5 * x * (1 + tanh_inner)\n\n        # Hardtanh: clamp between -1 and 1\n        hardtanh_output = tl.minimum(tl.maximum(gelu_output, -1.0), 1.0)\n\n        # Now store the result\n        output_block_ptr = output_ptr + output_row_start + n_offsets\n        tl.store(output_block_ptr, hardtanh_output, mask=mask_n)\n\n26. But note: the input and output are in float32? The original was float32. The kernel uses float32.\n\n27. We must also note: the kernel must be compiled for the specific in_features and out_features? We pass them as arguments.\n\n28. We'll define the kernel with:\n\n        input_ptr: pointer to input (float32)\n        weight_ptr: pointer to weight (float32)\n        bias_ptr: pointer to bias (float32)\n        add_value_ptr: pointer to add_value (float32)\n        output_ptr: pointer to output (float32)\n        in_features: int\n        out_features: int\n        BLOCK_SIZE_K: tl.constexpr\n        BLOCK_SIZE_N: tl.constexpr\n\n29. In the forward function:\n\n        batch_size, in_feats = x.shape\n        out_feats = self.matmul.out_features\n\n        # Make sure the tensors are contiguous\n        x = x.contiguous()\n        weight = self.matmul.weight.contiguous()\n        bias = self.matmul.bias.contiguous()\n        add_value = self.add_value.contiguous()\n\n        output = torch.empty((batch_size, out_feats), device=x.device, dtype=x.dtype)\n\n        # Grid: (batch_size, number of blocks in the output feature dimension)\n        grid = (batch_size, triton.cdiv(out_feats, BLOCK_SIZE_N))\n\n        # We need to choose BLOCK_SIZE_N and BLOCK_SIZE_K. We can set them to 128 and 64? But we can autotune.\n\n        # We'll define the kernel with autotune. We'll provide several configs.\n\n30. We'll use autotune:\n\n    configs = [\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128}, num_warps=8),\n    ]\n\n    But note: we have two block sizes: N and K. And we also have num_warps and num_stages? We can set num_warps to 4 or 8.\n\n    We'll set num_warps based on the block size? Or let autotune decide.\n\n    We'll define:\n\n        key = ['in_features', 'out_features']   # but these are fixed? Actually, they are fixed per model. So we might not need to autotune for different sizes? \n\n        But the problem says the model is for fixed in_features=1024, out_features=512.\n\n    So we can fix the block sizes? But we don't know which is best. We'll let autotune run for the fixed sizes.\n\n31. However, note: the kernel is long and might use a lot of registers? We might need to set num_stages.\n\n    We'll start without num_stages and let Triton choose? Or we can set num_stages=3? \n\n    We can leave it to the autotuner to try different num_warps and num_stages? \n\n    We can define:\n\n        triton.Config(dict(BLOCK_SIZE_N=128, BLOCK_SIZE_K=64), num_warps=4, num_stages=3),\n        ... \n\n    But note: the problem says we are on H100 (SM8.9). We can use up to 8 warps (256 threads).\n\n32. We'll write the kernel and hope it compiles.\n\n33. We must also note: the kernel uses tl.sigmoid, tl.tanh, and tl.tanh again (for GELU). These are supported.\n\n34. We'll now write the code.\n\nBut note: the problem says we must maintain the same inputs and outputs.\n\nWe are replacing the entire forward pass with a Triton kernel. The original used:\n\n    x = self.matmul(x)\n    ... \n\nWe are now using the parameters of self.matmul and self.add_value and computing with the kernel.\n\n35. We must be cautious: the original linear layer was with bias? Yes, because we are using nn.Linear.\n\n36. We'll now write the entire block.\n\nNote: We are going to replace the current ModelNew class and the functions below.\n\nWe must keep the batch_size, in_features, out_features, add_value_shape, get_inputs, get_init_inputs.\n\nBut note: the ModelNew class now uses the kernel in the forward pass.\n\n37. We'll import triton and triton.language as tl.\n\n38. We are going to write:\n\n    import math\n    import torch\n    import torch.nn as nn\n    import triton\n    import triton.language as tl\n\n    # Then the kernel definition with autotune\n\n    # Then the ModelNew class\n\n    # Then the same batch_size, etc. and get_inputs, get_init_inputs.\n\n39. We must note: the kernel might be large and we are limited by the context. We'll try to write it concisely.\n\n40. We are going to write the kernel and hope for the best.\n\nGiven the complexity, we might do:\n\n    @triton.autotune(...)\n    @triton.jit\n    def _forward_kernel(...):\n        ... \n\nBut note: the kernel is long. We might break it? But we cannot.\n\n41. We'll write the kernel as described.\n\n42. We must test the boundaries: in_features and out_features might not be divisible by the block sizes.\n\n43. We are using masks in the loads.\n\n44. We are not using shared memory? We are doing a straightforward block matmul.\n\n45. We are not using tensor cores? We are using FP32. But the problem says the GPU has FP16 tensor cores. However, our data is FP32. We cannot use tensor cores for FP32? \n\n    Actually, H100 supports FP32 on tensor cores? I think not. Tensor cores are for lower precision. So we are using the CUDA cores.\n\n46. We are not optimizing for tensor cores because the data type is FP32.\n\n47. We hope that the fused activations and the block matmul will be faster than the separate PyTorch operations.\n\n48. We'll now write the code.\n\nBut note: the problem says to make small iterative steps. This is a big step? But the previous code did not use Triton at all.\n\n49. We'll do it.\n\nHowever, note: the problem says \"Small iterative steps: Modify \u226420 lines per revision\". We are modifying more than 20 lines? \n\nBut the previous code did not have a Triton kernel. We are adding one. The entire block is being replaced.\n\nWe are allowed because we are starting from the reference.\n\nWe'll write the code.\n\nLet me write the kernel and the model.\n\nBut note: the kernel might have errors. We are going to try to compile it.\n\nWe are going to output the entire block.\n\nIMPORTANT: We must not change the class name or the function names (get_inputs, get_init_inputs). We are only changing the implementation of the forward pass and adding the kernel.\n\nWe'll write:\n</think>\nTo improve performance, we rewrite the forward pass using a Triton kernel that fuses the matrix multiplication and all subsequent operations. This reduces overhead and leverages GPU parallelism more efficiently. The key optimizations include:\n\n1. Fused kernel implementation combining:\n   - Matrix multiplication\n   - Bias and value addition\n   - Swish, Tanh, GELU, and Hardtanh activations\n2. Optimized memory access patterns\n3. Configurable block sizes with autotuning\n4. Boundary checks to ensure correctness\n\nHere's the complete implementation:\n\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128}, num_warps=8),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef _forward_kernel(\n    input_ptr, weight_ptr, bias_ptr, add_value_ptr, output_ptr,\n    in_features, out_features, batch_size,\n    stride_input0, stride_input1,\n    stride_weight0, stride_weight1,\n    stride_output0, stride_output1,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    n_offsets = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = n_offsets < out_features\n    \n    input_row_start = pid_batch * stride_input0\n    output_row_start = pid_batch * stride_output0\n    \n    accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        mask_k = k_offsets < in_features\n        \n        input_block = tl.load(\n            input_ptr + input_row_start + k_offsets,\n            mask=mask_k, other=0.0\n        )\n        \n        weight_block_ptr = weight_ptr + n_offsets[:, None] * stride_weight0 + k_offsets[None, :]\n        weight_block = tl.load(\n            weight_block_ptr,\n            mask=mask_n[:, None] & mask_k[None, :], other=0.0\n        )\n        \n        accumulator += tl.sum(weight_block * input_block[None, :], axis=1)\n    \n    bias_block = tl.load(bias_ptr + n_offsets, mask=mask_n, other=0.0)\n    accumulator += bias_block\n    \n    add_value_block = tl.load(add_value_ptr + n_offsets, mask=mask_n, other=0.0)\n    accumulator += add_value_block\n    \n    swished = accumulator * tl.sigmoid(accumulator)\n    tanhed = tl.tanh(swished)\n    inner = tanhed * 0.7978845608028654 * (1 + 0.044715 * (tanhed * tanhed))\n    gelu_output = 0.5 * tanhed * (1 + tl.tanh(inner))\n    hardtanh_output = tl.minimum(tl.maximum(gelu_output, -1.0), 1.0)\n    \n    output_block_ptr = output_ptr + output_row_start + n_offsets\n    tl.store(output_block_ptr, hardtanh_output, mask=mask_n)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape))\n        self.in_features = in_features\n        self.out_features = out_features\n\n    def forward(self, x):\n        output = torch.empty(\n            (x.size(0), self.out_features), \n            device=x.device, dtype=x.dtype\n        )\n        \n        grid = (x.size(0), triton.cdiv(self.out_features, 128))\n        \n        _forward_kernel[grid](\n            x, self.matmul.weight, self.matmul.bias, self.add_value, output,\n            self.in_features, self.out_features, x.size(0),\n            x.stride(0), x.stride(1),\n            self.matmul.weight.stride(0), self.matmul.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_N=128, BLOCK_SIZE_K=64\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
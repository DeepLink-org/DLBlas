{"id": "c1341ebd-2e97-4d81-a412-9711dc6be01d", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_forward_kernel(\n    x_ptr, weight_ptr, bias_ptr, add_value_ptr, output_ptr,\n    batch_size, in_features, out_features,\n    BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_FEAT: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_feat = tl.program_id(1)\n    \n    batch_offsets = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feat_offsets = pid_feat * BLOCK_SIZE_FEAT + tl.arange(0, BLOCK_SIZE_FEAT)\n    k_offsets = tl.arange(0, BLOCK_SIZE_K)\n    \n    batch_mask = batch_offsets < batch_size\n    feat_mask = feat_offsets < out_features\n    \n    accumulator = tl.zeros((BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_mask = k_offsets < (in_features - k)\n        input_mask = batch_mask[:, None] & k_mask[None, :]\n        weight_mask = feat_mask[:, None] & k_mask[None, :]\n        \n        x_offset = batch_offsets[:, None] * in_features + (k + k_offsets[None, :])\n        w_offset = feat_offsets[:, None] * in_features + (k + k_offsets[None, :])\n        \n        x_tile = tl.load(x_ptr + x_offset, mask=input_mask, other=0.0)\n        w_tile = tl.load(weight_ptr + w_offset, mask=weight_mask, other=0.0)\n        \n        accumulator += tl.dot(x_tile, w_tile, trans_b=True)\n    \n    bias_vals = tl.load(bias_ptr + feat_offsets, mask=feat_mask, other=0.0)\n    add_vals = tl.load(add_value_ptr + feat_offsets, mask=feat_mask, other=0.0)\n    \n    result = accumulator + bias_vals[None, :] + add_vals[None, :]\n    \n    # Fused activations\n    swish = tl.sigmoid(result) * result\n    tanh = tl.tanh(swish)\n    gelu = tanh * 0.5 * (1.0 + tl.erf(tanh * 0.7071067811865475))\n    hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)\n    \n    output_offset = batch_offsets[:, None] * out_features + feat_offsets[None, :]\n    output_mask = batch_mask[:, None] & feat_mask[None, :]\n    tl.store(output_ptr + output_offset, hardtanh, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape))\n    \n    def forward(self, x):\n        weight = self.matmul.weight\n        bias = self.matmul.bias\n        add_value = self.add_value\n        \n        batch_size, in_dim = x.shape\n        out_dim = weight.shape[0]\n        output = torch.empty((batch_size, out_dim), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_BATCH = 32\n        BLOCK_SIZE_FEAT = 32\n        BLOCK_SIZE_K = 64\n        \n        grid = (\n            triton.cdiv(batch_size, BLOCK_SIZE_BATCH),\n            triton.cdiv(out_dim, BLOCK_SIZE_FEAT)\n        )\n        \n        _fused_forward_kernel[grid](\n            x, weight, bias, add_value, output,\n            batch_size, in_dim, out_dim,\n            BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT, BLOCK_SIZE_K\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "ccb7a9d3-2d42-4435-b130-7193e31b53f9", "generation": 1, "timestamp": 1754585839.0206857, "iteration_found": 2, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_95_Matmul_Add_Swish_Tanh_GELU_Hardtanh", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp9gv9r10p.py\", line 78, in forward\n    _fused_forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 0.0872, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0782, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0872, "speed_up": 1.115, "custom_timing": 0.0782}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0782\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.115, 'custom_timing': 0.0782}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0782, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.115, 'custom_timing': 0.0782}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, adds a value, applies Swish, Tanh, GELU, and Hardtanh activation functions.\n    \"\"\"\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x + self.add_value\n        x = torch.sigmoid(x) * x # Swish\n        x = torch.tanh(x)\n        x = torch.nn.functional.gelu(x) # GELU\n        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0782), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.115, 'custom_timing': 0.0782})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, adds a value, applies Swish, Tanh, GELU, and Hardtanh activation functions.\n    \"\"\"\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x + self.add_value\n        x = torch.sigmoid(x) * x # Swish\n        x = torch.tanh(x)\n        x = torch.nn.functional.gelu(x) # GELU\n        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Matrix multiplication (via nn.Linear)\n  2. Adds a parameter (broadcasted)\n  3. Applies Swish (sigmoid * x)\n  4. Tanh\n  5. GELU\n  6. Hardtanh (clamp between -1 and 1)\n\nThe goal is to rewrite this model using Triton to speed up the operations.\n\nHowever, note that the current implementation is entirely in PyTorch and runs on the GPU. The operations are:\n  - Linear (matmul + bias) -> but note: the nn.Linear has a weight and bias, but we are also adding an extra parameter (self.add_value)\n  - Then a sequence of pointwise operations.\n\nWe cannot replace the entire model with a single Triton kernel because the model has multiple operations. But note that the problem says we are allowed to replace the reference_code with custom_code (Triton). However, the example provided in the context was a single function. \n\nLooking at the structure of the program, we are required to output a class `ModelNew` and also the batch_size, in_features, etc. and the functions `get_inputs` and `get_init_inputs`.\n\nWe are allowed to change only the Triton kernel source? But note the current program does not have any Triton code. So we are to introduce Triton to replace some parts.\n\nBut note: the problem says \"Generate functionally equivalent but faster Triton implementations (custom_code)\".\n\nWe are allowed to change the forward method? The constraints say:\n  \u274c NEVER change @triton.jit function signatures or parameter names\n  \u274c NEVER modify grid configuration, output tensor shapes, or PID logic\n\nBut we are not given any Triton kernel. So we have to design one.\n\nHow to approach:\n  The current model does:\n      x = self.matmul(x)   -> [batch, out_features]\n      x = x + self.add_value  [broadcasted to [batch, out_features]]\n      Then pointwise operations.\n\nThe matmul is the most expensive part. We can write a Triton kernel for the matmul? But note that the nn.Linear also has a bias. And then we are adding an extra parameter.\n\nAlternatively, we can fuse the entire operation (matmul, add, then the activation functions) into one kernel? That would be a big kernel.\n\nHowever, note that the activation functions are complex: Swish, Tanh, GELU, Hardtanh. We can write a kernel that does:\n  - Loads the input matrix in tiles and does the matmul, then adds the two bias terms (the linear bias and the extra add_value) and then applies the activations.\n\nBut note: the linear layer has weights and bias. We have to load the weights. The input is [batch, in_features] and the weights are [out_features, in_features]. We are doing: x @ weight.t() + bias.\n\nSo the entire operation is:\n  x = input @ weight.t() + bias   (from linear)\n  x = x + self.add_value\n  then the activations.\n\nWe can write a kernel that does:\n  For each row in the batch and each column (output feature), we compute the dot product (with tiling) and then add the two biases and then apply the activations.\n\nBut note: the activations are applied element-wise.\n\nAlternatively, we can break the kernel into two parts? But we want to fuse as much as possible.\n\nHowever, note that the problem says: \"Small iterative steps\". We are allowed to do multiple kernels? But that might not be efficient.\n\nGiven the complexity, let's consider fusing the entire operation.\n\nSteps for the kernel:\n\n  We are going to write a kernel that computes one output element per thread? Or per block? We have to tile.\n\n  We can do:\n    - Let the kernel have a grid of (batch_size, out_features). But note: batch_size=128, out_features=512 -> 65536 elements. That might be too many threads? Actually, we can have each program ID compute one row of the output? But then we have to load the entire row of the input and the entire column of the weights? That would be inefficient.\n\n  Alternatively, we can tile the batch and the output features. We can do:\n\n    BLOCK_SIZE_BATCH: 32, BLOCK_SIZE_FEATURES: 32\n\n    Then we have grid of (ceil(batch_size / BLOCK_SIZE_BATCH), ceil(out_features / BLOCK_SIZE_FEATURES))\n\n    Each block computes a tile of [BLOCK_SIZE_BATCH, BLOCK_SIZE_FEATURES] of the output.\n\n    Steps:\n      1. Load a block of input: [BLOCK_SIZE_BATCH, in_features] -> but we cannot load the entire input at once. We have to tile over in_features.\n\n    We can do:\n      Let BLOCK_SIZE_K = 32 (for the inner dimension)\n\n      For k in range(0, in_features, BLOCK_SIZE_K):\n        Load a tile of input: [BLOCK_SIZE_BATCH, BLOCK_SIZE_K] from the input matrix at the current batch block and the current inner tile.\n        Load a tile of weights: [BLOCK_SIZE_FEATURES, BLOCK_SIZE_K] from the weight matrix at the current output feature block and the current inner tile.\n        Then compute the dot product for the current tile and accumulate.\n\n    Then we have the matmul result for the tile.\n\n    Then we add the bias (from linear) and the add_value. But note: the bias is of shape [out_features] and the add_value is [out_features]. So we can pre-add them? But we have to load the bias and add_value for the current output feature block.\n\n    Then apply the activations.\n\n    But note: the activations are element-wise and independent.\n\n  However, this is a complex kernel. And we have to write it correctly and efficiently.\n\nGiven the constraints of the problem (small iterative steps) and that we are currently at a reference code (PyTorch) and we want to get at least 5% speedup, we might start by replacing only the matmul? But then the pointwise operations are still in PyTorch. That might not be enough.\n\nAlternatively, we can write a kernel for the entire forward pass? That would be a big step.\n\nBut note: the problem says that the current stage is \"reference code\", meaning we are starting from scratch. We have to write a Triton kernel that replaces the entire forward pass.\n\nWe are allowed to change the forward method to call our Triton kernel? Yes, because the constraints only lock the Triton kernel signature? Actually, the constraints say:\n\n  \u274c NEVER change @triton.jit function signatures or parameter names\n\nSo we are free to design the kernel as we wish, as long as we don't change the grid configuration? Actually, the constraints say \"NEVER modify grid configuration, output tensor shapes, or PID logic\" in the Triton kernel? But we are designing the kernel from scratch.\n\nHowever, note that the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" But in the current code, there is no Triton kernel. So we are going to introduce one.\n\nBut the current code block includes the entire model and the batch_size, etc. So we are allowed to change the entire block? The problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\nWe are going to:\n\n  1. Write a Triton kernel for the entire forward pass (fused).\n  2. Change the forward method of ModelNew to call this kernel.\n\nBut note: the model also has parameters (weight, bias of linear, and add_value). We have to pass these to the kernel.\n\nWe are going to design:\n\n  Let the kernel be called `_fused_forward_kernel`.\n\n  Inputs to the kernel:\n    - input_ptr: pointer to input tensor [batch, in_features]\n    - weight_ptr: pointer to weight tensor [out_features, in_features]\n    - bias_ptr: pointer to bias tensor [out_features] (from linear)\n    - add_value_ptr: pointer to the extra parameter [out_features]\n    - output_ptr: pointer to output tensor [batch, out_features]\n\n  We will also pass the sizes: batch_size, in_features, out_features.\n\n  We will use tiling for the matmul part.\n\n  Steps in the kernel:\n\n    pid_batch = tl.program_id(0)   # along batch dimension\n    pid_feature = tl.program_id(1)  # along output feature dimension\n\n    We are going to use blocks for batch and features? Actually, we can have each program ID handle a block of batch and a block of features? Or we can have each program ID handle one element? That would be inefficient because we need to load a row of input and a column of weights for one element.\n\n  Instead, we do:\n\n    Let BLOCK_SIZE_BATCH = 32, BLOCK_SIZE_FEATURES = 32, BLOCK_SIZE_K = 64.\n\n    We have a grid of (ceil(batch_size / BLOCK_SIZE_BATCH), ceil(out_features / BLOCK_SIZE_FEATURES)).\n\n    Then in the kernel:\n\n      batch_index = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n      feature_index = pid_feature * BLOCK_SIZE_FEATURES + tl.arange(0, BLOCK_SIZE_FEATURES)\n\n      We create masks for batch and feature to avoid out-of-bound.\n\n      Then we initialize an accumulator for the matmul for the current block.\n\n      For k in range(0, in_features, BLOCK_SIZE_K):\n          k_index = k + tl.arange(0, BLOCK_SIZE_K)\n          # Load input tile: [BLOCK_SIZE_BATCH, BLOCK_SIZE_K]\n          input_mask = (batch_index[:, None] < batch_size) & (k_index[None, :] < in_features)\n          input_tile = tl.load(input_ptr + batch_index[:, None] * in_features + k_index[None, :], mask=input_mask, other=0.0)\n          # Load weight tile: [BLOCK_SIZE_FEATURES, BLOCK_SIZE_K]\n          weight_mask = (feature_index[:, None] < out_features) & (k_index[None, :] < in_features)\n          weight_tile = tl.load(weight_ptr + feature_index[:, None] * in_features + k_index[None, :], mask=weight_mask, other=0.0)\n          # We have to transpose weight_tile? Actually, the weight is stored as [out_features, in_features]. We are loading a tile of [BLOCK_SIZE_FEATURES, BLOCK_SIZE_K]. Then we do:\n          #   accumulator += tl.dot(input_tile, weight_tile, trans_b=True)   -> but wait, the dot product is between input_tile (BLOCK_SIZE_BATCH, BLOCK_SIZE_K) and weight_tile (BLOCK_SIZE_FEATURES, BLOCK_SIZE_K). We want to do: input_tile @ weight_tile.T -> which is (BLOCK_SIZE_BATCH, BLOCK_SIZE_FEATURES). So we can do:\n          accumulator += tl.dot(input_tile, weight_tile, trans_b=True)\n\n      Then we load the bias and add_value for the current feature block.\n        bias_tile = tl.load(bias_ptr + feature_index, mask=feature_index < out_features, other=0.0)\n        add_value_tile = tl.load(add_value_ptr + feature_index, mask=feature_index < out_features, other=0.0)\n\n      Then:\n        result = accumulator + bias_tile[None, :] + add_value_tile[None, :]\n\n      Then we apply the activations pointwise to the result tile.\n\n      Then store the result to the output.\n\n  But note: the activations are:\n      x = torch.sigmoid(x) * x   # Swish\n      x = torch.tanh(x)\n      x = torch.nn.functional.gelu(x)   # GELU\n      x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1)   # Hardtanh\n\n  We have to write these in Triton.\n\n  Triton has:\n      tl.sigmoid, tl.tanh, and we can write GELU as: x * 0.5 * (1.0 + tl.erf(x * 0.7071067811865475)), and hardtanh as tl.min(tl.max(x, -1.0), 1.0).\n\n  So in the kernel:\n\n      # Swish\n      swish = tl.sigmoid(result) * result\n      # Tanh\n      tanh = tl.tanh(swish)\n      # GELU: approximate version? or exact? The PyTorch GELU is tanh by default? But note: the exact formula is x * 0.5 * (1 + erf(x/sqrt(2))). We can use that.\n      gelu = tanh * 0.5 * (1.0 + tl.erf(tanh * 0.7071067811865475))   # but wait, the GELU is applied to the tanh output? The original code: after tanh, then GELU? That is not standard. We have to follow the order.\n\n      Actually, the original code:\n          x = torch.sigmoid(x) * x   -> Swish\n          x = torch.tanh(x)           -> Tanh on the Swish\n          x = torch.nn.functional.gelu(x) -> GELU on the Tanh\n          x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1)   -> clamp\n\n      So we do:\n\n          x0 = result\n          x1 = tl.sigmoid(x0) * x0   # Swish\n          x2 = tl.tanh(x1)            # Tanh\n          # GELU: we use the exact formula for GELU? But note: Triton might have a built-in for GELU? Actually, it doesn't. We use the erf version.\n          #   GELU(x) = x * 0.5 * (1 + erf(x / sqrt(2)))\n          x3 = x2 * 0.5 * (1.0 + tl.erf(x2 * 0.7071067811865475))   # 0.7071067811865475 is 1/sqrt(2)\n          x4 = tl.min(tl.max(x3, -1.0), 1.0)   # Hardtanh\n\n      Then we store x4.\n\n  But note: the GELU implementation in PyTorch might be the tanh approximation by default? However, the documentation says: \n        torch.nn.functional.gelu(x) = x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n      when approximate='none'. And the default is 'none'? Actually, in the current version, the default is 'tanh'. But the problem does not specify. We have to match the original.\n\n  However, the problem says we must be numerically correct within float32 tolerance. So we have to use the same GELU as the original.\n\n  Let's check the original code: \n        x = torch.nn.functional.gelu(x)\n\n  The default in PyTorch for gelu is the tanh approximation? Actually, the documentation says: \n        When the approximate argument is 'tanh', Gelu is estimated with: \n            x * 0.5 * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n        Otherwise, it uses the erf version.\n\n  The default is approximate='none'? Actually, the default in the function is approximate='none'. But note: the nn.GELU module uses approximate='none' by default? Let me check: \n        In PyTorch 1.12.1: the functional gelu has an argument `approximate` that defaults to None, which then uses the exact method? Actually, in the source code, if approximate is None, it uses the exact method.\n\n  But to be safe, we should use the exact method? Because the problem does not specify. And the tolerance is float32.\n\n  However, the exact method (erf) is more expensive? But we are in a kernel. We have to decide.\n\n  But note: the original code does not specify an approximate method. So we use the exact method to be safe.\n\n  Alternatively, we can use the approximate method? But the problem says we must be numerically correct. We have to match the original.\n\n  How about we run the original and see what it uses? We cannot. So we assume the exact method.\n\n  However, the original code uses the functional gelu without arguments -> so it uses the exact method? Actually, in the documentation of PyTorch 1.13: \n        torch.nn.functional.gelu(input, approximate='none') -> Tensor\n\n  So we use the erf version.\n\n  Therefore, we do:\n        x3 = x2 * 0.5 * (1.0 + tl.erf(x2 * 0.7071067811865475))\n\n  But note: the erf function in Triton is available in the `libdevice` module? Actually, in triton.language we have `tl.erf`.\n\n  So we can do.\n\n  However, note that the activations are applied element-wise. So we can write them as above.\n\n  Now, we have to write the kernel and then call it in the forward method.\n\n  We also have to handle the boundaries.\n\n  We are going to use:\n\n    BLOCK_SIZE_BATCH = 32\n    BLOCK_SIZE_FEATURES = 32\n    BLOCK_SIZE_K = 64   # for the inner dimension\n\n  We can tune these? But we are not required to do so in this step. We can set them as constants.\n\n  But note: the problem allows adaptive tuning. We can use autotune? But the problem says \"small iterative steps\". We can leave autotune for later.\n\n  We are going to write the kernel without autotune first.\n\n  Then in the forward method of ModelNew, we will:\n\n      - Check that the input is contiguous.\n      - Allocate the output tensor.\n      - Set the grid: \n            grid = (triton.cdiv(batch_size, BLOCK_SIZE_BATCH), triton.cdiv(out_features, BLOCK_SIZE_FEATURES))\n      - Call the kernel.\n\n  We have to pass the parameters (weight, bias, add_value) as tensors.\n\n  Note: the model currently has:\n        self.matmul: nn.Linear -> has weight and bias. We can get them by: self.matmul.weight, self.matmul.bias.\n        self.add_value: the extra parameter.\n\n  So in the forward method:\n\n        weight = self.matmul.weight   # [out_features, in_features]\n        bias = self.matmul.bias       # [out_features]\n        add_value = self.add_value    # [out_features]\n\n        # We have to make sure they are on the same device and contiguous.\n\n        output = torch.empty((x.shape[0], out_features), device=x.device, dtype=x.dtype)\n\n        # Check that the tensors are contiguous and of the right dtype? The input x is float32? The weights and biases are float32? The add_value is float32? The output will be float32.\n\n        # Then call the kernel.\n\n  However, note: the kernel must be compiled only once? We can use a static function.\n\n  We are going to write the kernel inside the class? Or outside? We can write it outside the class.\n\n  But note: the problem requires the entire code block. We are going to write the kernel as a top-level function in the EVOLVE-BLOCK.\n\n  Steps:\n\n    We will:\n        import triton and triton.language as tl\n        write the kernel function with the above design.\n\n    Then in the forward method, we call the kernel.\n\n  But note: the problem says we must not change the function signatures? We are designing the kernel from scratch, so we are free.\n\n  However, the constraints say: \"NEVER change @triton.jit function signatures or parameter names\" once we have them. But we are writing the first version.\n\n  We are going to write the kernel and then call it.\n\n  But note: the current model is for a specific batch size? Actually, the batch_size is set to 128. But the model should work for any batch size? Actually, the forward method uses the input x which has shape [batch, in_features]. So we have to set the grid accordingly.\n\n  We are going to write:\n\n      def forward(self, x):\n          # Get the parameters\n          weight = self.matmul.weight\n          bias = self.matmul.bias\n          add_value = self.add_value\n\n          batch_size, in_features = x.shape\n          out_features = weight.shape[0]\n          output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n\n          # Check that the parameters are contiguous\n          x = x.contiguous()\n          weight = weight.contiguous()\n          bias = bias.contiguous()\n          add_value = add_value.contiguous()\n\n          # Define the kernel grid and block sizes\n          BLOCK_SIZE_BATCH = 32\n          BLOCK_SIZE_FEATURES = 32\n          BLOCK_SIZE_K = 64\n\n          grid = (triton.cdiv(batch_size, BLOCK_SIZE_BATCH), triton.cdiv(out_features, BLOCK_SIZE_FEATURES))\n\n          # Launch the kernel\n          _fused_forward_kernel[grid](\n              x, weight, bias, add_value, output,\n              batch_size, in_features, out_features,\n              BLOCK_SIZE_BATCH, BLOCK_SIZE_FEATURES, BLOCK_SIZE_K\n          )\n\n          return output\n\n  But note: the kernel must be defined with the correct parameters.\n\n  However, note: the problem says we are to return the entire code block. We are going to write the kernel inside the EVOLVE-BLOCK.\n\n  But note: the problem says \"NO explanations, NO markdown, NO additional text\". So we just write the code.\n\n  We are going to write the kernel and the model.\n\n  Let's code.\n\n  IMPORTANT: We are not allowed to remove boundary checks. We have to mask every load.\n\n  We are going to write the kernel carefully.\n\n  However, note: this is a big step. But the problem says we are at the beginning (stage: reference code) and we are to generate Triton.\n\n  We hope that this fused kernel will be faster than the PyTorch implementation because we are fusing the matmul and the activations and avoiding multiple passes over the data.\n\n  But note: the matmul in PyTorch is highly optimized (using cuBLAS). So we have to hope that our tiled matmul is efficient? And the fusion of the activations might save memory bandwidth.\n\n  We are using the H100 with Tensor Cores? The problem says: \"FP16 Tensor Core 32\u00d716\u00d716\". But our tensors are float32? The original code uses float32? We are going to keep float32 for now.\n\n  Alternatively, we can use fp16? But the problem does not specify. We must be numerically equivalent. The original code uses float32? We have to check the original.\n\n  The original code: \n        x = torch.randn(batch_size, in_features) -> which is float32 by default.\n\n  So we do float32.\n\n  But note: the Tensor Cores can do fp32? Yes, they support tf32 and fp32. But we have to set the kernel to use tensor cores? How? We can use the `tl.dot` instruction and if the inputs are fp16 or bf16 or tf32, it might use tensor cores? For fp32, it might not? \n\n  Alternatively, we can use tf32? But that might lose precision. We are required to be within float32 tolerance.\n\n  We can leave it as fp32 and hope that the tiling is efficient.\n\n  Alternatively, we can convert the weights and input to fp16? But the original code is in fp32. We are not allowed to change the dtype.\n\n  So we do fp32.\n\n  Let's write the kernel.\n\n  Note: We are not using shared memory? We are using the dot product directly. Triton will automatically use shared memory for the tiling? We don't have to.\n\n  But note: the Triton dot product uses shared memory internally? We don't manage it.\n\n  We are going to write the kernel.\n\n  However, note: the problem says \"small iterative steps\". This kernel is about 100 lines? We are allowed to change up to 20 lines? But we are starting from scratch. The problem says we are allowed to replace the entire forward pass.\n\n  But note: the problem says \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" and we are replacing the entire block? Yes, because we are introducing the kernel and changing the forward method.\n\n  We are going to write the entire code block.\n\n  We hope that this will compile and be correct.\n\n  If not, we can fix in the next iteration.\n\n  We are going to write the code now.\n\n  IMPORTANT: We are not allowed to remove the boundary checks. We have to mask every load.\n\n  We are going to do:\n\n      input_tile = tl.load(input_ptr + ... , mask=mask, other=0.0)\n\n  Similarly for weights.\n\n  And for the bias and add_value.\n\n  And for the store.\n\n  Let's write the kernel.\n\n  We are going to define the kernel with the following parameters:\n\n      input_ptr, weight_ptr, bias_ptr, add_value_ptr, output_ptr,\n      batch_size, in_features, out_features,\n      BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_FEATURES: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n\n  Then:\n\n      pid_batch = tl.program_id(0)\n      pid_feature = tl.program_id(1)\n\n      # Create ranges for the batch and feature blocks\n      batch_offsets = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n      feature_offsets = pid_feature * BLOCK_SIZE_FEATURES + tl.arange(0, BLOCK_SIZE_FEATURES)\n\n      # Create masks for the batch and feature dimensions\n      batch_mask = batch_offsets < batch_size\n      feature_mask = feature_offsets < out_features\n\n      # We will accumulate in a block of [BLOCK_SIZE_BATCH, BLOCK_SIZE_FEATURES]\n      accumulator = tl.zeros((BLOCK_SIZE_BATCH, BLOCK_SIZE_FEATURES), dtype=tl.float32)\n\n      # Loop over K (in_features) in blocks of BLOCK_SIZE_K\n      for k in range(0, in_features, BLOCK_SIZE_K):\n          k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n          k_mask = k_offsets < in_features\n\n          # Load input: [BLOCK_SIZE_BATCH, BLOCK_SIZE_K]\n          input_mask = batch_mask[:, None] & k_mask[None, :]\n          input_ptr_offset = (batch_offsets[:, None] * in_features + k_offsets[None, :])\n          input_tile = tl.load(input_ptr + input_ptr_offset, mask=input_mask, other=0.0)\n\n          # Load weight: [BLOCK_SIZE_FEATURES, BLOCK_SIZE_K]\n          weight_mask = feature_mask[:, None] & k_mask[None, :]\n          weight_ptr_offset = (feature_offsets[:, None] * in_features + k_offsets[None, :])\n          weight_tile = tl.load(weight_ptr + weight_ptr_offset, mask=weight_mask, other=0.0)\n\n          # Compute the dot product: input_tile (BLOCK_SIZE_BATCH, BLOCK_SIZE_K) @ weight_tile (BLOCK_SIZE_FEATURES, BLOCK_SIZE_K).T\n          #   -> [BLOCK_SIZE_BATCH, BLOCK_SIZE_FEATURES]\n          accumulator += tl.dot(input_tile, weight_tile, trans_b=True)\n\n      # Now load the bias and add_value for the current feature block\n      bias_tile = tl.load(bias_ptr + feature_offsets, mask=feature_mask, other=0.0)   # [BLOCK_SIZE_FEATURES]\n      add_value_tile = tl.load(add_value_ptr + feature_offsets, mask=feature_mask, other=0.0)   # [BLOCK_SIZE_FEATURES]\n\n      # Add the bias and add_value (broadcasted to the batch dimension)\n      result = accumulator + bias_tile[None, :] + add_value_tile[None, :]\n\n      # Now apply the activations:\n\n      # Swish: sigmoid(result) * result\n      swish = tl.sigmoid(result) * result\n\n      # Tanh\n      tanh = tl.tanh(swish)\n\n      # GELU: exact version\n      gelu = tanh * 0.5 * (1.0 + tl.erf(tanh * 0.7071067811865475))\n\n      # Hardtanh: clamp between -1 and 1\n      hardtanh = tl.min(tl.max(gelu, -1.0), 1.0)\n\n      # Now store the result\n      output_ptr_offset = batch_offsets[:, None] * out_features + feature_offsets[None, :]\n      output_mask = batch_mask[:, None] & feature_mask[None, :]\n      tl.store(output_ptr + output_ptr_offset, hardtanh, mask=output_mask)\n\n  But note: the above GELU is applied to the tanh output? The original code does:\n        x = torch.sigmoid(x) * x   -> Swish\n        x = torch.tanh(x)           -> Tanh\n        x = torch.nn.functional.gelu(x) -> GELU on the result of tanh\n        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1)\n\n  So we are correct.\n\n  However, note: the GELU formula we used is for the input to the GELU, which in our case is `tanh`. So we are doing:\n        gelu = tanh * 0.5 * (1 + erf(tanh / sqrt(2)))   [but note: we multiplied by 0.707... which is 1/sqrt(2) so it's the same as dividing by sqrt(2) in the erf argument? Actually, we multiplied the input by 0.707... which is the same as dividing by sqrt(2) because 1/sqrt(2)=0.707...]\n\n  So it's correct.\n\n  Now, we have to write the model class and the rest.\n\n  We are going to replace the entire EVOLVE-BLOCK with:\n\n      import ... (including triton and triton.language)\n\n      # Define the kernel\n      @triton.jit\n      def _fused_forward_kernel(...):\n          ... (code above)\n\n      class ModelNew(nn.Module):\n          ... (same __init__ as before)\n\n          def forward(self, x):\n              ... (code above that calls the kernel)\n\n      ... (the rest: batch_size, get_inputs, get_init_inputs)\n\n  But note: the kernel is defined at the top level.\n\n  Let's write.\n\n  IMPORTANT: We are not allowed to change the class name: it must be ModelNew.\n\n  We hope that this will compile and run.\n\n  However, note: the kernel might be slow? But we are fusing operations and saving memory bandwidth.\n\n  We are going to output the entire block.\n\n  Note: We are using triton, so we have to import it.\n\n  We add:\n        import triton\n        import triton.language as tl\n\n  And we have to make sure that the kernel is compiled only once? It will be JIT compiled.\n\n  We are ready.\n\n  We write the code.\n\n  Note: We are using the same parameters for the model: in_features, out_features, add_value_shape.\n\n  We are going to set the BLOCK sizes as constants in the kernel call? We passed them as kernel arguments? And then we set them as tl.constexpr.\n\n  We set:\n        BLOCK_SIZE_BATCH = 32\n        BLOCK_SIZE_FEATURES = 32\n        BLOCK_SIZE_K = 64\n\n  We can try to tune these later.\n\n  Now, the code.\n\n  IMPORTANT: We are not allowed to change the grid configuration? We are designing it, so it's okay.\n\n  We are done.\n\n  Let's write the code.\n</think>\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_forward_kernel(\n    x_ptr, weight_ptr, bias_ptr, add_value_ptr, output_ptr,\n    batch_size, in_features, out_features,\n    BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_FEAT: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_feat = tl.program_id(1)\n    \n    batch_offsets = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feat_offsets = pid_feat * BLOCK_SIZE_FEAT + tl.arange(0, BLOCK_SIZE_FEAT)\n    k_offsets = tl.arange(0, BLOCK_SIZE_K)\n    \n    batch_mask = batch_offsets < batch_size\n    feat_mask = feat_offsets < out_features\n    \n    accumulator = tl.zeros((BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_mask = k_offsets < (in_features - k)\n        input_mask = batch_mask[:, None] & k_mask[None, :]\n        weight_mask = feat_mask[:, None] & k_mask[None, :]\n        \n        x_offset = batch_offsets[:, None] * in_features + (k + k_offsets[None, :])\n        w_offset = feat_offsets[:, None] * in_features + (k + k_offsets[None, :])\n        \n        x_tile = tl.load(x_ptr + x_offset, mask=input_mask, other=0.0)\n        w_tile = tl.load(weight_ptr + w_offset, mask=weight_mask, other=0.0)\n        \n        accumulator += tl.dot(x_tile, w_tile, trans_b=True)\n    \n    bias_vals = tl.load(bias_ptr + feat_offsets, mask=feat_mask, other=0.0)\n    add_vals = tl.load(add_value_ptr + feat_offsets, mask=feat_mask, other=0.0)\n    \n    result = accumulator + bias_vals[None, :] + add_vals[None, :]\n    \n    # Fused activations\n    swish = tl.sigmoid(result) * result\n    tanh = tl.tanh(swish)\n    gelu = tanh * 0.5 * (1.0 + tl.erf(tanh * 0.7071067811865475))\n    hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)\n    \n    output_offset = batch_offsets[:, None] * out_features + feat_offsets[None, :]\n    output_mask = batch_mask[:, None] & feat_mask[None, :]\n    tl.store(output_ptr + output_offset, hardtanh, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape))\n    \n    def forward(self, x):\n        weight = self.matmul.weight\n        bias = self.matmul.bias\n        add_value = self.add_value\n        \n        batch_size, in_dim = x.shape\n        out_dim = weight.shape[0]\n        output = torch.empty((batch_size, out_dim), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_BATCH = 32\n        BLOCK_SIZE_FEAT = 32\n        BLOCK_SIZE_K = 64\n        \n        grid = (\n            triton.cdiv(batch_size, BLOCK_SIZE_BATCH),\n            triton.cdiv(out_dim, BLOCK_SIZE_FEAT)\n        )\n        \n        _fused_forward_kernel[grid](\n            x, weight, bias, add_value, output,\n            batch_size, in_dim, out_dim,\n            BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT, BLOCK_SIZE_K\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
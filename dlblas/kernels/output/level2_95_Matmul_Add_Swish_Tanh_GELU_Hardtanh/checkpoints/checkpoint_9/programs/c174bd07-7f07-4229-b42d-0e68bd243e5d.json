{"id": "c174bd07-7f07-4229-b42d-0e68bd243e5d", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = torch.nn.Parameter(torch.randn(out_features))\n        self.add_value = torch.nn.Parameter(torch.randn(add_value_shape))\n        self.in_features = in_features\n        self.out_features = out_features\n\n    @triton.jit\n    def _forward_kernel(\n        x_ptr, w_ptr, b_ptr, add_ptr, output_ptr,\n        in_features, out_features,\n        BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_FEAT: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n    ):\n        pid0 = tl.program_id(0)\n        pid1 = tl.program_id(1)\n        \n        batch_offset = pid0 * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n        feat_offset = pid1 * BLOCK_SIZE_FEAT + tl.arange(0, BLOCK_SIZE_FEAT)\n        k_offset = tl.arange(0, BLOCK_SIZE_K)\n        \n        batch_mask = batch_offset < tl.num_programs(0) * BLOCK_SIZE_BATCH\n        feat_mask = feat_offset < out_features\n        \n        acc = tl.zeros((BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT), dtype=tl.float32)\n        \n        for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n            k_offset = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n            k_mask = k_offset < in_features\n            \n            x_ptr_offset = (batch_offset[:, None] * in_features + k_offset[None, :])\n            w_ptr_offset = (feat_offset[:, None] * in_features + k_offset[None, :])\n            \n            x_block = tl.load(x_ptr + x_ptr_offset, mask=batch_mask[:, None] & k_mask[None, :], other=0.0)\n            w_block = tl.load(w_ptr + w_ptr_offset, mask=feat_mask[:, None] & k_mask[None, :], other=0.0)\n            \n            acc += tl.dot(x_block, w_block, allow_tf32=True)\n        \n        b = tl.load(b_ptr + feat_offset, mask=feat_mask, other=0.0)\n        add_val = tl.load(add_ptr + feat_offset, mask=feat_mask, other=0.0)\n        \n        res = acc + b[None, :] + add_val[None, :]\n        res = res * tl.sigmoid(res)  # Swish\n        res = tl.tanh(res)\n        res = res * 0.5 * (1.0 + tl.erf(res * 0.7071067811865475))  # GELU\n        res = tl.minimum(tl.maximum(res, -1.0), 1.0)  # Hardtanh\n        \n        out_offset = (batch_offset[:, None] * out_features + feat_offset[None, :])\n        tl.store(output_ptr + out_offset, res, mask=batch_mask[:, None] & feat_mask[None, :])\n\n    def forward(self, x):\n        output = torch.empty((x.size(0), self.out_features), device=x.device, dtype=x.dtype)\n        batch_size = x.size(0)\n        \n        grid_batch = triton.cdiv(batch_size, 32)\n        grid_feat = triton.cdiv(self.out_features, 128)\n        \n        self._forward_kernel[(grid_batch, grid_feat)](\n            x, self.weight, self.bias, self.add_value, output,\n            self.in_features, self.out_features,\n            BLOCK_SIZE_BATCH=32, BLOCK_SIZE_FEAT=128, BLOCK_SIZE_K=64\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "ccb7a9d3-2d42-4435-b130-7193e31b53f9", "generation": 1, "timestamp": 1754586406.8668795, "iteration_found": 3, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_95_Matmul_Add_Swish_Tanh_GELU_Hardtanh", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp5p4g2suk.py\", line 64, in forward\n    self._forward_kernel[(grid_batch, grid_feat)](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 0.0872, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0782, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0872, "speed_up": 1.115, "custom_timing": 0.0782}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0782\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.115, 'custom_timing': 0.0782}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0782, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.115, 'custom_timing': 0.0782}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, adds a value, applies Swish, Tanh, GELU, and Hardtanh activation functions.\n    \"\"\"\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x + self.add_value\n        x = torch.sigmoid(x) * x # Swish\n        x = torch.tanh(x)\n        x = torch.nn.functional.gelu(x) # GELU\n        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0782), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.115, 'custom_timing': 0.0782})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, adds a value, applies Swish, Tanh, GELU, and Hardtanh activation functions.\n    \"\"\"\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x + self.add_value\n        x = torch.sigmoid(x) * x # Swish\n        x = torch.tanh(x)\n        x = torch.nn.functional.gelu(x) # GELU\n        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Matrix multiplication (nn.Linear)\n  2. Add a parameter vector (broadcasted)\n  3. Apply Swish (sigmoid * x)\n  4. Apply Tanh\n  5. Apply GELU\n  6. Apply Hardtanh (clamp between -1 and 1)\n\nThe current implementation is in PyTorch and runs on the GPU. We are to rewrite it using Triton to speed up the operations.\n\nHowever, note that the model uses multiple pointwise operations (add, swish, tanh, gelu, hardtanh) and one linear layer.\n\nThe linear layer is the most computationally heavy part. The pointwise operations are memory bound.\n\nWe cannot change the function signatures or the overall behavior. We must maintain the same input and output.\n\nBut note: the current code is entirely in PyTorch. We are allowed to replace parts with Triton kernels.\n\nWe can break the operations into two main parts:\n  A. The linear layer (matmul) and the addition: these are the heavy operations.\n  B. The sequence of activation functions.\n\nHowever, note that the linear layer in PyTorch is already optimized. We might not gain much by replacing it with Triton unless we can fuse the operations.\n\nBut the problem is that the entire sequence (linear, add, swish, tanh, gelu, hardtanh) is done in separate steps. We can try to fuse all these operations into one or two kernels to reduce memory traffic.\n\nGiven that the operations are pointwise (except the linear), we can consider:\n  1. Writing a Triton kernel for the entire forward pass? That would require a kernel that does:\n      - Load a block of input, do the matrix multiplication for that block (if we can break the matmul) and then apply the rest pointwise.\n      But note: the linear layer is a full matrix multiplication (with weights of shape [out_features, in_features]) and the input is [batch_size, in_features]. The output is [batch_size, out_features]. Then the add is with a vector of [out_features]. Then the activations.\n\n  2. Alternatively, we can break it into:\n      a. A Triton kernel for the linear layer (with or without fusing the add and some activations?).\n      b. Then a fused kernel for the activation sequence.\n\nHowever, note the constraints:\n  - We cannot change the grid configuration arbitrarily? Actually, we are allowed to change the Triton kernel's grid as long as the output tensor shapes remain the same.\n\nBut note: the problem says \"NEVER modify grid configuration, output tensor shapes, or PID logic\" in the context of the Triton kernel that we are evolving? Actually, the current program does not have a Triton kernel. We are introducing one.\n\nSo we are free to design the grid? But we must be cautious because the problem says that the grid configuration in the existing Triton code (if any) should not be changed. Since there is none, we can design.\n\nPlan:\n  We can write a fused kernel that does:\n      output = (input @ weight.T) + bias   [but note: the current model does not have a bias in the linear? Actually, nn.Linear has bias by default. But in the current code, we have an extra add with a parameter vector. So it's:\n          x = self.matmul(x)   -> [batch_size, out_features] = [batch_size, in_features] @ [out_features, in_features]^T + bias (if any)\n          x = x + self.add_value\n\n      Then the activations.\n\n  However, the model has two sets of parameters: the linear layer (weight and bias) and the add_value.\n\n  We can create a single kernel that:\n      - Takes the input, weight, linear_bias (if exists) and add_value as parameters.\n      - Computes the linear layer for a block of rows and then applies the add and the activations.\n\n  But note: the linear layer itself is a matmul. We can break the input matrix (batch_size x in_features) into blocks of rows and then do the matmul for that block? However, the weight matrix is shared.\n\n  Alternatively, we can do:\n      Each program ID will handle a row of the output (i.e., one example in the batch and one row of the output features). But then the inner product would be over the in_features.\n\n  Steps for one row:\n      - Load one row of input (of length in_features)\n      - For each output feature, compute the dot product with the corresponding row of the weight matrix (which is of length in_features) and then add the linear bias (if any) and then add the add_value for that feature? Actually, the add_value is a vector of length out_features. So for each output feature, we have:\n          out[i] = (input_row * weight[i]).sum() + linear_bias[i] + add_value[i]\n      - Then apply the activations.\n\n  We can do:\n      pid = tl.program_id(0)  # which row in the batch? [0..batch_size-1]\n      and then for each feature, we can have a loop? But that would be inefficient.\n\n  Alternatively, we can have a 2D grid: [batch_size, out_features] but that would be too many threads and not coalesced.\n\n  Better: use block for the output features? We can let each program handle a block of output features for one row.\n\n  Proposed kernel structure:\n\n      pid_batch = tl.program_id(0)   # row index in the batch\n      pid_feature = tl.program_id(1)  # block index for the output features\n\n      We will have BLOCK_SIZE_FEATURES for the output features.\n\n      Steps:\n        - Load a block of the input row (for the entire row? Actually, we need the whole row to compute the dot product with multiple weight rows). We can load the entire row? But the row might be long (in_features=1024) and we are limited by shared memory.\n\n      We can break the inner product by blocks of the in_features? This becomes a block matrix multiplication.\n\n  However, note: the entire model is small. The batch_size=128, in_features=1024, out_features=512. The total operations are 128*1024*512 = 67M. But we are memory bound for the activations.\n\n  Alternatively, we can use Triton to write a fused kernel that does:\n      linear + add + swish + tanh + gelu + hardtanh\n\n  and we can do it in two steps:\n\n  Step 1: Do the linear layer (matmul) using a Triton kernel that is optimized for small sizes? We can use the Triton matmul example.\n\n  Step 2: Then do a fused pointwise kernel for the rest: add, swish, tanh, gelu, hardtanh.\n\n  But note: the add is with a vector (broadcasted over the batch). Then the activations are independent per element.\n\n  We can do:\n\n      Kernel 1: matmul (with bias and then add the add_value) and then the activations? Actually, we can do the entire thing in one kernel? That would save memory.\n\n  However, the matmul kernel is complex. We can use the Triton matmul kernel and then extend it to do the pointwise operations at the end? But note the pointwise operations are applied after the entire matmul.\n\n  Alternatively, we can break the matmul into blocks and then for each block of output, we can do the pointwise operations? But that would require storing intermediate results and then reloading? Not good.\n\n  Considering the complexity, we might start by writing a fused kernel for the pointwise operations only? And leave the linear layer as is? That might not give much speedup because the linear layer is the heavy part.\n\n  But note: the current PyTorch linear layer is already optimized (using cuBLAS). So we might not be able to beat it with a custom Triton kernel for such a small matmul? However, we can try to fuse the entire operation (linear, add, and activations) to avoid writing the intermediate results to global memory.\n\n  We can write a kernel that does the linear layer for a tile and then immediately applies the pointwise operations? But note: the linear layer requires the entire input row to compute one output element? Actually, we break the matmul by blocks of K (the inner dimension). We accumulate in registers.\n\n  We can do:\n\n      pid = tl.program_id(0)   # each program handles a block of output rows? and a block of output columns? (i.e., batch and feature blocks)\n\n      Let BLOCK_SIZE_BATCH and BLOCK_SIZE_FEATURES.\n\n      Then for each block, we compute:\n          for k in range(0, in_features, BLOCK_SIZE_K):\n              load a block of input [BLOCK_SIZE_BATCH, BLOCK_SIZE_K] and a block of weights [BLOCK_SIZE_FEATURES, BLOCK_SIZE_K]\n              compute the partial matmul.\n\n          Then we have the matmul result for [BLOCK_SIZE_BATCH, BLOCK_SIZE_FEATURES] in a register.\n\n          Then we add the linear_bias (broadcasted over the batch for the feature block) and then add the add_value (for the feature block) and then apply the activations.\n\n          Then store the result.\n\n      This avoids writing the intermediate matmul result and then reading it again for the pointwise operations.\n\n  This is a big kernel. We can start with a simple kernel that does the entire forward pass for one row? But that would not be efficient.\n\n  Given the time, we might try to write a kernel that does the entire forward pass for a block of rows and a block of features? And use the standard matmul block approach.\n\n  Steps:\n\n      pid_batch = tl.program_id(0)  # block index for batch\n      pid_feature = tl.program_id(1) # block index for feature\n\n      We have:\n          batch_block = pid_batch * BLOCK_SIZE_BATCH\n          feature_block = pid_feature * BLOCK_SIZE_FEATURES\n\n      We allocate an accumulator array of size [BLOCK_SIZE_BATCH, BLOCK_SIZE_FEATURES] in registers.\n\n      Initialize accumulator to linear_bias? Actually, the linear layer has bias. We can add the bias at the end? Or we can break the bias into blocks? Actually, the bias is a vector of length out_features. We can load the bias for the current feature block and add it to the accumulator.\n\n      Similarly, we will load the add_value for the current feature block.\n\n      Then for k in range(0, in_features, BLOCK_SIZE_K):\n          # Load input block: [BLOCK_SIZE_BATCH, BLOCK_SIZE_K]\n          input_block = load_input(batch_block, k, BLOCK_SIZE_BATCH, BLOCK_SIZE_K)\n          # Load weight block: [BLOCK_SIZE_FEATURES, BLOCK_SIZE_K] -> note: the weight matrix is stored as [out_features, in_features]\n          weight_block = load_weight(feature_block, k, BLOCK_SIZE_FEATURES, BLOCK_SIZE_K)\n\n          # Compute: accumulator += input_block @ weight_block.T\n\n      Then:\n          accumulator = accumulator + linear_bias_block   # [BLOCK_SIZE_BATCH, BLOCK_SIZE_FEATURES] + [1, BLOCK_SIZE_FEATURES] (broadcasted)\n          accumulator = accumulator + add_value_block      # [BLOCK_SIZE_BATCH, BLOCK_SIZE_FEATURES] + [1, BLOCK_SIZE_FEATURES]\n\n      Then apply the activations pointwise on the accumulator.\n\n      Then store the accumulator.\n\n  We must be cautious about the boundaries.\n\n  However, note: the problem says we are not allowed to remove boundary checks.\n\n  This kernel is complex. Given the constraints of the problem (only 20 lines per revision) and that we are starting from a PyTorch implementation, we might not be able to write the entire kernel in one go.\n\n  Alternatively, we can try to fuse only the pointwise operations? And leave the linear layer as the PyTorch linear. But that would not save much.\n\n  Considering the problem statement that we are to rewrite the program to improve performance, and the fact that the current speedup is 1.115 (which is already above 1, meaning the custom implementation is faster) but the score is 0 because the current program is the reference? Actually, the current program is the reference and we are to write the Triton version.\n\n  Let me re-read: \n      \"Current performance metrics: ... stage: reference code ... passed: 0.0000 ... speed_up: 1.0000 ... meta: ... speed_up: 1.115 ...\"\n\n  It seems the reference timing is 0.0872 ms and the custom timing (if any) is 0.0782 ms? But the current program is the reference.\n\n  We are to write the Triton version to replace the entire forward pass.\n\n  Given the complexity, we might break the task:\n\n  Step 1: Write a Triton kernel for the linear layer (matmul) and the add (the two adds: linear bias and add_value) and then leave the activations to be done in a separate kernel? But that would still require two kernels and intermediate storage.\n\n  Step 2: Then we can try to fuse the activations.\n\n  But note: the activations are independent and pointwise. We can do a single kernel for the entire activation sequence? That would be easy and save memory round trips.\n\n  However, the problem says we are allowed to change the Triton kernel source only within the EVOLVE-BLOCK. And we are to return a complete new program.\n\n  We are going to replace the entire forward function of ModelNew with a Triton kernel call? And we must initialize the parameters appropriately.\n\n  We must be cautious: the model is an nn.Module. We have to store the weight, bias (from the linear layer) and the add_value as parameters.\n\n  Proposed plan for the code:\n\n      We remove the nn.Linear and store the weight and bias as parameters. Then in the forward, we call our Triton kernel.\n\n      Steps:\n\n        class ModelNew(nn.Module):\n            def __init__(self, in_features, out_features, add_value_shape):\n                super(ModelNew, self).__init__()\n                # Instead of nn.Linear, we create parameters\n                self.weight = nn.Parameter(torch.randn(out_features, in_features))\n                self.bias = nn.Parameter(torch.randn(out_features))\n                self.add_value = nn.Parameter(torch.randn(add_value_shape))\n\n            def forward(self, x):\n                # x: [batch_size, in_features]\n                # We want output: [batch_size, out_features]\n                output = torch.empty((x.shape[0], self.weight.shape[0]), device=x.device, dtype=x.dtype)\n\n                # We will write a Triton kernel that does:\n                #   output = (x @ self.weight.T) + self.bias + self.add_value\n                #   then apply the activations: swish, tanh, gelu, hardtanh\n\n                # Call the fused kernel\n                # We need to specify grid and block sizes.\n                # Let BLOCK_SIZE_BATCH = 32, BLOCK_SIZE_FEATURES = 128? We can tune.\n\n                # We'll design the kernel to be called with a 2D grid: [ceil(batch_size / BLOCK_SIZE_BATCH), ceil(out_features / BLOCK_SIZE_FEATURES)]\n\n                # We must also pass the activation functions? We'll code them inside the kernel.\n\n                # The kernel will be defined as:\n                #   @triton.jit\n                #   def _forward_kernel( ... )\n\n                # We'll define the kernel in this file.\n\n                # But note: the problem says to only output the code within the EVOLVE-BLOCK. So we have to put everything inside the block.\n\n                ... kernel code ...\n\n        We also have to set the batch_size, in_features, etc. as in the original.\n\n  Given the complexity and the line limit, we might not be able to write the entire optimized matmul kernel in one go. We can start with a simple kernel that does the entire forward pass for one output element? And then optimize step by step.\n\n  However, the problem says: \"Small iterative steps: Modify \u226420 lines per revision\"\n\n  We are starting from scratch. We are allowed to write a kernel that is not fully optimized and then evolve it? The problem says we are in Phase 1 (compilation pass) and then Phase 2 (correctness) and then Phase 3 (performance).\n\n  We are to write the entire Triton kernel in one block.\n\n  Considering the time, I will write a kernel that does the entire forward pass in a fused manner but without advanced matmul tiling. We'll do:\n\n      Each program handles one row of the batch and one output feature? That would be inefficient but simple.\n\n  Steps for the kernel:\n\n      pid = tl.program_id(0)   # which is the row index in the batch? and also the feature index? Actually, we can have a 1D grid of size batch_size * out_features.\n\n      Then:\n          row = pid // out_features\n          feat = pid % out_features\n\n          # Load the entire input row? But that would be inefficient because multiple threads in the same row would load the same input.\n\n  Alternatively, we can do:\n\n      pid = tl.program_id(0)   # row index in the batch\n      Then we loop over the features? But then we don't parallelize over features.\n\n  Given the constraints, we can do:\n\n      Let the grid be (batch_size, triton.cdiv(out_features, BLOCK_SIZE_FEATURES))\n\n      Then each program handles one row and a block of features.\n\n      We'll use:\n\n          pid_row = tl.program_id(0)\n          pid_feat_block = tl.program_id(1)\n\n          offsets_feat = pid_feat_block * BLOCK_SIZE_FEATURES + tl.arange(0, BLOCK_SIZE_FEATURES)\n          mask_feat = offsets_feat < out_features\n\n          # Load the weight block: [BLOCK_SIZE_FEATURES, in_features] -> but we don't have enough memory to load the entire weight block? We have to break the inner dimension.\n\n          # We break the inner dimension (in_features) into blocks of size BLOCK_SIZE_INNER.\n\n          accumulator = tl.zeros((BLOCK_SIZE_FEATURES,), dtype=tl.float32)\n          linear_bias_block = tl.load(linear_bias_ptr + offsets_feat, mask=mask_feat, other=0)\n          add_value_block = tl.load(add_value_ptr + offsets_feat, mask=mask_feat, other=0)\n\n          for i in range(0, in_features, BLOCK_SIZE_INNER):\n              offsets_inner = i + tl.arange(0, BLOCK_SIZE_INNER)\n              mask_inner = offsets_inner < in_features\n\n              # Load input block: [BLOCK_SIZE_INNER]\n              input_block = tl.load(input_ptr + pid_row * in_features + offsets_inner, mask=mask_inner, other=0)\n\n              # Load weight block: [BLOCK_SIZE_FEATURES, BLOCK_SIZE_INNER]\n              weight_block = tl.load(weight_ptr + offsets_feat[:, None] * in_features + offsets_inner[None, :], \n                                    mask=mask_feat[:, None] & mask_inner[None, :], \n                                    other=0)\n\n              accumulator += tl.sum(input_block[None, :] * weight_block, axis=1)\n\n          # Add bias and add_value\n          accumulator = accumulator + linear_bias_block + add_value_block\n\n          # Apply activations:\n          #   swish: sigmoid(accumulator) * accumulator\n          swish = accumulator * tl.sigmoid(accumulator)\n          #   tanh\n          tanh_val = tl.tanh(swish)\n          #   gelu: approximately? or use exact? Triton has tl.gelu?\n          #   Note: tl.gelu is available in triton.language.extra but we don't have that. Alternatively, we can approximate.\n          #   We'll use the exact GELU: x * 0.5 * (1 + erf(x / sqrt(2)))\n          #   But Triton has tl.erf? Yes, in the math module.\n          #   gelu = tanh_val * 0.5 * (1 + tl.erf(tanh_val / math.sqrt(2)))   -> wait, no: the standard gelu is for the input, but here we are applying after tanh? \n          #   Actually, we are applying in sequence: after tanh we do gelu? That is unusual, but we have to follow.\n\n          #   So: input to gelu is tanh_val.\n          gelu = tanh_val * 0.5 * (1 + tl.erf(tanh_val * 0.7071067811865476))   # 0.7071 ~= 1/sqrt(2)\n\n          #   hardtanh: clamp between -1 and 1\n          hardtanh = gelu\n          hardtanh = tl.where(hardtanh < -1, -1, hardtanh)\n          hardtanh = tl.where(hardtanh > 1, 1, hardtanh)\n\n          # Store the result for this block of features in the output row.\n          tl.store(output_ptr + pid_row * out_features + offsets_feat, hardtanh, mask=mask_feat)\n\n  This kernel is not optimized for shared memory or tensor cores, but it should work.\n\n  We must set the grid and block appropriately.\n\n  We'll choose:\n      BLOCK_SIZE_FEATURES = 128   # must be power of two? Not necessarily, but efficient.\n      BLOCK_SIZE_INNER = 32\n\n  We'll autotune these later.\n\n  Now, in the forward function:\n\n      output = torch.empty(x.shape[0], self.weight.shape[0], device=x.device, dtype=x.dtype)\n\n      grid = (x.shape[0], triton.cdiv(self.weight.shape[0], BLOCK_SIZE_FEATURES))\n\n      _forward_kernel[grid](x, self.weight, self.bias, self.add_value, output, ... )\n\n  We must pass the pointers and the constants: in_features, out_features.\n\n  We'll define the kernel above the forward function.\n\n  Given the complexity and the line limit, we will write the kernel and hope it compiles.\n\n  We'll use:\n      import triton\n      import triton.language as tl\n\n  But note: the original code did not import triton. We must import it.\n\n  We are allowed to change the entire block.\n\n  Let's write the code.\n\n  Important: We must not change the get_inputs and get_init_inputs.\n\n  We'll keep the batch_size, in_features, out_features, add_value_shape as in the original.\n\n  Note: the add_value_shape is (out_features,). So it is a vector.\n\n  Steps:\n\n      Replace the ModelNew class to have parameters: weight, bias, add_value.\n      Then in the forward, call the Triton kernel.\n\n  We'll define the kernel inside the class? Or outside? We can define it inside the EVOLVE-BLOCK.\n\n  We'll define it at the top of the block.\n\n  Given the line constraints, we'll write the kernel and hope it is within 20 lines? Actually, the kernel will be long. But the problem says \"Modify \u226420 lines per revision\" but we are starting from scratch. We are replacing the entire block.\n\n  We are allowed to replace the entire block.\n\n  We'll write:\n\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def _forward_kernel( ... ):\n          ... body ...\n\n  Then in the ModelNew.forward, we call the kernel.\n\n  But note: the kernel must be compiled and we must pass the constants.\n\n  Let's do it.\n\n  Note: We are not using tensor cores (FP16) because the input is float32? The original input is from torch.randn which is float32. We can change to float16? But the problem doesn't specify.\n\n  We'll assume float32.\n\n  We'll write the kernel in float32.\n\n  Let's code.\n</think>\nTo improve performance, I'll rewrite the model using Triton to fuse all operations into a single optimized kernel. This eliminates intermediate memory transfers and leverages GPU parallelism efficiently.\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = torch.nn.Parameter(torch.randn(out_features))\n        self.add_value = torch.nn.Parameter(torch.randn(add_value_shape))\n        self.in_features = in_features\n        self.out_features = out_features\n\n    @triton.jit\n    def _forward_kernel(\n        x_ptr, w_ptr, b_ptr, add_ptr, output_ptr,\n        in_features, out_features,\n        BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_FEAT: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n    ):\n        pid0 = tl.program_id(0)\n        pid1 = tl.program_id(1)\n        \n        batch_offset = pid0 * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n        feat_offset = pid1 * BLOCK_SIZE_FEAT + tl.arange(0, BLOCK_SIZE_FEAT)\n        k_offset = tl.arange(0, BLOCK_SIZE_K)\n        \n        batch_mask = batch_offset < tl.num_programs(0) * BLOCK_SIZE_BATCH\n        feat_mask = feat_offset < out_features\n        \n        acc = tl.zeros((BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT), dtype=tl.float32)\n        \n        for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n            k_offset = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n            k_mask = k_offset < in_features\n            \n            x_ptr_offset = (batch_offset[:, None] * in_features + k_offset[None, :])\n            w_ptr_offset = (feat_offset[:, None] * in_features + k_offset[None, :])\n            \n            x_block = tl.load(x_ptr + x_ptr_offset, mask=batch_mask[:, None] & k_mask[None, :], other=0.0)\n            w_block = tl.load(w_ptr + w_ptr_offset, mask=feat_mask[:, None] & k_mask[None, :], other=0.0)\n            \n            acc += tl.dot(x_block, w_block, allow_tf32=True)\n        \n        b = tl.load(b_ptr + feat_offset, mask=feat_mask, other=0.0)\n        add_val = tl.load(add_ptr + feat_offset, mask=feat_mask, other=0.0)\n        \n        res = acc + b[None, :] + add_val[None, :]\n        res = res * tl.sigmoid(res)  # Swish\n        res = tl.tanh(res)\n        res = res * 0.5 * (1.0 + tl.erf(res * 0.7071067811865475))  # GELU\n        res = tl.minimum(tl.maximum(res, -1.0), 1.0)  # Hardtanh\n        \n        out_offset = (batch_offset[:, None] * out_features + feat_offset[None, :])\n        tl.store(output_ptr + out_offset, res, mask=batch_mask[:, None] & feat_mask[None, :])\n\n    def forward(self, x):\n        output = torch.empty((x.size(0), self.out_features), device=x.device, dtype=x.dtype)\n        batch_size = x.size(0)\n        \n        grid_batch = triton.cdiv(batch_size, 32)\n        grid_feat = triton.cdiv(self.out_features, 128)\n        \n        self._forward_kernel[(grid_batch, grid_feat)](\n            x, self.weight, self.bias, self.add_value, output,\n            self.in_features, self.out_features,\n            BLOCK_SIZE_BATCH=32, BLOCK_SIZE_FEAT=128, BLOCK_SIZE_K=64\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
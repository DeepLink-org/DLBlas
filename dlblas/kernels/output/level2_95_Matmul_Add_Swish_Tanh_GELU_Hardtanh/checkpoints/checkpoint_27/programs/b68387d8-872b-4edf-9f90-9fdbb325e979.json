{"id": "b68387d8-872b-4edf-9f90-9fdbb325e979", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_activations_kernel(\n    x_ptr,\n    add_ptr,\n    output_ptr,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    col_mask = col_offsets < n_cols\n    \n    row_start = row_idx * n_cols\n    x_ptrs = x_ptr + row_start + col_offsets\n    add_ptrs = add_ptr + col_offsets\n    \n    # Vectorized load for better memory coalescing\n    x_vals = tl.load(x_ptrs, mask=col_mask, other=0.0)\n    add_vals = tl.load(add_ptrs, mask=col_mask, other=0.0)\n    \n    # Fused operations with intermediate reuse\n    y = x_vals + add_vals\n    sig_y = tl.sigmoid(y)\n    swish = y * sig_y\n    \n    # Use built-in tanh for faster computation\n    tanh = tl.tanh(swish)\n    \n    # Optimized GELU computation\n    z = tanh * 0.7071067811865475\n    erf_z = tl.erf(z)\n    gelu = 0.5 * tanh * (1.0 + erf_z)\n    \n    hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)\n    \n    # Vectorized store\n    tl.store(output_ptr + row_start + col_offsets, hardtanh, mask=col_mask)\n\ndef fused_activations(x, add_value):\n    output = torch.empty_like(x)\n    n_rows, n_cols = x.shape\n    grid = (n_rows,)\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    # Adjust warps based on block size\n    num_warps = max(1, min(BLOCK_SIZE // 32, 16))\n    _fused_activations_kernel[grid](\n        x, add_value, output, n_cols,\n        BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = fused_activations(x, self.add_value)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "f2421fbc-c876-4778-b6a9-81d214d003e3", "generation": 4, "timestamp": 1754589315.2270656, "iteration_found": 26, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_95_Matmul_Add_Swish_Tanh_GELU_Hardtanh", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmptf9w7zdc.py\", line 66, in forward\n    x = fused_activations(x, self.add_value)\n  File \"/tmp/tmptf9w7zdc.py\", line 52, in fused_activations\n    _fused_activations_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 0.0872, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 37.99999999999999, "speed_up": 1.18, "runtime_ms": 0.0739, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0872, "speed_up": 1.18, "custom_timing": 0.0739}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 1.0000\n- score: 38.0000\n- speed_up: 1.1800\n- runtime_ms: 0.0739\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.18, 'custom_timing': 0.0739}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: score, speed_up. Consider continuing with similar changes.\n- Metrics showing regression: runtime_ms. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 33.7000, speed_up: 1.1370, runtime_ms: 0.0767, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.137, 'custom_timing': 0.0767}\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 36.4000, speed_up: 1.1640, runtime_ms: 0.0749, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.164, 'custom_timing': 0.0749}\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 38.0000, speed_up: 1.1800, runtime_ms: 0.0739, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.18, 'custom_timing': 0.0739}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 38.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_activations_kernel(\n    x_ptr,\n    add_ptr,\n    output_ptr,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    col_mask = col_offsets < n_cols\n    \n    row_start = row_idx * n_cols\n    x_ptrs = x_ptr + row_start + col_offsets\n    add_ptrs = add_ptr + col_offsets\n    \n    # Vectorized load for better memory coalescing\n    x_vals = tl.load(x_ptrs, mask=col_mask, other=0.0)\n    add_vals = tl.load(add_ptrs, mask=col_mask, other=0.0)\n    \n    # Fused operations with intermediate reuse\n    y = x_vals + add_vals\n    sig_y = tl.sigmoid(y)\n    swish = y * sig_y\n    \n    # Optimized tanh computation\n    sig_swish = tl.sigmoid(2.0 * swish)\n    tanh = 2.0 * sig_swish - 1.0\n    \n    # Optimized GELU computation\n    z = tanh * 0.7071067811865475\n    erf_z = tl.erf(z)\n    gelu = 0.5 * tanh * (1.0 + erf_z)\n    \n    hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)\n    \n    # Vectorized store\n    tl.store(output_ptr + row_start + col_offsets, hardtanh, mask=col_mask)\n\ndef fused_activations(x, add_value):\n    output = torch.empty_like(x)\n    n_rows, n_cols = x.shape\n    grid = (n_rows,)\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    # Adjust warps based on block size\n    num_warps = max(1, min(BLOCK_SIZE // 32, 16))\n    _fused_activations_kernel[grid](\n        x, add_value, output, n_cols,\n        BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = fused_activations(x, self.add_value)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (38.0000), Performs well on speed_up (1.1800), Performs well on runtime_ms (0.0739), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.18, 'custom_timing': 0.0739})\n\n\n### Program 2 (Score: 36.4000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_activations_kernel(\n    x_ptr,\n    add_ptr,\n    output_ptr,\n    n_rows,\n    n_cols,\n    BLOCK_SIZE_R: tl.constexpr,\n    BLOCK_SIZE_C: tl.constexpr,\n):\n    pid_row = tl.program_id(0)\n    pid_col = tl.program_id(1)\n    \n    row_start = pid_row * BLOCK_SIZE_R\n    col_start = pid_col * BLOCK_SIZE_C\n    \n    col_offsets = col_start + tl.arange(0, BLOCK_SIZE_C)\n    col_mask = col_offsets < n_cols\n    \n    add_ptrs = add_ptr + col_offsets\n    add_vals = tl.load(add_ptrs, mask=col_mask, other=0.0)\n    \n    for r in range(BLOCK_SIZE_R):\n        row_idx = row_start + r\n        if row_idx < n_rows:\n            x_ptrs = x_ptr + row_idx * n_cols + col_offsets\n            x_vals = tl.load(x_ptrs, mask=col_mask, other=0.0)\n            \n            # Fused operations\n            y = x_vals + add_vals\n            swish = y * tl.sigmoid(y)\n            tanh = 2 * tl.sigmoid(2 * swish) - 1\n            gelu = 0.5 * tanh * (1.0 + tl.erf(tanh * 0.7071067811865475))\n            hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)\n            \n            out_ptrs = output_ptr + row_idx * n_cols + col_offsets\n            tl.store(out_ptrs, hardtanh, mask=col_mask)\n\ndef fused_activations(x, add_value):\n    output = torch.empty_like(x)\n    n_rows, n_cols = x.shape\n    BLOCK_SIZE_R = 4\n    BLOCK_SIZE_C = 128\n    grid = (triton.cdiv(n_rows, BLOCK_SIZE_R), triton.cdiv(n_cols, BLOCK_SIZE_C))\n    num_warps = BLOCK_SIZE_C // 32\n    _fused_activations_kernel[grid](\n        x, add_value, output, n_rows, n_cols,\n        BLOCK_SIZE_R=BLOCK_SIZE_R, \n        BLOCK_SIZE_C=BLOCK_SIZE_C,\n        num_warps=num_warps\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = fused_activations(x, self.add_value)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (36.4000), Performs well on speed_up (1.1640), Performs well on runtime_ms (0.0749), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.164, 'custom_timing': 0.0749})\n\n\n### Program 3 (Score: 33.7000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_activations_kernel(\n    x_ptr,\n    add_ptr,\n    output_ptr,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    col_mask = col_offsets < n_cols\n    \n    row_start = row_idx * n_cols\n    x_ptrs = x_ptr + row_start + col_offsets\n    add_ptrs = add_ptr + col_offsets\n    \n    x_vals = tl.load(x_ptrs, mask=col_mask, other=0.0)\n    add_vals = tl.load(add_ptrs, mask=col_mask, other=0.0)\n    \n    # Fused operations\n    y = x_vals + add_vals\n    swish = y * tl.sigmoid(y)\n    # Replace tl.tanh with equivalent: tanh(x) = 2*sigmoid(2*x) - 1\n    tanh = 2 * tl.sigmoid(2 * swish) - 1\n    gelu = 0.5 * tanh * (1.0 + tl.erf(tanh * 0.7071067811865475))\n    hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)\n    \n    tl.store(output_ptr + row_start + col_offsets, hardtanh, mask=col_mask)\n\ndef fused_activations(x, add_value):\n    output = torch.empty_like(x)\n    n_rows, n_cols = x.shape\n    grid = (n_rows,)\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    num_warps = BLOCK_SIZE // 32\n    _fused_activations_kernel[grid](\n        x, add_value, output, n_cols,\n        BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = fused_activations(x, self.add_value)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (33.7000), Performs well on speed_up (1.1370), Performs well on runtime_ms (0.0767), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0872, 'speed_up': 1.137, 'custom_timing': 0.0767})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_R': 1, 'BLOCK_SIZE_C': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_R': 2, 'BLOCK_SIZE_C': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_R': 4, 'BLOCK_SIZE_C': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_R': 8, 'BLOCK_SIZE_C': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_R': 1, 'BLOCK_SIZE_C': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_R': 2, 'BLOCK_SIZE_C': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_R': 4, 'BLOCK_SIZE_C': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_R': 1, 'BLOCK_SIZE_C': 64}, num_warps=2),\n        triton.Config({'BLOCK_SIZE_R': 2, 'BLOCK_SIZE_C': 64}, num_warps=2),\n        triton.Config({'BLOCK_SIZE_R': 4, 'BLOCK_SIZE_C': 64}, num_warps=2),\n    ],\n    key=['n_rows', 'n_cols'],\n)\n@triton.jit\ndef _fused_activations_kernel(\n    x_ptr,\n    add_ptr,\n    output_ptr,\n    n_rows,\n    n_cols,\n    BLOCK_SIZE_R: tl.constexpr,\n    BLOCK_SIZE_C: tl.constexpr,\n):\n    pid_row = tl.program_id(0)\n    pid_col = tl.program_id(1)\n    \n    row_start = pid_row * BLOCK_SIZE_R\n    col_start = pid_col * BLOCK_SIZE_C\n    \n    col_offsets = col_start + tl.arange(0, BLOCK_SIZE_C)\n    col_mask = col_offsets < n_cols\n    \n    add_ptrs = add_ptr + col_offsets\n    add_vals = tl.load(add_ptrs, mask=col_mask, other=0.0)\n    \n    for r in range(BLOCK_SIZE_R):\n        row_idx = row_start + r\n        if row_idx < n_rows:\n            x_ptrs = x_ptr + row_idx * n_cols + col_offsets\n            x_vals = tl.load(x_ptrs, mask=col_mask, other=0.0)\n            \n            # Fused operations\n            y = x_vals + add_vals\n            swish = y * tl.sigmoid(y)\n            tanh = 2 * tl.sigmoid(2 * swish) - 1\n            gelu = 0.5 * tanh * (1.0 + tl.erf(tanh * 0.7071067811865475))\n            hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)\n            \n            out_ptrs = output_ptr + row_idx * n_cols + col_offsets\n            tl.store(out_ptrs, hardtanh, mask=col_mask)\n\ndef fused_activations(x, add_value):\n    output = torch.empty_like(x)\n    n_rows, n_cols = x.shape\n    grid = (triton.cdiv(n_rows, 4), triton.cdiv(n_cols, 128))\n    _fused_activations_kernel[grid](\n        x, add_value, output, n_rows, n_cols,\n        BLOCK_SIZE_R=4, \n        BLOCK_SIZE_C=128,\n        num_warps=4\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = fused_activations(x, self.add_value)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128}, num_warps=8),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef _forward_kernel(\n    input_ptr, weight_ptr, bias_ptr, add_value_ptr, output_ptr,\n    in_features, out_features, batch_size,\n    stride_input0, stride_input1,\n    stride_weight0, stride_weight1,\n    stride_output0, stride_output1,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    n_offsets = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = n_offsets < out_features\n    \n    input_row_start = pid_batch * stride_input0\n    output_row_start = pid_batch * stride_output0\n    \n    accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        mask_k = k_offsets < in_features\n        \n        input_block = tl.load(\n            input_ptr + input_row_start + k_offsets,\n            mask=mask_k, other=0.0\n        )\n        \n        weight_block_ptr = weight_ptr + n_offsets[:, None] * stride_weight0 + k_offsets[None, :]\n        weight_block = tl.load(\n            weight_block_ptr,\n            mask=mask_n[:, None] & mask_k[None, :], other=0.0\n        )\n        \n        accumulator += tl.sum(weight_block * input_block[None, :], axis=1)\n    \n    bias_block = tl.load(bias_ptr + n_offsets, mask=mask_n, other=0.0)\n    accumulator += bias_block\n    \n    add_value_block = tl.load(add_value_ptr + n_offsets, mask=mask_n, other=0.0)\n    accumulator += add_value_block\n    \n    swished = accumulator * tl.sigmoid(accumulator)\n    tanhed = tl.tanh(swished)\n    inner = tanhed * 0.7978845608028654 * (1 + 0.044715 * (tanhed * tanhed))\n    gelu_output = 0.5 * tanhed * (1 + tl.tanh(inner))\n    hardtanh_output = tl.minimum(tl.maximum(gelu_output, -1.0), 1.0)\n    \n    output_block_ptr = output_ptr + output_row_start + n_offsets\n    tl.store(output_block_ptr, hardtanh_output, mask=mask_n)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape))\n        self.in_features = in_features\n        self.out_features = out_features\n\n    def forward(self, x):\n        output = torch.empty(\n            (x.size(0), self.out_features), \n            device=x.device, dtype=x.dtype\n        )\n        \n        grid = (x.size(0), triton.cdiv(self.out_features, 128))\n        \n        _forward_kernel[grid](\n            x, self.matmul.weight, self.matmul.bias, self.add_value, output,\n            self.in_features, self.out_features, x.size(0),\n            x.stride(0), x.stride(1),\n            self.matmul.weight.stride(0), self.matmul.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_N=128, BLOCK_SIZE_K=64\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n@triton.jit\ndef _fused_activations_kernel(\n    x_ptr,\n    add_ptr,\n    output_ptr,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    col_mask = col_offsets < n_cols\n    \n    row_start = row_idx * n_cols\n    x_ptrs = x_ptr + row_start + col_offsets\n    add_ptrs = add_ptr + col_offsets\n    \n    x_vals = tl.load(x_ptrs, mask=col_mask, other=0.0)\n    add_vals = tl.load(add_ptrs, mask=col_mask, other=0.0)\n    \n    # Fused operations\n    y = x_vals + add_vals\n    swish = y * tl.sigmoid(y)\n    tanh = libdevice.tanh(swish)\n    \n    # Optimized GELU approximation\n    gelu = 0.5 * tanh * (1.0 + tl.tanh(0.7978845608 * (tanh + 0.044715 * tanh * tanh * tanh)))\n    hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)\n    \n    tl.store(output_ptr + row_start + col_offsets, hardtanh, mask=col_mask)\n\ndef fused_activations(x, add_value):\n    output = torch.empty_like(x)\n    n_rows, n_cols = x.shape\n    grid = (n_rows,)\n    BLOCK_SIZE = min(triton.next_power_of_2(n_cols), 1024)\n    num_warps = 4 if BLOCK_SIZE <= 256 else 8\n    _fused_activations_kernel[grid](\n        x, add_value, output, n_cols,\n        BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = fused_activations(x, self.add_value)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_activations_kernel(\n    x_ptr,\n    add_ptr,\n    output_ptr,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    col_mask = col_offsets < n_cols\n    \n    row_start = row_idx * n_cols\n    x_ptrs = x_ptr + row_start + col_offsets\n    add_ptrs = add_ptr + col_offsets\n    \n    x_vals = tl.load(x_ptrs, mask=col_mask, other=0.0)\n    add_vals = tl.load(add_ptrs, mask=col_mask, other=0.0)\n    \n    # Fused operations\n    y = x_vals + add_vals\n    swish = y * tl.sigmoid(y)\n    tanh = tl.tanh(swish)\n    gelu = 0.5 * tanh * (1.0 + tl.erf(tanh * 0.7071067811865475))\n    hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)\n    \n    tl.store(output_ptr + row_start + col_offsets, hardtanh, mask=col_mask)\n\ndef fused_activations(x, add_value):\n    output = torch.empty_like(x)\n    n_rows, n_cols = x.shape\n    grid = (n_rows,)\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    num_warps = BLOCK_SIZE // 32\n    _fused_activations_kernel[grid](\n        x, add_value, output, n_cols,\n        BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = fused_activations(x, self.add_value)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_R': 1, 'BLOCK_SIZE_C': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_R': 2, 'BLOCK_SIZE_C': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_R': 4, 'BLOCK_SIZE_C': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_R': 8, 'BLOCK_SIZE_C': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_R': 1, 'BLOCK_SIZE_C': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_R': 2, 'BLOCK_SIZE_C': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_R': 4, 'BLOCK_SIZE_C': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_R': 1, 'BLOCK_SIZE_C': 64}, num_warps=2),\n        triton.Config({'BLOCK_SIZE_R': 2, 'BLOCK_SIZE_C': 64}, num_warps=2),\n        triton.Config({'BLOCK_SIZE_R': 4, 'BLOCK_SIZE_C': 64}, num_warps=2),\n    ],\n    key=['n_rows', 'n_cols'],\n)\n@triton.jit\ndef _fused_activations_kernel(\n    x_ptr,\n    add_ptr,\n    output_ptr,\n    n_rows,\n    n_cols,\n    BLOCK_SIZE_R: tl.constexpr,\n    BLOCK_SIZE_C: tl.constexpr,\n):\n    pid_row = tl.program_id(0)\n    pid_col = tl.program_id(1)\n    \n    row_start = pid_row * BLOCK_SIZE_R\n    col_start = pid_col * BLOCK_SIZE_C\n    \n    col_offsets = col_start + tl.arange(0, BLOCK_SIZE_C)\n    col_mask = col_offsets < n_cols\n    \n    add_ptrs = add_ptr + col_offsets\n    add_vals = tl.load(add_ptrs, mask=col_mask, other=0.0)\n    \n    for r in range(BLOCK_SIZE_R):\n        row_idx = row_start + r\n        if row_idx < n_rows:\n            x_ptrs = x_ptr + row_idx * n_cols + col_offsets\n            x_vals = tl.load(x_ptrs, mask=col_mask, other=0.0)\n            \n            # Fused operations\n            y = x_vals + add_vals\n            swish = y * tl.sigmoid(y)\n            tanh = 2 * tl.sigmoid(2 * swish) - 1\n            gelu = 0.5 * tanh * (1.0 + tl.erf(tanh * 0.7071067811865475))\n            hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)\n            \n            out_ptrs = output_ptr + row_idx * n_cols + col_offsets\n            tl.store(out_ptrs, hardtanh, mask=col_mask)\n\ndef fused_activations(x, add_value):\n    output = torch.empty_like(x)\n    n_rows, n_cols = x.shape\n    grid = (triton.cdiv(n_rows, 4), triton.cdiv(n_cols, 128))\n    _fused_activations_kernel[grid](\n        x, add_value, output, n_rows, n_cols,\n        BLOCK_SIZE_R=4, \n        BLOCK_SIZE_C=128,\n        num_warps=4\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = fused_activations(x, self.add_value)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: 0.0000, Type: Exploratory)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, adds a value, applies Swish, Tanh, GELU, and Hardtanh activation functions.\n    \"\"\"\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x + self.add_value\n        x = torch.sigmoid(x) * x # Swish\n        x = torch.tanh(x)\n        x = torch.nn.functional.gelu(x) # GELU\n        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nUnique approach: Excellent compiled (1.000), Excellent correctness (1.000), Alternative passed approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128}, num_warps=8),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef _forward_kernel(\n    input_ptr, weight_ptr, bias_ptr, add_value_ptr, output_ptr,\n    in_features, out_features, batch_size,\n    stride_input0, stride_input1,\n    stride_weight0, stride_weight1,\n    stride_output0, stride_output1,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    n_offsets = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = n_offsets < out_features\n    \n    input_row_start = pid_batch * stride_input0\n    output_row_start = pid_batch * stride_output0\n    \n    accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        mask_k = k_offsets < in_features\n        \n        input_block = tl.load(\n            input_ptr + input_row_start + k_offsets,\n            mask=mask_k, other=0.0\n        )\n        \n        weight_block_ptr = weight_ptr + n_offsets[:, None] * stride_weight0 + k_offsets[None, :]\n        weight_block = tl.load(\n            weight_block_ptr,\n            mask=mask_n[:, None] & mask_k[None, :], other=0.0\n        )\n        \n        accumulator += tl.sum(weight_block * input_block[None, :], axis=1)\n    \n    bias_block = tl.load(bias_ptr + n_offsets, mask=mask_n, other=0.0)\n    accumulator += bias_block\n    \n    add_value_block = tl.load(add_value_ptr + n_offsets, mask=mask_n, other=0.0)\n    accumulator += add_value_block\n    \n    swished = accumulator * tl.sigmoid(accumulator)\n    tanhed = tl.tanh(swished)\n    inner = tanhed * 0.7978845608028654 * (1 + 0.044715 * (tanhed * tanhed))\n    gelu_output = 0.5 * tanhed * (1 + tl.tanh(inner))\n    hardtanh_output = tl.minimum(tl.maximum(gelu_output, -1.0), 1.0)\n    \n    output_block_ptr = output_ptr + output_row_start + n_offsets\n    tl.store(output_block_ptr, hardtanh_output, mask=mask_n)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape))\n        self.in_features = in_features\n        self.out_features = out_features\n\n    def forward(self, x):\n        output = torch.empty(\n            (x.size(0), self.out_features), \n            device=x.device, dtype=x.dtype\n        )\n        \n        grid = (x.size(0), triton.cdiv(self.out_features, 128))\n        \n        _forward_kernel[grid](\n            x, self.matmul.weight, self.matmul.bias, self.add_value, output,\n            self.in_features, self.out_features, x.size(0),\n            x.stride(0), x.stride(1),\n            self.matmul.weight.stride(0), self.matmul.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_N=128, BLOCK_SIZE_K=64\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_activations_kernel(\n    x_ptr,\n    add_ptr,\n    output_ptr,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    col_mask = col_offsets < n_cols\n    \n    row_start = row_idx * n_cols\n    x_ptrs = x_ptr + row_start + col_offsets\n    add_ptrs = add_ptr + col_offsets\n    \n    # Vectorized load for better memory coalescing\n    x_vals = tl.load(x_ptrs, mask=col_mask, other=0.0)\n    add_vals = tl.load(add_ptrs, mask=col_mask, other=0.0)\n    \n    # Fused operations with intermediate reuse\n    y = x_vals + add_vals\n    sig_y = tl.sigmoid(y)\n    swish = y * sig_y\n    \n    # Optimized tanh computation\n    sig_swish = tl.sigmoid(2.0 * swish)\n    tanh = 2.0 * sig_swish - 1.0\n    \n    # Optimized GELU computation\n    z = tanh * 0.7071067811865475\n    erf_z = tl.erf(z)\n    gelu = 0.5 * tanh * (1.0 + erf_z)\n    \n    hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)\n    \n    # Vectorized store\n    tl.store(output_ptr + row_start + col_offsets, hardtanh, mask=col_mask)\n\ndef fused_activations(x, add_value):\n    output = torch.empty_like(x)\n    n_rows, n_cols = x.shape\n    grid = (n_rows,)\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    # Adjust warps based on block size\n    num_warps = max(1, min(BLOCK_SIZE // 32, 16))\n    _fused_activations_kernel[grid](\n        x, add_value, output, n_cols,\n        BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = fused_activations(x, self.add_value)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program which is performing well (score 38, speed_up 1.18) but we are always looking to improve.\n\nThe current kernel uses a 1D grid with each block processing a row. The block size is set to the next power of two of the number of columns (n_cols). The operations are fused and we have optimized the computation of tanh and gelu.\n\nHowever, note that the current implementation computes the tanh and gelu in a way that might not be the most efficient. Also, we are using a single block per row, which might be efficient for rows that are long enough, but we might be able to optimize the memory access patterns further.\n\nLooking at the inspiration programs, we see that some use a 2D grid and autotuning. However, the top performer (Program 1) uses a 1D grid and vectorized loads/stores.\n\nAreas for potential improvement:\n\n1. We are using an optimized tanh and gelu, but note that the gelu is computed on the tanh result. However, the original sequence was:\n      y = x + add\n      swish = y * sigmoid(y)\n      tanh = tanh(swish)   [but we are computing tanh via sigmoid: 2*sigmoid(2*x)-1]\n      gelu = 0.5 * tanh * (1 + erf(tanh / sqrt(2)))   [but note: we are using tanh * 0.707... which is tanh / sqrt(2) for erf?]\n\nBut wait, the current code does:\n      z = tanh * 0.7071067811865475   [which is 1/sqrt(2)]\n      erf_z = tl.erf(z)\n      gelu = 0.5 * tanh * (1.0 + erf_z)\n\nThis is actually the gelu of `tanh`? But the original model was supposed to do:\n      x -> linear -> add -> swish -> tanh -> gelu -> hardtanh\n\nSo we are applying gelu to the tanh result? That might be the requirement.\n\nHowever, note that the GELU formula is: GELU(x) = 0.5 * x * (1 + erf(x / sqrt(2)))\n\nIn our case, we are computing:\n      tanh = ...   (from the swish)\n      then we want gelu(tanh) = 0.5 * tanh * (1 + erf(tanh / sqrt(2)))\n\nSo the current code is correct.\n\nBut we can consider using an approximation for erf to speed up? However, Triton's `tl.erf` is likely optimized.\n\nAlternatively, we can try to fuse the operations to reduce the number of passes.\n\nBut note: the current code is already fused.\n\nAnother idea: we are computing the tanh via two sigmoids? Actually, we have:\n   swish = y * sigmoid(y)   [one sigmoid]\n   then for tanh: we do sigmoid(2*swish) and then linear transform -> that's two sigmoids per element.\n\nWe are also doing an erf in the gelu. So that's a lot of transcendental functions.\n\nWe might try to use fewer operations by using approximations? But we must be numerically correct.\n\nAlternatively, we can try to use the hardware's fast math capabilities by using vectorized operations and ensuring we are using the right data types.\n\nHowever, the current program is already using vectorized loads and stores.\n\nAnother idea: the block size is set to the next power of two of n_cols. This might lead to some inefficiency when n_cols is not a power of two because of masking. But that is handled.\n\nBut note: the top performer (Program 1) has a speedup of 1.18. We want to push it further.\n\nLooking at the Inspiration 1, they used a different approximation for GELU that avoids erf? They did:\n    gelu = 0.5 * tanh * (1.0 + tl.tanh(0.7978845608 * (tanh + 0.044715 * tanh * tanh * tanh)))\n\nBut that is an approximation for the GELU of the original input, not for the tanh? And it's applied to tanh? That might not be the same.\n\nAlternatively, we can try to rearrange the operations to reduce the number of passes.\n\nBut note: the current kernel does:\n\n    y = x_vals + add_vals\n    sig_y = tl.sigmoid(y)   [1]\n    swish = y * sig_y        [2]\n    sig_swish = tl.sigmoid(2.0 * swish)   [3]\n    tanh = 2.0 * sig_swish - 1.0   [4]\n    z = tanh * 0.7071067811865475   [5]\n    erf_z = tl.erf(z)               [6]\n    gelu = 0.5 * tanh * (1.0 + erf_z)   [7]\n    hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)   [8]\n\nThat's 8 steps per element.\n\nWe might try to combine some steps? Or use fewer transcendental functions.\n\nAlternatively, note that the current kernel does not use any shared memory. Since the same `add_vals` are used for every row, we could load them into shared memory to reduce global memory access? But the add_vals are only loaded once per block (and each block handles one row). So we are already loading them once per row. And the kernel is row-parallel. So that might not help.\n\nHowever, if we change the kernel to process multiple rows per block, then we could reuse the add_vals. But that would require a different grid structure.\n\nBut note: the current top performer uses one row per block. And the number of rows is 128 (batch size) and columns 512. So we have 128 blocks. That is not too many, but we might be able to process multiple rows per block to reduce the overhead of loading add_vals? However, the add_vals are only 512 elements, and we are loading them once per row (so 128 times). That is 128*512=65536 elements, which is the same as the entire add_value tensor. So we are reading the entire add_value tensor 128 times. That is inefficient.\n\nWe can change the kernel to load the add_value tensor once and then reuse it for all rows. How?\n\nWe could have the entire add_value tensor in shared memory? But note: the add_value tensor is of size (out_features) = 512. We can load it into shared memory once per block, and then the block processes multiple rows.\n\nSo we can restructure the kernel to have a 1D grid of blocks, each block processing multiple rows (e.g., 4, 8, ...). Then:\n\n  1. Load the entire add_value tensor into shared memory (if it fits). We have 512 elements, so 512*4=2048 bytes for float32, which fits in shared memory (64KB per block).\n\n  2. Then, for each row in the block, we do:\n        load a row of x\n        compute y = x_row + add_vals (from shared memory)\n        ... then the rest ...\n\nThis would reduce the global memory accesses for add_value from (n_rows * n_cols) to (n_cols) per block (and then shared memory for each row in the block).\n\nBut note: we are currently reading add_value from global memory once per row (so 128 times). With the new approach, we would read it once per block (so 128 / rows_per_block times). And then we read it from shared memory for each row in the block.\n\nLet's assume we set rows_per_block = 4. Then we would read add_value 32 times (128/4) from global memory. That is a 4x reduction.\n\nHowever, we must be cautious because shared memory is limited. We have 512 elements per add_value. If we use float32, that's 2048 bytes. We can have up to 48 KB shared memory per block (on H100). So we can easily fit multiple rows and the add_value.\n\nBut note: we are also using the block for the columns. The current BLOCK_SIZE is 512 (next power of two of 512 is 512). So we have 512 columns. We are already using 512 threads per block? That is the maximum number of threads per block (1024 for H100, but 512 is safe). So we cannot have multiple rows per block without reducing the number of threads per row? Or we can use a 2D block?\n\nAlternatively, we can use a 1D block that handles multiple rows by looping. For example:\n\n  Let BLOCK_SIZE_R = number of rows per block (e.g., 4)\n  Let BLOCK_SIZE_C = number of columns per block (must be <= 1024, but we have 512 so we can use 512 threads)\n\n  But then we have 4*512=2048 threads per block? That is too many (max 1024). So we cannot.\n\nInstead, we can have a 2D block? But Triton currently only supports 1D blocks.\n\nAlternatively, we can have a 1D block that loops over multiple rows. We assign each thread to multiple rows? For example, we can have:\n\n  blockDim = 512 (threads)\n  We have 4 rows per block -> each thread must process 4 elements (one per row) in the same column.\n\nBut then we can do:\n\n  We load the entire add_value into shared memory (512 elements) for the block.\n\n  Then, we have:\n      row_offset = 0 to 3 (for 4 rows)\n      col_offset = thread index\n\n  Then, each thread loads 4 elements from x (from 4 consecutive rows at the same column) and then adds the add_value at that column (from shared memory). Then it computes the activations for all 4.\n\n  We would then store 4 elements.\n\nThis would require that we have 512 threads per block and we process 4 rows at a time.\n\nThe grid would then be (n_rows // 4, )? But if n_rows is 128, then grid would be 32.\n\nBut note: the kernel currently uses a grid of (n_rows,). We are changing it to (ceil(n_rows / rows_per_block),). And we set rows_per_block=4.\n\nWe would then have:\n\n  pid = tl.program_id(0)\n  row_block_start = pid * rows_per_block\n  row_indices = [row_block_start + i for i in range(rows_per_block)]\n\n  Then, for each row_index in row_indices (if valid), we do the computation.\n\nBut how do we handle the case when n_rows is not divisible by rows_per_block? We use masking for the rows.\n\nThis approach would reduce the number of times we load the add_value tensor from global memory by a factor of rows_per_block.\n\nHowever, note that we are also loading the x tensor. Currently, we are loading x in a contiguous manner per row. In the new approach, we would load non-contiguous elements (if rows are consecutive, then they are contiguous in memory? Actually, the rows are contiguous in memory? The tensor is (n_rows, n_cols). So consecutive rows are consecutive in memory. But we are loading the same column across multiple rows? That is strided. However, we are using vectorized loads? We can do:\n\n  We can have each thread loading 4 elements from the same column? That would be 4*4=16 bytes (if float32) per thread, and 512 threads -> 8KB. But we are loading from 4 consecutive rows? The addresses for the same column in 4 consecutive rows are:\n\n      row0: base + col\n      row1: base + n_cols + col\n      row2: base + 2*n_cols + col\n      row3: base + 3*n_cols + col\n\n  So the addresses are not contiguous. This might cause uncoalesced access.\n\nAlternatively, we can load the entire block of 4 rows and 512 columns in a coalesced manner? We can use a 2D block in Triton? But Triton doesn't support 2D blocks. We can use a 1D block and then have each thread load multiple contiguous columns for one row? But then we are back to the original design for the row.\n\nGiven the complexity and the fact that we are already at speedup 1.18, and that the add_value is only 512 elements (so 2048 bytes for float32) and we are reading it 128 times (262144 bytes) which is not a lot compared to the entire x tensor (128*512*4=262144 bytes) and the output (same). So the total global memory traffic is:\n\n   x: 262144\n   add_value: 128*2048? -> no, we are reading the entire add_value (2048 bytes) 128 times -> 2048*128 = 262144 bytes\n   output: 262144\n\nTotal: 262144*3 = 786432 bytes.\n\nIf we reduce the add_value reads to 32 times (if we do 4 rows per block), then add_value traffic becomes 32*2048=65536 bytes, which is a reduction of 196608 bytes. That is about 25% of the total traffic? So it might help.\n\nBut we must change the memory access pattern for x and output to strided, which might be slower.\n\nAlternatively, we can load the add_value once per block into shared memory and then each thread loads one element from shared memory (for the column) and then uses it for 4 rows. Then, for x, we do:\n\n   for row_index in range(4):\n        x_ptr = ... + (row_block_start + row_index)*n_cols + col_offset\n\n   and then load 4 elements from x (strided) and then add the same add_value element (from shared memory) to each.\n\nBut the strided load for x might be inefficient.\n\nGiven the trade-off, it might not be worth it. And note that the L1 cache might be capturing the add_value reads because the same 2048 bytes are being read repeatedly.\n\nAlternatively, we can try to use the constant memory for add_value? But Triton does not have explicit constant memory.\n\nConsidering the above, and that we are already at a good speedup, we might try a different approach: optimize the computation by reducing the number of transcendental functions.\n\nWe note that we are computing two sigmoids and one erf per element. The sigmoid and erf are expensive. Can we approximate the entire function chain?\n\nBut we must be numerically correct.\n\nAlternatively, we can try to use the fast math flags in Triton? But we are not using any.\n\nAnother idea: we are computing the tanh via two sigmoids? We could use the built-in `tl.tanh` for the tanh? But the built-in tanh might be implemented with the same approximation? And we are using an approximation for tanh that is numerically stable? We might try to replace:\n\n   tanh = tl.tanh(swish)\n\nand then remove the two sigmoids for tanh? That would save one sigmoid.\n\nBut note: the current code for tanh is:\n\n   sig_swish = tl.sigmoid(2.0 * swish)\n   tanh = 2.0 * sig_swish - 1.0\n\nThat is one sigmoid. And the built-in `tl.tanh` might be just as fast? Or faster? We don't know.\n\nLet's try to replace the two lines with:\n\n   tanh = tl.tanh(swish)\n\nThen we have:\n\n   y = x_vals + add_vals\n   sig_y = tl.sigmoid(y)   [one sigmoid]\n   swish = y * sig_y\n   tanh = tl.tanh(swish)   [one tanh]\n   ... rest ...\n\nThat reduces the number of sigmoids from two to one? (because we removed the sigmoid for the tanh approximation). But note: we still have the sigmoid for swish and then the tanh.\n\nSo we are replacing one sigmoid by one tanh. Is that faster? We don't know, but we can try.\n\nAlternatively, we can use the approximation for tanh that uses exp? But Triton's built-in tanh is likely optimized.\n\nLet's change the code to use `tl.tanh` for the tanh computation and see if it improves.\n\nWe'll change:\n\n   # Remove: sig_swish = tl.sigmoid(2.0 * swish)\n   # Remove: tanh = 2.0 * sig_swish - 1.0\n   # Replace with:\n   tanh = tl.tanh(swish)\n\nThen, we also remove the two lines.\n\nThis change reduces the number of operations.\n\nWe must be cautious because the numerical results might change slightly, but the tolerance in the test is float32.\n\nLet's try this change.\n\nAdditionally, we can try to use the built-in gelu? Triton has `tl.gelu`? But looking at the Triton language documentation, there is no built-in gelu. So we have to keep our erf-based gelu.\n\nAlternatively, we can use an approximation for gelu? But the current one is exact? And we are already using the erf.\n\nBut note: the gelu is applied to the tanh result. We are doing:\n\n   gelu = 0.5 * tanh * (1 + erf(tanh / sqrt(2)))\n\nThat is the exact gelu of the tanh.\n\nBut the original model intended to do:\n\n   x -> linear -> add -> swish -> tanh -> gelu -> hardtanh\n\nSo we are doing the exact computation.\n\nGiven the above, we will try to replace the tanh computation with the built-in `tl.tanh` to save one sigmoid.\n\nWe also note that the current code for the gelu uses an erf and a multiplication. We cannot avoid that.\n\nSo the plan:\n\n  1. Replace the custom tanh (via sigmoid) with the built-in `tl.tanh`.\n  2. This will remove one sigmoid per element.\n\nWe hope that the built-in `tl.tanh` is at least as fast as our custom one (which required one sigmoid and some arithmetic).\n\nLet's write the new kernel code:\n\n    y = x_vals + add_vals\n    sig_y = tl.sigmoid(y)\n    swish = y * sig_y\n    tanh = tl.tanh(swish)   # This is the change\n    z = tanh * 0.7071067811865475\n    erf_z = tl.erf(z)\n    gelu = 0.5 * tanh * (1.0 + erf_z)\n    hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)\n\nWe keep the rest the same.\n\nWe also note that the current program uses a variable `BLOCK_SIZE` and adjusts the number of warps based on the block size. We leave that unchanged.\n\nWe output the entire code block.\n\nNote: We are only changing the kernel body (a few lines). We are not changing the function signature, grid, or anything else.\n\nLet's do it.\n\nBut note: the current program is the top performer. We are making a small change to the computation.\n\nWe must also note that the built-in `tl.tanh` might be more accurate than our approximation? Or less? But it should be within tolerance.\n\nWe output the entire code block with the change.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_activations_kernel(\n    x_ptr,\n    add_ptr,\n    output_ptr,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    col_mask = col_offsets < n_cols\n    \n    row_start = row_idx * n_cols\n    x_ptrs = x_ptr + row_start + col_offsets\n    add_ptrs = add_ptr + col_offsets\n    \n    # Vectorized load for better memory coalescing\n    x_vals = tl.load(x_ptrs, mask=col_mask, other=0.0)\n    add_vals = tl.load(add_ptrs, mask=col_mask, other=0.0)\n    \n    # Fused operations with intermediate reuse\n    y = x_vals + add_vals\n    sig_y = tl.sigmoid(y)\n    swish = y * sig_y\n    \n    # Use built-in tanh for faster computation\n    tanh = tl.tanh(swish)\n    \n    # Optimized GELU computation\n    z = tanh * 0.7071067811865475\n    erf_z = tl.erf(z)\n    gelu = 0.5 * tanh * (1.0 + erf_z)\n    \n    hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)\n    \n    # Vectorized store\n    tl.store(output_ptr + row_start + col_offsets, hardtanh, mask=col_mask)\n\ndef fused_activations(x, add_value):\n    output = torch.empty_like(x)\n    n_rows, n_cols = x.shape\n    grid = (n_rows,)\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    # Adjust warps based on block size\n    num_warps = max(1, min(BLOCK_SIZE // 32, 16))\n    _fused_activations_kernel[grid](\n        x, add_value, output, n_cols,\n        BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape)) \n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = fused_activations(x, self.add_value)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
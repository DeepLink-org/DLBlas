{"id": "415e20fd-c003-440e-81d8-b002f33d3404", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_forward_kernel(\n    x_ptr, weight_ptr, bias_ptr, add_value_ptr, output_ptr,\n    batch_size, in_features, out_features,\n    BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_FEAT: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_feat = tl.program_id(1)\n    \n    batch_offsets = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feat_offsets = pid_feat * BLOCK_SIZE_FEAT + tl.arange(0, BLOCK_SIZE_FEAT)\n    k_offsets = tl.arange(0, BLOCK_SIZE_K)\n    \n    batch_mask = batch_offsets < batch_size\n    feat_mask = feat_offsets < out_features\n    \n    accumulator = tl.zeros((BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_mask = k_offsets < (in_features - k)\n        input_mask = batch_mask[:, None] & k_mask[None, :]\n        weight_mask = feat_mask[:, None] & k_mask[None, :]\n        \n        x_offset = batch_offsets[:, None] * in_features + (k + k_offsets[None, :])\n        w_offset = feat_offsets[:, None] * in_features + (k + k_offsets[None, :])\n        \n        x_tile = tl.load(x_ptr + x_offset, mask=input_mask, other=0.0)\n        w_tile = tl.load(weight_ptr + w_offset, mask=weight_mask, other=0.0)\n        \n        # Fixed dot product without trans_b\n        w_tile_trans = tl.trans(w_tile)\n        accumulator += tl.dot(x_tile, w_tile_trans)\n    \n    bias_vals = tl.load(bias_ptr + feat_offsets, mask=feat_mask, other=0.0)\n    add_vals = tl.load(add_value_ptr + feat_offsets, mask=feat_mask, other=0.0)\n    \n    result = accumulator + bias_vals[None, :] + add_vals[None, :]\n    \n    # Fused activations with optimized tanh\n    swish = tl.sigmoid(result) * result\n    tanh = 2 * tl.sigmoid(2 * swish) - 1  # Faster approximation\n    gelu_input = tanh * 0.7071067811865475\n    gelu = 0.5 * tanh * (1.0 + tl.erf(gelu_input))\n    hardtanh = tl.minimum(tl.maximum(gelu, -1.0), 1.0)\n    \n    output_offset = batch_offsets[:, None] * out_features + feat_offsets[None, :]\n    output_mask = batch_mask[:, None] & feat_mask[None, :]\n    tl.store(output_ptr + output_offset, hardtanh, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, add_value_shape):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.add_value = nn.Parameter(torch.randn(add_value_shape))\n    \n    def forward(self, x):\n        weight = self.matmul.weight\n        bias = self.matmul.bias\n        add_value = self.add_value\n        \n        batch_size, in_dim = x.shape\n        out_dim = weight.shape[0]\n        output = torch.empty((batch_size, out_dim), device=x.device, dtype=x.dtype)\n        \n        # Optimized block sizes for Tensor Cores\n        BLOCK_SIZE_BATCH = 64\n        BLOCK_SIZE_FEAT = 64\n        BLOCK_SIZE_K = 64\n        \n        grid = (\n            triton.cdiv(batch_size, BLOCK_SIZE_BATCH),\n            triton.cdiv(out_dim, BLOCK_SIZE_FEAT)\n        )\n        \n        _fused_forward_kernel[grid](\n            x, weight, bias, add_value, output,\n            batch_size, in_dim, out_dim,\n            BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT, BLOCK_SIZE_K,\n            num_warps=8\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nadd_value_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, add_value_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "3a4982e6-babf-4a23-832d-d71594e8955e", "generation": 3, "timestamp": 1754589756.5561078, "iteration_found": 28, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 81.80000000000001, "speed_up": 1.618, "runtime_ms": 0.0539, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0872, "speed_up": 1.618, "custom_timing": 0.0539}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_95_Matmul_Add_Swish_Tanh_GELU_Hardtanh", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\nTypeError: dot() got an unexpected keyword argument 'trans_b'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpsns60l5k.py\", line 85, in forward\n    _fused_forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 29:23:\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_mask = k_offsets < (in_features - k)\n        input_mask = batch_mask[:, None] & k_mask[None, :]\n        weight_mask = feat_mask[:, None] & k_mask[None, :]\n\n        x_offset = batch_offsets[:, None] * in_features + (k + k_offsets[None, :])\n        w_offset = feat_offsets[:, None] * in_features + (k + k_offsets[None, :])\n\n        x_tile = tl.load(x_ptr + x_offset, mask=input_mask, other=0.0)\n        w_tile = tl.load(weight_ptr + w_offset, mask=weight_mask, other=0.0)\n\n        accumulator += tl.dot(x_tile, w_tile, trans_b=True)\n                       ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\nTypeError: dot() got an unexpected keyword argument 'trans_b'\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0872, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
{"id": "e478aa4f-5227-4fa3-802a-98b6f36a7155", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    offs_m = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid1 * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    offs_cm = offs_m[:, None]\n    offs_cn = offs_n[None, :]\n    c_ptrs = c_ptr + stride_cm * offs_cm + stride_cn * offs_cn\n    c_mask = (offs_cm < M) & (offs_cn < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        M, K = A.shape[1], A.shape[0]\n        N = B.shape[0]\n        \n        C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_M']), triton.cdiv(N, META['BLOCK_N']))\n        \n        matmul_kernel[grid](\n            A, B, C,\n            M, N, K,\n            stride_am=1, stride_ak=M,\n            stride_bk=1, stride_bn=K,\n            stride_cm=N, stride_cn=1,\n        )\n        return C\n\nM = 1024\nK = 4096\nN = 2048\n\ndef get_inputs():\n    A = torch.randn(K, M, device='cuda')\n    B = torch.randn(N, K, device='cuda')\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "1e9f6d71-8b62-4b7a-9581-f6565da82c89", "generation": 1, "timestamp": 1754634840.0285478, "iteration_found": 0, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_18_Matmul_with_transposed_both", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "max_difference": ["0.252289", "0.249115", "0.254669", "0.247803", "0.261566"], "avg_difference": ["0.039738", "0.039681", "0.039760", "0.039682", "0.039761"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0325, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.371, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0325, "speed_up": 0.088, "custom_timing": 0.371}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.3710\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0325, 'speed_up': 0.088, 'custom_timing': 0.371}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.3710, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0325, 'speed_up': 0.088, 'custom_timing': 0.371}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A.T, B.T)\n\nM = 1024\nK = 4096\nN = 2048\n\ndef get_inputs():\n    A = torch.randn(K, M)\n    B = torch.randn(N, K)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.3710), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0325, 'speed_up': 0.088, 'custom_timing': 0.371})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A.T, B.T)\n\nM = 1024\nK = 4096\nN = 2048\n\ndef get_inputs():\n    A = torch.randn(K, M)\n    B = torch.randn(N, K)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a simple model that does: `C = A * B` via `torch.matmul(A.T, B.T)`\n However, note the shapes: \n   A is (M, K) in the function, but we are given A of shape (K, M) in get_inputs? \n   Similarly, B is (K, N) in the function, but get_inputs returns B of shape (N, K).\n\n Actually, the forward function does:\n   A.T: becomes (K, M).T -> (M, K)   [if original A is (K, M) then A.T is (M, K)]\n   B.T: becomes (K, N).T -> (N, K)   [if original B is (N, K) then B.T is (K, N)]\n Then the matmul: (M, K) @ (K, N) -> (M, N)\n\n But note: the get_inputs returns:\n   A = torch.randn(K, M)   -> shape (K, M)\n   B = torch.randn(N, K)   -> shape (N, K)\n\n So in the forward:\n   A.T: (K, M) -> (M, K)\n   B.T: (N, K) -> (K, N)   -> wait, this is not the same as (K, N) but (N, K) transposed to (K, N) is the same as having a tensor of shape (K, N) but the data is transposed.\n\n However, the operation (M, K) @ (K, N) is a standard matmul.\n\n The problem: the current implementation uses `torch.matmul` which is efficient but we are asked to rewrite for Triton.\n\n But note: the current performance metrics show:\n   reference_timing_ms: 0.0325, custom_timing: 0.371 -> so the current custom (which is the reference) is actually the PyTorch matmul and it's fast.\n\n However, the task says: \"Rewrite the program to improve its performance on the specified metrics.\"\n\n How can we improve? We can write a Triton kernel for matrix multiplication.\n\n Steps:\n\n 1. We will write a Triton kernel for matrix multiplication: C = A @ B, where A is (M, K) and B is (K, N), producing C (M, N).\n\n 2. But note: the inputs we get in `forward` are:\n      A: tensor of shape (M, K)   [because the argument says A: (M, K) and we are given (K, M) in get_inputs? Actually, no: the get_inputs returns (K, M) and then we do A.T -> (M, K). So in the forward, A is (M, K) and B is (K, N)].\n\n 3. However, in the current code, the forward does:\n        return torch.matmul(A.T, B.T)\n\n    But wait: if A is of shape (M, K) then A.T is (K, M). And B is (K, N) then B.T is (N, K). Then (K, M) @ (N, K) is not defined? Actually, no: the matmul requires the second tensor to be of shape (K, N) but we have (N, K). So we must transpose B to (K, N) and then we do (M, K) @ (K, N) -> (M, N). \n\n    But the current code does: A.T (which is (K, M)) and B.T (which is (N, K)) and then (K, M) @ (N, K) is not defined? \n\n    Actually, let me check: \n        A is (M, K) -> A.T is (K, M)\n        B is (K, N) -> B.T is (N, K)\n        Then (K, M) @ (N, K) is not valid because the inner dimensions (M and N) don't match? \n\n    This code is incorrect! \n\n    Correction: \n        The docstring says:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Then the operation should be: \n            C = A @ B   # (M, K) @ (K, N) -> (M, N)\n\n        But the code does: \n            torch.matmul(A.T, B.T)\n\n        This is: (K, M) @ (N, K) -> which is invalid because M != N? \n\n    Actually, the code in the example might be intended for a different data layout.\n\n    However, the get_inputs returns:\n        A = torch.randn(K, M)   # (K, M)\n        B = torch.randn(N, K)   # (N, K)\n\n    Then in the forward, we have:\n        A is (K, M) and B is (N, K)   [because get_inputs returns [A, B] and then passed to forward(A, B)]\n\n    Then:\n        A.T: (K, M) -> (M, K)\n        B.T: (N, K) -> (K, N)\n\n        Then: (M, K) @ (K, N) -> (M, N)\n\n    So the forward is correct.\n\n    But now, if we are to write a Triton kernel, we note that we are effectively computing:\n        C = (A.T) @ (B.T)   but note that (A.T) is (M, K) and (B.T) is (K, N).\n\n    Alternatively, we can avoid the transposes by computing with the original A and B without transposing, but then we note:\n\n        We want C = A.T @ B.T = (B @ A).T   ??? \n\n        Actually: \n            (B @ A).T = (A.T @ B.T)   -> so we can compute: C = (B @ A).T\n\n        But note: \n            B is (N, K) and A is (K, M) -> then B @ A is (N, M), then transpose to (M, N).\n\n        This might be an alternative without transposing the entire matrices.\n\n    However, we are going to write a Triton kernel that multiplies two matrices: \n        Input: \n          A: (M, K)   [but note: we have the original A as (K, M) and we are going to use it without transposing?]\n          Actually, we have two options:\n\n        Option 1: \n          We can write the kernel to multiply (M, K) * (K, N) -> (M, N)\n\n        But our inputs are stored as:\n          A: (K, M) and we want to use it as (M, K) -> so we can treat it as row-major (M, K) if we change the strides? \n          Alternatively, we can avoid transposing by changing the access pattern.\n\n    However, note: the Triton kernel can handle non-contiguous tensors by using strides.\n\n    Let me design:\n\n        We want to compute: C = A.T @ B.T\n\n        Without transposing, we can think of A.T as a matrix of shape (M, K) but stored in the original A (which is (K, M)) in column-major? \n        Actually, A is (K, M) and if we want A.T, that is (M, K) and it is the transpose of A. The transpose of a row-major matrix (K, M) is a column-major matrix? Not exactly: it's stored non-contiguously.\n\n        We can write the kernel to read A and B in a transposed way.\n\n    Alternatively, we can change the inputs: instead of transposing, we can pass the original A and B to the kernel and have the kernel compute the equivalent of A.T @ B.T.\n\n        Note: \n          A.T @ B.T = (B @ A).T\n\n        So we can compute D = B @ A   [which is (N, M)] and then take the transpose to get (M, N). But that requires an extra transpose.\n\n        We can avoid the extra transpose by writing the kernel to output the result in the transposed layout? But then the output would be (N, M) and we want (M, N). \n\n    Alternatively, we can write a kernel that multiplies two matrices that are stored in column-major? \n\n    However, note: the current PyTorch matmul is very efficient. We are only going to beat it if we use a highly optimized kernel.\n\n    Given the constraints, we are going to write a Triton kernel that multiplies two matrices without transposing the inputs? \n\n    Let me define:\n\n        We have:\n          A: shape (K, M)   [in row-major: so element (i, j) is at i*M + j]\n          B: shape (N, K)   [in row-major: so element (i, j) is at i*K + j]\n\n        We want to compute:\n          C = A.T @ B.T   -> which is (M, K) @ (K, N) = (M, N)\n\n        How can we do that without transposing?\n\n        We can compute:\n          C[i, j] = sum_{k} A.T[i, k] * B.T[k, j] \n                   = sum_{k} A[k, i] * B[j, k]\n\n        So we can write a kernel that for each element (i, j) in C, it does:\n          s = 0\n          for k in range(K):\n             s += A[k, i] * B[j, k]\n\n        This is equivalent to:\n          C = torch.einsum('ki,jk->ij', A, B)\n\n        This is a valid way.\n\n    But note: the memory access pattern for A is by columns (because we fix i and vary k, so we are going down the column of A) and for B we fix j and vary k (so we are going along the row of B). This is not coalesced.\n\n    Alternatively, we can tile the computation and use shared memory to coalesce the reads.\n\n    We will write a standard matrix multiplication kernel in Triton that is optimized for the given shapes.\n\n    Steps for the kernel:\n\n      Let's assume we have:\n        A: shape (K, M)   -> we are going to access as A[k, i] for fixed i and varying k -> column-major in the original A? \n        But note: we are going to read A in a way that is not contiguous. We can preload tiles of A and B into shared memory (using Triton's block pointers) and then compute.\n\n      Alternatively, we can note that the problem is equivalent to multiplying two matrices that are stored in row-major but the first one is actually the transpose of what we normally use and the second one is also the transpose.\n\n      We can write the kernel for C = X @ Y, where X is (M, K) and Y is (K, N) and then set:\n          X = A.T   -> but we don't want to create a new tensor\n          Y = B.T\n\n      Instead, we can pass the pointers to A and B and then use strides to read them as if they were transposed.\n\n    How about we write the kernel for C = X @ Y, but we pass:\n        X = A   [with shape (K, M)] but we want to read it as (M, K) -> so we set the stride for the first dimension to be 1 (column-major) and the second dimension to be K? \n        Actually, the stride for A in the original is (M, 1) for (dim0, dim1) because it's row-major? \n          Element (i, j) is at i * M + j -> so stride0 = M, stride1 = 1.\n\n        To read it as if it were a matrix of shape (M, K) (which is A.T), we would need to swap the strides: \n          stride0 for the new first dimension (dim0) would be 1, and stride1 for the new second dimension (dim1) would be M.\n\n        Similarly for B: original shape (N, K) -> element (i, j) at i*K+j -> stride0 = K, stride1 = 1.\n        To read as (K, N) (which is B.T), we would use: \n          for the new first dimension (dim0) stride = 1, and for the new second dimension (dim1) stride = K.\n\n    Therefore, we can write a standard row-major matrix multiplication kernel that expects:\n        X: (M, K) with strides (stride_x0, stride_x1) -> for row-major, we expect stride_x0 = K, stride_x1 = 1? \n        But note: in the kernel, we are going to read by blocks. We can set:\n\n          ptr_x + i * stride_x0 + k * stride_x1   for element (i,k) in X.\n\n        But if we set:\n          stride_x0 = 1   [because we want the inner dimension to be contiguous? Actually, no: for row-major (M, K) the inner dimension is K?]\n\n        Actually, let me clarify:\n\n          In row-major for a matrix of shape (M, K):\n            element (i, j) is at base + i * stride_i + j * stride_j.\n            Typically, we set stride_i = K and stride_j = 1.\n\n          But if we have a matrix that is stored as column-major (like the transpose of a row-major matrix) and we want to interpret it as row-major, then we set:\n            stride_i = 1 and stride_j = M.\n\n        So for A (which is (K, M) row-major) we want to use it as (M, K) row-major -> then the stride for the first dimension (which is the row) should be 1 (because consecutive rows are stored in consecutive memory locations? Actually, no: consecutive elements in a row of the new matrix are stored in consecutive memory? \n\n        In the new matrix (M, K) row-major, the element (i, k) is the same as the element (k, i) in the original (K, M) row-major. And in the original, the element (k, i) is at k * M + i.\n\n        So the new element (i, k) is at k * M + i.\n\n        Therefore, the stride for the row dimension (i) is M? and the stride for the column dimension (k) is 1? \n\n        Actually, if we fix k and vary i by 1, we move 1 element -> so the stride for the row is 1? \n        But if we fix i and vary k by 1, we move M elements -> so the stride for the column is M.\n\n        So for the new matrix (M, K) we have:\n          stride for row (i) = 1\n          stride for column (k) = M\n\n        Similarly, for B: originally (N, K) and we want to use it as (K, N) row-major? \n          The element (k, j) in the new matrix (K, N) is the element (j, k) in the original, which is at j * K + k.\n          So in the new matrix, to access (k, j) we do: base + k * stride_k + j * stride_j.\n          If we fix k and vary j by 1, we move 1 element -> so stride_j = 1? \n          If we fix j and vary k by 1, we move by K? -> so stride_k = K.\n\n        Actually, wait: \n          The new matrix (K, N) row-major: \n            element (k, j) is stored at base + k * N + j   [if it were contiguous row-major]? \n          But we are not going to make it contiguous? We are just using the original data.\n\n          We want to compute the address: \n            original index: (j, k) -> at base + j * K + k.\n\n          So the new element (k, j) is at base + j * K + k.\n\n          Therefore, the strides for the new matrix (K, N) are:\n            stride for k (row) = K   [because increasing k by 1 moves by K? Actually, if we fix j and increase k by 1, we move by 1? No, wait: \n                (k, j) -> j * K + k\n                (k+1, j) -> j * K + (k+1) -> difference = 1 -> so the stride for k (the row) is 1? \n                (k, j+1) -> (j+1)*K + k -> difference = K -> so the stride for j (the column) is K.\n\n          So for the new matrix (K, N) we have:\n            stride for row (k) = 1\n            stride for column (j) = K\n\n        But note: in the kernel, we are going to read by blocks. We can set the strides accordingly.\n\n    So we can write a standard matrix multiplication kernel that expects:\n        Input A: (M, K) with strides (stride_am, stride_ak) = (1, M)   [so for element (i, k): offset = i*1 + k*M]\n        Input B: (K, N) with strides (stride_bk, stride_bn) = (1, K)   [so for element (k, j): offset = k*1 + j*K]\n\n        Then we compute C (M, N) with strides (stride_cm, stride_cn) = (N, 1) for row-major? \n        But we want the output to be (M, N) row-major? \n\n        However, note: the output of the kernel is going to be stored in a tensor that we create of shape (M, N). We can set it to row-major: \n          stride_cm = N, stride_cn = 1.\n\n    How to write the kernel:\n\n        We'll use a 2D grid: \n          pid_m = tl.program_id(0)\n          pid_n = tl.program_id(1)\n\n        And we'll have two inner dimensions: \n          BLOCK_SIZE_M, BLOCK_SIZE_N, and BLOCK_SIZE_K.\n\n        We'll iterate over k in chunks of BLOCK_SIZE_K.\n\n        We'll load a block of A of size (BLOCK_SIZE_M, BLOCK_SIZE_K) and a block of B of size (BLOCK_SIZE_K, BLOCK_SIZE_N) and then compute the dot product.\n\n        The addresses for A and B will use the strides we defined.\n\n        Steps:\n\n          offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n          offs_ak = tl.arange(0, BLOCK_SIZE_K)\n          a_ptrs = A + offs_am[:, None] * stride_am + offs_ak[None, :] * stride_ak   # [BLOCK_SIZE_M, BLOCK_SIZE_K]\n\n          Similarly for B:\n          offs_bk = tl.arange(0, BLOCK_SIZE_K)\n          offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n          b_ptrs = B + offs_bk[:, None] * stride_bk + offs_bn[None, :] * stride_bn   # [BLOCK_SIZE_K, BLOCK_SIZE_N] -> but note: we need to transpose B for the dot product? \n\n        Actually, we are going to do:\n          c = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n          for k in range(0, K, BLOCK_SIZE_K):\n             a = tl.load(a_ptrs, mask= ...)   # [BLOCK_SIZE_M, BLOCK_SIZE_K]\n             b = tl.load(b_ptrs, mask= ...)   # [BLOCK_SIZE_K, BLOCK_SIZE_N] -> but we want to multiply a and b? \n             c += tl.dot(a, b)   # This requires that a and b are in the right layout.\n\n        However, note: the dot product in Triton expects a to be (M, K) and b to be (K, N) and produces (M, N). So we are good.\n\n        But wait: our a_ptrs and b_ptrs are set to the correct blocks? \n\n        Actually, the a_ptrs we set is for a block of A: \n          We are loading a block at (pid_m block in M, and k_block in K) -> that is (BLOCK_SIZE_M, BLOCK_SIZE_K)\n\n        Similarly, for b_ptrs: we are loading a block of B at (k_block in K, and pid_n block in N) -> that is (BLOCK_SIZE_K, BLOCK_SIZE_N)\n\n        Then we can do a dot product of a (BLOCK_SIZE_M, BLOCK_SIZE_K) and b (BLOCK_SIZE_K, BLOCK_SIZE_N) to get (BLOCK_SIZE_M, BLOCK_SIZE_N).\n\n        Then we accumulate.\n\n        We must update the pointers for the next k block? \n\n        How to update:\n          a_ptrs += BLOCK_SIZE_K * stride_ak   # because we move in the k dimension of A by BLOCK_SIZE_K\n          b_ptrs += BLOCK_SIZE_K * stride_bk   # but note: in B, the k dimension is the row? and we are moving to the next block in the row? Actually, we are moving to the next block of K, so we move in the k dimension of B? \n\n        However, note: the stride_bk is 1 for the k dimension? and we are moving by BLOCK_SIZE_K in the k dimension -> so we add BLOCK_SIZE_K * 1.\n\n        But wait: in the kernel, we set the strides for A and B as:\n          A: (stride_am, stride_ak) = (1, M)\n          B: (stride_bk, stride_bn) = (1, K)\n\n        Then:\n          a_ptrs: for the next k block, we move by BLOCK_SIZE_K in the ak dimension -> add BLOCK_SIZE_K * M\n          b_ptrs: for the next k block, we move by BLOCK_SIZE_K in the bk dimension -> add BLOCK_SIZE_K * 1\n\n        However, note: the next block in A is in the same row (same i) but next k? So we add M * BLOCK_SIZE_K? \n        But our A matrix is stored by: element (i, k) at base + i*1 + k*M -> so if we increase k by BLOCK_SIZE_K, we add BLOCK_SIZE_K * M.\n\n        Similarly, for B: element (k, j) at base + k*1 + j*K -> so if we increase k by BLOCK_SIZE_K, we add BLOCK_SIZE_K * 1.\n\n    So we do:\n\n        a_ptrs = a_ptrs + (BLOCK_SIZE_K * stride_ak)  # which is BLOCK_SIZE_K * M\n        b_ptrs = b_ptrs + (BLOCK_SIZE_K * stride_bk)  # which is BLOCK_SIZE_K * 1\n\n    But note: we are iterating over k in steps of BLOCK_SIZE_K, so we are moving along the k dimension.\n\n    We also need to handle the boundaries: use masks.\n\n    Finally, we write the block of c to the output.\n\n    We'll use autotuning for BLOCK_SIZE_M, BLOCK_SIZE_N, and BLOCK_SIZE_K.\n\n    Given the sizes: M=1024, K=4096, N=2048, we can choose block sizes that are multiples of 16 or 32.\n\n    We'll set:\n        BLOCK_SIZE_M = 64, BLOCK_SIZE_N = 64, BLOCK_SIZE_K = 32\n\n    But we will autotune.\n\n    Steps:\n\n        We are going to replace the forward function with a call to our Triton kernel.\n\n        We note that the inputs to the forward are:\n            A: (K, M)   [from get_inputs: (4096, 1024)]\n            B: (N, K)   [from get_inputs: (2048, 4096)]\n\n        And we want to compute C = A.T @ B.T -> (M, N) = (1024, 2048)\n\n        We'll create an output tensor of shape (M, N) = (1024, 2048) and then call the kernel.\n\n        We must set the grid: \n            grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n\n        And we pass the strides for A and B as we computed.\n\n    Let's code the kernel.\n\n    Note: we must be cautious of the data types. We are using float32.\n\n    We'll write the kernel in the ModelNew class? Actually, we can write it as a separate function and then call it in the forward.\n\n    However, the EVOLVE-BLOCK must contain the entire code? \n\n    We are going to write:\n\n        import triton\n        import triton.language as tl\n\n        @triton.autotune(...)\n        @triton.jit\n        def _matmul_kernel(...):\n            ...\n\n        Then in the forward, we call this kernel.\n\n    We must be cautious: the kernel must be compiled and run.\n\n    We'll do:\n\n        C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n        # set strides for the kernel: for A we want to interpret as (M, K) with strides (1, M)\n        stride_am = 1\n        stride_ak = M\n        # for B: (K, N) with strides (1, K)\n        stride_bk = 1\n        stride_bn = K   # note: this is the original K (4096) from the dimension of A? Actually, the K in the problem is 4096.\n\n        But note: in the kernel, the stride_bn is the stride for the column dimension of B? which we set to K (the original K=4096). However, in the new matrix (K, N) the column dimension is N? and the stride for the column is the original K? -> yes.\n\n        Then call the kernel.\n\n    But note: the kernel expects the entire matrix A to be of shape (M, K) and B (K, N). We are passing the base pointers and the strides.\n\n    We must also check the boundaries: use masks.\n\n    Let's write the kernel.\n\n    IMPORTANT: We must not change the function signature of the ModelNew.forward. We are only changing the implementation.\n\n    We'll keep the same inputs and outputs.\n\n    We are going to remove the torch.matmul and replace it with our kernel.\n\n    We note: the current implementation uses A.T and B.T which are contiguous? But our kernel does not require contiguous.\n\n    We'll try to write the kernel and then autotune.\n\n    Due to the complexity, we will write a simple kernel first and then optimize.\n\n    However, the problem says: the current reference timing is 0.0325 ms and our custom timing is 0.371 ms (which is slower). We want to beat 0.0325 ms.\n\n    We must write a highly optimized kernel.\n\n    We can use the Triton matrix multiplication tutorial as a base.\n\n    We'll write:\n\n        _matmul_kernel(\n            A, B, C,\n            M, N, K,\n            stride_am, stride_ak,   # for A\n            stride_bk, stride_bn,   # for B\n            stride_cm, stride_cn,   # for C: we set C as row-major -> (stride_cm = N, stride_cn = 1)\n            BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n            ... \n        )\n\n    We'll follow the tutorial: https://triton-lang.org/master/getting-started/tutorials/03-matrix-multiplication.html\n\n    But note: we have non-standard strides.\n\n    We'll adapt the tutorial to our strides.\n\n    Steps in the kernel:\n\n        pid0 = tl.program_id(0)   # for M\n        pid1 = tl.program_id(1)   # for N\n\n        # create ranges\n        rm = pid0 * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        rn = pid1 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        rk = tl.arange(0, BLOCK_SIZE_K)\n\n        # get pointers to the data\n        A_ptr = A + (rm[:, None] * stride_am + rk[None, :] * stride_ak)\n        B_ptr = B + (rk[:, None] * stride_bk + rn[None, :] * stride_bn)\n        C_ptr = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n\n        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n            # load a and b\n            a = tl.load(A_ptr, mask=rk[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n            b = tl.load(B_ptr, mask=rk[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n            acc += tl.dot(a, b)\n            # update pointers for next block\n            A_ptr += BLOCK_SIZE_K * stride_ak\n            B_ptr += BLOCK_SIZE_K * stride_bk\n\n        # write back\n        tl.store(C_ptr, acc, mask= ...)   # mask: rm and rn within bounds.\n\n    But note: the mask in the load for A and B: we are masking along the k dimension. We must check that the current k block is within K.\n\n    Also, the update: we are moving by BLOCK_SIZE_K in the k dimension.\n\n    However, the above loop does not use k in the pointer update? We are updating the pointers inside the loop by a fixed step.\n\n    Alternatively, we can precompute the base pointers for the current k block? \n\n    We can do:\n\n        for k_offset in range(0, K, BLOCK_SIZE_K):\n            a = tl.load(A_ptr + k_offset * stride_ak, ...)\n            b = tl.load(B_ptr + k_offset * stride_bk, ...)\n\n    But we already set A_ptr and B_ptr for the start of the row? \n\n    Actually, the tutorial uses a different approach: they use a block pointer that moves along K.\n\n    We'll do:\n\n        for k in range(0, K, BLOCK_SIZE_K):\n            # load a block of A and B\n            a = tl.load(A_ptr)\n            b = tl.load(B_ptr)\n            acc += tl.dot(a, b)\n            A_ptr += BLOCK_SIZE_K * stride_ak\n            B_ptr += BLOCK_SIZE_K * stride_bk\n\n    But note: the initial A_ptr and B_ptr are set for k_offset=0.\n\n    We must mask the load for the last block if K is not divisible by BLOCK_SIZE_K.\n\n    We can compute:\n\n        k_remaining = K - k\n        a = tl.load(A_ptr, mask=rk[None, :] < k_remaining, other=0.0)\n        b = tl.load(B_ptr, mask=rk[:, None] < k_remaining, other=0.0)\n\n    But note: our rk is of size BLOCK_SIZE_K, and we are at the block starting at k, so we require that the indices in the k dimension are less than k_remaining.\n\n    Alternatively, we can precompute the mask for the entire block? \n\n    We'll do:\n\n        mask_a = (rm < M) & (rk[None, :] < K)   # but we are iterating by k blocks? \n\n    Actually, the mask for A should be: \n        mask_a = (rm < M) and ( (k_offset + rk) < K )   but we are only loading one block at a time.\n\n    We can do:\n\n        for k_offset in range(0, K, BLOCK_SIZE_K):\n            a = tl.load(A_ptr, mask=(rm[:, None] < M) & ( (rk[None, :] < K - k_offset) ), other=0.0)\n            ... \n\n    But note: rm and rk are fixed for the entire block? Actually, they are defined at the beginning. \n\n    We must redefine rk for the current k_offset? \n\n    Alternatively, we can define the k index as: rk = tl.arange(0, BLOCK_SIZE_K) + k_offset\n\n    But we are not storing the entire k dimension? \n\n    We'll do:\n\n        for k_offset in range(0, K, BLOCK_SIZE_K):\n            k_indices = k_offset + tl.arange(0, BLOCK_SIZE_K)\n            a_mask = (rm < M) & (k_indices[None, :] < K)\n            a = tl.load(A_ptr, mask=a_mask, other=0.0)\n            b_mask = (k_indices[:, None] < K) & (rn < N)\n            b = tl.load(B_ptr, mask=b_mask, other=0.0)\n            acc += tl.dot(a, b)\n            A_ptr += BLOCK_SIZE_K * stride_ak\n            B_ptr += BLOCK_SIZE_K * stride_bk\n\n    But note: the pointer update is the same regardless of the mask.\n\n    We'll write the kernel accordingly.\n\n    However, the tutorial uses a different approach without recomputing the mask per block? \n\n    Given the complexity, we will start with a simple kernel without masking in the inner loop? and then add the masking.\n\n    But we must have masking to avoid out-of-bound reads.\n\n    Alternatively, we can split the kernel into two: one for the main blocks and one for the boundary? But that is complex.\n\n    We'll do the mask per block.\n\n    Also, the output store must be masked.\n\n    We'll set:\n\n        mask_c = (rm < M) & (rn < N)\n\n        tl.store(C_ptr, acc, mask=mask_c)\n\n    Now, let's code.\n\n    We must choose BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K.\n\n    We'll set some initial values and then autotune.\n\n    We'll use the autotune configs from the example.\n\n    We'll define:\n\n        configs = [\n            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=8),\n            ... \n        ]\n\n    We'll try to cover common block sizes.\n\n    Finally, we note that the kernel might be slower than the PyTorch matmul? We must hope that we can beat 0.0325 ms.\n\n    But note: the current PyTorch matmul is done on contiguous tensors? And we are avoiding the transposes? \n\n    However, the transposes in the current code are done by PyTorch and are very fast? \n\n    We are saving two transposes? But we are doing a kernel that has to do non-contiguous access? \n\n    We must hope that the kernel can be faster.\n\n    Let's try.\n\n    We'll write the code.\n\n    Due to the length, we'll write the kernel and then the forward.\n\n    IMPORTANT: We must not change the get_inputs and get_init_inputs.\n\n    We'll replace the entire forward function.\n\n    We'll also define M, K, N as global constants? But they are defined at the bottom. We can use the same M, K, N.\n\n    However, in the kernel call, we need to know M, N, K. We can pass them.\n\n    Steps:\n\n        In the forward:\n\n            # A is (K, M), B is (N, K)\n            M = A.shape[1]   # because A is (K, M) -> M = A.shape[1]\n            K = A.shape[0]   # and K = A.shape[0]\n            N = B.shape[0]   # B is (N, K) -> N = B.shape[0]\n\n            # Create output C: (M, N)\n            C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n\n            # Strides for A, B, and C:\n            stride_am = 1\n            stride_ak = M   # because to get next k in A (which is the row in the original) we step by M? \n            # But note: the original A is (K, M) and we are interpreting it as (M, K). The element (i, k) in the new view is at base + i*1 + k*M.\n\n            stride_bk = 1\n            stride_bn = K   # for B: we interpret as (K, N): element (k, j) at base + k*1 + j*K.\n\n            stride_cm = N   # for C: row-major (M, N): element (i, j) at base + i*N + j\n            stride_cn = 1\n\n            # Grid\n            grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n\n            # Call kernel\n            _matmul_kernel[grid](A, B, C, M, N, K, \n                                 stride_am, stride_ak, \n                                 stride_bk, stride_bn,\n                                 stride_cm, stride_cn,\n                                 BLOCK_SIZE_M=BLOCK_SIZE_M, ...)\n\n        But we are using autotune, so we don't fix the block sizes.\n\n    We'll now write the entire code.\n\n    Due to the constraints of the problem, we must not change anything outside the EVOLVE-BLOCK.\n\n    We'll write the kernel inside the EVOLVE-BLOCK? \n\n    We are going to write:\n\n        import math\n        import triton\n        import triton.language as tl\n\n        @triton.autotune(...)\n        @triton.jit\n        def _matmul_kernel(...):\n            ...\n\n        class ModelNew(nn.Module):\n            ...\n\n    But note: we are only allowed to output the code within the EVOLVE-BLOCK.\n\n    We'll do:\n\n        # ================== EVOLVE-BLOCK-START ==================\n        ... (our code)\n\n    We'll include the necessary imports.\n\n    However, note: the imports might be already present? But the problem says: \"all other code is LOCKED\" so we cannot change imports outside the block? \n\n    We must include the imports inside the block? \n\n    We'll do:\n\n        import math\n        import torch\n        import torch.nn as nn\n        import triton\n        import triton.language as tl\n\n        ... then the kernel and the model.\n\n    But note: the original code did not import triton. We are allowed to import? \n\n    The problem says: we are to generate Triton implementations. So we assume the environment has triton.\n\n    We'll do it.\n\n    We'll write the kernel and then the model.\n\n    Given the complexity and the fact that we are not sure if it will be faster, we hope that with autotuning we can beat PyTorch.\n\n    Let's code.\n\n    IMPORTANT: We must not remove the M, K, N and get_inputs, get_init_inputs at the bottom.\n\n    We'll keep them.\n\n    We are going to replace the ModelNew and the forward, and then add the kernel.\n\n    Due to the length, we hope it fits.\n\n    We'll try.\n</think>\nTo optimize the matrix multiplication operation using Triton, we'll implement a highly optimized kernel that leverages the GPU's capabilities. The key improvements include:\n\n1. Using Triton's matrix multiplication capabilities with proper memory coalescing\n2. Implementing block tiling for efficient Tensor Core utilization\n3. Adding autotuning to find optimal block configurations\n4. Using non-contiguous memory accesses to avoid explicit transposes\n5. Proper boundary checks and masking\n\nHere's the optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    offs_m = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid1 * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    offs_cm = offs_m[:, None]\n    offs_cn = offs_n[None, :]\n    c_ptrs = c_ptr + stride_cm * offs_cm + stride_cn * offs_cn\n    c_mask = (offs_cm < M) & (offs_cn < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        M, K = A.shape[1], A.shape[0]\n        N = B.shape[0]\n        \n        C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_M']), triton.cdiv(N, META['BLOCK_N']))\n        \n        matmul_kernel[grid](\n            A, B, C,\n            M, N, K,\n            stride_am=1, stride_ak=M,\n            stride_bk=1, stride_bn=K,\n            stride_cm=N, stride_cn=1,\n        )\n        return C\n\nM = 1024\nK = 4096\nN = 2048\n\ndef get_inputs():\n    A = torch.randn(K, M, device='cuda')\n    B = torch.randn(N, K, device='cuda')\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "4a4374cc-d0c3-4ba7-95c9-3e23aa53354c", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef cross_entropy_kernel(\n    predictions_ptr,\n    targets_ptr,\n    output_ptr,\n    stride_pred,\n    batch_size,\n    num_classes: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    row_idx = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = row_idx < batch_size\n\n    # Load target indices for the block\n    target_indices = tl.load(targets_ptr + row_idx, mask=mask, other=0)\n    \n    # Compute row start positions in predictions tensor\n    row_starts = row_idx * stride_pred\n    row_offsets = row_starts[:, None] + tl.arange(0, num_classes)[None, :]\n    \n    # Load prediction rows with boundary mask\n    pred_rows = tl.load(\n        predictions_ptr + row_offsets,\n        mask=mask[:, None] & (tl.arange(0, num_classes)[None, :] < num_classes),\n        other=-float('inf')\n    )\n    \n    # Compute row-wise max for numerical stability\n    row_max = tl.max(pred_rows, axis=1)\n    # Compute logsumexp\n    exp_vals = tl.exp(pred_rows - row_max[:, None])\n    log_sumexp = row_max + tl.log(tl.sum(exp_vals, axis=1))\n    \n    # Extract target values using advanced indexing\n    target_vals = tl.sum(\n        tl.where(\n            tl.arange(0, num_classes)[None, :] == target_indices[:, None],\n            pred_rows,\n            0.0\n        ),\n        axis=1\n    )\n    \n    # Compute per-row loss: log_sumexp - target_vals\n    per_row_loss = log_sumexp - target_vals\n    tl.store(output_ptr + row_idx, per_row_loss, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, predictions, targets):\n        predictions = predictions.contiguous()\n        targets = targets.contiguous().to(torch.int32)\n        \n        batch_size, num_classes = predictions.shape\n        per_row_loss = torch.empty(batch_size, device=predictions.device, dtype=torch.float32)\n        \n        # Configure kernel launch parameters\n        BLOCK_SIZE = 128\n        grid = (triton.cdiv(batch_size, BLOCK_SIZE),)\n        \n        cross_entropy_kernel[grid](\n            predictions,\n            targets,\n            per_row_loss,\n            predictions.stride(0),\n            batch_size,\n            num_classes,\n            BLOCK_SIZE\n        )\n        \n        return per_row_loss.mean()\n\nbatch_size = 4096\nnum_classes = 10\ninput_shape = (num_classes, )\ndim = 1\n\ndef get_inputs():\n    return [torch.randn(batch_size, *input_shape), torch.randint(0, num_classes, (batch_size,))]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "cb977197-f054-4177-984a-db6fb880bf09", "generation": 1, "timestamp": 1754987923.333756, "iteration_found": 1, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_95_CrossEntropyLoss", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\n    raise ValueError(\"arange's range must be a power of 2\")\nValueError: arange's range must be a power of 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp1llj3oxt.py\", line 70, in forward\n    cross_entropy_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 19:40:\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    row_idx = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = row_idx < batch_size\n\n    # Load target indices for the block\n    target_indices = tl.load(targets_ptr + row_idx, mask=mask, other=0)\n\n    # Compute row start positions in predictions tensor\n    row_starts = row_idx * stride_pred\n    row_offsets = row_starts[:, None] + tl.arange(0, num_classes)[None, :]\n                                        ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\n    raise ValueError(\"arange's range must be a power of 2\")\nValueError: arange's range must be a power of 2\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0249, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0245, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0249, "speed_up": 1.016, "custom_timing": 0.0245}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0245\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0249, 'speed_up': 1.016, 'custom_timing': 0.0245}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0245, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0249, 'speed_up': 1.016, 'custom_timing': 0.0245}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that computes Cross Entropy Loss for multi-class classification tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.cross_entropy(predictions, targets)\n\nbatch_size = 4096\nnum_classes = 10\ninput_shape = (num_classes, )  # Output for each class\ndim = 1\n\ndef get_inputs():\n    return [torch.randn(batch_size, *input_shape), torch.randint(0, num_classes, (batch_size,))]\n\ndef get_init_inputs():\n    return []\n\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0245), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0249, 'speed_up': 1.016, 'custom_timing': 0.0245})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that computes Cross Entropy Loss for multi-class classification tasks.\n\n    Parameters:\n        None\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, predictions, targets):\n        return torch.nn.functional.cross_entropy(predictions, targets)\n\nbatch_size = 4096\nnum_classes = 10\ninput_shape = (num_classes, )  # Output for each class\ndim = 1\n\ndef get_inputs():\n    return [torch.randn(batch_size, *input_shape), torch.randint(0, num_classes, (batch_size,))]\n\ndef get_init_inputs():\n    return []\n\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch reference implementation for cross entropy loss.\n The current implementation uses `torch.nn.functional.cross_entropy`.\n The goal is to rewrite the forward pass of the ModelNew class to use a Triton kernel for improved performance.\n\n However, note that the cross entropy loss involves:\n   log_softmax + negative log likelihood loss.\n\n We can break it down into two steps:\n   1. Compute log_softmax over the class dimension (dim=1 in our case, because predictions are [batch_size, num_classes])\n   2. Then, for each element in the batch, pick the log_softmax value at the target class and take the negative.\n\n But note: the standard cross_entropy function also handles class weights, ignore_index, reduction, etc.\n For simplicity, we assume:\n   - reduction is 'mean' (the default in F.cross_entropy)\n   - no class weights, no ignore_index.\n\n Steps for the Triton implementation:\n\n   We can write two kernels or one fused kernel? Fusing the entire cross entropy might be complex.\n\n   Alternatively, we can break the problem:\n   Step 1: Compute log_softmax for the entire predictions tensor. This is a common operation and we can write a Triton kernel for log_softmax.\n   Step 2: Then, we gather the log_softmax values at the target indices and compute the negative and then the mean.\n\n   However, note that the standard implementation is highly optimized in PyTorch. We must be cautious.\n\n   But the problem says: the current implementation has a speed_up of 1.016 (which is above 1, meaning the reference is slightly slower) but note that the reference is the PyTorch F.cross_entropy.\n\n   We are to write a Triton version that is faster.\n\n   However, note: the current program is using the PyTorch function. We are to replace it with a custom Triton implementation.\n\n   We are allowed to change only the EVOLVE-BLOCK.\n\n   We will write:\n\n      forward(self, predictions, targets):\n          ... our Triton-based cross entropy ...\n\n   We must be careful: the loss must be the same as the PyTorch version.\n\n   We note that the reduction is by default 'mean'. We have to compute the mean over the batch.\n\n   Plan:\n\n      We can do:\n        log_softmax = predictions.log_softmax(dim=1)   # [batch_size, num_classes]\n        loss = -log_softmax[torch.arange(batch_size), targets].mean()\n\n      But we want to accelerate the entire thing with Triton.\n\n   However, note: the log_softmax itself is a common operation and we can write a Triton kernel for it.\n\n   But the problem: we have to compute the loss, not the entire log_softmax tensor.\n\n   We can compute the loss without materializing the entire log_softmax tensor? Yes.\n\n   How?\n\n      We can compute:\n        loss = - (predictions[targets] - logsumexp(predictions, dim=1)).mean()\n\n      But note: the log_softmax for a row is: predictions - logsumexp(predictions, dim=1, keepdim=True)\n\n      So the value at the target class for a row i is:\n          predictions[i, target_i] - logsumexp(predictions[i, :])\n\n      Therefore, the loss for the batch is:\n          loss = mean_i ( logsumexp(predictions[i, :]) - predictions[i, target_i] )\n\n   So we can write a kernel that for each row i:\n        - computes the logsumexp of the row\n        - subtracts the element at the target_i\n        - then we take the mean over the batch.\n\n   We can do:\n\n        loss = (logsumexp(row) - row[target_i]).mean()\n\n   Steps for a Triton kernel:\n\n        We will launch a grid of batch_size blocks? Or we can have each row handled by a program?\n\n        Since each row is independent, we can have one program per row.\n\n        However, the row length is num_classes (which is 10). This is very small.\n\n        We can use a block size of 32 (which is the warp size) and have one warp per row? But 10 is less than 32.\n\n        Alternatively, we can have multiple rows per block? But the batch_size is 4096, which is large.\n\n        We can do:\n\n          BLOCK_SIZE = 1024   (number of threads per block)\n\n          Then, each block can handle multiple rows? How many rows per block? We can do:\n\n            rows_per_block = BLOCK_SIZE // num_classes? But 1024 // 10 = 102.4 -> 102 rows per block.\n\n          But note: we have to compute the logsumexp per row and then subtract the target element.\n\n        However, because the row length is small (10), we can do:\n\n          - Each row is handled by one thread? That would be inefficient because we have 10 elements and we want to compute logsumexp which requires a reduction.\n\n        Alternatively, we can have a two-stage kernel:\n\n          Stage 1: Each row is reduced by a warp? But the row is only 10 elements. We can use a single warp to process multiple rows.\n\n        Given the small row size, we can assign one warp per row? That would be 32 threads for 10 elements. We can use a warp-level reduction.\n\n        However, 4096 rows and 4096 warps (which is 128 blocks of 32 warps) is acceptable? But note: each warp has 32 threads, so 4096 warps is 4096*32 = 131072 threads, which is acceptable for a GPU.\n\n        But we are going to have 4096 warps (each row one warp). That is 4096/32 = 128 blocks? (if we set 32 warps per block). Actually, we can set 8 warps per block (256 threads) and then 4096/8 = 512 blocks.\n\n        Steps:\n\n          Kernel input: predictions (shape [batch_size, num_classes]), targets (shape [batch_size])\n          Kernel output: loss (scalar) OR we can output an intermediate tensor of [batch_size] and then do the mean on the host? But note: the mean is over 4096 elements, which is cheap. We can do the mean on the host.\n\n          Alternatively, we can do the entire mean in the kernel? We can do a two-step reduction.\n\n        We design:\n\n          Step 1: Each row i computes: row_loss_i = logsumexp(row_i) - row_i[targets[i]]\n          Step 2: Then we compute the mean of row_loss_i.\n\n        We can do:\n\n          - Each row i is handled by one warp (or one thread block? but we want to use warp-level primitives).\n\n        Since the row is small, we can do:\n\n          - Load the row into shared memory? But we have 10 elements. We can load the row into registers for the entire warp.\n\n        How to compute logsumexp for a row of 10?\n\n          We can do:\n\n            max_val = tl.max(row, axis=0)   # but note: we are in a warp, we can do a warp-level max\n            exp_sum = tl.sum(tl.exp(row - max_val), axis=0)\n            log_sum = max_val + tl.log(exp_sum)\n\n          Then, we subtract the element at the target index.\n\n        But note: we have to load the row. The row is contiguous? The predictions tensor is [batch_size, num_classes] -> row-major.\n\n        We can do:\n\n          row_start = i * stride\n          Then load 10 elements.\n\n        However, we have to be cautious of the boundaries.\n\n        Implementation:\n\n          pid = program_id(0)  # we will have batch_size program_ids? Or we can have a grid of (ceil(batch_size / rows_per_block), ) and then within the block, multiple rows.\n\n        Given that the row is small, we can assign one row per program? And then we have batch_size programs.\n\n        But the batch_size is 4096, which is acceptable.\n\n        So:\n\n          pid = tl.program_id(0)\n          if pid >= batch_size: return\n\n          row = predictions[pid, :]   # we can load the entire row with a stride\n\n          How to load the row? We can use:\n\n            offsets = pid * stride_row + tl.arange(0, num_classes)\n            row = tl.load(predictions_ptr + offsets, mask=tl.arange(0, num_classes) < num_classes, other=-float('inf'))\n\n          Then, compute max_val = tl.max(row, axis=0)   # this is a scalar for the row\n\n          Then: exp_sum = tl.sum(tl.exp(row - max_val), axis=0)\n\n          log_sum = max_val + tl.log(exp_sum)\n\n          Then, we need the target value: target_index = tl.load(targets_ptr + pid)\n\n          Then, the element at target_index: we can use row[target_index]? But we have the row in registers.\n\n          However, we don't have the row as a list of 10 elements? We have it as a block of 10 elements. We can use:\n\n            target_val = tl.load(predictions_ptr + pid * stride_row + target_index)   # but we already loaded the row? We can avoid this load by storing the row in a variable.\n\n          Alternatively, we can use:\n\n            # We have the row in the variable `row` but it's a block of 10. We can use tl.arange to index? Not directly.\n\n          We can do:\n\n            # We precomputed the row. We know the target_index. We can do:\n            #   target_val = row[target_index]   -> but we cannot index by a variable in Triton.\n\n          Instead, we can use tl.where? But that would be inefficient.\n\n          Alternatively, we can note that the row is small. We can load the entire row and then use a loop? But we cannot use loops for indexing by a variable.\n\n        Solution: we can use tl.advance to get the pointer to the element? Or we can load the row and then scatter? Not helpful.\n\n        We can avoid loading the entire row again by storing the row in a register array? But Triton doesn't support arrays.\n\n        Alternatively, we can recompute the address for the target element and load it. That would be one load per row.\n\n        So:\n\n          target_ptr = targets_ptr + pid\n          target_index = tl.load(target_ptr)\n\n          Then, we compute the address of the target element in predictions:\n\n            target_element_ptr = predictions_ptr + pid * stride_row + target_index\n\n          But note: we already loaded the entire row? We don't need to load again. However, we don't have the row stored in a way that we can index arbitrarily. So we have to do:\n\n          We can do:\n\n            row = tl.load(predictions_ptr + pid * stride_row + tl.arange(0, num_classes), ...)\n            # Now, we want to extract the element at target_index. We can use:\n\n            # Create a mask: tl.arange(0, num_classes) == target_index? Then use tl.where?\n            #   mask = tl.arange(0, num_classes) == target_index\n            #   target_val = tl.sum(tl.where(mask, row, 0.0), axis=0)   # because there is exactly one True\n\n          This is acceptable for a small num_classes (10).\n\n        Steps for one row:\n\n          row = ... load the row (size 10) with a mask for the row (if the row is not padded, but we know it's 10)\n          mask = tl.arange(0, num_classes) == target_index   # target_index is a scalar? We have to broadcast it to the same shape as the row.\n\n          How to broadcast? We can do:\n\n            target_index_broadcast = tl.broadcast_to(target_index, [num_classes])\n\n          Then: mask = tl.arange(0, num_classes) == target_index_broadcast\n\n          Then: target_val = tl.sum(tl.where(mask, row, 0.0), axis=0)\n\n          Then: row_loss = log_sum - target_val\n\n        Then, we store row_loss in an output tensor of [batch_size].\n\n        Then, after the kernel, we compute the mean on the host? Or we can do a second kernel for reduction? But the batch_size is 4096, which is small.\n\n        We'll do the mean on the host.\n\n        But note: the problem says the inputs are [batch_size, *input_shape] and targets [batch_size]. Our kernel will output a tensor of [batch_size] for the per-row loss.\n\n        Then, we do:\n\n          loss = -output_tensor.mean()   # but note: in the formula we have (log_sum - target_val) and the target_val is the logit at the target. But the standard cross entropy is negative log probability, so we have:\n\n          loss = (log_sum - target_val)   # this is the negative log probability? \n\n          Let me check:\n\n            log_softmax = log(exp(x_i) / sum(exp(x_j))) = x_i - log_sum\n            so the log probability at the target is: x_target - log_sum\n\n            then the negative log probability is: - (x_target - log_sum) = log_sum - x_target\n\n          So we have computed the per-row loss as: log_sum - x_target.\n\n          Therefore, we then take the mean.\n\n        So the kernel output is a tensor of [batch_size] with the per-row loss.\n\n        Then we do:\n\n          total_loss = tl.sum(per_row_loss)   # but we want the mean.\n\n        Alternatively, we can do:\n\n          We can compute the mean in the kernel? We can do a two-step reduction.\n\n        However, for simplicity, we do:\n\n          Step 1: kernel1 computes per_row_loss of size [batch_size]\n          Step 2: on host: loss = per_row_loss.mean()\n\n        But note: the host is CPU and we are doing a single mean. This is acceptable.\n\n   However, we are going to write two kernels? We want to do it in one kernel? Actually, we can do the entire reduction in one kernel? But the problem is that we have to compute per-row logsumexp and then also the overall mean.\n\n        We can do:\n\n          Step 1: Each row computes its loss and then we do a reduction across the entire batch? But that would require a global reduction and we have 4096 rows. We can do:\n\n            - We break the batch into blocks, each block computes the sum of the per-row loss for the rows in the block, then we do a second block to sum the block sums.\n\n          But that complicates the kernel.\n\n        Given that 4096 is small, we can do the reduction on the host.\n\n   Steps:\n\n        We'll write a kernel that:\n\n          @triton.jit\n          def _cross_entropy_forward_kernel(\n              predictions_ptr, \n              targets_ptr, \n              output_ptr, \n              stride_row, \n              batch_size, \n              num_classes, \n              BLOCK_SIZE: tl.constexpr   # BLOCK_SIZE should be at least num_classes? Actually, we are using one row per program, so BLOCK_SIZE is not used for the row. We are using the grid for the batch.\n\n          ):\n\n            pid = tl.program_id(0)\n            if pid >= batch_size:\n                return\n\n            # Load the target for this row\n            target_index = tl.load(targets_ptr + pid)\n\n            # Compute the row in predictions\n            row_offsets = pid * stride_row + tl.arange(0, num_classes)\n            row = tl.load(predictions_ptr + row_offsets, mask=tl.arange(0, num_classes) < num_classes, other=-float('inf'))\n\n            # Compute max of the row\n            max_val = tl.max(row, axis=0)\n\n            # Subtract max and exponentiate\n            exp_row = tl.exp(row - max_val)\n            exp_sum = tl.sum(exp_row, axis=0)\n            log_sum = max_val + tl.log(exp_sum)\n\n            # Now, get the target value: we create a mask for the target index\n            col_indices = tl.arange(0, num_classes)\n            mask = col_indices == tl.broadcast_to(target_index, [num_classes])\n            target_val = tl.sum(tl.where(mask, row, 0.0), axis=0)\n\n            # Compute the per-row loss: log_sum - target_val\n            per_row_loss = log_sum - target_val\n\n            # Store the per-row loss in the output\n            tl.store(output_ptr + pid, per_row_loss)\n\n        Then, in the forward function:\n\n          per_row_loss = torch.empty(batch_size, device=predictions.device, dtype=torch.float32)\n          grid = (batch_size,)\n          _cross_entropy_forward_kernel[grid](predictions, targets, per_row_loss, ...)\n\n          loss = per_row_loss.mean()\n\n        But note: the original function returns a scalar tensor.\n\n   However, we must check the numerical stability? The log_sum computation is stable.\n\n   But note: what if exp_sum is 0? Then log(0) is -inf. But we clamp? The PyTorch version doesn't clamp. We do the same.\n\n   Also, we are using the same formula.\n\n   We must set the grid and the kernel parameters.\n\n   How to set the stride_row? It's the stride of the predictions tensor in the row dimension (dim=0). We can compute:\n\n        stride_row = predictions.stride(0)   # but note: the tensor is [batch_size, num_classes] -> row-major: stride(0) = num_classes? Not necessarily if it's not contiguous.\n\n   We are given that the input is from `get_inputs` which returns contiguous tensors? We should make the input contiguous.\n\n   We can do in the forward:\n\n        predictions = predictions.contiguous()\n        targets = targets.contiguous()\n\n   Then, stride_row = predictions.stride(0)   # which is num_classes? Actually, for a contiguous tensor of [batch_size, num_classes], stride(0) = num_classes.\n\n   But note: the tensor might be non-contiguous? We are given from the test. We should make it contiguous.\n\n   Alternatively, we can use:\n\n        stride_row = predictions.stride(0)\n\n   But we have to pass it to the kernel.\n\n   We also have to pass batch_size and num_classes.\n\n   We note: num_classes is 10.\n\n   We can set BLOCK_SIZE to the next power of two of num_classes? But we are using a fixed num_classes=10. We can set BLOCK_SIZE=16? But the kernel doesn't use BLOCK_SIZE for the row because we are using a fixed row length.\n\n   Actually, the kernel doesn't require a block size? We are using one program per row, and the row is loaded with a fixed-length vector of 10.\n\n   However, we are using a vector operation of length 10. We can set the BLOCK_SIZE to 16? But it's not used.\n\n   We can remove the BLOCK_SIZE? But the kernel requires at least one constexpr. We can remove it? Or we can leave it and set it to 16.\n\n   Alternatively, we can use:\n\n        BLOCK_SIZE: tl.constexpr = num_classes   # but num_classes is not a constant? We can pass it as a constant? We can set it as a config.\n\n   Actually, we can pass num_classes as a tl.constexpr? Yes.\n\n   So:\n\n        @triton.jit\n        def _cross_entropy_forward_kernel(\n            predictions_ptr, \n            targets_ptr, \n            output_ptr, \n            stride_row, \n            batch_size, \n            num_classes: tl.constexpr,   # we make it a compile-time constant\n            ...\n        ):\n\n   Then, we call the kernel with num_classes=10.\n\n   Steps in the forward function:\n\n        predictions = predictions.contiguous()\n        targets = targets.contiguous()\n\n        batch_size = predictions.shape[0]\n        num_classes = predictions.shape[1]\n\n        per_row_loss = torch.empty(batch_size, device=predictions.device, dtype=torch.float32)\n\n        grid = (batch_size,)\n\n        # We need the stride for the row dimension (dim0) of predictions\n        stride_row = predictions.stride(0)\n\n        _cross_entropy_forward_kernel[grid](\n            predictions, \n            targets, \n            per_row_loss, \n            stride_row, \n            batch_size, \n            num_classes\n        )\n\n        loss = per_row_loss.mean()\n\n        return loss\n\n   But note: the original function returns a scalar tensor. Our loss is a scalar tensor.\n\n   However, we must test for correctness.\n\n   But note: the problem says we are at the reference code stage? We are to write a Triton kernel to replace it.\n\n   We must also consider performance: we are doing one kernel launch with batch_size blocks. Each block has one thread? Actually, the kernel is written for one program per row, and each program is executed by a set of threads? We haven't specified the block shape.\n\n   How do we set the number of threads per block? We can let Triton choose? Or we can set:\n\n        We are using a vector of length 10. We can set 32 threads per block? And then each block handles 32 rows? \n\n        Actually, we can restructure the kernel to process multiple rows per block.\n\n        We can do:\n\n          pid = tl.program_id(0)   # block index\n          block_start = pid * BLOCK_SIZE_ROWS\n          Then, we process BLOCK_SIZE_ROWS rows in the block.\n\n        But we are constrained by the shared memory? We don't use shared memory.\n\n        Alternatively, we can do:\n\n          Each thread in the block processes one row? Then we can have a block of 1024 threads and process 1024 rows per block.\n\n        Then, the grid would be ceil(batch_size / 1024) blocks.\n\n        But note: the row is small (10) and each row requires 10*4=40 bytes? So we can load 10 elements per thread? That is acceptable.\n\n        We can rewrite the kernel to process multiple rows per block.\n\n        However, given the time, we start with one row per program and then we can optimize later.\n\n        But note: the batch_size is 4096. We can set the grid to 4096 and each program one row. Then, we set the block size to 1? That would be inefficient because we are only using 10 threads? Actually, we are using a vector operation of 10, so we can set the block to 32 threads? And then each block handles 32 rows? \n\n        Actually, we can set:\n\n          grid = (triton.cdiv(batch_size, 32),)   # 4096/32 = 128 blocks\n          block = (32,)\n\n        Then, in the kernel:\n\n          pid = tl.program_id(0)   # from 0 to 127\n          tid = tl.program_id(1)   # no, we are in 1D grid and 1D block? Actually, we are using 1D block.\n\n          Let me use:\n\n            row_idx = pid * BLOCK_SIZE_ROWS + tl.arange(0, BLOCK_SIZE_ROWS)\n            mask_row = row_idx < batch_size\n\n          But we want to process 32 rows per block? Actually, we can assign each thread in the block to one row? Then:\n\n            row_index = pid * 32 + tid\n            if row_index < batch_size:\n                ... process one row ...\n\n          We set BLOCK_SIZE_ROWS = 32.\n\n        How to set the block: (32,)\n\n        Then, the grid is (ceil(batch_size/32),)\n\n        This would be 4096/32 = 128 blocks.\n\n        Then, each thread does:\n\n          row_index = pid * 32 + tid\n          if row_index >= batch_size: return\n\n          target_index = tl.load(targets_ptr + row_index)\n\n          row_offsets = row_index * stride_row + tl.arange(0, num_classes)\n          row = tl.load(predictions_ptr + row_offsets, mask=tl.arange(0, num_classes) < num_classes, other=-float('inf'))\n\n          ... then compute logsumexp and target_val ...\n\n          per_row_loss = log_sum - target_val\n\n          tl.store(output_ptr + row_index, per_row_loss)\n\n        This is efficient because we have 32 threads per block, each thread processes one row.\n\n        We set num_warps: we can set 1 warp per block? Since 32 threads is one warp.\n\n        So we set num_warps=1.\n\n        We can also try to use more warps? But one warp per block is fine.\n\n        We can write:\n\n          @triton.jit\n          def _cross_entropy_forward_kernel(\n              predictions_ptr, \n              targets_ptr, \n              output_ptr, \n              stride_row, \n              batch_size, \n              num_classes: tl.constexpr, \n              BLOCK_SIZE_ROWS: tl.constexpr   # number of rows per block, which is the block size (32)\n          ):\n              pid = tl.program_id(0)\n              tid = tl.local_program_id(0)   # thread index in the block\n\n              row_index = pid * BLOCK_SIZE_ROWS + tid\n              if row_index >= batch_size:\n                  return\n\n              # Load the target for this row\n              target_index = tl.load(targets_ptr + row_index)\n\n              # Compute the row in predictions\n              row_offsets = row_index * stride_row + tl.arange(0, num_classes)\n              row = tl.load(predictions_ptr + row_offsets, mask=tl.arange(0, num_classes) < num_classes, other=-float('inf'))\n\n              # Compute max of the row\n              max_val = tl.max(row, axis=0)\n\n              # Subtract max and exponentiate\n              exp_row = tl.exp(row - max_val)\n              exp_sum = tl.sum(exp_row, axis=0)\n              log_sum = max_val + tl.log(exp_sum)\n\n              # Create mask for the target index\n              col_indices = tl.arange(0, num_classes)\n              mask = col_indices == tl.broadcast_to(target_index, [num_classes])\n              target_val = tl.sum(tl.where(mask, row, 0.0), axis=0)\n\n              per_row_loss = log_sum - target_val\n              tl.store(output_ptr + row_index, per_row_loss)\n\n        Then, in the forward:\n\n          BLOCK_SIZE_ROWS = 32\n          grid = (triton.cdiv(batch_size, BLOCK_SIZE_ROWS),)\n          _cross_entropy_forward_kernel[grid](..., BLOCK_SIZE_ROWS=BLOCK_SIZE_ROWS)\n\n   We must set the num_warps? We can set:\n\n        num_warps = BLOCK_SIZE_ROWS // 32? But BLOCK_SIZE_ROWS is 32, which is one warp. So we set num_warps=1.\n\n        How? We can set:\n\n          @triton.jit\n          def kernel(...):\n              ...\n\n          Then, when we call it, we set the number of warps? Actually, we can set it in the decorator? Or we can set it by autotuning? We can set:\n\n          @triton.jit\n          def kernel(...):   # without specifying num_warps, then it uses the block size to infer? We set the block to (32,), so it will use one warp.\n\n        Alternatively, we can set:\n\n          kernel[grid, stream, num_warps=1](...)\n\n        But we are not specifying the block? In Triton, the block is the same as the block size we are using? Actually, we are using a 1D block of size BLOCK_SIZE_ROWS (32). Then, by default, it will use one warp? Because 32 threads = 1 warp.\n\n   We'll write the kernel accordingly.\n\n   Let me write the complete code.\n\n   We note: the kernel uses tl.exp and tl.log. These are available.\n\n   We also note: we are using tl.broadcast_to? But we can do:\n\n        mask = col_indices == target_index   # and target_index is a scalar? But col_indices is [num_classes]. This will broadcast.\n\n   So we don't need tl.broadcast_to? Because the scalar target_index will be broadcasted to the shape of col_indices.\n\n   So:\n\n        mask = col_indices == target_index\n\n   Then, target_val = tl.sum(tl.where(mask, row, 0.0), axis=0)\n\n   We are ready.\n\n   We must also note: the data types. The predictions are float32? The targets are int64? But we are storing per_row_loss as float32.\n\n   In the kernel, we load targets as int32? But the targets are generated by torch.randint(0, num_classes, ...) which is int64. We can load as int32? Or we can change the kernel to load as int64.\n\n   We can do:\n\n        target_index = tl.load(targets_ptr + row_index)   # by default, it loads as int32? But if the tensor is int64, we need to specify the type.\n\n   We can specify:\n\n        target_index = tl.load(targets_ptr + row_index, mask=None).to(tl.int32)\n\n   Or we can change the targets to int32? But the original function accepts int64.\n\n   We can do in the forward:\n\n        targets = targets.to(torch.int32)\n\n   Alternatively, we can load as int64? Triton supports int64? Yes.\n\n   We can do:\n\n        target_index = tl.load(targets_ptr + row_index).to(tl.int64)\n\n   But note: the mask we create uses col_indices which is int32? We should cast target_index to int32? Or we can use int64 for col_indices? But the num_classes is 10, so int32 is enough.\n\n   We can cast:\n\n        target_index = tl.load(targets_ptr + row_index).to(tl.int32)\n\n   So:\n\n        targets = targets.to(torch.int32)   # on the host, to avoid kernel complexity.\n\n   We do:\n\n        targets = targets.contiguous().to(torch.int32)\n\n   Then, in the kernel, we load as int32.\n\n   Alternatively, we can do the conversion in the kernel? We can load as int64 and then convert to int32? But that is inefficient.\n\n   We choose to convert on the host.\n\n   Steps:\n\n        predictions = predictions.contiguous()\n        targets = targets.contiguous().to(torch.int32)\n\n   Then, the kernel uses int32 for target_index.\n\n   We write the kernel accordingly.\n\n   We also note: the kernel must be compiled for the specific num_classes=10? We pass it as a constant.\n\n   We are ready to code.\n\n   We must also note: the original function returns a scalar tensor. Our function returns the same.\n\n   We are going to replace the forward function of ModelNew.\n\n   We note: the batch_size and num_classes are fixed? The batch_size is 4096 and num_classes is 10? Actually, the model should work for any batch_size and num_classes? But the problem says the inputs are from get_inputs which returns batch_size=4096 and num_classes=10.\n\n   However, we are to write a general model? We should use the actual shape.\n\n   We do:\n\n        batch_size = predictions.shape[0]\n        num_classes = predictions.shape[1]\n\n   We pass these to the kernel.\n\n   We set:\n\n        BLOCK_SIZE_ROWS = 32   # we can make it a constant in the kernel call.\n\n   We write the kernel and the forward function.\n\n   We must also note: we are to return the loss as a scalar tensor. Our per_row_loss is of shape [batch_size] and then we take the mean, which is a scalar.\n\n   We return loss.\n\n   We are ready.\n\n   However, note: the original function might have other parameters? The F.cross_entropy has parameters like weight, reduction, etc. We are ignoring them. We assume reduction='mean' and no weight.\n\n   The problem says: the original function is:\n\n        return torch.nn.functional.cross_entropy(predictions, targets)\n\n   So we are matching that.\n\n   We write the code accordingly.\n\n   Let me write the entire EVOLVE-BLOCK.\n\n   We note: we are not allowed to change the signature of the ModelNew.forward function.\n\n   We are going to write:\n\n        import triton\n        import triton.language as tl\n\n   And then define the kernel and the forward.\n\n   We must also note: the kernel must be defined outside the class.\n\n   We put the kernel at the top of the block.\n\n   We write:\n\n        import math\n        import torch\n        import torch.nn as nn\n        import triton\n        import triton.language as tl\n\n        @triton.jit\n        def _cross_entropy_forward_kernel(\n            predictions_ptr, \n            targets_ptr, \n            output_ptr, \n            stride_row, \n            batch_size, \n            num_classes: tl.constexpr, \n            BLOCK_SIZE_ROWS: tl.constexpr\n        ):\n            pid = tl.program_id(0)\n            tid = tl.local_program_id(0)\n\n            row_index = pid * BLOCK_SIZE_ROWS + tid\n            if row_index >= batch_size:\n                return\n\n            target_index = tl.load(targets_ptr + row_index).to(tl.int32)\n\n            # Load the row of predictions\n            col_offsets = tl.arange(0, num_classes)\n            row_offsets = row_index * stride_row + col_offsets\n            row = tl.load(predictions_ptr + row_offsets, mask=col_offsets < num_classes, other=-float('inf'))\n\n            # Compute logsumexp for the row\n            max_val = tl.max(row, axis=0)\n            exp_row = tl.exp(row - max_val)\n            exp_sum = tl.sum(exp_row, axis=0)\n            log_sum = max_val + tl.log(exp_sum)\n\n            # Extract the target value\n            mask = col_offsets == target_index\n            target_val = tl.sum(tl.where(mask, row, 0.0), axis=0)\n\n            per_row_loss = log_sum - target_val\n            tl.store(output_ptr + row_index, per_row_loss)\n\n        class ModelNew(nn.Module):\n            def __init__(self):\n                super(ModelNew, self).__init__()\n\n            def forward(self, predictions, targets):\n                # Make sure the inputs are contiguous and targets are int32\n                predictions = predictions.contiguous()\n                targets = targets.contiguous().to(torch.int32)\n\n                batch_size, num_classes = predictions.shape\n                per_row_loss = torch.empty(batch_size, device=predictions.device, dtype=torch.float32)\n\n                BLOCK_SIZE_ROWS = 32\n                grid = (triton.cdiv(batch_size, BLOCK_SIZE_ROWS),)\n\n                # We need the stride for the row dimension (dim0) of predictions\n                stride_row = predictions.stride(0)\n\n                _cross_entropy_forward_kernel[grid](\n                    predictions, \n                    targets, \n                    per_row_loss, \n                    stride_row, \n                    batch_size, \n                    num_classes, \n                    BLOCK_SIZE_ROWS\n                )\n\n                loss = per_row_loss.mean()\n                return loss\n\n        batch_size = 4096\n        num_classes = 10\n        input_shape = (num_classes, )  # Output for each class\n        dim = 1\n\n        def get_inputs():\n            return [torch.randn(batch_size, *input_shape), torch.randint(0, num_classes, (batch_size,))]\n\n        def get_init_inputs():\n            return []\n\n   We hope this will be faster.\n\n   However, note: the PyTorch implementation is highly optimized. We are doing a kernel that might be memory bound? And we are doing one row at a time.\n\n   We hope that the kernel will be faster because we are fusing the operations and avoiding materializing the entire log_softmax tensor.\n\n   We'll see.\n\n   If it doesn't improve, we might need to optimize further.\n\n   But we are required to change only the EVOLVE-BLOCK.\n\n   We output the entire code.\n\n   Note: We are using the same batch_size and num_classes as before.\n\n   We note: the get_inputs function returns the same as before.\n\n   We are done.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef cross_entropy_kernel(\n    predictions_ptr,\n    targets_ptr,\n    output_ptr,\n    stride_pred,\n    batch_size,\n    num_classes: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    row_idx = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = row_idx < batch_size\n\n    # Load target indices for the block\n    target_indices = tl.load(targets_ptr + row_idx, mask=mask, other=0)\n    \n    # Compute row start positions in predictions tensor\n    row_starts = row_idx * stride_pred\n    row_offsets = row_starts[:, None] + tl.arange(0, num_classes)[None, :]\n    \n    # Load prediction rows with boundary mask\n    pred_rows = tl.load(\n        predictions_ptr + row_offsets,\n        mask=mask[:, None] & (tl.arange(0, num_classes)[None, :] < num_classes),\n        other=-float('inf')\n    )\n    \n    # Compute row-wise max for numerical stability\n    row_max = tl.max(pred_rows, axis=1)\n    # Compute logsumexp\n    exp_vals = tl.exp(pred_rows - row_max[:, None])\n    log_sumexp = row_max + tl.log(tl.sum(exp_vals, axis=1))\n    \n    # Extract target values using advanced indexing\n    target_vals = tl.sum(\n        tl.where(\n            tl.arange(0, num_classes)[None, :] == target_indices[:, None],\n            pred_rows,\n            0.0\n        ),\n        axis=1\n    )\n    \n    # Compute per-row loss: log_sumexp - target_vals\n    per_row_loss = log_sumexp - target_vals\n    tl.store(output_ptr + row_idx, per_row_loss, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, predictions, targets):\n        predictions = predictions.contiguous()\n        targets = targets.contiguous().to(torch.int32)\n        \n        batch_size, num_classes = predictions.shape\n        per_row_loss = torch.empty(batch_size, device=predictions.device, dtype=torch.float32)\n        \n        # Configure kernel launch parameters\n        BLOCK_SIZE = 128\n        grid = (triton.cdiv(batch_size, BLOCK_SIZE),)\n        \n        cross_entropy_kernel[grid](\n            predictions,\n            targets,\n            per_row_loss,\n            predictions.stride(0),\n            batch_size,\n            num_classes,\n            BLOCK_SIZE\n        )\n        \n        return per_row_loss.mean()\n\nbatch_size = 4096\nnum_classes = 10\ninput_shape = (num_classes, )\ndim = 1\n\ndef get_inputs():\n    return [torch.randn(batch_size, *input_shape), torch.randint(0, num_classes, (batch_size,))]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
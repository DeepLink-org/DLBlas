{"id": "9ad399de-a104-415a-998d-0d65707f510a", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef linear_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    in_features, out_features, batch_size,\n    stride_x, stride_out,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    pid_batch = pid // out_features\n    pid_feature = pid % out_features\n\n    if pid_batch >= batch_size or pid_feature >= out_features:\n        return\n\n    output_offset = pid_batch * stride_out + pid_feature\n    bias_val = tl.load(bias_ptr + pid_feature)\n    acc = bias_val\n\n    # Vectorization for improved memory throughput\n    VEC_SIZE = 4\n    for block in range(0, in_features, BLOCK_SIZE):\n        input_block_ptr = input_ptr + pid_batch * stride_x + block\n        weight_block_ptr = weight_ptr + pid_feature * in_features + block\n\n        for i in range(0, BLOCK_SIZE, VEC_SIZE):\n            idx = block + i + tl.arange(0, VEC_SIZE)\n            mask = idx < in_features\n            \n            # Load vectorized inputs and weights\n            x_vec = tl.load(input_block_ptr + i + tl.arange(0, VEC_SIZE), mask=mask, other=0.0)\n            w_vec = tl.load(weight_block_ptr + i + tl.arange(0, VEC_SIZE), mask=mask, other=0.0)\n            \n            # Accumulate using vector dot product\n            acc += tl.dot(x_vec, w_vec)\n\n    tl.store(output_ptr + output_offset, acc)\n\nclass TritonLinear(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.in_features = in_features\n        self.out_features = out_features\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        output = torch.empty(x.size(0), self.out_features, device=x.device, dtype=x.dtype)\n        flat_x = x.view(-1, x.size(-1))\n        batch_size, in_features = flat_x.shape\n\n        grid = (batch_size * self.out_features,)\n        BLOCK_SIZE = 128\n        \n        linear_kernel[grid](\n            flat_x, self.weight, self.bias, output,\n            in_features, self.out_features, batch_size,\n            flat_x.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        # VGG16 architecture: 5 blocks of convolutional layers\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv\u6781d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=\u6781),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 2\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 3\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 4\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Block 5\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        # Fully connected layers with Triton acceleration\n        self.classifier = nn.Sequential(\n            TritonLinear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.0),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.0),\n            nn.Linear(4096, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "535b03d7-ff14-41dc-9d9f-70b02a9d9a06", "generation": 2, "timestamp": 1754590686.153294, "iteration_found": 27, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "module 'torch.nn' has no attribute 'Conv\u6781d'\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/evaluator.py\", line 173, in _verification_worker\n    result: KernelExecResult = eval_kernel_against_ref(\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 484, in eval_kernel_against_ref\n    custom_model = ModelNew(*init_args, **init_kwargs)\n  File \"/tmp/tmpqbo0j393.py\", line 85, in __init__\n    nn.Conv\u6781d(3, 64, kernel_size=3, padding=1),\nAttributeError: module 'torch.nn' has no attribute 'Conv\u6781d'. Did you mean: 'Conv1d'?\n", "exec_log": "[Child] \u7ed1\u5b9a GPU 6, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 10, in <module>\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 844, in jit\n    return decorator(fn)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 832, in decorator\n    return JITFunction(\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 635, in __init__\n    self.starting_line_number = inspect.getsourcelines(fn)[1]\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/inspect.py\", line 1121, in getsourcelines\n    lines, lnum = findsource(object)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/inspect.py\", line 958, in findsource\n    raise OSError('could not get source code')\nOSError: could not get source code\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.145, "runtime_ms": 5.67, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.824, "speed_up": 0.145, "custom_timing": 5.67}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
{"id": "f556f01c-a97f-4dac-97f8-05a3eb59165c", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Define optimized autotune configurations\nlinear_configs = [\n    triton.Config({'BLOCK_SIZE_IN': 128, 'BLOCK_SIZE_OUT': 64}, num_warps=4),\n    triton.Config({'BLOCK_SIZE_IN': 256, 'BLOCK_SIZE_OUT': 32}, num_warps=4),\n    triton.Config({'BLOCK_SIZE_IN': 64, 'BLOCK_SIZE_OUT': 128}, num_warps=4),\n    triton.Config({'BLOCK_SIZE_IN': 128, 'BLOCK_SIZE_OUT': 128}, num_warps=8),\n    triton.Config({'BLOCK_SIZE_IN': 256, 'BLOCK_SIZE_OUT': 64}, num_warps=8),\n]\n\n@triton.autotune(\n    configs=linear_configs,\n    key=['in_features', 'out_features']\n)\n@triton.jit\ndef _linear_relu_forward(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    out_ptr,\n    in_features,\n    out_features,\n    batch_size,\n    BLOCK_SIZE_IN: tl.constexpr,\n    BLOCK_SIZE_OUT: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_out = tl.program_id(1)\n    \n    # Check batch boundaries\n    if pid_batch >= batch_size:\n        return\n    \n    # Output feature indices for this block\n    offs_out = pid_out * BLOCK_SIZE_OUT + tl.arange(0, BLOCK_SIZE_OUT)\n    mask_out = offs_out < out_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_OUT,), dtype=tl.float32)\n    \n    # Loop over input features in blocks\n    for k in range(0, in_features, BLOCK_SIZE_IN):\n        offs_in = k + tl.arange(0, BLOCK_SIZE_IN)\n        mask_in = offs_in < in_features\n        \n        # Load input block: for current batch and current input block\n        x_vals = tl.load(x_ptr + pid_batch * in_features + offs_in, mask=mask_in, other=0.0)\n        \n        # Load weight block: for current output block and current input block\n        w_ptrs = w_ptr + offs_out[:, None] * in_features + offs_in[None, :]\n        w_vals = tl.load(w_ptrs, mask=mask_out[:, None] & mask_in[None, :], other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(w_vals * x_vals[None, :], axis=1)\n    \n    # Add bias\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + offs_out, mask=mask_out, other=0.0)\n        acc += bias\n    \n    # Apply ReLU activation\n    acc = tl.maximum(acc, 0.0)\n    \n    # Store output block: for current batch and current output block\n    out_ptrs = out_ptr + pid_batch * out_features + offs_out\n    tl.store(out_ptrs, acc, mask=mask_out)\n\n@triton.autotune(\n    configs=linear_configs,\n    key=['in_features', 'out_features']\n)\n@triton.jit\ndef _linear_forward(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    out_ptr,\n    in_features,\n    out_features,\n    batch_size,\n    BLOCK_SIZE_IN: tl.constexpr,\n    BLOCK_SIZE_OUT: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_out = tl.program_id(1)\n    \n    # Check batch boundaries\n    if pid_batch >= batch_size:\n        return\n    \n    # Output feature indices for this block\n    offs_out = pid_out * BLOCK_SIZE_OUT + tl.arange(0, BLOCK_SIZE_OUT)\n    mask_out = offs_out < out_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_OUT,), dtype=tl.float32)\n    \n    # Loop over input features in blocks\n    for k in range(0, in_features, BLOCK_SIZE_IN):\n        offs_in = k + tl.arange(0, BLOCK_SIZE_IN)\n        mask_in = offs_in < in_features\n        \n        # Load input block: for current batch and current input block\n        x_vals = tl.load(x_ptr + pid_batch * in_features + offs_in, mask=mask_in, other=0.0)\n        \n        # Load weight block: for current output block and current input block\n        w_ptrs = w_ptr + offs_out[:, None] * in_features + offs_in[None, :]\n        w_vals = tl.load(w_ptrs, mask=mask_out[:, None] & mask_in[None, :], other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(w_vals * x_vals[None, :], axis=1)\n    \n    # Add bias\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + offs_out, mask=mask_out, other=0.0)\n        acc += bias\n    \n    # Store output block: for current batch and current output block\n    out_ptrs = out_ptr + pid_batch * out_features + offs_out\n    tl.store(out_ptrs, acc, mask=mask_out)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_layer_sizes, output_size):\n        super(ModelNew, self).__init__()\n        self.layers = nn.ModuleList()\n        self.activations = []\n        \n        current_size = input_size\n        for hidden_size in hidden_layer_sizes:\n            linear = nn.Linear(current_size, hidden_size)\n            self.layers.append(linear)\n            self.activations.append('relu')\n            current_size = hidden_size\n        \n        output_layer = nn.Linear(current_size, output_size)\n        self.layers.append(output_layer)\n        self.activations.append(None)\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        for layer, activation in zip(self.layers, self.activations):\n            in_features = layer.weight.size(1)\n            out_features = layer.weight.size(0)\n            \n            out = torch.empty(batch_size, out_features, device=x.device, dtype=x.dtype)\n            \n            # Optimized grid configuration\n            grid = (\n                batch_size,\n                triton.cdiv(out_features, linear_configs[0].kwargs['BLOCK_SIZE_OUT'])\n            )\n            \n            if activation == 'relu':\n                _linear_relu_forward[grid](\n                    x, \n                    layer.weight, \n                    layer.bias,\n                    out,\n                    in_features,\n                    out_features,\n                    batch_size,\n                )\n            else:\n                _linear_forward[grid](\n                    x, \n                    layer.weight, \n                    layer.bias,\n                    out,\n                    in_features,\n                    out_features,\n                    batch_size,\n                )\n            x = out\n        \n        return x\n\n# Test code\nbatch_size = 1\ninput_size = 1000\nhidden_layer_sizes = [50, 50, 50, 50, 50, 50, 50, 50]\noutput_size = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_layer_sizes, output_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "e8ea0347-0af5-4fd6-846a-6995fda50654", "generation": 3, "timestamp": 1754596842.0819616, "iteration_found": 26, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_3_DeepNarrowMLP", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "max_difference": ["0.127469", "0.016945", "0.016547", "0.016870", "0.016889"], "avg_difference": ["0.072950", "0.007735", "0.007549", "0.007686", "0.007598"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.227, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.538, "runtime_ms": 0.422, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.227, "speed_up": 0.538, "custom_timing": 0.422}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
{"id": "e5672de7-a856-45b0-9485-9ea04ea273fa", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE\": 256}, num_warps=1),\n        triton.Config({\"BLOCK_SIZE\": 512}, num_warps=1),\n        triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=1),\n        triton.Config({\"BLOCK_SIZE\": 2048}, num_warps=2),\n        triton.Config({\"BLOCK_SIZE\": 4096}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 8192}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n@triton.jit\ndef fused_activation_kernel(\n    output_ptr,\n    input_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Optimized fused ReLU + HardSwish with minimal operations\n    x = tl.maximum(x, 0.0)  # Efficient ReLU\n    # Compute HardSwish with fused arithmetic\n    hardswish = x * tl.minimum((x + 3.0) * 0.16666667, 1.0)\n    tl.store(output_ptr + offsets, hardswish, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_activation_kernel[grid](output, x, n_elements)\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "d5068699-c76e-49b8-b61f-14e126ab42ec", "generation": 3, "timestamp": 1754576015.396031, "iteration_found": 22, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.829, "runtime_ms": 0.0887, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0735, "speed_up": 0.829, "custom_timing": 0.0887}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.874, "runtime_ms": 0.0841, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0735, "speed_up": 0.874, "custom_timing": 0.0841}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.8740\n- runtime_ms: 0.0841\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0735, 'speed_up': 0.874, 'custom_timing': 0.0841}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.0100, runtime_ms: 7.4000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0735, 'speed_up': 0.01, 'custom_timing': 7.4}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 25.2000, speed_up: 1.0520, runtime_ms: 0.0699, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0735, 'speed_up': 1.052, 'custom_timing': 0.0699}\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 26.1000, speed_up: 1.0610, runtime_ms: 0.0693, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0735, 'speed_up': 1.061, 'custom_timing': 0.0693}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 26.1000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_activation_kernel(\n    output_ptr,\n    input_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Optimized fused ReLU + HardSwish\n    x = tl.maximum(x, 0.0)  # ReLU with efficient primitive\n    hardswish = x * tl.minimum((x + 3.0) * 0.16666667, 1.0)  # Combined operations\n    tl.store(output_ptr + offsets, hardswish, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        output = torch.empty_like(x)\n        fused_activation_kernel[grid](output, x, n_elements, BLOCK_SIZE=1024)\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (26.1000), Performs well on speed_up (1.0610), Performs well on runtime_ms (0.0693), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0735, 'speed_up': 1.061, 'custom_timing': 0.0693})\n\n\n### Program 2 (Score: 25.2000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_activation_kernel(\n    output_ptr,\n    input_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Fused ReLU + HardSwish\n    relu = tl.where(x >= 0, x, 0.0)\n    hardswish = relu * tl.minimum(tl.maximum(relu + 3.0, 0.0), 6.0) / 6.0\n    tl.store(output_ptr + offsets, hardswish, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_activation_kernel[grid](output, x, n_elements, BLOCK_SIZE=1024)\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (25.2000), Performs well on speed_up (1.0520), Performs well on runtime_ms (0.0699), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0735, 'speed_up': 1.052, 'custom_timing': 0.0699})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_conv_activation_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    in_channels, kernel_size, height, width,\n    out_height, out_width,\n    stride_xb, stride_xc, stride_xh, stride_xw,\n    stride_woc, stride_wic, stride_wh, stride_ww,\n    out_channels,\n    BLOCK_SIZE: tl.constexpr,\n):\n    # 3D grid: (ow, oh, batch_oc)\n    ow = tl.program_id(0)\n    oh = tl.program_id(1)\n    batch_oc = tl.program_id(2)\n    \n    # Calculate batch and output channel indices\n    b = batch_oc // out_channels\n    oc = batch_oc % out_channels\n    \n    acc = 0.0\n    # Optimized convolution loop\n    for ic in range(in_channels):\n        for kh in range(kernel_size):\n            for kw in range(kernel_size):\n                ih = oh + kh\n                iw = ow + kw\n                if ih < height and iw < width:\n                    x_off = b * stride_xb + ic * stride_xc + ih * stride_xh + iw * stride_xw\n                    w_off = oc * stride_woc + ic * stride_wic + kh * stride_wh + kw * stride_ww\n                    x_val = tl.load(x_ptr + x_off)\n                    w_val = tl.load(weight_ptr + w_off)\n                    acc += x_val * w_val\n    \n    # Add bias\n    b_val = tl.load(bias_ptr + oc)\n    acc += b_val\n    \n    # Fused ReLU + HardSwish\n    acc = tl.maximum(acc, 0.0)\n    tmp = (acc + 3.0) / 6.0\n    tmp = tl.minimum(tl.maximum(tmp, 0.0), 1.0)\n    acc = acc * tmp\n    \n    # Store result\n    out_off = b * out_channels * out_height * out_width + oc * out_height * out_width + oh * out_width + ow\n    tl.store(output_ptr + out_off, acc)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, kernel_size, kernel_size))\n        self.bias = nn.Parameter(torch.empty(out_channels))\n        # Initialize weights using Kaiming uniform\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        # Initialize bias\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch, in_channels, height, width = x.shape\n        out_height = height - self.kernel_size + 1\n        out_width = width - self.kernel_size + 1\n        \n        # Ensure contiguous memory access\n        x = x.contiguous()\n        weight = self.weight.contiguous()\n        bias = self.bias.contiguous()\n        \n        # Prepare output tensor\n        output = torch.empty((batch, self.out_channels, out_height, out_width), \n                            device=x.device, dtype=x.dtype)\n        \n        # Extract tensor strides\n        stride_xb, stride_xc, stride_xh, stride_xw = x.stride()\n        stride_woc, stride_wic, stride_wh, stride_ww = weight.stride()\n        \n        # Configure 3D grid\n        grid = (out_width, out_height, batch * self.out_channels)\n        \n        # Launch optimized kernel\n        fused_conv_activation_kernel[grid](\n            x, weight, bias, output,\n            self.in_channels, self.kernel_size, height, width,\n            out_height, out_width,\n            stride_xb, stride_xc, stride_xh, stride_xw,\n            stride_woc, stride_wic, stride_wh, stride_ww,\n            self.out_channels,\n            BLOCK_SIZE=1\n        )\n        \n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.0100), Performs well on runtime_ms (7.4000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0735, 'speed_up': 0.01, 'custom_timing': 7.4})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE\": 256}, num_warps=1),\n        triton.Config({\"BLOCK_SIZE\": 512}, num_warps=1),\n        triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=1),\n        triton.Config({\"BLOCK_SIZE\": 2048}, num_warps=2),\n        triton.Config({\"BLOCK_SIZE\": 4096}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 8192}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n@triton.jit\ndef fused_activation_kernel(\n    output_ptr,\n    input_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Fused ReLU + HardSwish in single expression\n    result = tl.where(x < 0, 0.0, x * tl.minimum(x + 3.0, 6.0) * (1.0 / 6.0))\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_activation_kernel[grid](output, x, n_elements)\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_activation_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    x = tl.load(input_ptr + offsets, mask=mask)\n    # Fused ReLU: max(x, 0)\n    x = tl.maximum(x, 0.0)\n    # Fused HardSwish: x * clamp((x+3)/6, 0, 1)\n    y = (x + 3.0) * (1.0 / 6.0)\n    y = tl.minimum(tl.maximum(y, 0.0), 1.0)\n    out = x * y\n    tl.store(output_ptr + offsets, out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_activation_kernel[grid](x.view(-1), output.view(-1), n_elements, BLOCK_SIZE=1024)\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 26.1000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_activation_kernel(\n    output_ptr,\n    input_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Optimized fused ReLU + HardSwish\n    x = tl.maximum(x, 0.0)  # ReLU with efficient primitive\n    hardswish = x * tl.minimum((x + 3.0) * 0.16666667, 1.0)  # Combined operations\n    tl.store(output_ptr + offsets, hardswish, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        output = torch.empty_like(x)\n        fused_activation_kernel[grid](output, x, n_elements, BLOCK_SIZE=1024)\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 4}, num_warps=4, num_stages=1),\n        triton.Config({'BLOCK_SIZE': 2048, 'VEC_SIZE': 4}, num_warps=4, num_stages=1),\n        triton.Config({'BLOCK_SIZE': 4096, 'VEC_SIZE': 4}, num_warps=8, num_stages=1),\n        triton.Config({'BLOCK_SIZE': 8192, 'VEC_SIZE': 4}, num_warps=8, num_stages=1),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 8}, num_warps=4, num_stages=1),\n        triton.Config({'BLOCK_SIZE': 2048, 'VEC_SIZE': 8}, num_warps=4, num_stages=1),\n        triton.Config({'BLOCK_SIZE': 4096, 'VEC_SIZE': 8}, num_warps=8, num_stages=1),\n        triton.Config({'BLOCK_SIZE': 8192, 'VEC_SIZE': 8}, num_warps=8, num_stages=1),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef fused_activation_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n    VEC_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE, VEC_SIZE)\n    mask = (offsets + VEC_SIZE - 1) < n_elements\n    \n    x_vec = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    \n    # Vectorized fused activation\n    x_vec = tl.maximum(x_vec, 0.0)\n    t_vec = (x_vec + 3.0) * 0.16666667\n    t_vec = tl.minimum(t_vec, 1.0)\n    y_vec = x_vec * t_vec\n    \n    tl.store(output_ptr + offsets, y_vec, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_activation_kernel[grid](x, x, n_elements)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE\": 256}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 512}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 2048}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 4096}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n@triton.jit\ndef fused_activation_kernel(\n    output_ptr,\n    input_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Optimized fused ReLU + HardSwish with minimal operations\n    x = tl.maximum(x, 0.0)  # ReLU\n    # Compute HardSwish with fused arithmetic operations\n    t = (x + 3.0) * 0.16666667  # Fused add-multiply (1/6)\n    t = tl.minimum(t, 1.0)       # Clamp upper bound\n    hardswish = x * t            # Final multiplication\n    tl.store(output_ptr + offsets, hardswish, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        # Operate in-place to avoid memory allocation\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_activation_kernel[grid](x, x, n_elements)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: 25.2000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_activation_kernel(\n    output_ptr,\n    input_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Fused ReLU + HardSwish\n    relu = tl.where(x >= 0, x, 0.0)\n    hardswish = relu * tl.minimum(tl.maximum(relu + 3.0, 0.0), 6.0) / 6.0\n    tl.store(output_ptr + offsets, hardswish, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_activation_kernel[grid](output, x, n_elements, BLOCK_SIZE=1024)\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies ReLU, and applies HardSwish activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = x * torch.clamp((x + 3) / 6, 0, 1)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nUnique approach: Excellent compiled (1.000), Excellent correctness (1.000), Alternative passed approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE\": 256}, num_warps=1),\n        triton.Config({\"BLOCK_SIZE\": 512}, num_warps=1),\n        triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=1),\n        triton.Config({\"BLOCK_SIZE\": 2048}, num_warps=2),\n        triton.Config({\"BLOCK_SIZE\": 4096}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 8192}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n@triton.jit\ndef fused_activation_kernel(\n    output_ptr,\n    input_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Fused ReLU + HardSwish in single expression\n    result = tl.where(x < 0, 0.0, x * tl.minimum(x + 3.0, 6.0) * (1.0 / 6.0))\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_activation_kernel[grid](output, x, n_elements)\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program which is a Triton kernel for a fused ReLU + HardSwish activation applied after a convolution.\n The current program uses autotune for the BLOCK_SIZE and has a kernel that uses a single expression for the activation.\n\n However, the current performance (speed_up: 0.874) is worse than the reference (PyTorch). We need to improve it.\n\n Observations:\n - The current kernel uses a `tl.where` and a complex expression for the fused activation.\n - We note from the top-performing programs (Program 1 and Program 2) that they break down the activation into two steps:\n      Step 1: ReLU (using `tl.maximum(x, 0.0)`)\n      Step 2: HardSwish (using `x * tl.minimum((x + 3.0) * 0.16666667, 1.0)`)\n - The top program (Program 1) achieved a speedup of 1.061.\n\n Let's consider why the current approach might be slower:\n   - The `tl.where` might be more expensive than the `tl.maximum` for ReLU.\n   - The expression for HardSwish in the current kernel: `x * tl.minimum(x + 3.0, 6.0) * (1.0/6.0)` is mathematically equivalent to:\n        `x * (min(x+3, 6) / 6)`\n     but note that this is not the same as the standard HardSwish, which is:\n        `x * (min(max(x+3, 0), 6) / 6)`\n     Actually, the current kernel does not clamp the lower bound to 0. This might be a correctness issue? \n     However, note that if x < -3, then `x+3` is negative, and `min(x+3, 6)` would be negative, which then multiplied by x (which is negative) gives a positive? \n     But the standard HardSwish for x < -3 is 0. So the current kernel does not match.\n\n Correction: The standard HardSwish is defined as:\n        x * (min(max(x+3, 0), 6) / 6\n\n The current kernel does:\n        result = tl.where(x < 0, 0.0, x * tl.minimum(x + 3.0, 6.0) * (1.0 / 6.0))\n\n But note: for x>=0, it does `x * min(x+3, 6) / 6` which is correct for x>=-3? Actually, for x in [-3,0) the `tl.where` sets it to 0, but the standard HardSwish for x in [-3,0) is not zero? \n Let me check: \n   - For x in [-3,0): \n        The current kernel: because x<0, it returns 0.\n        The standard: (x+3) is in [0,3) -> then multiplied by x (negative) and divided by 6 -> negative? \n        Actually, the standard HardSwish is: \n            if x<=-3: 0\n            if x>=-3 and x<=3: x*(x+3)/6\n            if x>=3: x\n        But note: the common definition is: \n            HardSwish(x) = x * ReLU6(x+3) / 6\n\n        And ReLU6(x+3) = min(max(x+3,0),6)\n\n        So for x in [-3,0): \n            x is negative, and (x+3) is in [0,3) -> so ReLU6(x+3)=x+3, then output = x*(x+3)/6 -> which is negative.\n\n        But in the current kernel, for x in [-3,0) we get 0 because of the `tl.where(x<0, 0.0, ...)`. \n\n        This is a correctness issue! \n\n        Therefore, the current kernel is not numerically correct.\n\n However, the problem states that the current program passed correctness (correctness: 1.0000). How?\n        Note: the reference PyTorch code is not shown, but the problem says that the dataset contains PyTorch reference.\n\n        Let me recall: the original reference might be:\n\n            x = self.conv(x)\n            x = torch.relu(x)   # ? or not? \n            # then HardSwish?\n\n        But note: the model in the current program does:\n            After convolution, it applies a fused activation that is ReLU + HardSwish.\n\n        Actually, the problem says: \"Each file contains PyTorch reference (reference_code)\".\n\n        Without the reference, we cannot be sure. But note the top programs do:\n\n            Program 1: \n                x = tl.maximum(x, 0.0)   # ReLU\n                hardswish = x * tl.minimum((x + 3.0) * 0.16666667, 1.0)\n\n            This is: ReLU then HardSwish? But note: the ReLU sets negatives to 0, then HardSwish on non-negatives.\n\n        The standard HardSwish does not have a ReLU first. \n\n        Looking at the problem context, it says \"fused ReLU + HardSwish\". So the intended operation is:\n            output = HardSwish(ReLU(x))\n\n        But note: ReLU(x) = max(x,0). Then HardSwish of that.\n\n        So for x<0: ReLU(x)=0, then HardSwish(0)=0.\n        For x>=0: HardSwish(x) = x * min(max(x+3,0),6) / 6.\n\n        However, in the top program, they do:\n            x = tl.maximum(x,0)   # so now x is non-negative\n            then: hardswish = x * tl.minimum((x+3.0)*0.16666667, 1.0)\n\n        This is equivalent to: \n            = x * min( (x+3)/6, 1 ) \n            = x * min( (x+3), 6 ) / 6   [because (x+3)/6 <= 1 is equivalent to x+3<=6]\n\n        And note that since x>=0, then x+3>=3>0, so the max(...,0) is not needed.\n\n        Therefore, the top program is correct for the fused ReLU+HardSwish.\n\n        The current program, however, does:\n            For x>=0: it does x * min(x+3,6) / 6 -> which is the same as above.\n            For x<0: it returns 0.\n\n        So it is also correct? Because the ReLU sets x<0 to 0, and then the HardSwish of 0 is 0.\n\n        Therefore, the current kernel is numerically correct.\n\n Why then is the performance worse?\n   - The expression in the current kernel for x>=0: `x * tl.minimum(x + 3.0, 6.0) * (1.0 / 6.0)` requires:\n        - One condition (the `tl.where` for the whole expression) -> but note that the `tl.where` is over the entire activation, so it does both the ReLU and the condition for the HardSwish? \n        - Actually, the ReLU part is done by the `tl.where` (for x<0, set to 0). Then for x>=0, it does the multiplication.\n\n   - Alternatively, the top program breaks it into two steps without a conditional.\n\n   - The top program uses two steps: \n        Step1: ReLU with `tl.maximum` (which is a fast operation)\n        Step2: Then compute `(x+3)*0.16666667` and then `tl.minimum` with 1.0, then multiply by x.\n\n   - The current program uses one conditional and then for the non-negative branch does the same operations.\n\n   - The `tl.where` might be more expensive than the two-step approach because it requires storing a mask and then blending.\n\n   - Also note: the top program does not have an autotune decorator? Actually, the top program (Program 1) does not use autotune. However, the current program does.\n\n   - But the autotune might be causing overhead? Actually, autotune is done at the first call and then cached.\n\n   - The current kernel also uses a complex expression that might not be as optimized.\n\n Proposed change:\n   We will rewrite the kernel to use the two-step approach from the top program, which is:\n        x = tl.maximum(x, 0.0)   # ReLU\n        hardswish = x * tl.minimum((x + 3.0) * 0.16666667, 1.0)\n\n   We will keep the autotune because it can help in choosing the best block size.\n\n   Also note: the top program used a fixed BLOCK_SIZE=1024 in the call, but we are using autotune so we leave that.\n\n   We must not change the function signature.\n\n   Let's rewrite the kernel body.\n\n   Also note: we must maintain the same input and output.\n\n   We are also going to change the kernel to operate in-place? The current program creates an output tensor. The top program also creates an output tensor. We are not changing that.\n\n   We are only changing the computation.\n\n   The new kernel body:\n\n        pid = tl.program_id(axis=0)\n        block_start = pid * BLOCK_SIZE\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_elements\n        x = tl.load(input_ptr + offsets, mask=mask)\n\n        # Step 1: ReLU\n        x = tl.maximum(x, 0.0)\n        # Step 2: HardSwish\n        # Note: 1/6 \u2248 0.16666667\n        hardswish = x * tl.minimum((x + 3.0) * 0.16666667, 1.0)\n        tl.store(output_ptr + offsets, hardswish, mask=mask)\n\n   This is the same as the top program.\n\n   We hope this will be faster because:\n        - `tl.maximum` is a fast operation.\n        - Then we do two arithmetic operations (add and multiply) and a `tl.minimum`, and then a multiply.\n\n   And we avoid the `tl.where` which might be more expensive.\n\n   Also note: the current program's expression for the non-negative branch was:\n        x * tl.minimum(x + 3.0, 6.0) * (1.0 / 6.0)\n        = x * (min(x+3,6) / 6)\n\n   Which is the same as:\n        x * min( (x+3)/6, 1 )\n\n   So mathematically equivalent.\n\n   Therefore, we are changing the implementation without changing the math.\n\n   We also note that the top program (Program 1) achieved a speedup of 1.061.\n\n   Let's hope we can get similar improvement.\n\n   We are going to output the entire code block with this change.\n\n   IMPORTANT: We are only changing the kernel body. The rest of the code (the ModelNew class, the batch size, etc.) remains.\n\n   We are also keeping the autotune.\n\n   We are changing the kernel body from:\n\n        # Fused ReLU + HardSwish in single expression\n        result = tl.where(x < 0, 0.0, x * tl.minimum(x + 3.0, 6.0) * (1.0 / 6.0))\n\n   to:\n\n        # Optimized fused ReLU + HardSwish\n        x = tl.maximum(x, 0.0)  # ReLU\n        hardswish = x * tl.minimum((x + 3.0) * 0.16666667, 1.0)  # Combined operations\n        result = hardswish\n\n   But note: we can just store `hardswish`.\n\n   Alternatively, we can write without a temporary variable.\n\n   Let me write:\n\n        x = tl.maximum(x, 0.0)\n        result = x * tl.minimum((x + 3.0) * 0.16666667, 1.0)\n\n   Then store.\n\n   We'll do that.\n\n   Let's write the new kernel.\n\n   Also note: the expression `(x + 3.0) * 0.16666667` is equivalent to `(x+3)/6`. We use the constant to avoid a division (which is more expensive) but note that Triton might optimize division by constant to multiplication. However, using a constant multiplier is safe.\n\n   We use 0.16666667 which is approximately 1/6.\n\n   We are not changing the autotune configs.\n\n   We output the entire code block as required.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE\": 256}, num_warps=1),\n        triton.Config({\"BLOCK_SIZE\": 512}, num_warps=1),\n        triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=1),\n        triton.Config({\"BLOCK_SIZE\": 2048}, num_warps=2),\n        triton.Config({\"BLOCK_SIZE\": 4096}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 8192}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n@triton.jit\ndef fused_activation_kernel(\n    output_ptr,\n    input_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Optimized fused ReLU + HardSwish with minimal operations\n    x = tl.maximum(x, 0.0)  # Efficient ReLU\n    # Compute HardSwish with fused arithmetic\n    hardswish = x * tl.minimum((x + 3.0) * 0.16666667, 1.0)\n    tl.store(output_ptr + offsets, hardswish, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_activation_kernel[grid](output, x, n_elements)\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
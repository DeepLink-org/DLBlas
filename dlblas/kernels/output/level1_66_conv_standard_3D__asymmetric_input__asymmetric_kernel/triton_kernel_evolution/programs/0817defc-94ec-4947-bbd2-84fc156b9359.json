{"id": "0817defc-94ec-4947-bbd2-84fc156b9359", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), dilation: tuple = (1, 1, 1), groups: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        \n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels // groups, *kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_channels))\n        else:\n            self.bias = None\n            \n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if bias:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() != 5:\n            raise ValueError(\"Input tensor must be 5D (batch, channels, depth, height, width)\")\n            \n        N, C, D, H, W = x.shape\n        Kd, Kh, Kw = self.kernel_size\n        stride_d, stride_h, stride_w = self.stride\n        pad_d, pad_h, pad_w = self.padding\n        dil_d, dil_h, dil_w = self.dilation\n        \n        # Calculate output dimensions\n        D_out = (D + 2 * pad_d - dil_d * (Kd - 1) - 1) // stride_d + 1\n        H_out = (H + 2 * pad_h - dil_h * (Kh - 1) - 1) // stride_h + 1\n        W_out = (W + 2 * pad_w - dil_w * (Kw - 1) - 1) // stride_w + 1\n        \n        output = torch.empty((N, self.out_channels, D_out, H_out, W_out), \n                             device=x.device, dtype=torch.float32)\n        \n        # Convert to FP16 for Tensor Core operations\n        x_fp16 = x.to(torch.float16)\n        weight_fp16 = self.weight.to(torch.float16)\n        \n        # Launch Triton kernel\n        grid = lambda opt: (triton.cdiv(N, opt.BLOCK_N), \n                           triton.cdiv(D_out, opt.BLOCK_D),\n                           triton.cdiv(H_out * W_out, opt.BLOCK_HW))\n        \n        conv3d_kernel[grid](\n            x_fp16, weight_fp16, output,\n            self.bias if self.bias is not None else None,\n            C, D, H, W,\n            self.out_channels, Kd, Kh, Kw,\n            stride_d, stride_h, stride_w,\n            pad_d, pad_h, pad_w,\n            dil_d, dil_h, dil_w,\n            D_out, H_out, W_out,\n            x_fp16.stride(0), x_fp16.stride(1), x_fp16.stride(2), x_fp16.stride(3), x_fp16.stride(4),\n            weight_fp16.stride(0), weight_fp16.stride(1), weight_fp16.stride(2), weight_fp16.stride(3), weight_fp16.stride(4),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4),\n            BLOCK_N=4, BLOCK_C=32, BLOCK_O=32, BLOCK_D=1, BLOCK_HW=16,\n            USE_BIAS=self.bias is not None\n        )\n        \n        return output\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_N\": 4, \"BLOCK_C\": 32, \"BLOCK_O\": 32, \"BLOCK_D\": 1, \"BLOCK_HW\": 16}, num_warps=4),\n        triton.Config({\"BLOCK_N\": 2, \"BLOCK_C\": 32, \"BLOCK_O\": 64, \"BLOCK_D\": 1, \"BLOCK_HW\": 32}, num_warps=8),\n        triton.Config({\"BLOCK_N\": 1, \"BLOCK_C\": 64, \"BLOCK_O\": 64, \"BLOCK_D\": 2, \"BLOCK_HW\": 64}, num_warps=8),\n    ],\n    key=[\"C\", \"D\", \"H\", \"W\", \"out_channels\", \"Kd\", \"Kh\", \"Kw\"],\n)\n@triton.jit\ndef conv3d_kernel(\n    x_ptr, weight_ptr, output_ptr, bias_ptr,\n    C, D, H, W,\n    out_channels, Kd, Kh, Kw,\n    stride_d, stride_h, stride_w,\n    pad_d, pad_h, pad_w,\n    dil_d, dil_h, dil_w,\n    D_out, H_out, W_out,\n    x_sn, x_sc, x_sd, x_sh, x_sw,\n    weight_so, weight_sc, weight_sd, weight_sh, weight_sw,\n    out_sn, out_so, out_sd, out_sh, out_sw,\n    BLOCK_N: tl.constexpr, BLOCK_C: tl.constexpr, BLOCK_O: tl.constexpr,\n    BLOCK_D: tl.constexpr, BLOCK_HW: tl.constexpr, USE_BIAS: tl.constexpr\n):\n    pid_n = tl.program_id(0)\n    pid_d = tl.program_id(1)\n    pid_hw = tl.program_id(2)\n    \n    # Create ranges for block dimensions\n    n_offs = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    d_offs = pid_d * BLOCK_D + tl.arange(0, BLOCK_D)\n    hw_offs = pid_hw * BLOCK_HW + tl.arange(0, BLOCK_HW)\n    \n    # Split HW offset into H and W components\n    h_offs = hw_offs // W_out\n    w_offs = hw_offs % W_out\n    \n    # Compute input window start positions\n    d_in_start = d_offs * stride_d - pad_d\n    h_in_start = h_offs * stride_h - pad_h\n    w_in_start = w_offs * stride_w - pad_w\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_N, BLOCK_D, BLOCK_HW, BLOCK_O), dtype=tl.float32)\n    \n    # Loop over input channels and kernel dimensions\n    for c in range(0, C, BLOCK_C):\n        for kd in range(Kd):\n            d_in = d_in_start + kd * dil_d\n            for kh in range(Kh):\n                h_in = h_in_start + kh * dil_h\n                for kw in range(Kw):\n                    w_in = w_in_start + kw * dil_w\n                    \n                    # Check input boundaries\n                    in_bounds = (d_in >= 0) & (d_in < D) & (h_in >= 0) & (h_in < H) & (w_in >= 0) & (w_in < W)\n                    \n                    # Load input block (FP16)\n                    x_vals = tl.load(\n                        x_ptr + \n                        n_offs[:, None, None, None] * x_sn + \n                        (c + tl.arange(0, BLOCK_C))[None, :, None, None] * x_sc +\n                        d_in[None, None, :, None] * x_sd +\n                        h_in[None, None, None, :] * x_sh +\n                        w_in * x_sw,\n                        mask=in_bounds & (n_offs[:, None, None, None] < N) &\n                             ((c + tl.arange(0, BLOCK_C))[None, :, None, None] < C),\n                        other=0.0\n                    )\n                    \n                    # Load weight block (FP16)\n                    w_vals = tl.load(\n                        weight_ptr +\n                        tl.arange(0, BLOCK_O)[:, None, None, None, None] * weight_so +\n                        (c + tl.arange(0, BLOCK_C))[None, :, None, None, None] * weight_sc +\n                        kd * weight_sd +\n                        kh * weight_sh +\n                        kw * weight_sw,\n                        mask=(c + tl.arange(0, BLOCK_C))[None, :, None, None, None] < C\n                    )\n                    \n                    # Convert to FP32 and accumulate\n                    acc += tl.sum(\n                        tl.dot(w_vals, x_vals, allow_tf32=True).to(tl.float32),\n                        axis=1\n                    )\n    \n    # Add bias if needed\n    if USE_BIAS:\n        bias = tl.load(\n            bias_ptr + tl.arange(0, BLOCK_O),\n            mask=tl.arange(0, BLOCK_O) < out_channels\n        )\n        acc += bias[None, None, None, :]\n    \n    # Store results\n    output_offs = (\n        n_offs[:, None, None, None] * out_sn +\n        tl.arange(0, BLOCK_O)[None, None, None, :] * out_so +\n        d_offs[None, :, None, None] * out_sd +\n        h_offs[None, None, :, None] * out_sh +\n        w_offs * out_sw\n    )\n    \n    tl.store(\n        output_ptr + output_offs,\n        acc,\n        mask=(\n            (n_offs[:, None, None, None] < N) &\n            (d_offs[None, :, None, None] < D_out) &\n            (h_offs[None, None, :, None] < H_out) &\n            (w_offs < W_out) &\n            (tl.arange(0, BLOCK_O)[None, None, None, :] < out_channels)\n        )\n    )\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)  # Asymmetric kernel size\ndepth = 16\nheight = 256\nwidth = 256\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "7fd8670a-6cb4-40b4-8983-36021e861d29", "generation": 1, "timestamp": 1754647198.4619753, "iteration_found": 1, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_66_conv_standard_3D__asymmetric_input__asymmetric_kernel", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp75qkd1uf.py\", line 58, in forward\n    conv3d_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 135, in _bench\n    raise ValueError(f\"Conflicting meta-parameters: {', '.join(conflicts)}.\"\nValueError: Conflicting meta-parameters: BLOCK_D, BLOCK_N, BLOCK_O, BLOCK_HW, BLOCK_C. Make sure that you don't re-define auto-tuned symbols.\n", "runtime_error_name": "builtins.ValueError", "reference_timing_ms": 0.0443, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 31.2, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0443, "speed_up": 0.001, "custom_timing": 31.2}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
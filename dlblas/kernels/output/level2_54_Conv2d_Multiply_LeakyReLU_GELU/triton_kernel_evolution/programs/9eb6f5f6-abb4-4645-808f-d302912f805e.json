{"id": "9eb6f5f6-abb4-4645-808f-d302912f805e", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_ops_kernel(\n    x_ptr,\n    multiplier_ptr,\n    output_ptr,\n    negative_slope,\n    n_elements,\n    channel_size: tl.constexpr,\n    C: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Load input element\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    \n    # Calculate channel index\n    channel_idx = (offsets // channel_size) % C\n    \n    # Load multiplier for corresponding channel\n    multiplier = tl.load(multiplier_ptr + channel_idx, mask=mask, other=1.0)\n    \n    # Fused operations: Multiply -> LeakyReLU -> GELU\n    x = x * multiplier\n    x = tl.where(x >= 0, x, x * negative_slope)\n    gelu_x = 0.5 * x * (1.0 + tl.erf(x * 0.7071067811865475))\n    \n    tl.store(output_ptr + offsets, gelu_x, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, multiplier_shape):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.multiplier = nn.Parameter(torch.randn(multiplier_shape))\n        self.leaky_relu = nn.LeakyReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        B, C, H, W = x.shape\n        n_elements = B * C * H * W\n        channel_size = H * W\n        neg_slope = self.leaky_relu.negative_slope\n        \n        # Prepare tensors\n        x_flat = x.contiguous().view(-1)\n        output_flat = torch.empty_like(x_flat)\n        multiplier_flat = self.multiplier.view(-1).contiguous()\n        \n        # Configure kernel grid\n        BLOCK_SIZE = 1024\n        grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n        \n        # Launch fused kernel\n        fused_ops_kernel[grid](\n            x_flat, multiplier_flat, output_flat, neg_slope,\n            n_elements, channel_size, C, BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output_flat.view(B, C, H, W)\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nmultiplier_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, multiplier_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "17df51c0-74d1-40b6-98b6-c5eaed9f667d", "generation": 1, "timestamp": 1754574032.6516213, "iteration_found": 8, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.921, "runtime_ms": 0.0848, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0781, "speed_up": 0.921, "custom_timing": 0.0848}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0632, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0781, "speed_up": 1.236, "custom_timing": 0.0632}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
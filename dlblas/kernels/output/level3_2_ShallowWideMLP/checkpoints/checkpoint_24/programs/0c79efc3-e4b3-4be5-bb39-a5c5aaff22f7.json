{"id": "0c79efc3-e4b3-4be5-bb39-a5c5aaff22f7", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Triton kernel for fused linear + ReLU operation\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128}, num_warps=4),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64}, num_warps=8),\n    ],\n    key=['M', 'N'],\n)\n@triton.jit\ndef linear_relu_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_im, stride_in,\n    stride_wk, stride_wn,\n    stride_om, stride_on,\n    ACTIVATION: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    pid_m = pid % num_pid_m\n    pid_n = pid // num_pid_m\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    input_ptr += offs_m[:, None] * stride_im + offs_k[None, :] * stride_in\n    weight_ptr += offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        mask_input = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n        mask_weight = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n        \n        input = tl.load(input_ptr, mask=mask_input, other=0.0)\n        weight = tl.load(weight_ptr, mask=mask_weight, other=0.0)\n        \n        acc += tl.dot(input, weight)\n        input_ptr += BLOCK_K * stride_in\n        weight_ptr += BLOCK_K * stride_wk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += bias[None, :]\n\n    if ACTIVATION:\n        acc = tl.where(acc > 0, acc, 0.0)\n    \n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    mask_output = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    output_ptr += offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    tl.store(output_ptr, acc, mask=mask_output)\n\n# Optimized linear layer with Triton\nclass TritonLinear(nn.Module):\n    def __init__(self, in_features, out_features, activation=False):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.activation = activation\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        # Initialize parameters\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        output = torch.empty(x.shape[0], self.out_features, device=x.device, dtype=x.dtype)\n        \n        # Kernel configuration\n        grid = lambda opt: (triton.cdiv(x.shape[0], opt.BLOCK_M) * triton.cdiv(self.out_features, opt.BLOCK_N),)\n        \n        # Launch kernel\n        linear_relu_kernel[grid](\n            x, self.weight, self.bias, output,\n            x.shape[0], self.out_features, self.in_features,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            ACTIVATION=1 if self.activation else 0,\n            BLOCK_K=64\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_layer_sizes, output_size):\n        super(ModelNew, self).__init__()\n        \n        layers = []\n        current_input_size = input_size\n        \n        # Hidden layers with fused linear + ReLU\n        for hidden_size in hidden_layer_sizes:\n            layers.append(TritonLinear(current_input_size, hidden_size, activation=True))\n            current_input_size = hidden_size\n        \n        # Final layer without activation\n        layers.append(TritonLinear(current_input_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.network(x)\n\n# Test code\nbatch_size = 1\ninput_size = 1000\nhidden_layer_sizes = [2000, 2000]\noutput_size = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_layer_sizes, output_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "50e918d8-3cbd-4386-b8c7-024d51ff2f5a", "generation": 1, "timestamp": 1754589637.9031165, "iteration_found": 2, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_2_ShallowWideMLP", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp6lqg5idk.py\", line 120, in forward\n    return self.network(x)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\n    input = module(input)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp6lqg5idk.py\", line 91, in forward\n    linear_relu_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 160, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 146, in kernel_call\n    self.fn.run(\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 610, in run\n    grid = grid(bound_args)\n  File \"/tmp/tmp6lqg5idk.py\", line 88, in <lambda>\n    grid = lambda opt: (triton.cdiv(x.shape[0], opt.BLOCK_M) * triton.cdiv(self.out_features, opt.BLOCK_N),)\nAttributeError: 'dict' object has no attribute 'BLOCK_M'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 0.0851, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0766, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0851, "speed_up": 1.111, "custom_timing": 0.0766}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0766\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0851, 'speed_up': 1.111, 'custom_timing': 0.0766}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0766, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0851, 'speed_up': 1.111, 'custom_timing': 0.0766}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_layer_sizes, output_size):\n        \"\"\"\n        :param input_size: The number of input features\n        :param hidden_layer_sizes: A list of ints containing the sizes of each hidden layer\n        :param output_size: The number of output features\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        layers = []\n        current_input_size = input_size\n        \n        for hidden_size in hidden_layer_sizes:\n            layers.append(nn.Linear(current_input_size, hidden_size))\n            layers.append(nn.ReLU())\n            current_input_size = hidden_size\n        \n        layers.append(nn.Linear(current_input_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        return self.network(x)\n\n# Test code\nbatch_size = 1\ninput_size = 1000\nhidden_layer_sizes = [2000, 2000]  # Example of deep and narrow layers\noutput_size = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_layer_sizes, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0766), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0851, 'speed_up': 1.111, 'custom_timing': 0.0766})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_layer_sizes, output_size):\n        \"\"\"\n        :param input_size: The number of input features\n        :param hidden_layer_sizes: A list of ints containing the sizes of each hidden layer\n        :param output_size: The number of output features\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        layers = []\n        current_input_size = input_size\n        \n        for hidden_size in hidden_layer_sizes:\n            layers.append(nn.Linear(current_input_size, hidden_size))\n            layers.append(nn.ReLU())\n            current_input_size = hidden_size\n        \n        layers.append(nn.Linear(current_input_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        return self.network(x)\n\n# Test code\nbatch_size = 1\ninput_size = 1000\nhidden_layer_sizes = [2000, 2000]  # Example of deep and narrow layers\noutput_size = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_layer_sizes, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that is a simple multi-layer perceptron (MLP) with ReLU activations.\n The current implementation uses `nn.Sequential` and standard `nn.Linear` and `nn.ReLU` modules.\n\n However, the task is to rewrite the program to improve performance. The current metrics show:\n   - reference_timing_ms: 0.0851\n   - custom_timing: 0.0766 (which gives a speed_up of 1.111)\n\n We are to rewrite the model using Triton for the matrix multiplications and activations to achieve better performance.\n\n Steps:\n 1. We note that the model has multiple linear layers and ReLU activations.\n 2. We can write a Triton kernel for the linear layer (matmul) and then use ReLU (which is pointwise and cheap).\n 3. However, note that the model is defined with dynamic hidden layer sizes and input/output sizes.\n\n Approach:\n We will create a custom TritonLinear module that replaces `nn.Linear` and uses a Triton kernel for the forward pass.\n\n Important: The model must have the same inputs and outputs.\n\n We are going to:\n   - Keep the same structure: input -> hidden layers (each: linear + ReLU) -> output linear.\n   - Replace each `nn.Linear` with our own `TritonLinear` module.\n\n Constraints for TritonLinear:\n   - The kernel must handle any input and output dimensions (with appropriate blocking).\n   - We must handle the bias (if present) but note that the original model does not use bias? Actually, `nn.Linear` has bias by default. We must include it.\n\n However, note: the original model does not have any bias=False, so we use bias.\n\n Steps for TritonLinear kernel:\n   - We'll write a Triton kernel for matrix multiplication with bias and ReLU? Actually, no: the ReLU is a separate module. But note that the model has ReLU after every linear except the last. We are not going to fuse ReLU in the linear kernel for now because the model structure is defined as linear then ReLU. We can do it separately? But note: we are replacing only the linear part.\n\n Alternatively, we can fuse the ReLU with the linear for the hidden layers? That might be more efficient.\n\n Let's reconsider the model structure:\n\n   self.network = nn.Sequential(\n        nn.Linear(input_size, hidden_size1),\n        nn.ReLU(),\n        nn.Linear(hidden_size1, hidden_size2),\n        nn.ReLU(),\n        ...,\n        nn.Linear(hidden_sizeN, output_size)\n   )\n\n We can create a custom module for each hidden layer that does: Linear + ReLU in one Triton kernel? That would reduce the number of kernel launches and improve performance.\n\n However, the last layer does not have ReLU, so we must keep it separate.\n\n We can design:\n   - TritonLinear: for the linear transformation (with bias) without activation.\n   - TritonLinearReLU: for the linear transformation (with bias) followed by ReLU.\n\n Then we can build the network as:\n\n   layers = []\n   current_input_size = input_size\n   for i, hidden_size in enumerate(hidden_layer_sizes):\n        # For all hidden layers (except the last one) we use Linear+ReLU\n        layers.append(TritonLinearReLU(current_input_size, hidden_size))\n        current_input_size = hidden_size\n\n   layers.append(TritonLinear(current_input_size, output_size))   # last layer without ReLU\n\n But note: the original model uses `nn.ReLU()` as a separate module. We are fusing in the hidden layers.\n\n Why fuse? Because it reduces the number of kernel launches and we can do the ReLU without writing the result to global memory and then reading it again.\n\n However, the task says: \"maintain the same inputs and outputs\". We are not changing the functional behavior because ReLU is the same.\n\n Implementation of TritonLinear and TritonLinearReLU:\n\n   We will write a Triton kernel that does:\n      Y = X @ W^T + bias   [and then for ReLU: Y = max(Y, 0)]\n\n   We need to handle:\n      X: (batch_size, in_features)\n      W: (out_features, in_features)\n      bias: (out_features,)\n\n   Output: (batch_size, out_features)\n\n   We note that the batch size can be any, and the feature sizes can be any.\n\n   We will use a kernel that uses block-wise matrix multiplication for each row of X and the entire W matrix? Actually, we can do:\n\n      Each program will compute one row of the output.\n\n   But note: the matrix W is of size (out_features, in_features). If out_features is large, we might need to split the output row.\n\n   Alternatively, we can use a 2D grid: \n        batch_size x (ceil(out_features / BLOCK_SIZE))\n\n   However, note that the batch size might be 1 in the test code (but we should be general).\n\n   We design:\n\n      pid_b = tl.program_id(0)   # batch index\n      pid_o = tl.program_id(1)    # output feature block index\n\n   Each block will compute a contiguous block of output features for one batch element.\n\n   Steps in kernel:\n\n      Load the row of X (for batch index pid_b) in blocks (since in_features might be large).\n      For each block of input features, load the corresponding block of weights (for the current output block and the input block).\n      Accumulate the dot product for the current output block.\n\n   Then add bias and apply ReLU (if needed).\n\n   However, note that the entire weight matrix for the output block (pid_o) is of size (BLOCK_SIZE, in_features). We can't load it all at once. So we do:\n\n      for i in range(0, in_features, BLOCK_K):\n          load a block of X (size [BLOCK_K]) and a block of W (size [BLOCK_SIZE, BLOCK_K]) and do the dot product.\n\n   This is a standard matmul kernel for A (1, in_features) and B (out_features, in_features) -> C (1, out_features).\n\n   We can use the same kernel for the entire batch by having the batch index.\n\n   But note: the batch dimension is independent. We can have a grid (batch_size, out_features // BLOCK_SIZE).\n\n   Steps:\n\n      pid_b = tl.program_id(0)\n      pid_o = tl.program_id(1)\n\n      offs_o = pid_o * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)   # indices for output features in this block\n      offs_i = tl.arange(0, BLOCK_K)   # for input features\n\n      acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n      for i in range(0, in_features, BLOCK_K):\n          # Load a block of X\n          offs_x = i + offs_i\n          mask_x = offs_i < (in_features - i)   # actually, we can pre-calculate the number of blocks, but we use mask for the last block\n          x = tl.load(X_ptr + pid_b * in_features + offs_x, mask=mask_x, other=0.0)\n\n          # Load a block of W: for the current output block and the current input block\n          w = tl.load(W_ptr + offs_o[:, None] * in_features + offs_x[None, :], \n                     mask=(offs_o[:, None] < out_features) & mask_x[None, :], \n                     other=0.0)\n\n          acc += tl.sum(x[None, :] * w, axis=1)\n\n      # Now add bias\n      if bias_ptr is not None:\n          b = tl.load(bias_ptr + offs_o, mask=offs_o < out_features, other=0.0)\n          acc += b\n\n      # Apply ReLU if needed\n      if activation == \"relu\":\n          acc = tl.where(acc > 0, acc, 0)\n\n      # Write the output block\n      tl.store(Output_ptr + pid_b * out_features + offs_o, acc, mask=offs_o < out_features)\n\n   However, note: the above kernel is for one row of the batch and one block of the output.\n\n   We must also consider the data types: the model uses float32? The original model uses the default (float32). We are going to use float32 in Triton as well.\n\n   But note: the Triton kernel above uses float32 for accumulation. We must load the inputs as float32.\n\n   We have to design the kernel to be configurable with BLOCK_SIZE and BLOCK_K.\n\n   We will use autotuning for BLOCK_SIZE and BLOCK_K? But note: the sizes of the layers are fixed at initialization. We can set the kernel to be autotuned for different sizes.\n\n   However, the problem says: the hidden layers can be of any size (given at model initialization). We can choose the block sizes at runtime? But Triton requires the block sizes to be compile-time constants.\n\n   We can use a fixed BLOCK_SIZE (say 128) and BLOCK_K (say 64) and hope that it works for all sizes? But we can also have multiple kernels and choose at runtime.\n\n   Alternatively, we can write a function that chooses the best configuration for the given sizes. But that is complex.\n\n   Since this is an example and the test uses batch_size=1 and input_size=1000, hidden_sizes=[2000,2000], output_size=10, we can optimize for these.\n\n   However, we must be general.\n\n   We can use the autotune decorator to try different configurations and then cache the best for the given shape.\n\n   Steps:\n\n      We will write a Triton kernel that takes:\n          X_ptr, W_ptr, bias_ptr, Output_ptr, in_features, out_features, activation: tl.constexpr (0 for none, 1 for ReLU)\n\n      We use autotune for BLOCK_SIZE and BLOCK_K.\n\n   But note: the grid dimensions are (batch_size, ceil(out_features / BLOCK_SIZE)). We must compute the grid accordingly.\n\n   We create a wrapper function for the kernel that will set the grid and call the kernel with the best configuration.\n\n   However, note: the kernel must be jit compiled for each set of parameters (in_features, out_features) and for each configuration? We can cache the best configuration per (in_features, out_features, BLOCK_SIZE, BLOCK_K) but that is too much.\n\n   Alternatively, we can predefine a set of configurations and choose the one that fits the problem size.\n\n   Given the complexity, we will start with a fixed configuration and then later we can autotune.\n\n   We choose:\n        BLOCK_SIZE = 128   # for output features\n        BLOCK_K = 64       # for input features\n\n   We note: the input features (in_features) might not be divisible by BLOCK_K, so we use masking.\n\n   We also note: the output features (out_features) might not be divisible by BLOCK_SIZE, so we use masking.\n\n   We design the kernel:\n\n      kernel_name: _triton_linear_forward\n\n   We will create a module `TritonLinear` that uses this kernel.\n\n   However, note: the model is defined with `hidden_layer_sizes` and we are replacing the linear layers.\n\n   We must also handle the parameters (weight and bias). We will use the same as `nn.Linear`: initialize with the same method? Actually, the model is built by the user and the parameters are randomly initialized by `nn.Linear`. We are just replacing the implementation.\n\n   We will create:\n\n        class TritonLinear(nn.Module):\n            __constants__ = ['in_features', 'out_features']\n            def __init__(self, in_features, out_features, bias=True, activation=None):\n                super().__init__()\n                self.in_features = in_features\n                self.out_features = out_features\n                self.activation = activation   # None or 'relu'\n\n                self.weight = nn.Parameter(torch.empty(out_features, in_features))\n                if bias:\n                    self.bias = nn.Parameter(torch.empty(out_features))\n                else:\n                    self.register_parameter('bias', None)\n\n                # initialize the parameters\n                nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n                if self.bias is not None:\n                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n                    bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n                    nn.init.uniform_(self.bias, -bound, bound)\n\n            def forward(self, x):\n                # x: (batch_size, in_features)\n                batch_size, _ = x.shape\n                output = torch.empty((batch_size, self.out_features), device=x.device, dtype=x.dtype)\n\n                # We have to choose the grid: (batch_size, ceil(self.out_features / BLOCK_SIZE))\n                grid = (batch_size, triton.cdiv(self.out_features, BLOCK_SIZE))\n\n                # call the kernel\n                _triton_linear_forward[grid](\n                    x, self.weight, output, \n                    self.bias if self.bias is not None else None,\n                    self.in_features, self.out_features,\n                    activation=1 if self.activation == 'relu' else 0,\n                    BLOCK_SIZE=BLOCK_SIZE, BLOCK_K=BLOCK_K\n                )\n\n                return output\n\n        But note: we haven't defined the kernel yet.\n\n   However, the kernel must be defined outside the class.\n\n   We also note: the kernel must be compiled only once? Actually, Triton will compile for each different set of constant arguments. We are using BLOCK_SIZE and BLOCK_K as constexpr.\n\n   We define:\n\n        @triton.jit\n        def _triton_linear_forward(\n            x_ptr, w_ptr, output_ptr, bias_ptr,\n            in_features, out_features,\n            activation: tl.constexpr,\n            BLOCK_SIZE: tl.constexpr,\n            BLOCK_K: tl.constexpr\n        ):\n            ... as above ...\n\n   But note: we have two program IDs: pid_b and pid_o.\n\n   We also note: we are using row-major for everything.\n\n   However, the weight matrix is stored as (out_features, in_features). We access it as: w_ptr[off_o * in_features + off_i]\n\n   But in the kernel we are doing:\n\n        w = tl.load(w_ptr + offs_o[:, None] * in_features + offs_x[None, :])\n\n   This is correct.\n\n   But note: the input x is stored as (batch, in_features). We are loading a block of x for the current batch and the current input block.\n\n   However, the kernel above has a bug: the accumulation is done by `acc += tl.sum(x[None, :] * w, axis=1)`. This is not a standard matrix multiplication. We should do:\n\n        acc += tl.dot(w, x, allow_tf32=False)   # but w is [BLOCK_SIZE, BLOCK_K] and x is [BLOCK_K] -> we get [BLOCK_SIZE] vector\n\n   But Triton doesn't have a dot product between a matrix and a vector? We can use:\n\n        acc += tl.sum(w * x[None, :], axis=1)\n\n   However, this is the same as above. But note: we are doing a reduction over the K dimension. We can use tl.dot if we reshape? Alternatively, we can use tl.sum.\n\n   But note: the above method is not the most efficient. We can use tl.dot by making x a vector and w a matrix? Actually, we can do:\n\n        w_block = tl.load(...)   # [BLOCK_SIZE, BLOCK_K]\n        x_block = tl.load(...)   # [BLOCK_K]\n        # Now we want to compute: w_block @ x_block -> [BLOCK_SIZE]\n        # We can do:\n        acc += tl.sum(w_block * x_block[None, :], axis=1)\n\n   This is acceptable.\n\n   Alternatively, we can use the fact that the input x is a vector and we are doing a matrix-vector product? But note: we are doing one row of the batch at a time.\n\n   We can also use a more efficient method by using tl.dot with the input vector as a row vector? But then we would have to transpose w_block? \n\n   Since the kernel is for one row of the batch, and we are doing block in the output, we are doing a block of the output vector. This is standard.\n\n   We'll proceed with the above.\n\n   However, note: the kernel must be efficient. We are using shared memory? No, because we are not using shared memory for the entire weight matrix (which is too big). We are loading one block of the weight matrix at a time.\n\n   We are also loading the same block of x for multiple output blocks? Actually, no: each output block is processed independently. So we are loading the same block of x for each output block. This is inefficient.\n\n   We could restructure the kernel to compute multiple output blocks for the same x block? But that would require more shared memory and we are limited by the number of registers.\n\n   Alternatively, we can use a different tiling: tile the input features and then for each tile, load the entire tile of x and then the corresponding tile of the weight matrix for all output features? But that would require loading the entire weight matrix for that tile? Which might be too big.\n\n   Given the complexity and the fact that the batch size is 1 in the test, we might not see a big improvement. However, we are also targeting the hidden layers which are 2000x2000.\n\n   We can try to use a kernel that does the entire output for one batch element? But the output features are 2000, which might be too big for one block.\n\n   We are constrained by the number of threads per block and shared memory.\n\n   We will stick to the above kernel for now.\n\n   We note: the kernel above is for one batch element and one block of output features. We are loading the same x block multiple times (for each output block). This is inefficient in terms of memory bandwidth.\n\n   We can restructure to load the entire x vector once and then reuse it for all output blocks? But we cannot because we are limited by the shared memory (which is 48KB per block). The entire x vector (for 1000 or 2000 elements) is 4KB or 8KB, which might fit. But note: the kernel is launched for each batch element and each output block. We cannot share across output blocks? Because they are in different program_ids.\n\n   We must change the kernel to have a 1D grid of batch elements and then each block computes the entire output row? But then the output row might be too big (2000 elements) and we would need to split the reduction? Actually, we can do:\n\n        We tile the input features and then for each tile, we load a block of x and a block of the entire weight matrix? That is not possible because the entire weight matrix is too big.\n\n   Alternatively, we can do:\n\n        Each block (for a batch element) loads the entire x vector into shared memory? Then we can reuse it for all output blocks? But the entire x vector is 2000 elements (8KB) which might fit in shared memory (48KB). Then we can compute the entire output row in one block? But the output row has 2000 elements and we can use 128 threads? Then each thread would compute 2000/128 ~ 16 elements? That might be acceptable.\n\n   We design a kernel for one batch element:\n\n        pid = tl.program_id(0)   # batch index\n\n        # Allocate shared memory for the entire x vector? But the size is dynamic. We cannot.\n\n   So we must know the in_features at compile time? We don't.\n\n   Therefore, we stick to the original kernel that tiles the output features and the input features.\n\n   We hope that for the given problem sizes (1000, 2000) the kernel is efficient.\n\n   We choose BLOCK_SIZE=128 and BLOCK_K=64.\n\n   We write the kernel:\n\n        @triton.jit\n        def _triton_linear_forward(\n            x_ptr, w_ptr, output_ptr, bias_ptr,\n            in_features, out_features,\n            activation: tl.constexpr,\n            BLOCK_SIZE: tl.constexpr,   # for output features\n            BLOCK_K: tl.constexpr,       # for input features\n        ):\n            pid_b = tl.program_id(0)\n            pid_o = tl.program_id(1)\n\n            # Offsets for the output block\n            offs_o = pid_o * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n            offs_i = tl.arange(0, BLOCK_K)\n\n            # We'll accumulate in a block of BLOCK_SIZE\n            acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n            # Loop over the input features in blocks of BLOCK_K\n            for i in range(0, in_features, BLOCK_K):\n                # Load a block of x for the current batch and input block\n                offs_x = i + offs_i\n                mask_x = offs_i < (in_features - i)   # mask for the last block\n                x = tl.load(x_ptr + pid_b * in_features + offs_x, mask=mask_x, other=0.0)\n\n                # Load a block of weights: for the current output block and the current input block\n                w_offs = offs_o[:, None] * in_features + offs_x[None, :]\n                mask_w = (offs_o[:, None] < out_features) & mask_x[None, :]\n                w = tl.load(w_ptr + w_offs, mask=mask_w, other=0.0)\n\n                # Compute the dot product for this block\n                # We do: for each row in w (which is BLOCK_SIZE rows) and the vector x: dot product = sum(w_row * x)\n                # But note: we are doing a block of the row? Actually, we are doing a block of the row and the entire x block? \n                # We want: acc[j] += w[j, k] * x[k] for k in the current block.\n                # We can do:\n                #   acc += tl.sum(w * x[None, :], axis=1)   # but note: we are adding to the entire block of acc\n                # However, we must mask the multiplication? The w and x are already masked to 0 where out of bounds.\n                # But note: the mask for w is for the row (output feature) and the column (input feature). The mask for x is for the input feature.\n                # When we do w * x[None,:] and then sum, the masked elements become 0 and don't contribute.\n                acc += tl.sum(w * x[None, :], axis=1)\n\n            # Now add bias if present\n            if bias_ptr is not None:\n                b = tl.load(bias_ptr + offs_o, mask=offs_o < out_features, other=0.0)\n                acc += b\n\n            # Apply ReLU if activation is set\n            if activation:\n                acc = tl.where(acc > 0, acc, 0.0)\n\n            # Write the output block\n            tl.store(output_ptr + pid_b * out_features + offs_o, acc, mask=offs_o < out_features)\n\n   But note: the above kernel uses a lot of registers because we are storing a block of BLOCK_SIZE and doing a reduction? We might run into performance issues.\n\n   We can try to use a more efficient method by using tl.dot? We can reshape the w block to be 2D and the x block to be 1D and then do:\n\n        acc += tl.dot(w, x, allow_tf32=False)   # w is [BLOCK_SIZE, BLOCK_K] and x is [BLOCK_K] -> output [BLOCK_SIZE]\n\n   However, the Triton dot operation supports this? We have to check.\n\n   According to Triton documentation, tl.dot can do:\n        A: (M, K) and B: (K, N) -> (M, N)\n        or A: (M, K) and B: (K) -> (M)\n\n   So we can do:\n\n        w_block = w   # [BLOCK_SIZE, BLOCK_K]\n        x_block = x   # [BLOCK_K]\n        acc += tl.dot(w_block, x_block)\n\n   This is more efficient.\n\n   We change the inner loop to:\n\n        # Load w and x as above\n        # Then:\n        acc += tl.dot(w_block, x_block)\n\n   This should be faster and use less registers? Actually, it uses the tensor cores? But note: we are using float32 and the tensor cores are for FP16? We are not using tensor cores.\n\n   But it is still a better way to do the matrix-vector product.\n\n   We change the code accordingly.\n\n   However, note: the tl.dot function for a matrix and a vector is not directly supported? Let me check the Triton language:\n\n        tl.dot(a, b) where a and b are tensors.\n\n        We have:\n            a: [BLOCK_SIZE, BLOCK_K] \n            b: [BLOCK_K] \n            -> we want a vector of [BLOCK_SIZE]\n\n        But the dot product for a matrix and a vector is defined as the matrix multiplied by the vector (which is a column vector) and gives a vector.\n\n        We can do: \n            tl.dot(w_block, x_block[:, None])   -> [BLOCK_SIZE, 1]? Then we have to squeeze.\n\n        Alternatively, we can use:\n\n            tl.dot(w_block, x_block)   # This is not defined for a matrix and a vector? We might get an error.\n\n   We check the Triton documentation: it says the dimensions must be 2D? Actually, the dot product in Triton is for 2D tensors.\n\n   We can do:\n\n        x_block_2d = x_block[None, :]   # [1, BLOCK_K]\n        Then: tl.dot(w_block, x_block_2d, allow_tf32=False)   # [BLOCK_SIZE, 1]\n        Then: we add to acc [BLOCK_SIZE] by doing: tl.sum(dot_result, axis=1) is not needed? Actually, we can do:\n\n        dot_result = tl.dot(w_block, x_block_2d)   # [BLOCK_SIZE, 1]\n        Then: acc += dot_result[:, 0]\n\n   But that is inefficient.\n\n   Alternatively, we can use:\n\n        w_block = w_block   # [BLOCK_SIZE, BLOCK_K]\n        x_block = x_block   # [BLOCK_K]\n        # We want: for each row j in w_block: dot(w_block[j, :], x_block) -> a scalar for each j.\n        # We can do: tl.sum(w_block * x_block[None,:], axis=1)   # as before.\n\n   We stick to the original method.\n\n   Given the complexity, we will use the original method and hope that the compiler optimizes it.\n\n   We write the kernel as described.\n\n   We then replace the linear layers in the model with our TritonLinear layers.\n\n   For the hidden layers, we use TritonLinearReLU (which is TritonLinear with activation='relu').\n\n   For the last layer, we use TritonLinear without activation.\n\n   We also note: the original model uses `nn.ReLU()` as a separate module. We are fusing it in the hidden layers. So we remove the separate ReLU modules.\n\n   We build the network:\n\n        layers = []\n        current_input_size = input_size\n        for hidden_size in hidden_layer_sizes:\n            layers.append(TritonLinearReLU(current_input_size, hidden_size))\n            current_input_size = hidden_size\n        layers.append(TritonLinear(current_input_size, output_size))\n\n   Then the model becomes:\n\n        self.network = nn.Sequential(*layers)\n\n   We hope that this will be faster.\n\n   We note: the test code uses batch_size=1. We expect that the Triton kernel will be faster for the large hidden layers (2000x2000) because it avoids the overhead of PyTorch's large matrix multiplication for small batch? Actually, PyTorch uses cuBLAS which is optimized for large matrices. But our kernel is optimized for small batch and large features? We'll see.\n\n   We must test on the given hardware.\n\n   However, we are only allowed to output the code.\n\n   We write the code accordingly.\n\n   Note: We must include the necessary imports and the kernel definition.\n\n   We also note: the kernel must be defined at the top level? Yes, because Triton requires it.\n\n   We structure the code:\n\n        import ... (as before)\n\n        # Define the kernel\n        @triton.jit\n        def _triton_linear_forward(...):\n            ...\n\n        # Define TritonLinear and TritonLinearReLU\n        class TritonLinear(nn.Module):\n            ...\n\n        class TritonLinearReLU(TritonLinear):\n            # We can inherit and set activation='relu'\n            def __init__(self, in_features, out_features, bias=True):\n                super().__init__(in_features, out_features, bias, activation='relu')\n\n        # Then define ModelNew as before, but using these layers.\n\n   We also note: the test code (get_inputs and get_init_inputs) remains the same.\n\n   We hope this will improve the speed.\n\n   But note: the current reference timing is 0.0851 ms and the custom timing (which is the original model) is 0.0766 ms. We are replacing the model with a Triton version. We hope to get a speedup.\n\n   We set BLOCK_SIZE and BLOCK_K to 128 and 64 respectively.\n\n   However, we might want to autotune. We skip autotune for now to keep the code simple.\n\n   Let's write the code.\n\n   IMPORTANT: We must not change the test code (get_inputs and get_init_inputs). We only change the model.\n\n   We note: the kernel might be slower for very small sizes? But the test sizes are 1000, 2000, 2000, 10.\n\n   We output the entire code block.\n\n   We also note: the original model is for inference? We are not training.\n\n   We write the code.\n\n   However, note: the kernel uses float32. The model is in float32.\n\n   We are ready.\n\n   But note: the kernel must handle the case when bias is None. We pass a null pointer? In Triton, we can pass None for the bias_ptr and then check `if bias_ptr is not None`.\n\n   We do:\n\n        if bias_ptr is not None: ... \n\n   This is safe.\n\n   We also note: the kernel must be compiled for the given device. We are using the same device as the input.\n\n   We hope it works.\n\n   We output the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Triton kernel for fused linear + ReLU operation\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128}, num_warps=4),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64}, num_warps=8),\n    ],\n    key=['M', 'N'],\n)\n@triton.jit\ndef linear_relu_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_im, stride_in,\n    stride_wk, stride_wn,\n    stride_om, stride_on,\n    ACTIVATION: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    pid_m = pid % num_pid_m\n    pid_n = pid // num_pid_m\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    input_ptr += offs_m[:, None] * stride_im + offs_k[None, :] * stride_in\n    weight_ptr += offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        mask_input = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n        mask_weight = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n        \n        input = tl.load(input_ptr, mask=mask_input, other=0.0)\n        weight = tl.load(weight_ptr, mask=mask_weight, other=0.0)\n        \n        acc += tl.dot(input, weight)\n        input_ptr += BLOCK_K * stride_in\n        weight_ptr += BLOCK_K * stride_wk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += bias[None, :]\n\n    if ACTIVATION:\n        acc = tl.where(acc > 0, acc, 0.0)\n    \n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    mask_output = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    output_ptr += offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    tl.store(output_ptr, acc, mask=mask_output)\n\n# Optimized linear layer with Triton\nclass TritonLinear(nn.Module):\n    def __init__(self, in_features, out_features, activation=False):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.activation = activation\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        # Initialize parameters\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        output = torch.empty(x.shape[0], self.out_features, device=x.device, dtype=x.dtype)\n        \n        # Kernel configuration\n        grid = lambda opt: (triton.cdiv(x.shape[0], opt.BLOCK_M) * triton.cdiv(self.out_features, opt.BLOCK_N),)\n        \n        # Launch kernel\n        linear_relu_kernel[grid](\n            x, self.weight, self.bias, output,\n            x.shape[0], self.out_features, self.in_features,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            ACTIVATION=1 if self.activation else 0,\n            BLOCK_K=64\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_layer_sizes, output_size):\n        super(ModelNew, self).__init__()\n        \n        layers = []\n        current_input_size = input_size\n        \n        # Hidden layers with fused linear + ReLU\n        for hidden_size in hidden_layer_sizes:\n            layers.append(TritonLinear(current_input_size, hidden_size, activation=True))\n            current_input_size = hidden_size\n        \n        # Final layer without activation\n        layers.append(TritonLinear(current_input_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.network(x)\n\n# Test code\nbatch_size = 1\ninput_size = 1000\nhidden_layer_sizes = [2000, 2000]\noutput_size = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_layer_sizes, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
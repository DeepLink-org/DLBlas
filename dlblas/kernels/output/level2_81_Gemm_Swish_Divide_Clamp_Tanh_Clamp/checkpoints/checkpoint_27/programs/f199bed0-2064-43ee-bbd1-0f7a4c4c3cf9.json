{"id": "f199bed0-2064-43ee-bbd1-0f7a4c4c3cf9", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Fused element-wise operations kernel\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=32),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef _fused_ops_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Load input element\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Swish: x * sigmoid(x)\n    sigmoid_x = 1.0 / (1.0 + tl.exp(-x))\n    x = x * sigmoid_x\n    \n    # Divide by 2\n    x = x / 2.0\n    \n    # Clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(x, -1.0), 1.0)\n    \n    # Tanh activation using exponential implementation\n    exp_2x = tl.exp(2 * x)\n    tanh_x = (exp_2x - 1) / (exp_2x + 1)\n    \n    # Final clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(tanh_x, -1.0), 1.0)\n    \n    # Store result\n    tl.store(output_ptr + offsets, x, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        # Compute GEMM\n        x = self.gemm(x)\n        \n        # Fused element-wise operations\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _fused_ops_kernel[grid](x, output, n_elements)\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "c78a7657-c4a0-4686-a193-4aac8599e96d", "generation": 3, "timestamp": 1754582105.1884325, "iteration_found": 27, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 39.10000000000001, "speed_up": 1.191, "runtime_ms": 0.0808, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0962, "speed_up": 1.191, "custom_timing": 0.0808}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 32.1, "speed_up": 1.121, "runtime_ms": 0.0858, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0962, "speed_up": 1.121, "custom_timing": 0.0858}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 1.0000\n- score: 32.1000\n- speed_up: 1.1210\n- runtime_ms: 0.0858\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0962, 'speed_up': 1.121, 'custom_timing': 0.0858}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 32.1000, speed_up: 1.1210, runtime_ms: 0.0858, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0962, 'speed_up': 1.121, 'custom_timing': 0.0858}\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 41.8000, speed_up: 1.2180, runtime_ms: 0.0790, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0962, 'speed_up': 1.218, 'custom_timing': 0.079}\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 42.9000, speed_up: 1.2290, runtime_ms: 0.0783, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0962, 'speed_up': 1.229, 'custom_timing': 0.0783}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 42.9000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Fused element-wise operations kernel\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef _fused_ops_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Load input element\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Swish: x * sigmoid(x)\n    sigmoid_x = 1.0 / (1.0 + tl.exp(-x))\n    x = x * sigmoid_x\n    \n    # Divide by 2\n    x = x / 2.0\n    \n    # Clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(x, -1.0), 1.0)\n    \n    # Tanh activation using exponential implementation\n    exp_2x = tl.exp(2 * x)\n    tanh_x = (exp_2x - 1) / (exp_2x + 1)\n    \n    # Final clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(tanh_x, -1.0), 1.0)\n    \n    # Store result\n    tl.store(output_ptr + offsets, x, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        # Compute GEMM\n        x = self.gemm(x)\n        \n        # Fused element-wise operations\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _fused_ops_kernel[grid](x, output, n_elements, BLOCK_SIZE=1024)\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (42.9000), Performs well on speed_up (1.2290), Performs well on runtime_ms (0.0783), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0962, 'speed_up': 1.229, 'custom_timing': 0.0783})\n\n\n### Program 2 (Score: 41.8000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Fused element-wise operations kernel\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef _fused_ops_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Load input element\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Swish: x * sigmoid(x)\n    sigmoid_x = 1.0 / (1.0 + tl.exp(-x))\n    x = x * sigmoid_x\n    \n    # Divide by 2\n    x = x / 2.0\n    \n    # Clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(x, -1.0), 1.0)\n    \n    # Tanh activation using identity: tanh(x) = 2*sigmoid(2*x) - 1\n    two_x = 2.0 * x\n    sigmoid_2x = 1.0 / (1.0 + tl.exp(-two_x))\n    x = 2.0 * sigmoid_2x - 1.0\n    \n    # Final clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(x, -1.0), 1.0)\n    \n    # Store result\n    tl.store(output_ptr + offsets, x, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        # Compute GEMM\n        x = self.gemm(x)\n        \n        # Fused element-wise operations\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _fused_ops_kernel[grid](x, output, n_elements, BLOCK_SIZE=1024)\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (41.8000), Performs well on speed_up (1.2180), Performs well on runtime_ms (0.0790), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0962, 'speed_up': 1.218, 'custom_timing': 0.079})\n\n\n### Program 3 (Score: 32.1000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Fused element-wise operations kernel\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef _fused_ops_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Load input element\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Swish: x * sigmoid(x)\n    sigmoid_x = 1.0 / (1.0 + tl.exp(-x))\n    x = x * sigmoid_x\n    \n    # Divide by 2\n    x = x / 2.0\n    \n    # Clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(x, -1.0), 1.0)\n    \n    # Tanh activation using exponential implementation\n    exp_2x = tl.exp(2 * x)\n    tanh_x = (exp_2x - 1) / (exp_2x + 1)\n    \n    # Final clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(tanh_x, -1.0), 1.0)\n    \n    # Store result\n    tl.store(output_ptr + offsets, x, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        # Compute GEMM\n        x = self.gemm(x)\n        \n        # Fused element-wise operations\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _fused_ops_kernel[grid](x, output, n_elements, BLOCK_SIZE=1024)\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (32.1000), Performs well on speed_up (1.1210), Performs well on runtime_ms (0.0858), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0962, 'speed_up': 1.121, 'custom_timing': 0.0858})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 24.3000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_warps=8),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef _fused_forward_kernel(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    batch_size, in_features, out_features,\n    stride_x0, stride_x1,\n    stride_w0, stride_w1,\n    stride_out0, stride_out1,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_x0 + offs_k[None, :] * stride_x1\n    w_ptrs = w_ptr + offs_k[:, None] * stride_w0 + offs_n[None, :] * stride_w1\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_remaining = in_features - k * BLOCK_SIZE_K\n        k_mask = offs_k < k_remaining\n        \n        x_mask = (offs_m[:, None] < batch_size) & k_mask[None, :]\n        w_mask = k_mask[:, None] & (offs_n[None, :] < out_features)\n        \n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x, w)\n        x_ptrs += BLOCK_SIZE_K * stride_x1\n        w_ptrs += BLOCK_SIZE_K * stride_w0\n    \n    if HAS_BIAS:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < out_features, other=0.0)\n        acc += bias[None, :]\n    \n    # Swish activation: x * sigmoid(x)\n    sig = tl.sigmoid(acc)\n    acc = acc * sig\n    # Divide by 2.0\n    acc = acc / 2.0\n    # Clamp between -1 and 1\n    acc = tl.minimum(acc, 1.0)\n    acc = tl.maximum(acc, -1.0)\n    # Tanh activation using identity: tanh(x) = (exp(2x) - 1) / (exp(2x) + 1)\n    two_x = 2.0 * acc\n    exp2x = tl.exp(two_x)\n    tanh_x = (exp2x - 1.0) / (exp2x + 1.0)\n    acc = tanh_x\n    # Clamp again between -1 and 1\n    acc = tl.minimum(acc, 1.0)\n    acc = tl.maximum(acc, -1.0)\n    \n    offs_out_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_out_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    out_ptrs = output_ptr + offs_out_m[:, None] * stride_out0 + offs_out_n[None, :] * stride_out1\n    out_mask = (offs_out_m[:, None] < batch_size) & (offs_out_n[None, :] < out_features)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features)) if bias else None\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        x = x.contiguous()\n        w_t = self.weight.t().contiguous()\n        bias = self.bias.contiguous() if self.bias is not None else None\n        \n        output = torch.empty((x.size(0), self.out_features), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (\n            triton.cdiv(x.size(0), META['BLOCK_SIZE_M']),\n            triton.cdiv(self.out_features, META['BLOCK_SIZE_N']),\n        )\n        \n        _fused_forward_kernel[grid](\n            x, w_t, bias, output,\n            x.size(0), self.in_features, self.out_features,\n            x.stride(0), x.stride(1),\n            w_t.stride(0), w_t.stride(1),\n            output.stride(0), output.stride(1),\n            HAS_BIAS=bias is not None\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_warps=2, num_stages=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _fused_forward_kernel(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wk, stride_wn,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_offs = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offs < K\n        \n        x_vals = tl.load(\n            x_ptr + offs_m[:, None] * stride_xm + k_offs[None, :] * stride_xk,\n            mask=k_mask[None, :] & (offs_m[:, None] < M),\n            other=0.0\n        )\n        w_vals = tl.load(\n            w_ptr + k_offs[:, None] * stride_wk + offs_n[None, :] * stride_wn,\n            mask=k_mask[:, None] & (offs_n[None, :] < N),\n            other=0.0\n        )\n        acc += tl.dot(x_vals, w_vals)\n    \n    b = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n    \n    swish = acc * tl.sigmoid(acc)\n    div = swish / 2.0\n    clamp1 = tl.minimum(tl.maximum(div, -1.0), 1.0)\n    tanh = tl.tanh(clamp1)\n    clamp2 = tl.minimum(tl.maximum(tanh, -1.0), 1.0)\n    \n    offs_out = offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    tl.store(\n        output_ptr + offs_out, \n        clamp2,\n        mask=(offs_m[:, None] < M) & (offs_n[None, :] < N)\n    )\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n        \n    def forward(self, x):\n        M, K = x.shape\n        N = self.gemm.out_features\n        \n        x = x.contiguous()\n        weight_t = self.gemm.weight.t().contiguous()\n        bias = self.gemm.bias if self.gemm.bias is not None else torch.zeros(N, device=x.device, dtype=x.dtype)\n        \n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (\n            triton.cdiv(M, META['BLOCK_SIZE_M']), \n            triton.cdiv(N, META['BLOCK_SIZE_N']),\n        )\n        \n        _fused_forward_kernel[grid](\n            x, weight_t, bias, output,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 42.9000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Fused element-wise operations kernel\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef _fused_ops_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Load input element\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Swish: x * sigmoid(x)\n    sigmoid_x = 1.0 / (1.0 + tl.exp(-x))\n    x = x * sigmoid_x\n    \n    # Divide by 2\n    x = x / 2.0\n    \n    # Clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(x, -1.0), 1.0)\n    \n    # Tanh activation using exponential implementation\n    exp_2x = tl.exp(2 * x)\n    tanh_x = (exp_2x - 1) / (exp_2x + 1)\n    \n    # Final clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(tanh_x, -1.0), 1.0)\n    \n    # Store result\n    tl.store(output_ptr + offsets, x, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        # Compute GEMM\n        x = self.gemm(x)\n        \n        # Fused element-wise operations\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _fused_ops_kernel[grid](x, output, n_elements, BLOCK_SIZE=1024)\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Fused element-wise operations kernel\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef _fused_ops_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Load input element\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Swish: x * sigmoid(x)\n    sigmoid_x = 1.0 / (1.0 + tl.exp(-x))\n    x = x * sigmoid_x\n    \n    # Divide by 2\n    x = x / 2.0\n    \n    # Clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(x, -1.0), 1.0)\n    \n    # Replace manual tanh implementation with hardware-accelerated tl.tanh\n    x = tl.tanh(x)\n    \n    # Store result\n    tl.store(output_ptr + offsets, x, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        # Compute GEMM\n        x = self.gemm(x)\n        \n        # Fused element-wise operations\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _fused_ops_kernel[grid](x, output, n_elements, BLOCK_SIZE=1024)\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 41.8000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Fused element-wise operations kernel\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef _fused_ops_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Load input element\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Swish: x * sigmoid(x)\n    sigmoid_x = 1.0 / (1.0 + tl.exp(-x))\n    x = x * sigmoid_x\n    \n    # Divide by 2\n    x = x / 2.0\n    \n    # Clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(x, -1.0), 1.0)\n    \n    # Tanh activation using identity: tanh(x) = 2*sigmoid(2*x) - 1\n    two_x = 2.0 * x\n    sigmoid_2x = 1.0 / (1.0 + tl.exp(-two_x))\n    x = 2.0 * sigmoid_2x - 1.0\n    \n    # Final clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(x, -1.0), 1.0)\n    \n    # Store result\n    tl.store(output_ptr + offsets, x, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        # Compute GEMM\n        x = self.gemm(x)\n        \n        # Fused element-wise operations\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _fused_ops_kernel[grid](x, output, n_elements, BLOCK_SIZE=1024)\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_warps=2, num_stages=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _fused_forward_kernel(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wk, stride_wn,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_offs = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offs < K\n        \n        x_vals = tl.load(\n            x_ptr + offs_m[:, None] * stride_xm + k_offs[None, :] * stride_xk,\n            mask=k_mask[None, :] & (offs_m[:, None] < M),\n            other=0.0\n        )\n        w_vals = tl.load(\n            w_ptr + k_offs[:, None] * stride_wk + offs_n[None, :] * stride_wn,\n            mask=k_mask[:, None] & (offs_n[None, :] < N),\n            other=0.0\n        )\n        acc += tl.dot(x_vals, w_vals)\n    \n    b = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n    \n    swish = acc * tl.sigmoid(acc)\n    div = swish / 2.0\n    clamp1 = tl.minimum(tl.maximum(div, -1.0), 1.0)\n    tanh = tl.tanh(clamp1)\n    clamp2 = tl.minimum(tl.maximum(tanh, -1.0), 1.0)\n    \n    offs_out = offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    tl.store(\n        output_ptr + offs_out, \n        clamp2,\n        mask=(offs_m[:, None] < M) & (offs_n[None, :] < N)\n    )\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n        \n    def forward(self, x):\n        M, K = x.shape\n        N = self.gemm.out_features\n        \n        x = x.contiguous()\n        weight_t = self.gemm.weight.t().contiguous()\n        bias = self.gemm.bias if self.gemm.bias is not None else torch.zeros(N, device=x.device, dtype=x.dtype)\n        \n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (\n            triton.cdiv(M, META['BLOCK_SIZE_M']), \n            triton.cdiv(N, META['BLOCK_SIZE_N']),\n        )\n        \n        _fused_forward_kernel[grid](\n            x, weight_t, bias, output,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 24.3000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_warps=8),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef _fused_forward_kernel(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    batch_size, in_features, out_features,\n    stride_x0, stride_x1,\n    stride_w0, stride_w1,\n    stride_out0, stride_out1,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_x0 + offs_k[None, :] * stride_x1\n    w_ptrs = w_ptr + offs_k[:, None] * stride_w0 + offs_n[None, :] * stride_w1\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_remaining = in_features - k * BLOCK_SIZE_K\n        k_mask = offs_k < k_remaining\n        \n        x_mask = (offs_m[:, None] < batch_size) & k_mask[None, :]\n        w_mask = k_mask[:, None] & (offs_n[None, :] < out_features)\n        \n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x, w)\n        x_ptrs += BLOCK_SIZE_K * stride_x1\n        w_ptrs += BLOCK_SIZE_K * stride_w0\n    \n    if HAS_BIAS:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < out_features, other=0.0)\n        acc += bias[None, :]\n    \n    # Swish activation: x * sigmoid(x)\n    sig = tl.sigmoid(acc)\n    acc = acc * sig\n    # Divide by 2.0\n    acc = acc / 2.0\n    # Clamp between -1 and 1\n    acc = tl.minimum(acc, 1.0)\n    acc = tl.maximum(acc, -1.0)\n    # Tanh activation using identity: tanh(x) = (exp(2x) - 1) / (exp(2x) + 1)\n    two_x = 2.0 * acc\n    exp2x = tl.exp(two_x)\n    tanh_x = (exp2x - 1.0) / (exp2x + 1.0)\n    acc = tanh_x\n    # Clamp again between -1 and 1\n    acc = tl.minimum(acc, 1.0)\n    acc = tl.maximum(acc, -1.0)\n    \n    offs_out_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_out_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    out_ptrs = output_ptr + offs_out_m[:, None] * stride_out0 + offs_out_n[None, :] * stride_out1\n    out_mask = (offs_out_m[:, None] < batch_size) & (offs_out_n[None, :] < out_features)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features)) if bias else None\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        x = x.contiguous()\n        w_t = self.weight.t().contiguous()\n        bias = self.bias.contiguous() if self.bias is not None else None\n        \n        output = torch.empty((x.size(0), self.out_features), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (\n            triton.cdiv(x.size(0), META['BLOCK_SIZE_M']),\n            triton.cdiv(self.out_features, META['BLOCK_SIZE_N']),\n        )\n        \n        _fused_forward_kernel[grid](\n            x, w_t, bias, output,\n            x.size(0), self.in_features, self.out_features,\n            x.stride(0), x.stride(1),\n            w_t.stride(0), w_t.stride(1),\n            output.stride(0), output.stride(1),\n            HAS_BIAS=bias is not None\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Fused element-wise operations kernel\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef _fused_ops_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Load input element\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Swish: x * sigmoid(x)\n    sigmoid_x = 1.0 / (1.0 + tl.exp(-x))\n    x = x * sigmoid_x\n    \n    # Divide by 2\n    x = x / 2.0\n    \n    # Clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(x, -1.0), 1.0)\n    \n    # Tanh activation using exponential implementation\n    exp_2x = tl.exp(2 * x)\n    tanh_x = (exp_2x - 1) / (exp_2x + 1)\n    \n    # Final clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(tanh_x, -1.0), 1.0)\n    \n    # Store result\n    tl.store(output_ptr + offsets, x, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        # Compute GEMM\n        x = self.gemm(x)\n        \n        # Fused element-wise operations\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _fused_ops_kernel[grid](x, output, n_elements, BLOCK_SIZE=1024)\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program that has a speedup of 1.121 (score 32.1) and we see that the top-performing program had a speedup of 1.229 (score 42.9). The current program uses a Triton kernel for fused element-wise operations after a GEMM.\n\nThe current kernel does:\n  1. Swish: x * sigmoid(x)\n  2. Divide by 2\n  3. Clamp between [-1, 1]\n  4. Tanh via: (exp(2*x) - 1) / (exp(2*x) + 1)\n  5. Clamp again between [-1, 1]\n\nLooking at the top program (Program 1) in the history, it had the same structure but achieved a higher speedup. However, the current program we are given is actually the same as Program 3 (which has a lower speedup). We note that the top program (Program 1) had a different implementation for tanh? Actually, upon inspection, the top program (Program 1) is identical to the current program. \n\nBut wait: the history shows that the top program had a speedup of 1.229, but the current program has 1.121. This discrepancy might be due to different runs or hardware. However, we are to improve the current program.\n\nAreas for improvement:\n- The current kernel uses two exp operations (one for sigmoid and two for tanh). We might try to use the built-in `tl.tanh` for the tanh part, which might be faster and more accurate. However, in the Inspiration 2 program (score 10.0) they tried using `tl.tanh` but that had a lower score.\n\nAlternatively, we note that the top program (Program 1) used the same tanh implementation as the current program but achieved a higher score. So the difference might be in the autotuning or the block size.\n\nHowever, the current program uses autotuning with different num_warps. The grid is set to use a fixed BLOCK_SIZE of 1024. We might try to make the block size tunable by the autotuner.\n\nIn the current kernel, the BLOCK_SIZE is fixed at 1024 in the call. We can instead let the autotuner choose the block size. We see that the kernel is called with `BLOCK_SIZE=1024`. Instead, we can set the BLOCK_SIZE as a tunable parameter.\n\nLet's change the kernel call to use the tunable BLOCK_SIZE and adjust the autotuner configs to try different block sizes.\n\nSteps:\n1. In the autotune configs, we will include different block sizes (like 512, 1024, 2048, etc.). But note that the block size must be a power of two and not exceed the maximum number of threads per block (which is 1024 for current GPUs? Actually, Triton might handle larger block sizes by splitting, but the kernel is written for 1D block).\n\n2. We change the grid function to use the meta parameter for BLOCK_SIZE.\n\n3. We change the kernel call to use the tunable BLOCK_SIZE.\n\nBut note: the autotuner key is 'n_elements'. We are already using that. We can add BLOCK_SIZE as a tunable parameter.\n\nWe will modify the autotune configs to include different BLOCK_SIZE values. We have to be cautious because the block size must be a power of two and also the number of warps must be consistent (since each warp is 32 threads, the block size must be divisible by 32). We can set the block sizes to 256, 512, 1024, 2048? But note that the kernel uses `tl.arange(0, BLOCK_SIZE)`, so the block size must be within the maximum number of threads.\n\nActually, the Triton compiler may use multiple warps to handle a large block. However, the maximum block size in CUDA is 1024, so we should not exceed that. Therefore, we set the block sizes to 256, 512, 1024.\n\nWe will restructure the autotune configs to try different block sizes and different numbers of warps. Note that the number of warps is determined by the block size: for example, block size 256 requires 8 warps (256/32=8). But we can set the number of warps independently? Actually, the block size in Triton is the number of threads? And the number of warps is the number of warps per block. So the block size must be 32 * num_warps? Not necessarily: the block can be any size, and Triton will use as many warps as needed. However, we must set the num_warps to at least ceil(BLOCK_SIZE/32). But we can let Triton handle that? Actually, we specify the num_warps and the block size independently? The block size in the kernel is independent of the num_warps. The num_warps is the number of warps per block, so the block size must be <= num_warps * 32.\n\nTherefore, we must set the num_warps appropriately for the block size. We can do:\n\n  block_size = 256 -> num_warps = 8 (since 256/32=8)\n  block_size = 512 -> num_warps = 16? But wait, 16 warps is 512 threads -> that's the maximum for CUDA. However, Triton might not allow that? Actually, the Triton documentation says that the num_warps is the number of warps per block, and the block size is the number of threads per block. They are related by: block_size = num_warps * 32. Therefore, we cannot set them independently? \n\nLooking at the Triton code: the kernel is launched with a grid of blocks, and each block has `BLOCK_SIZE` threads? But note: in the kernel, we have:\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\nSo the block in the grid has `BLOCK_SIZE` threads? Then the num_warps must be set to `BLOCK_SIZE // 32` (if BLOCK_SIZE is divisible by 32). And we must set the num_warps accordingly.\n\nTherefore, we will set the num_warps in the config to be BLOCK_SIZE // 32. We can do:\n\n    triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n    triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n    triton.Config({'BLOCK_SIZE': 1024}, num_warps=32),\n\nBut wait: 1024 threads per block is the maximum in CUDA, and 32 warps (1024/32=32) is the maximum. However, Triton might not support 32 warps? Actually, the H100 supports up to 64 warps per block? But let me check: the maximum threads per block for NVIDIA GPUs is 1024, so 1024/32=32 warps. So 32 warps is acceptable.\n\nBut note: the autotuner might not accept a config with 32 warps? We have to try.\n\nAlternatively, we can set the num_warps to be the minimum number of warps required (ceil(BLOCK_SIZE/32)). But we must set it exactly.\n\nWe change the autotune configs to:\n\n    configs = [\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=32),\n    ]\n\nBut note: the kernel parameter BLOCK_SIZE is a tl.constexpr, so it can be set by the autotuner.\n\nAlso, we must change the grid function to use the tunable block size.\n\nIn the forward function, we call the kernel with:\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _fused_ops_kernel[grid](x, output, n_elements, BLOCK_SIZE=meta['BLOCK_SIZE'])\n\nBut wait: the kernel call must pass the BLOCK_SIZE as a keyword argument? Actually, the kernel call uses the current meta for the block size. We are passing the meta from the grid function.\n\nWe change the kernel call to use the meta['BLOCK_SIZE'].\n\nHowever, note that the kernel definition has a parameter `BLOCK_SIZE: tl.constexpr`. We are now setting it via the autotuner.\n\nBut the autotuner will run different configurations, and the grid function uses the same meta.\n\nWe'll do:\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    # Then in the kernel call, we pass BLOCK_SIZE=meta['BLOCK_SIZE']? Actually, we don't have to because the autotuner will set the BLOCK_SIZE for the kernel.\n\nBut note: the kernel call has the argument `BLOCK_SIZE=...`. We are currently passing `BLOCK_SIZE=1024` (a fixed value). We must change that to use the meta parameter.\n\nWe change the kernel call to:\n    _fused_ops_kernel[grid](x, output, n_elements, BLOCK_SIZE=meta['BLOCK_SIZE'])\n\nBut wait, the kernel call in Triton is done by passing the grid and then the arguments. The autotuner will use the config to set the BLOCK_SIZE for that run. So we can simply pass the BLOCK_SIZE as a kernel argument and it will be set by the autotuner? Actually, the autotuner sets the constexpr parameters. So we can remove the BLOCK_SIZE from the argument list? No, because the kernel expects it. We are passing it as an argument. But note: the autotuner sets the constexpr parameters, and they are not passed as regular arguments. We must remove the BLOCK_SIZE from the kernel arguments? \n\nLooking at the Triton documentation: the constexpr parameters are passed as keyword arguments to the kernel and are set at compile time. They are not passed as regular arguments. Therefore, we should not pass them as regular arguments. So we remove the BLOCK_SIZE from the kernel arguments? But then how do we set it? The autotuner sets it via the config.\n\nActually, in the kernel call, we do not pass the constexpr parameters as arguments. They are set by the autotuner. Therefore, we should remove the BLOCK_SIZE from the kernel's argument list? But wait, the kernel signature is:\n\n    def _fused_ops_kernel(\n        input_ptr,\n        output_ptr,\n        n_elements,\n        BLOCK_SIZE: tl.constexpr,\n    )\n\nSo the BLOCK_SIZE is a constexpr. We are passing it as a keyword argument? In the call we have:\n\n    _fused_ops_kernel[grid](x, output, n_elements, BLOCK_SIZE=1024)\n\nBut now we want to set it via the autotuner. Therefore, we remove the argument in the call? Because the autotuner will set it. Actually, we can leave the call without the BLOCK_SIZE? But then the kernel would not have it set. Alternatively, we can pass it as a constexpr by not including it in the argument list? \n\nThe correct way is to let the autotuner set the constexpr. So we do:\n\n    _fused_ops_kernel[grid](x, output, n_elements)\n\nAnd then the autotuner will set the BLOCK_SIZE for each config.\n\nBut note: the kernel still requires the BLOCK_SIZE as a constexpr. We must define it in the config. Therefore, we change the autotune configs to set the BLOCK_SIZE, and we remove the BLOCK_SIZE from the kernel call.\n\nSo we change the kernel call to:\n    _fused_ops_kernel[grid](x, output, n_elements)\n\nAnd in the autotune configs, we set the BLOCK_SIZE.\n\nBut wait: the kernel signature still has BLOCK_SIZE as a parameter. How does the autotuner set it? It sets it as a constexpr. So we don't pass it as an argument.\n\nTherefore, we remove the BLOCK_SIZE from the kernel call.\n\nBut note: the kernel uses BLOCK_SIZE in the code. We are now setting it via the autotuner.\n\nWe must also adjust the grid function to use the same BLOCK_SIZE that the autotuner chooses.\n\nSo the plan:\n  1. Change the autotune configs to set BLOCK_SIZE and num_warps appropriately.\n  2. Remove the BLOCK_SIZE argument from the kernel call.\n  3. Change the grid function to use meta['BLOCK_SIZE'].\n\nLet me write the new autotune configs:\n\n    configs = [\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=32),\n        # Also include the original configs? They are for num_warps only. We can keep the above and also include some with different num_warps? But the num_warps must be consistent with the block size. So we stick to these.\n\n        # We can also try to use fewer warps? Actually, we cannot because the block size must be divisible by 32 and the num_warps must be block_size/32.\n    ]\n\nBut note: the current autotune configs are:\n\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n\nWe are replacing them with the above? But the above set the BLOCK_SIZE and the num_warps together. We are now tuning the block size and the number of warps together.\n\nAlternatively, we can try to keep the existing configs and add the new ones? But note: the existing configs do not set BLOCK_SIZE, so they would use the default? We don't have a default. We must set the BLOCK_SIZE in every config.\n\nWe remove the old configs and use the new ones.\n\nWe change the autotune decorator to:\n\n    @triton.autotune(\n        configs=[\n            triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n            triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n            triton.Config({'BLOCK_SIZE': 1024}, num_warps=32),\n        ],\n        key=['n_elements'],\n    )\n\nAnd then in the kernel, we use the BLOCK_SIZE as a constexpr.\n\nIn the forward function, we change the grid to:\n\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\nAnd the kernel call to:\n\n        _fused_ops_kernel[grid](x, output, n_elements)\n\nWe remove the BLOCK_SIZE argument.\n\nBut note: the kernel definition still has the BLOCK_SIZE parameter. We leave it.\n\nLet's write the new code accordingly.\n\nWe also note that the top program (Program 1) had a higher score. We hope that by tuning the block size we can get closer to that.\n\nAlternatively, we might try to optimize the math operations. For example, we are computing two exponentials for tanh: exp(2*x) twice. We can compute it once and reuse. Let's change:\n\n    # Instead of:\n    exp_2x = tl.exp(2 * x)\n    tanh_x = (exp_2x - 1) / (exp_2x + 1)\n\n    # We can do:\n    exp_2x = tl.exp(2 * x)\n    one = 1.0\n    tanh_x = (exp_2x - one) / (exp_2x + one)\n\nBut that doesn't save much. Alternatively, we can use the built-in tanh? But we tried that in Inspiration 2 and it had lower performance? Or maybe we can try:\n\n    x = tl.tanh(x)\n\nBut let's see: the built-in tanh might be faster? We can try. But we are going to do two things: autotune block size and try to use built-in tanh.\n\nHowever, we are instructed to make small iterative changes. We'll first try the autotuning of block size. Then, if we have room, we can try the built-in tanh.\n\nBut note: the current program already has two clamps and we are doing two clamps. We might be able to remove the last clamp because tanh is already in [-1,1]. But the problem says: \"Clamp between [-1, 1]\" and then \"Final clamp between [-1, 1]\". We might remove the last clamp? But the problem requires the same operations. So we must keep both.\n\nAlternatively, we can note that the output of tanh is in [-1,1] so the final clamp is redundant. But the original PyTorch code might have done the clamp again? We must be functionally equivalent. The original PyTorch code is:\n\n    reference_output = torch.clamp(torch.tanh(torch.clamp(swish_output / 2.0, min=-1, max=1)), min=-1, max=1)\n\nSo the inner clamp and the outer clamp are both there. Therefore, we cannot remove.\n\nGiven the above, we will focus on autotuning the block size.\n\nLet's rewrite the code accordingly.\n\nWe'll change:\n  - The autotune configs to set BLOCK_SIZE and num_warps.\n  - Remove the BLOCK_SIZE argument in the kernel call.\n  - Adjust the grid function to use meta['BLOCK_SIZE'].\n\nWe leave the rest of the kernel unchanged.\n\nWe note that the current kernel has the following operations:\n\n    # Swish: x * sigmoid(x)\n    sigmoid_x = 1.0 / (1.0 + tl.exp(-x))\n    x = x * sigmoid_x\n\n    # Then divide by 2, clamp, tanh, clamp.\n\nWe are not changing the operations.\n\nWe output the entire code block.\n\nIMPORTANT: We must not change the class name or the function signatures.\n\nLet's write the new code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Fused element-wise operations kernel\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=32),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef _fused_ops_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Load input element\n    x = tl.load(input_ptr + offsets, mask=mask)\n    \n    # Swish: x * sigmoid(x)\n    sigmoid_x = 1.0 / (1.0 + tl.exp(-x))\n    x = x * sigmoid_x\n    \n    # Divide by 2\n    x = x / 2.0\n    \n    # Clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(x, -1.0), 1.0)\n    \n    # Tanh activation using exponential implementation\n    exp_2x = tl.exp(2 * x)\n    tanh_x = (exp_2x - 1) / (exp_2x + 1)\n    \n    # Final clamp between [-1, 1]\n    x = tl.minimum(tl.maximum(tanh_x, -1.0), 1.0)\n    \n    # Store result\n    tl.store(output_ptr + offsets, x, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features, bias=bias)\n\n    def forward(self, x):\n        # Compute GEMM\n        x = self.gemm(x)\n        \n        # Fused element-wise operations\n        output = torch.empty_like(x)\n        n_elements = x.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _fused_ops_kernel[grid](x, output, n_elements)\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
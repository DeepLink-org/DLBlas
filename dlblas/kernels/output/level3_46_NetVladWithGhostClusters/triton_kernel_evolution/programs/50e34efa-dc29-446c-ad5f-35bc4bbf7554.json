{"id": "50e34efa-dc29-446c-ad5f-35bc4bbf7554", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_assignment_kernel(\n    x_ptr, \n    clusters_ptr,\n    running_mean_ptr,\n    scale_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_xb, stride_xn, stride_xd,\n    stride_cd, stride_ck,\n    stride_rmk, stride_sk, stride_bk,\n    stride_ob, stride_on, stride_ok,\n    D, clusters, cluster_size, max_sample,\n    BLOCK_D: tl.constexpr, BLOCK_CLUSTERS: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    b_idx = pid // max_sample\n    n_idx = pid % max_sample\n    \n    # Initialize accumulator\n    accum = tl.zeros((BLOCK_CLUSTERS,), dtype=tl.float32)\n    \n    # Compute matrix multiplication\n    for d_offset in range(0, D, BLOCK_D):\n        d_offsets = d_offset + tl.arange(0, BLOCK_D)\n        d_mask = d_offsets < D\n        \n        # Load input vector block\n        x_vals = tl.load(\n            x_ptr + b_idx*stride_xb + n_idx*stride_xn + d_offsets,\n            mask=d_mask, other=0.0\n        )\n        \n        # Load cluster weights block\n        cluster_offsets = d_offsets[:, None] * stride_cd + tl.arange(0, BLOCK_CLUSTERS)[None, :] * stride_ck\n        cluster_mask = d_mask[:, None] & (tl.arange(0, BLOCK_CLUSTERS)[None, :] < clusters)\n        clusters_block = tl.load(clusters_ptr + cluster_offsets, mask=cluster_mask, other=0.0)\n        \n        # Accumulate partial sums\n        partial = tl.dot(x_vals, clusters_block, allow_tf32=True)\n        accum += partial\n\n    # Load batch norm parameters\n    k_offsets = tl.arange(0, BLOCK_CLUSTERS)\n    k_mask = k_offsets < clusters\n    running_mean = tl.load(running_mean_ptr + k_offsets, mask=k_mask, other=0.0)\n    scale_vals = tl.load(scale_ptr + k_offsets, mask=k_mask, other=0.0)\n    bias_vals = tl.load(bias_ptr + k_offsets, mask=k_mask, other=0.0)\n    \n    # Apply batch norm\n    accum = (accum - running_mean) * scale_vals + bias_vals\n    \n    # Stable softmax\n    max_val = tl.max(accum, axis=0)\n    numerator = tl.exp(accum - max_val)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_out = numerator / denominator\n    \n    # Store non-ghost clusters\n    output_vals = softmax_out[:cluster_size]\n    output_offsets = b_idx*stride_ob + n_idx*stride_on + tl.arange(0, cluster_size)\n    tl.store(output_ptr + output_offsets, output_vals, mask=tl.arange(0, cluster_size) < cluster_size)\n\nclass ModelNew(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(ModelNew, self).__init__()\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n        \n        self.clusters = nn.Parameter(init_sc * torch.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        self.clusters2 = nn.Parameter(init_sc * torch.randn(1, feature_size, cluster_size))\n        self.out_dim = cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        B, N, D = x.shape\n        x_flat = x.reshape(-1, D)\n        \n        if not self.training:\n            # Precompute batch norm parameters\n            scale = self.batch_norm.weight / torch.sqrt(self.batch_norm.running_var + self.batch_norm.eps)\n            bias = self.batch_norm.bias\n            \n            # Prepare output\n            assignment_out = torch.empty((B, N, self.cluster_size), \n                                        device=x.device, dtype=x.dtype)\n            \n            # Launch kernel\n            grid = (B * N,)\n            fused_assignment_kernel[grid](\n                x_flat, self.clusters,\n                self.batch_norm.running_mean,\n                scale, bias,\n                assignment_out,\n                x_flat.stride(0), x_flat.stride(1), 0,\n                self.clusters.stride(0), self.clusters.stride(1),\n                0, 0, 0,\n                assignment_out.stride(0), assignment_out.stride(1), assignment_out.stride(2),\n                D, self.cluster_size + self.ghost_clusters, self.cluster_size, N,\n                BLOCK_D=128,\n                BLOCK_CLUSTERS=triton.next_power_of_2(self.cluster_size + self.ghost_clusters),\n            )\n        else:\n            # Training path\n            assignment = torch.matmul(x_flat, self.clusters)\n            assignment = self.batch_norm(assignment)\n            assignment = F.softmax(assignment, dim=1)\n            assignment_out = assignment[:, :self.cluster_size].view(B, N, self.cluster_size)\n        \n        # Compute VLAD features\n        a_sum = torch.sum(assignment_out, dim=1, keepdim=True)\n        a = a_sum * self.clusters2\n        vlad = torch.matmul(x.transpose(1, 2), assignment_out) - a\n        \n        # Normalizations\n        vlad = F.normalize(vlad, p=2, dim=1)\n        vlad = vlad.reshape(B, -1)\n        vlad = F.normalize(vlad, p=2, dim=1)\n        return vlad\n\nbatch_size = 32\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 16\n\ndef get_inputs():\n    return [torch.randn(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n    return [num_clusters, feature_size, ghost_clusters]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "137c5f95-87fb-47bd-a234-88d832fff46a", "generation": 2, "timestamp": 1754592881.2531629, "iteration_found": 20, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 25.899999999999995, "speed_up": 1.059, "runtime_ms": 0.236, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.25, "speed_up": 1.059, "custom_timing": 0.236}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_46_NetVladWithGhostClusters", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpdllmb7ah.py\", line 91, in forward\n    self._vlad_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 35:17:\n    vlad_val = tl.zeros((BLOCK_SIZE_D, BLOCK_SIZE_K), dtype=tl.float32)\n    a_sum = tl.zeros((BLOCK_SIZE_K,), dtype=tl.float32)\n\n    # Compute assignment and accumulate\n    for n in range(N):\n        # Load input feature\n        x_offsets = pid_b * stride_xb + n * stride_xn + d_offsets\n        x_val = tl.load(x_ptr + x_offsets, mask=d_mask, other=0.0)\n\n        # Compute assignment scores\n        scores = tl.sum(x_val[:, None] * clusters, axis=0)\n        scores = tl.softmax(scores, axis=0)\n                 ^\nTypeError(\"softmax() got an unexpected keyword argument 'axis'\")\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/inspect.py\", line 1517, in getcallargs\n    raise TypeError(\"%s() got an unexpected keyword argument %r\" %\nTypeError: softmax() got an unexpected keyword argument 'axis'\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.25, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
{"id": "e1cc025d-f803-4ff8-ac1f-8e2d3d549e23", "code": "# ================== EVOLVE-BLOCK-START ==================\n# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_D\": 256}, num_warps=8),\n        triton.Config({\"BLOCK_D\": 512}, num_warps=8),\n        triton.Config({\"BLOCK_D\": 1024}, num_warps=4),\n    ],\n    key=[\"D\"],\n)\n@triton.jit\ndef fused_assignment_kernel(\n    x_ptr, \n    clusters_ptr,\n    running_mean_ptr,\n    scale_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x_batch,\n    stride_x_d,\n    stride_clusters_d,\n    stride_running_mean_k,\n    stride_scale_k,\n    stride_bias_k,\n    stride_output_b, \n    stride_output_n, \n    stride_output_k,\n    D, \n    clusters, \n    cluster_size, \n    max_sample, \n    BLOCK_D: tl.constexpr, \n    BLOCK_CLUSTERS: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    b_idx = pid // max_sample\n    n_idx = pid % max_sample\n    \n    # Precompute offsets for coalesced memory access\n    x_offset = b_idx * stride_x_batch + n_idx * stride_x_d\n    clusters_offset = tl.arange(0, BLOCK_CLUSTERS)\n    \n    # Initialize accumulator\n    accum = tl.zeros((BLOCK_CLUSTERS,), dtype=tl.float32)\n    \n    # Vectorized matrix multiplication\n    for d_offset in range(0, D, BLOCK_D):\n        d_indices = d_offset + tl.arange(0, BLOCK_D)\n        mask_d = d_indices < D\n        \n        # Coalesced memory loads\n        x_vals = tl.load(x_ptr + x_offset + d_indices, mask=mask_d, other=0.0)\n        clusters_block = tl.load(\n            clusters_ptr + d_indices[:, None] * stride_clusters_d + clusters_offset[None, :],\n            mask=mask_d[:, None] & (clusters_offset[None, :] < clusters),\n            other=0.0\n        )\n        \n        # Accumulate partial sums\n        accum += tl.dot(x_vals, clusters_block)\n\n    # Load batch norm parameters with coalesced access\n    running_mean = tl.load(running_mean_ptr + clusters_offset, mask=clusters_offset < clusters, other=0.0)\n    scale_vals = tl.load(scale_ptr + clusters_offset, mask=clusters_offset < clusters, other=0.0)\n    bias_vals = tl.load(bias_ptr + clusters_offset, mask=clusters_offset < clusters, other=0.0)\n    \n    # Apply batch norm\n    accum = (accum - running_mean) * scale_vals + bias_vals\n    \n    # Stable softmax\n    max_val = tl.max(accum, axis=0)\n    accum = accum - max_val\n    exp_vals = tl.exp(accum)\n    exp_sum = tl.sum(exp_vals, axis=0)\n    softmax_out = exp_vals / exp_sum\n    \n    # Store only non-ghost clusters with vectorization\n    output_k_indices = tl.arange(0, cluster_size)\n    output_vals = softmax_out[0:cluster_size]\n    output_offset = b_idx * stride_output_b + n_idx * stride_output_n + output_k_indices\n    tl.store(output_ptr + output_offset, output_vals, mask=output_k_indices < cluster_size)\n\nclass ModelNew(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(ModelNew, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * torch.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * torch.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        \"\"\"Aggregates feature maps into a fixed size representation.\"\"\"\n        B, N, D = x.shape\n        x_flat = x.reshape(B*N, D)\n        \n        # Precompute batch norm parameters for inference\n        if not self.training:\n            scale = self.batch_norm.weight / torch.sqrt(self.batch_norm.running_var + self.batch_norm.eps)\n            bias = self.batch_norm.bias\n            \n            # Create output tensor\n            assignment_out = torch.empty((B, N, self.cluster_size), \n                                        device=x.device, dtype=x.dtype)\n            \n            # Launch Triton kernel with autotuning\n            grid = (B*N,)\n            fused_assignment_kernel[grid](\n                x_flat, \n                self.clusters,\n                self.batch_norm.running_mean,\n                scale,\n                bias,\n                assignment_out,\n                x_flat.stride(0),\n                x_flat.stride(1),\n                self.clusters.stride(0),\n                self.batch_norm.running_mean.stride(0),\n                scale.stride(0),\n                bias.stride(0),\n                assignment_out.stride(0),\n                assignment_out.stride(1),\n                assignment_out.stride(2),\n                D,\n                self.cluster_size + self.ghost_clusters,\n                self.cluster_size,\n                N,\n                BLOCK_D=256,\n                BLOCK_CLUSTERS=triton.next_power_of_2(self.cluster_size + self.ghost_clusters),\n            )\n        else:\n            # Training path (original implementation)\n            assignment = torch.matmul(x_flat, self.clusters)\n            assignment = self.batch_norm(assignment)\n            assignment = F.softmax(assignment, dim=1)\n            assignment = assignment[:, :self.cluster_size]\n            assignment_out = assignment.view(B, N, self.cluster_size)\n\n        a_sum = torch.sum(assignment_out, dim=1, keepdim=True)\n        a = a_sum * self.clusters2\n\n        # Optimized VLAD computation\n        vlad = torch.matmul(x.transpose(1,2), assignment_out)\n        vlad = vlad - a\n\n        # Normalizations\n        vlad = F.normalize(vlad, dim=1)\n        vlad = vlad.reshape(B, -1)\n        vlad = F.normalize(vlad)\n        return vlad\n\nbatch_size = 32\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 16\n\ndef get_inputs():\n  return [torch.randn(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "ecb95f21-cf11-41e4-b594-6572b5072ef5", "generation": 2, "timestamp": 1754592307.212907, "iteration_found": 15, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 26.400000000000006, "speed_up": 1.064, "runtime_ms": 0.235, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.25, "speed_up": 1.064, "custom_timing": 0.235}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 25.000000000000004, "speed_up": 1.05, "runtime_ms": 0.238, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.25, "speed_up": 1.05, "custom_timing": 0.238}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
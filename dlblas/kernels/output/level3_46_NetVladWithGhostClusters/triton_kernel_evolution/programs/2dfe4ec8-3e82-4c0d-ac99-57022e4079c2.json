{"id": "2dfe4ec8-3e82-4c0d-ac99-57022e4079c2", "code": "# ================== EVOLVE-BLOCK-START ==================\n# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCode modified from here\nhttps://github.com/albanie/collaborative-experts/blob/master/model/net_vlad.py\n\"\"\"\n\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch as th\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _softmax_reduce_kernel(\n    assignment_ptr,\n    assignment_output_ptr,\n    a_sum_ptr,\n    B,\n    N,\n    clusters,\n    cluster_size,\n    stride_asn,\n    stride_asc,\n    stride_outb,\n    stride_outn,\n    stride_outc,\n    stride_sumb,\n    stride_sumc,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    b_idx = pid // N\n    n_idx = pid % N\n    if b_idx >= B or n_idx >= N:\n        return\n\n    row_start = b_idx * stride_asn * N + n_idx * stride_asn\n    row = tl.load(\n        assignment_ptr + row_start + tl.arange(0, clusters) * stride_asc,\n        mask=tl.arange(0, clusters) < clusters,\n        other=-float('inf')\n    )\n\n    # Online softmax with stable computation\n    row_max = tl.max(row, axis=0)\n    numerator = tl.exp(row - row_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_row = numerator / denominator\n\n    # Store valid clusters and accumulate sums\n    for c in range(cluster_size):\n        val = softmax_row[c]\n        out_offset = b_idx * stride_outb + n_idx * stride_outn + c * stride_outc\n        tl.store(assignment_output_ptr + out_offset, val)\n        sum_offset = b_idx * stride_sumb + c * stride_sumc\n        tl.atomic_add(a_sum_ptr + sum_offset, val)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(ModelNew, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * th.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * th.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        max_sample = x.size()[1]\n        x_flat = x.view(-1, self.feature_size)  # B x N x D -> BN x D\n\n        if x_flat.device != self.clusters.device:\n            msg = f\"x.device {x_flat.device} != cluster.device {self.clusters.device}\"\n            raise ValueError(msg)\n\n        assignment = th.matmul(x_flat, self.clusters)  # (BN x D) x (D x (K+G)) -> BN x (K+G)\n        assignment = self.batch_norm(assignment)\n\n        B, N = x.size(0), x.size(1)\n        clusters_total = self.cluster_size + self.ghost_clusters\n        \n        # Create output buffers\n        assignment_output = th.empty((B, N, self.cluster_size), \n                                    device=assignment.device, dtype=assignment.dtype)\n        a_sum = th.zeros((B, self.cluster_size), \n                        device=assignment.device, dtype=assignment.dtype)\n        \n        # Launch Triton kernel\n        grid = (B * N,)\n        _softmax_reduce_kernel[grid](\n            assignment,\n            assignment_output,\n            a_sum,\n            B,\n            N,\n            clusters_total,\n            self.cluster_size,\n            assignment.stride(0),\n            assignment.stride(1),\n            assignment_output.stride(0),\n            assignment_output.stride(1),\n            assignment_output.stride(2),\n            a_sum.stride(0),\n            a_sum.stride(1),\n            BLOCK_SIZE=triton.next_power_of_2(clusters_total)\n        )\n        \n        a_sum = a_sum.unsqueeze(1)  # B x 1 x K\n        a = a_sum * self.clusters2\n\n        assignment = assignment_output.transpose(1, 2)  # B x K x N\n\n        x = x.view(B, N, self.feature_size)  # BN x D -> B x N x D\n        vlad = th.matmul(assignment, x)  # (B x K x N) x (B x N x D) -> B x K x D\n        vlad = vlad.transpose(1, 2)  # -> B x D x K\n        vlad = vlad - a\n\n        # L2 intra norm\n        vlad = F.normalize(vlad, p=2, dim=1)\n\n        # flattening + L2 norm\n        vlad = vlad.reshape(-1, self.cluster_size * self.feature_size)  # -> B x DK\n        vlad = F.normalize(vlad, p=2, dim=1)\n        return vlad  # B x DK\n\nbatch_size = 32\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 16\n\ndef get_inputs():\n  return [torch.randn(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "820c6335-456b-4a78-8a3b-8e373302a9e9", "generation": 1, "timestamp": 1754590498.0067813, "iteration_found": 3, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_46_NetVladWithGhostClusters", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 574, in arange\n    raise ValueError(\"arange's arguments must be of type tl.constexpr\")\nValueError: arange's arguments must be of type tl.constexpr\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp5og0qijc.py\", line 116, in forward\n    _softmax_reduce_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 27:13:\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    b_idx = pid // N\n    n_idx = pid % N\n    if b_idx >= B or n_idx >= N:\n        return\n\n    row_start = b_idx * stride_asn * N + n_idx * stride_asn\n    row = tl.load(\n        assignment_ptr + row_start + tl.arange(0, clusters) * stride_asc,\n        mask=tl.arange(0, clusters) < clusters,\n             ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 574, in arange\n    raise ValueError(\"arange's arguments must be of type tl.constexpr\")\nValueError: arange's arguments must be of type tl.constexpr\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.25, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.247, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.25, "speed_up": 1.012, "custom_timing": 0.247}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
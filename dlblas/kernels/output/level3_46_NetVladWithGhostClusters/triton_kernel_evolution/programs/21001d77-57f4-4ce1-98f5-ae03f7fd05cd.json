{"id": "21001d77-57f4-4ce1-98f5-ae03f7fd05cd", "code": "# ================== EVOLVE-BLOCK-START ==================\n# Copyright 2018 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_D': 256, 'BLOCK_S': 64}, num_warps=8, num_stages=1),\n        triton.Config({'BLOCK_D': 256, 'BLOCK_S': 32}, num_warps=8, num_stages=1),\n        triton.Config({'BLOCK_D': 512, 'BLOCK_S': 32}, num_warps=8, num_stages=1),\n        triton.Config({'BLOCK_D': 128, 'BLOCK_S': 128}, num_warps=8, num_stages=1),\n        triton.Config({'BLOCK_D': 512, 'BLOCK_S': 64}, num_warps=8, num_stages=1),\n        triton.Config({'BLOCK_D': 256, 'BLOCK_S': 128}, num_warps=8, num_stages=1),\n    ],\n    key=['D'],\n)\n@triton.jit\ndef fused_assignment_kernel(\n    x_ptr, \n    clusters_ptr,\n    running_mean_ptr,\n    scale_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x_batch,\n    stride_x_d,\n    stride_clusters_d,\n    stride_running_mean_k,\n    stride_scale_k,\n    stride_bias_k,\n    stride_output_b, \n    stride_output_n, \n    stride_output_k,\n    D, \n    clusters, \n    cluster_size, \n    max_sample, \n    BLOCK_D: tl.constexpr, \n    BLOCK_CLUSTERS: tl.constexpr,\n    BLOCK_S: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_sample_blocks = tl.cdiv(max_sample, BLOCK_S)\n    b_idx = pid // num_sample_blocks\n    s_block = pid % num_sample_blocks\n    \n    s_start = s_block * BLOCK_S\n    s_offsets = s_start + tl.arange(0, BLOCK_S)\n    mask_s = s_offsets < max_sample\n    \n    accum = tl.zeros((BLOCK_S, BLOCK_CLUSTERS), dtype=tl.float32)\n    \n    for d_offset in range(0, D, BLOCK_D):\n        d_indices = tl.arange(0, BLOCK_D) + d_offset\n        mask_d = d_indices < D\n        \n        # Load x block: [BLOCK_S, BLOCK_D] - convert to fp16 for Tensor Cores\n        x_ptrs = x_ptr + b_idx * stride_x_batch + s_offsets[:, None] * stride_x_d + d_indices[None, :]\n        mask_x = mask_s[:, None] & mask_d[None, :]\n        x_vals = tl.load(x_ptrs, mask=mask_x, other=0.0).to(tl.float16)\n        \n        # Load clusters block: [BLOCK_D, BLOCK_CLUSTERS] - convert to fp16\n        clusters_block = tl.load(\n            clusters_ptr + d_indices[:, None] * stride_clusters_d + tl.arange(0, BLOCK_CLUSTERS)[None, :],\n            mask=mask_d[:, None] & (tl.arange(0, BLOCK_CLUSTERS)[None, :] < clusters),\n            other=0.0\n        ).to(tl.float16)\n        \n        # Use FP16 Tensor Core matrix multiplication with accumulation in FP32\n        partial = tl.dot(x_vals, clusters_block, allow_tf32=True).to(tl.float32)\n        accum += partial\n\n    # Load batch norm parameters\n    k_indices = tl.arange(0, BLOCK_CLUSTERS)\n    running_mean = tl.load(running_mean_ptr + k_indices, mask=k_indices < clusters, other=0.0)\n    scale_vals = tl.load(scale_ptr + k_indices, mask=k_indices < clusters, other=0.0)\n    bias_vals = tl.load(bias_ptr + k_indices, mask=k_indices < clusters, other=0.0)\n    \n    # Apply batch norm\n    accum = (accum - running_mean[None, :]) * scale_vals[None, :] + bias_vals[None, :]\n    \n    # Apply softmax\n    max_vals = tl.max(accum, axis=1, keep_dims=True)\n    accum = accum - max_vals\n    exp_vals = tl.exp(accum)\n    exp_sums = tl.sum(exp_vals, axis=1, keep_dims=True)\n    softmax_out = exp_vals / exp_sums\n    \n    # Store non-ghost clusters\n    output_k_indices = tl.arange(0, cluster_size)\n    output_vals = softmax_out[:, :cluster_size]\n    output_ptrs = output_ptr + b_idx * stride_output_b + s_offsets[:, None] * stride_output_n + output_k_indices[None, :]\n    mask_store = mask_s[:, None] & (output_k_indices[None, :] < cluster_size)\n    tl.store(output_ptrs, output_vals, mask=mask_store)\n\nclass ModelNew(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(ModelNew, self).__init__()\n\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n\n        # The `clusters` weights are the `(w,b)` in the paper\n        self.clusters = nn.Parameter(init_sc * torch.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        # The `clusters2` weights are the visual words `c_k` in the paper\n        self.clusters2 = nn.Parameter(init_sc * torch.randn(1, feature_size, cluster_size))\n        self.out_dim = self.cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        \"\"\"Aggregates feature maps into a fixed size representation.\"\"\"\n        B, N, D = x.shape\n        x_flat = x.reshape(B*N, D)\n        \n        # Precompute batch norm parameters for inference\n        if not self.training:\n            scale = self.batch_norm.weight / torch.sqrt(self.batch_norm.running_var + self.batch_norm.eps)\n            bias = self.batch_norm.bias\n            \n            # Create output tensor\n            assignment_out = torch.empty((B, N, self.cluster_size), \n                                        device=x.device, dtype=x.dtype)\n            \n            # Launch Triton kernel with autotuned parameters\n            grid = lambda meta: (B * triton.cdiv(N, meta['BLOCK_S']),)\n            fused_assignment_kernel[grid](\n                x_flat, \n                self.clusters,\n                self.batch_norm.running_mean,\n                scale,\n                bias,\n                assignment_out,\n                x_flat.stride(0),\n                x_flat.stride(1),\n                self.clusters.stride(0),\n                self.batch_norm.running_mean.stride(0),\n                scale.stride(0),\n                bias.stride(0),\n                assignment_out.stride(0),\n                assignment_out.stride(1),\n                assignment_out.stride(2),\n                D,\n                self.cluster_size + self.ghost_clusters,\n                self.cluster_size,\n                N,\n                BLOCK_D=128,\n                BLOCK_CLUSTERS=triton.next_power_of_2(self.cluster_size + self.ghost_clusters),\n                BLOCK_S=64,\n            )\n        else:\n            # Training path (original implementation)\n            assignment = torch.matmul(x_flat, self.clusters)\n            assignment = self.batch_norm(assignment)\n            assignment = F.softmax(assignment, dim=1)\n            assignment = assignment[:, :self.cluster_size]\n            assignment_out = assignment.view(B, N, self.cluster_size)\n\n        a_sum = torch.sum(assignment_out, dim=1, keepdim=True)\n        a = a_sum * self.clusters2\n\n        # Optimized VLAD computation\n        vlad = torch.matmul(x.transpose(1,2), assignment_out)\n        vlad = vlad - a\n\n        # Normalizations\n        vlad = F.normalize(vlad, dim=1)\n        vlad = vlad.reshape(B, -1)\n        vlad = F.normalize(vlad)\n        return vlad\n\nbatch_size = 32\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 16\n\ndef get_inputs():\n  return [torch.randn(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n  return [num_clusters, feature_size, ghost_clusters]\n\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "eb6a4343-e370-4721-a900-08431f00e0b2", "generation": 4, "timestamp": 1754594078.5038347, "iteration_found": 25, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 20.4, "speed_up": 1.004, "runtime_ms": 0.249, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.25, "speed_up": 1.004, "custom_timing": 0.249}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 24.200000000000003, "speed_up": 1.042, "runtime_ms": 0.24, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.25, "speed_up": 1.042, "custom_timing": 0.24}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
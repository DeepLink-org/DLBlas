{"id": "ca1fa89d-17d7-4301-943b-1beb60bfbe93", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_D': 256}, num_warps=4),\n        triton.Config({'BLOCK_D': 256}, num_warps=8),\n        triton.Config({'BLOCK_D': 512}, num_warps=4),\n        triton.Config({'BLOCK_D': 512}, num_warps=8),\n    ],\n    key=['D']\n)\n@triton.jit\ndef fused_assignment_kernel(\n    x_ptr, \n    clusters_ptr,\n    running_mean_ptr,\n    scale_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_xb, \n    stride_xd,\n    stride_clusters_d,\n    stride_clusters_k,\n    stride_rmk,\n    stride_sk,\n    stride_bk,\n    stride_ob, \n    stride_on, \n    stride_ok,\n    D, \n    clusters, \n    cluster_size, \n    max_sample, \n    BLOCK_D: tl.constexpr, \n    BLOCK_CLUSTERS: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    b_idx = pid // max_sample\n    n_idx = pid % max_sample\n    \n    # Precompute base pointers for efficient access\n    x_base = x_ptr + b_idx * stride_xb + n_idx * stride_xd\n    output_base = output_ptr + b_idx * stride_ob + n_idx * stride_on\n    \n    # Initialize accumulator\n    accum = tl.zeros((BLOCK_CLUSTERS,), dtype=tl.float32)\n    \n    # Optimized matrix multiplication with vectorized loads\n    for d_offset in range(0, D, BLOCK_D):\n        d_idx = tl.arange(0, BLOCK_D)\n        d_mask = d_idx < D - d_offset\n        \n        # Coalesced load of input vector block\n        x_vals = tl.load(x_base + d_offset + d_idx, mask=d_mask, other=0.0)\n        \n        # Load cluster weights block with efficient offset calculation\n        clusters_offset = (d_offset + d_idx)[:, None] * stride_clusters_d\n        k_idx = tl.arange(0, BLOCK_CLUSTERS)[None, :]\n        clusters_mask = d_mask[:, None] & (k_idx < clusters)\n        clusters_vals = tl.load(\n            clusters_ptr + clusters_offset + k_idx * stride_clusters_k,\n            mask=clusters_mask, \n            other=0.0\n        )\n        \n        # Accumulate partial sums using tensor core optimization\n        partial = tl.dot(x_vals, clusters_vals, allow_tf32=True)\n        accum += partial\n\n    # Efficiently load batch norm parameters\n    k_idx = tl.arange(0, BLOCK_CLUSTERS)\n    k_mask = k_idx < clusters\n    running_mean = tl.load(running_mean_ptr + k_idx * stride_rmk, mask=k_mask, other=0.0)\n    scale_vals = tl.load(scale_ptr + k_idx * stride_sk, mask=k_mask, other=0.0)\n    bias_vals = tl.load(bias_ptr + k_idx * stride_bk, mask=k_mask, other=0.0)\n    \n    # Apply batch norm\n    accum = (accum - running_mean) * scale_vals + bias_vals\n    \n    # Stable softmax with vectorization\n    max_val = tl.max(accum, axis=0)\n    accum = accum - max_val\n    exp_vals = tl.exp(accum)\n    exp_sum = tl.sum(exp_vals, axis=0)\n    softmax_out = exp_vals / exp_sum\n    \n    # Store only non-ghost clusters\n    output_idx = tl.arange(0, cluster_size)\n    output_mask = output_idx < cluster_size\n    tl.store(\n        output_base + output_idx * stride_ok, \n        softmax_out[:cluster_size], \n        mask=output_mask\n    )\n\nclass ModelNew(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(ModelNew, self).__init__()\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n        init_sc = (1 / math.sqrt(feature_size))\n        clusters = cluster_size + ghost_clusters\n        \n        self.clusters = nn.Parameter(init_sc * torch.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        self.clusters2 = nn.Parameter(init_sc * torch.randn(1, feature_size, cluster_size))\n        self.out_dim = cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        B, N, D = x.shape\n        x_flat = x.reshape(-1, D)\n        \n        if not self.training:\n            # Precompute batch norm parameters\n            scale = self.batch_norm.weight / torch.sqrt(self.batch_norm.running_var + self.batch_norm.eps)\n            bias = self.batch_norm.bias\n            \n            # Prepare output\n            assignment_out = torch.empty((B, N, self.cluster_size), \n                                        device=x.device, dtype=x.dtype)\n            \n            # Launch kernel with autotuning\n            grid = (B * N,)\n            fused_assignment_kernel[grid](\n                x_flat, self.clusters,\n                self.batch_norm.running_mean,\n                scale, bias,\n                assignment_out,\n                x_flat.stride(0),  # stride_xb\n                x_flat.stride(1),  # stride_xd\n                self.clusters.stride(0),  # stride_clusters_d\n                self.clusters.stride(1),  # stride_clusters_k\n                self.batch_norm.running_mean.stride(0),  # stride_rmk\n                scale.stride(0),  # stride_sk\n                bias.stride(0),  # stride_bk\n                assignment_out.stride(0),  # stride_ob\n                assignment_out.stride(1),  # stride_on\n                assignment_out.stride(2),  # stride_ok\n                D, \n                self.cluster_size + self.ghost_clusters, \n                self.cluster_size, \n                N,\n                BLOCK_D=256,  # Default value (autotuner will override)\n                BLOCK_CLUSTERS=triton.next_power_of_2(self.cluster_size + self.ghost_clusters),\n            )\n        else:\n            # Training path\n            assignment = torch.matmul(x_flat, self.clusters)\n            assignment = self.batch_norm(assignment)\n            assignment = F.softmax(assignment, dim=1)\n            assignment_out = assignment[:, :self.cluster_size].view(B, N, self.cluster_size)\n        \n        # Compute VLAD features\n        a_sum = torch.sum(assignment_out, dim=1, keepdim=True)\n        a = a_sum * self.clusters2\n        vlad = torch.matmul(x.transpose(1, 2), assignment_out) - a\n        \n        # Normalizations\n        vlad = F.normalize(vlad, p=2, dim=1)\n        vlad = vlad.reshape(B, -1)\n        vlad = F.normalize(vlad, p=2, dim=1)\n        return vlad\n\nbatch_size = 32\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 16\n\ndef get_inputs():\n    return [torch.randn(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n    return [num_clusters, feature_size, ghost_clusters]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "50e34efa-dc29-446c-ad5f-35bc4bbf7554", "generation": 3, "timestamp": 1754593878.9855018, "iteration_found": 24, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.988, "runtime_ms": 0.253, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.25, "speed_up": 0.988, "custom_timing": 0.253}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 25.899999999999995, "speed_up": 1.059, "runtime_ms": 0.236, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.25, "speed_up": 1.059, "custom_timing": 0.236}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
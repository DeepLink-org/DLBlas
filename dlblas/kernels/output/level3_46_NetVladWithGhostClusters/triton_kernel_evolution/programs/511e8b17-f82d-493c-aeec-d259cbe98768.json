{"id": "511e8b17-f82d-493c-aeec-d259cbe98768", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_D': 128, 'BLOCK_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_D': 64, 'BLOCK_K': 128}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_D': 256, 'BLOCK_K': 32}, num_stages=2, num_warps=8),\n    ],\n    key=['D', 'K'],\n)\n@triton.jit\ndef netvlad_kernel(\n    x_ptr, clusters_ptr, output_ptr,\n    stride_xb, stride_xn, stride_xd,\n    stride_cd, stride_ck,\n    stride_ob, stride_on, stride_ok,\n    B, N, D, K, G,\n    BLOCK_D: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid_b = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    # Create block pointers\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(B, N, D),\n        strides=(stride_xb, stride_xn, stride_xd),\n        offsets=(pid_b, pid_n, 0),\n        block_shape=(1, 1, BLOCK_D),\n        order=(2, 1, 0)\n    )\n    clusters_block_ptr = tl.make_block_ptr(\n        base=clusters_ptr,\n        shape=(D, K+G),\n        strides=(stride_cd, stride_ck),\n        offsets=(0, 0),\n        block_shape=(BLOCK_D, BLOCK_K),\n        order=(1, 0)\n    )\n    \n    # Initialize accumulator\n    accum = tl.zeros((BLOCK_K,), dtype=tl.float32)\n    \n    # Compute dot product\n    for d in range(0, tl.cdiv(D, BLOCK_D)):\n        x = tl.load(x_block_ptr, boundary_check=(2,), padding_option=\"zero\")\n        clusters = tl.load(clusters_block_ptr, boundary_check=(0,1), padding_option=\"zero\")\n        \n        partial = tl.dot(clusters, x, allow_tf32=True)\n        accum += partial\n        \n        # Update pointers\n        x_block_ptr = tl.advance(x_block_ptr, (0, 0, BLOCK_D))\n        clusters_block_ptr = tl.advance(clusters_block_ptr, (BLOCK_D, 0))\n    \n    # Apply softmax (only non-ghost clusters)\n    max_val = tl.max(accum[:K], 0)\n    exp_vals = tl.exp(accum[:K] - max_val)\n    softmax_out = exp_vals / tl.sum(exp_vals, 0)\n    \n    # Store output\n    output_offsets = pid_b * stride_ob + pid_n * stride_on + tl.arange(0, K)\n    tl.store(output_ptr + output_offsets, softmax_out)\n\nclass ModelNew(nn.Module):\n    def __init__(self, cluster_size, feature_size, ghost_clusters):\n        super(ModelNew, self).__init__()\n        self.feature_size = feature_size\n        self.cluster_size = cluster_size\n        self.ghost_clusters = ghost_clusters\n        clusters = cluster_size + ghost_clusters\n        \n        # Initialize parameters\n        init_sc = (1 / math.sqrt(feature_size))\n        self.clusters = nn.Parameter(init_sc * torch.randn(feature_size, clusters))\n        self.batch_norm = nn.BatchNorm1d(clusters)\n        self.clusters2 = nn.Parameter(init_sc * torch.randn(1, feature_size, cluster_size))\n        self.out_dim = cluster_size * feature_size\n\n    def forward(self, x, mask=None):\n        B, N, D = x.shape\n        K, G = self.cluster_size, self.ghost_clusters\n        \n        # Precompute batch norm parameters\n        scale = self.batch_norm.weight / torch.sqrt(self.batch_norm.running_var + self.batch_norm.eps)\n        bias = self.batch_norm.bias - self.batch_norm.running_mean * scale\n        \n        # Apply batch norm to clusters\n        clusters = self.clusters * scale[None, :] + bias[None, :]\n        \n        # Compute assignments with Triton\n        assignment = torch.empty((B, N, K), device=x.device, dtype=torch.float32)\n        grid = (B, N)\n        netvlad_kernel[grid](\n            x, clusters, assignment,\n            x.stride(0), x.stride(1), x.stride(2),\n            clusters.stride(0), clusters.stride(1),\n            assignment.stride(0), assignment.stride(1), assignment.stride(2),\n            B, N, D, K, G,\n            BLOCK_D=triton.next_power_of_2(D),\n            BLOCK_K=triton.next_power_of_2(K+G)\n        )\n        \n        # Compute VLAD components\n        a_sum = torch.sum(assignment, dim=1, keepdim=True)\n        a = a_sum * self.clusters2\n        \n        # Optimized VLAD computation\n        vlad = torch.matmul(assignment.transpose(1,2), x)\n        vlad = vlad.transpose(1,2) - a\n        \n        # Normalizations\n        vlad = F.normalize(vlad, p=2, dim=1)\n        vlad = vlad.reshape(B, -1)\n        vlad = F.normalize(vlad, p=2, dim=1)\n        return vlad\n\nbatch_size = 32\nnum_features = 100\nnum_clusters = 32\nfeature_size = 512\nghost_clusters = 16\n\ndef get_inputs():\n    return [torch.randn(batch_size, num_features, feature_size)]\n\ndef get_init_inputs():\n    return [num_clusters, feature_size, ghost_clusters]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "820c6335-456b-4a78-8a3b-8e373302a9e9", "generation": 1, "timestamp": 1754590496.0211878, "iteration_found": 6, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_46_NetVladWithGhostClusters", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpoeloe9if.py\", line 100, in forward\n    netvlad_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 135, in _bench\n    raise ValueError(f\"Conflicting meta-parameters: {', '.join(conflicts)}.\"\nValueError: Conflicting meta-parameters: BLOCK_D, BLOCK_K. Make sure that you don't re-define auto-tuned symbols.\n", "runtime_error_name": "builtins.ValueError", "reference_timing_ms": 0.25, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.247, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.25, "speed_up": 1.012, "custom_timing": 0.247}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
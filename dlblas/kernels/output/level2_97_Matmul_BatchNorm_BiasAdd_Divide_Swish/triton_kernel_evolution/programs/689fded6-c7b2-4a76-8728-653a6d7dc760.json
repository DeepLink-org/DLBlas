{"id": "689fded6-c7b2-4a76-8728-653a6d7dc760", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_BATCH': 16, 'BLOCK_SIZE_FEATURE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_BATCH': 32, 'BLOCK_SIZE_FEATURE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_BATCH': 64, 'BLOCK_SIZE_FEATURE': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_BATCH': 16, 'BLOCK_SIZE_FEATURE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_BATCH': 32, 'BLOCK_SIZE_FEATURE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_BATCH': 64, 'BLOCK_SIZE_FEATURE': 128}, num_warps=8),\n    ],\n    key=['batch_size', 'out_features'],\n)\n@triton.jit\ndef fused_forward_kernel(\n    linear_out_ptr,\n    fused_scale_ptr,\n    fused_shift_ptr,\n    output_ptr,\n    batch_size,\n    out_features,\n    stride_lo0, stride_lo1,\n    stride_out0, stride_out1,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEATURE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_feature = tl.program_id(1)\n    \n    batch_idx = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feature_idx = pid_feature * BLOCK_SIZE_FEATURE + tl.arange(0, BLOCK_SIZE_FEATURE)\n    \n    batch_mask = batch_idx < batch_size\n    feature_mask = feature_idx < out_features\n    block_mask = batch_mask[:, None] & feature_mask[None, :]\n    \n    lo_offsets = batch_idx[:, None] * stride_lo0 + feature_idx[None, :] * stride_lo1\n    linear_out = tl.load(linear_out_ptr + lo_offsets, mask=block_mask, other=0.0)\n    \n    fused_scale = tl.load(fused_scale_ptr + feature_idx, mask=feature_mask, other=0.0)\n    fused_shift = tl.load(fused_shift_ptr + feature_idx, mask=feature_mask, other=0.0)\n    \n    normalized = linear_out * fused_scale[None, :] + fused_shift[None, :]\n    swish_out = normalized * tl.sigmoid(normalized)\n    \n    out_offsets = batch_idx[:, None] * stride_out0 + feature_idx[None, :] * stride_out1\n    tl.store(output_ptr + out_offsets, swish_out, mask=block_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        self.register_buffer('fused_scale', None)\n        self.register_buffer('fused_shift', None)\n        \n    def _precompute_fused_params(self):\n        with torch.no_grad():\n            inv_std = 1.0 / torch.sqrt(self.bn.running_var + self.bn.eps)\n            scale = self.bn.weight * inv_std\n            shift = self.bn.bias - self.bn.running_mean * scale\n            self.fused_scale = (scale / self.divide_value).contiguous()\n            self.fused_shift = ((shift + self.bias) / self.divide_value).contiguous()\n        \n    def forward(self, x):\n        linear_out = self.matmul(x)\n        \n        if self.training:\n            x = self.bn(linear_out)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            if self.fused_scale is None or self.fused_shift is None:\n                self._precompute_fused_params()\n                \n            batch_size, _ = linear_out.shape\n            output = torch.empty_like(linear_out)\n            \n            grid = lambda meta: (\n                triton.cdiv(batch_size, meta['BLOCK_SIZE_BATCH']),\n                triton.cdiv(linear_out.size(1), meta['BLOCK_SIZE_FEATURE'])\n            )\n            \n            fused_forward_kernel[grid](\n                linear_out,\n                self.fused_scale,\n                self.fused_shift,\n                output,\n                batch_size,\n                linear_out.size(1),\n                linear_out.stride(0), linear_out.stride(1),\n                output.stride(0), output.stride(1),\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "1cc04411-4fc1-4658-977b-3157d6a5afbb", "generation": 3, "timestamp": 1754590037.9618073, "iteration_found": 16, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.917, "runtime_ms": 0.12, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.11, "speed_up": 0.917, "custom_timing": 0.12}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 25.800000000000004, "speed_up": 1.058, "runtime_ms": 0.104, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.11, "speed_up": 1.058, "custom_timing": 0.104}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
{"id": "1cc04411-4fc1-4658-977b-3157d6a5afbb", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_BATCH': 16, 'BLOCK_SIZE_FEATURE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_BATCH': 32, 'BLOCK_SIZE_FEATURE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_BATCH': 64, 'BLOCK_SIZE_FEATURE': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_BATCH': 16, 'BLOCK_SIZE_FEATURE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_BATCH': 32, 'BLOCK_SIZE_FEATURE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_BATCH': 64, 'BLOCK_SIZE_FEATURE': 128}, num_warps=8),\n    ],\n    key=['batch_size', 'out_features'],\n)\n@triton.jit\ndef fused_forward_kernel(\n    linear_out_ptr,\n    bn_weight_ptr,\n    bn_bias_ptr,\n    running_mean_ptr,\n    running_var_ptr,\n    model_bias_ptr,\n    divide_value,\n    output_ptr,\n    batch_size,\n    out_features,\n    bn_eps,\n    stride_lo0, stride_lo1,\n    stride_out0, stride_out1,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEATURE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_feature = tl.program_id(1)\n    \n    batch_idx = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feature_idx = pid_feature * BLOCK_SIZE_FEATURE + tl.arange(0, BLOCK_SIZE_FEATURE)\n    \n    batch_mask = batch_idx < batch_size\n    feature_mask = feature_idx < out_features\n    block_mask = batch_mask[:, None] & feature_mask[None, :]\n    \n    # Calculate offsets for linear_out access\n    lo_offsets = batch_idx[:, None] * stride_lo0 + feature_idx[None, :] * stride_lo1\n    linear_out = tl.load(linear_out_ptr + lo_offsets, mask=block_mask, other=0.0)\n    \n    # Load batchnorm parameters for the feature block\n    bn_weight = tl.load(bn_weight_ptr + feature_idx, mask=feature_mask, other=0.0)\n    bn_bias = tl.load(bn_bias_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_mean = tl.load(running_mean_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_var = tl.load(running_var_ptr + feature_idx, mask=feature_mask, other=0.0)\n    \n    # Optimized batchnorm computation\n    inv_std = tl.math.rsqrt(running_var + bn_eps)\n    scale = bn_weight * inv_std\n    shift = bn_bias - running_mean * scale\n    bn_out = linear_out * scale[None, :] + shift[None, :]\n    \n    # Add model bias and divide\n    model_bias = tl.load(model_bias_ptr)\n    normalized = (bn_out + model_bias) / divide_value\n    \n    # Swish activation\n    swish_out = normalized * tl.sigmoid(normalized)\n    \n    # Store results\n    out_offsets = batch_idx[:, None] * stride_out0 + feature_idx[None, :] * stride_out1\n    tl.store(output_ptr + out_offsets, swish_out, mask=block_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        \n    def forward(self, x):\n        linear_out = self.matmul(x)\n        \n        if self.training:\n            x = self.bn(linear_out)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            # Inference path with fused Triton kernel\n            batch_size, _ = linear_out.shape\n            output = torch.empty_like(linear_out)\n            \n            grid = lambda meta: (\n                triton.cdiv(batch_size, meta['BLOCK_SIZE_BATCH']),\n                triton.cdiv(linear_out.size(1), meta['BLOCK_SIZE_FEATURE'])\n            )\n            \n            fused_forward_kernel[grid](\n                linear_out,\n                self.bn.weight,\n                self.bn.bias,\n                self.bn.running_mean,\n                self.bn.running_var,\n                self.bias,\n                self.divide_value,\n                output,\n                batch_size,\n                linear_out.size(1),\n                self.bn.eps,\n                linear_out.stride(0), linear_out.stride(1),\n                output.stride(0), output.stride(1),\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "fdd90896-683c-4ad5-97d4-acda3233ab92", "generation": 2, "timestamp": 1754588101.8325825, "iteration_found": 6, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 25.800000000000004, "speed_up": 1.058, "runtime_ms": 0.104, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.11, "speed_up": 1.058, "custom_timing": 0.104}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 21.89999999999999, "speed_up": 1.019, "runtime_ms": 0.108, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.11, "speed_up": 1.019, "custom_timing": 0.108}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 1.0000\n- score: 21.9000\n- speed_up: 1.0190\n- runtime_ms: 0.1080\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.019, 'custom_timing': 0.108}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 21.9000, speed_up: 1.0190, runtime_ms: 0.1080, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.019, 'custom_timing': 0.108}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 21.9000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_forward_kernel(\n    linear_out_ptr,\n    bn_weight_ptr,\n    bn_bias_ptr,\n    running_mean_ptr,\n    running_var_ptr,\n    model_bias_ptr,\n    divide_value,\n    output_ptr,\n    batch_size,\n    out_features,\n    bn_eps,\n    stride_lo0, stride_lo1,\n    stride_out0, stride_out1,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEATURE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_feature = tl.program_id(1)\n    \n    batch_idx = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feature_idx = pid_feature * BLOCK_SIZE_FEATURE + tl.arange(0, BLOCK_SIZE_FEATURE)\n    \n    batch_mask = batch_idx < batch_size\n    feature_mask = feature_idx < out_features\n    block_mask = batch_mask[:, None] & feature_mask[None, :]\n    \n    # Calculate offsets for linear_out access\n    lo_offsets = batch_idx[:, None] * stride_lo0 + feature_idx[None, :] * stride_lo1\n    linear_out = tl.load(linear_out_ptr + lo_offsets, mask=block_mask, other=0.0)\n    \n    # Load batchnorm parameters for the feature block\n    bn_weight = tl.load(bn_weight_ptr + feature_idx, mask=feature_mask, other=0.0)\n    bn_bias = tl.load(bn_bias_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_mean = tl.load(running_mean_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_var = tl.load(running_var_ptr + feature_idx, mask=feature_mask, other=0.0)\n    \n    # Compute batchnorm\n    scale = bn_weight / tl.sqrt(running_var + bn_eps)\n    bn_out = (linear_out - running_mean[None, :]) * scale[None, :] + bn_bias[None, :]\n    \n    # Add model bias and divide\n    model_bias = tl.load(model_bias_ptr)\n    normalized = (bn_out + model_bias) / divide_value\n    \n    # Swish activation\n    swish_out = normalized * tl.sigmoid(normalized)\n    \n    # Store results\n    out_offsets = batch_idx[:, None] * stride_out0 + feature_idx[None, :] * stride_out1\n    tl.store(output_ptr + out_offsets, swish_out, mask=block_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        \n    def forward(self, x):\n        linear_out = self.matmul(x)\n        \n        if self.training:\n            x = self.bn(linear_out)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            # Inference path with fused Triton kernel\n            batch_size, _ = linear_out.shape\n            output = torch.empty_like(linear_out)\n            \n            grid = lambda meta: (\n                triton.cdiv(batch_size, meta['BLOCK_SIZE_BATCH']),\n                triton.cdiv(linear_out.size(1), meta['BLOCK_SIZE_FEATURE'])\n            )\n            \n            fused_forward_kernel[grid](\n                linear_out,\n                self.bn.weight,\n                self.bn.bias,\n                self.bn.running_mean,\n                self.bn.running_var,\n                self.bias,\n                self.divide_value,\n                output,\n                batch_size,\n                linear_out.size(1),\n                self.bn.eps,\n                linear_out.stride(0), linear_out.stride(1),\n                output.stride(0), output.stride(1),\n                BLOCK_SIZE_BATCH=32,\n                BLOCK_SIZE_FEATURE=128\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (21.9000), Performs well on speed_up (1.0190), Performs well on runtime_ms (0.1080), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.019, 'custom_timing': 0.108})\n\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_forward_kernel(\n    linear_out_ptr,\n    bn_weight_ptr,\n    bn_bias_ptr,\n    running_mean_ptr,\n    running_var_ptr,\n    model_bias_ptr,\n    divide_value,\n    output_ptr,\n    batch_size,\n    out_features,\n    bn_eps,\n    stride_lo0, stride_lo1,\n    stride_out0, stride_out1,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEATURE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_feature = tl.program_id(1)\n    \n    batch_idx = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feature_idx = pid_feature * BLOCK_SIZE_FEATURE + tl.arange(0, BLOCK_SIZE_FEATURE)\n    \n    batch_mask = batch_idx < batch_size\n    feature_mask = feature_idx < out_features\n    block_mask = batch_mask[:, None] & feature_mask[None, :]\n    \n    # Calculate offsets for linear_out access\n    lo_offsets = batch_idx[:, None] * stride_lo0 + feature_idx[None, :] * stride_lo1\n    linear_out = tl.load(linear_out_ptr + lo_offsets, mask=block_mask, other=0.0)\n    \n    # Load batchnorm parameters for the feature block\n    bn_weight = tl.load(bn_weight_ptr + feature_idx, mask=feature_mask, other=0.0)\n    bn_bias = tl.load(bn_bias_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_mean = tl.load(running_mean_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_var = tl.load(running_var_ptr + feature_idx, mask=feature_mask, other=0.0)\n    \n    # Compute batchnorm\n    scale = bn_weight / tl.sqrt(running_var + bn_eps)\n    bn_out = (linear_out - running_mean[None, :]) * scale[None, :] + bn_bias[None, :]\n    \n    # Add model bias and divide\n    model_bias = tl.load(model_bias_ptr)\n    normalized = (bn_out + model_bias) / divide_value\n    \n    # Swish activation\n    swish_out = normalized * tl.sigmoid(normalized)\n    \n    # Store results\n    out_offsets = batch_idx[:, None] * stride_out0 + feature_idx[None, :] * stride_out1\n    tl.store(output_ptr + out_offsets, swish_out, mask=block_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        \n    def forward(self, x):\n        linear_out = self.matmul(x)\n        \n        if self.training:\n            x = self.bn(linear_out)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            # Inference path with fused Triton kernel\n            batch_size, _ = linear_out.shape\n            output = torch.empty_like(linear_out)\n            \n            grid = lambda meta: (\n                triton.cdiv(batch_size, meta['BLOCK_SIZE_BATCH']),\n                triton.cdiv(linear_out.size(1), meta['BLOCK_SIZE_FEATURE'])\n            )\n            \n            fused_forward_kernel[grid](\n                linear_out,\n                self.bn.weight,\n                self.bn.bias,\n                self.bn.running_mean,\n                self.bn.running_var,\n                self.bias,\n                self.divide_value,\n                output,\n                batch_size,\n                linear_out.size(1),\n                self.bn.eps,\n                linear_out.stride(0), linear_out.stride(1),\n                output.stride(0), output.stride(1),\n                BLOCK_SIZE_BATCH=32,\n                BLOCK_SIZE_FEATURE=128\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a Triton kernel that fuses batch normalization, adding a bias, scaling, and Swish activation.\n The current kernel has a speedup of 1.019 (about 1.9% faster than the reference). We are to improve this.\n\n Observations and potential improvements:\n\n 1. The kernel currently uses two program IDs (batch and feature) and loads a block of [BLOCK_SIZE_BATCH, BLOCK_SIZE_FEATURE].\n    This is a good approach for 2D parallelism.\n\n 2. The current block sizes are fixed at 32 for batch and 128 for feature. We might try to adjust these to better utilize the hardware.\n\n 3. The kernel does:\n    a. Load a block of linear_out.\n    b. Load batchnorm parameters (weight, bias, running_mean, running_var) for the feature block.\n    c. Compute batchnorm for the entire block.\n    d. Add a scalar model bias and divide by a scalar divide_value.\n    e. Apply Swish: normalized * sigmoid(normalized).\n\n 4. We note that the model_bias is a scalar (or a tensor of shape (1,)) and is the same for every element. \n    Currently, we load it once per block but we are not broadcasting it efficiently.\n\n 5. The Swish activation can be optimized by using an approximation or by using the built-in tl.sigmoid which might be optimized.\n\n 6. We might consider using vectorized loads/stores. Currently, the loads and stores are done with a mask and are not vectorized.\n\n 7. The current kernel does not use shared memory. Since the same batchnorm parameters are used for an entire feature column, \n    we might consider loading them into shared memory to reduce global memory accesses. However, note that the batchnorm parameters \n    are loaded per feature and then broadcast along the batch dimension. Since the feature dimension is split by blocks, each block \n    only loads the features it needs.\n\n 8. We note that the current kernel uses:\n        scale = bn_weight / tl.sqrt(running_var + bn_eps)\n        bn_out = (linear_out - running_mean[None, :]) * scale[None, :] + bn_bias[None, :]\n\n    This is the standard batch norm formula.\n\n 9. We can precompute the scale and the term (bn_bias - running_mean * scale) to reduce the number of operations.\n\n    Let: \n        inv_std = 1.0 / tl.sqrt(running_var + bn_eps)\n        scale = bn_weight * inv_std\n        shift = bn_bias - running_mean * scale   [then bn_out = linear_out * scale + shift]\n\n    This would save one broadcast and one subtraction per element.\n\n    But note: we are already doing (linear_out - running_mean) * scale + bn_bias.\n    We can rewrite as: linear_out * scale + (bn_bias - running_mean * scale)\n\n    So we precompute: shift = bn_bias - running_mean * scale\n\n    Then: bn_out = linear_out * scale + shift[None, :]\n\n    This replaces a subtraction and a multiply-add with a multiply-add and an add. \n    However, we are saving one broadcast of running_mean and one subtraction.\n\n    But note: the current kernel does:\n        (linear_out - running_mean[None, :]) * scale[None, :] -> two broadcasts and a subtraction and a multiplication\n        then add bn_bias[None, :] -> another broadcast and an add.\n\n    The proposed method:\n        linear_out * scale[None, :] -> one broadcast and a multiplication\n        then add shift[None, :] -> one broadcast and an add.\n\n    So we reduce one broadcast and one operation (the subtraction). This might be beneficial.\n\n10. We are currently loading the model_bias as a scalar and then adding it to the entire block. \n    This is efficient because we only load it once per block. However, note that the model_bias is a scalar (or a 1-element tensor) \n    and we are loading it for every block. We might consider passing it as a scalar to the kernel and then having it in a register.\n\n    But note: the kernel argument model_bias_ptr is a pointer. We are doing:\n        model_bias = tl.load(model_bias_ptr)\n\n    This loads the model_bias once per block. Since the block may be large, this is a single load. It's efficient.\n\n11. The divide by divide_value is a scalar, so that's cheap.\n\n12. The Swish activation: we do normalized * tl.sigmoid(normalized). This is the standard way.\n\n13. We might consider using the fast math flags if available, but Triton might already do that.\n\n14. We note that the current block size for batch (32) and feature (128) might not be optimal. We can try to tune these.\n\n    The H100 has 128 cores per SM, and we want to have enough blocks to cover the entire grid and to hide latency.\n\n    We have grid = (triton.cdiv(batch_size, BLOCK_SIZE_BATCH), triton.cdiv(out_features, BLOCK_SIZE_FEATURE))\n\n    For batch_size=128 and out_features=512, the grid is (4, 4) with the current block sizes.\n\n    We might try to increase the feature block size to 256 or even 512 to reduce the grid size and increase per-block work.\n\n    Alternatively, we might reduce the block sizes to have more blocks for better parallelism.\n\n    We should use Triton's autotuning to find the best block sizes.\n\n15. We are currently not using any vectorization. We can try to vectorize the loads and stores.\n\n    Since the linear_out is stored in row-major order, and we are accessing a contiguous block in the feature dimension, \n    we can use vectorized loads for the linear_out and the output.\n\n    Steps for vectorization:\n\n        - We require that BLOCK_SIZE_FEATURE is divisible by the vector size (e.g., 4, 8, 16).\n        - We can then load multiple elements at once.\n\n    We can also vectorize the stores.\n\n16. We can try to fuse the operations more efficiently to reduce the number of passes over the data.\n\n    The current computation flow:\n\n        linear_out = load(...)\n        bn_out = (linear_out - running_mean) * scale + bn_bias   [broadcasted]\n        normalized = (bn_out + model_bias) / divide_value\n        swish_out = normalized * tl.sigmoid(normalized)\n\n    We can try to combine the operations:\n\n        Let: x = linear_out\n        Let: s = scale[None, :]\n        Let: shift = (bn_bias - running_mean * scale)[None, :]   [precomputed per feature]\n\n        Then: bn_out = x * s + shift\n\n        normalized = (x * s + shift + model_bias) / divide_value\n\n        Let: t = (shift + model_bias)   [we can precompute this as a vector per feature?]\n\n        However, note that model_bias is a scalar. So:\n\n            normalized = (x * s + (shift + model_bias)) / divide_value\n\n        Let: total_shift = shift + model_bias   [a vector per feature]\n\n        Then: normalized = (x * s + total_shift) / divide_value\n\n        Then we do swish.\n\n    This reduces the number of operations.\n\n17. We note that the kernel currently does not use any shared memory. Since the batchnorm parameters (scale and total_shift) \n    are constant for an entire feature column, we could load them once per feature and then broadcast to all the batch elements in the block.\n\n    However, the current way of loading them and then broadcasting via TL operations is already efficient because the same value is used for an entire column.\n\n    But if we use shared memory, we could load the batchnorm parameters for the feature block once and then every thread in the block can read from shared memory.\n\n    The benefit: we are currently loading the same batchnorm parameters multiple times (once per thread in the batch dimension? Actually, no: the feature parameters are loaded per feature and then broadcast along the batch dimension in the block. The same feature parameter is used for all batch elements in the same feature. So we are already loading each feature parameter once per block and then broadcasting without additional cost).\n\n    So shared memory might not be necessary.\n\n18. We can try to use the following optimizations:\n\n    a. Precompute the batchnorm parameters (scale and shift) for the entire feature block in the beginning of the kernel.\n    b. Vectorize the loads and stores for linear_out and output.\n\n    Let's restructure the kernel:\n\n        - We'll break the feature dimension into chunks of BLOCK_SIZE_FEATURE, but we'll also break the batch dimension into chunks of BLOCK_SIZE_BATCH.\n        - We'll use vectorized loads of size VEC for the feature dimension. So we require BLOCK_SIZE_FEATURE % VEC == 0.\n\n    Steps:\n\n        pid_batch = tl.program_id(0)\n        pid_feature = tl.program_id(1)\n\n        batch_offsets = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n        feature_offsets = pid_feature * BLOCK_SIZE_FEATURE + tl.arange(0, BLOCK_SIZE_FEATURE)\n\n        # We can split the feature_offsets into vectors of size VEC\n        # Actually, we can use tl.reshape and then use vectorized loads.\n\n        But note: the linear_out is stored in row-major, so the feature dimension is contiguous. We can load a vector of VEC elements per row.\n\n        We can iterate over the batch block and within each row, load VEC features at a time.\n\n        However, the kernel currently loads the entire block at once. We can change to:\n\n            for i in range(0, BLOCK_SIZE_BATCH, 1):\n                for j in range(0, BLOCK_SIZE_FEATURE, VEC):\n                    ... load a vector of VEC features for row i and columns [j:j+VEC]\n\n        But this would be a nested loop and might not be efficient. Alternatively, we can use the fact that the entire block is contiguous in memory? \n        Actually, the linear_out tensor is row-major, so the block is contiguous only if BLOCK_SIZE_FEATURE is the entire row? No, we are loading a block.\n\n        The block we are loading is [BLOCK_SIZE_BATCH, BLOCK_SIZE_FEATURE] and it is not contiguous in memory because the row stride is stride_lo0 and the column stride is 1. \n        So we can load a vector per row.\n\n        We can do:\n\n            for i in range(BLOCK_SIZE_BATCH):\n                row_idx = batch_offsets[i]\n                if row_idx < batch_size:\n                    for j in range(0, BLOCK_SIZE_FEATURE, VEC):\n                        col_idx = feature_offsets[j:j+VEC]\n                        mask = (row_idx < batch_size) and (col_idx < out_features)   # but col_idx is a vector\n                        ptr = linear_out_ptr + row_idx * stride_lo0 + col_idx * stride_lo1   # stride_lo1 is 1? \n                        vec = tl.load(ptr, mask=col_idx < out_features, ...)\n\n        However, this would be a double loop and might not be efficient in Triton.\n\n    Alternatively, we can use the current method and try to vectorize the entire block by treating it as a 1D array of length BLOCK_SIZE_BATCH * BLOCK_SIZE_FEATURE, but that would require the block to be contiguous? Not necessarily.\n\n    We can try to vectorize along the feature dimension for each row. We can do:\n\n        # Let VEC = 4 (for example)\n        # We require that BLOCK_SIZE_FEATURE is divisible by VEC.\n\n        # We reshape the offsets for the block to be [BLOCK_SIZE_BATCH, BLOCK_SIZE_FEATURE // VEC, VEC]\n\n        But this might complicate the mask.\n\n    Given the complexity and the fact that the current kernel is already working, we will first try to optimize by:\n\n        - Precomputing the shift to reduce one broadcast and one operation.\n        - Using vectorized loads and stores along the feature dimension.\n\n    We will change:\n\n        feature_idx = pid_feature * BLOCK_SIZE_FEATURE + tl.arange(0, BLOCK_SIZE_FEATURE)\n        to:\n        feature_idx = pid_feature * BLOCK_SIZE_FEATURE + tl.arange(0, BLOCK_SIZE_FEATURE)   # same\n\n        But then we can split the feature_idx into vectors? Actually, we can use the built-in vectorization of Triton.\n\n    Triton supports vectorized loads by specifying the type. We can do:\n\n        linear_out_vec = tl.load(linear_out_ptr + lo_offsets, mask=block_mask, other=0.0, cache_modifier=\".cg\", ...)\n\n    but that doesn't vectorize. We need to reshape the offsets to be vectors.\n\n    We can do:\n\n        # Define a vector size, say VEC=4\n        # Then we require that BLOCK_SIZE_FEATURE is divisible by VEC.\n\n        # We change the feature_idx to be of length BLOCK_SIZE_FEATURE // VEC, and then we use a vector type.\n\n        feature_idx_base = pid_feature * BLOCK_SIZE_FEATURE + tl.arange(0, BLOCK_SIZE_FEATURE // VEC) * VEC\n        feature_idx = feature_idx_base[:, None] + tl.arange(0, VEC)[None, :]   # [BLOCK_SIZE_FEATURE//VEC, VEC]\n\n        Then we have to adjust the block_mask accordingly.\n\n    This complicates the code. We might try a simpler approach: use the current block and then use tl.dot to encourage vectorization? Not necessary.\n\n    We decide to try the following:\n\n        - Precompute the shift to reduce operations.\n        - Use a larger block size for features to improve memory efficiency and reduce grid size.\n\n    We'll also try to use autotuning for the block sizes.\n\n    However, note the constraints: we cannot change the grid configuration. But we can change the block sizes because they are passed as constexpr.\n\n    We are allowed to change the block sizes and the tuning parameters.\n\n    We will change the kernel to:\n\n        Precomputation per feature:\n            scale = bn_weight * tl.math.rsqrt(running_var + bn_eps)   # using rsqrt might be faster\n            shift = bn_bias - running_mean * scale\n\n        Then:\n            bn_out = linear_out * scale[None, :] + shift[None, :]\n\n        Then add model_bias and divide.\n\n    We also change the activation to use tl.sigmoid as before.\n\n    We also change the block sizes to be tunable and try to use a larger feature block.\n\n    We note that the current block sizes (32, 128) lead to 4x4=16 blocks. We might try (64, 256) for the same problem, which would be 2x2=4 blocks. \n    This might not be enough parallelism. Alternatively, we might try (16, 64) which would be 8x8=64 blocks.\n\n    We will use Triton's autotuning to select the best block sizes.\n\n    We can wrap the kernel with @triton.autotune.\n\n    However, the problem states that we are only to change the kernel within the EVOLVE-BLOCK. We are allowed to change the kernel and the class.\n\n    We will:\n\n        - Add autotuning to the kernel.\n        - Change the kernel to precompute the shift.\n        - Change the block sizes to be configurable via the autotune configs.\n\n    We note that the current kernel call in the model passes fixed block sizes (32 and 128). We will change that to use the meta parameters.\n\n    We change the kernel call to:\n\n        fused_forward_kernel[grid](\n            ...,\n            BLOCK_SIZE_BATCH=meta['BLOCK_SIZE_BATCH'],\n            BLOCK_SIZE_FEATURE=meta['BLOCK_SIZE_FEATURE']\n        )\n\n    And we define autotune configs for different block sizes.\n\n    We also change the kernel to precompute the shift.\n\n    Steps:\n\n        In the kernel:\n            # Precompute scale and shift per feature\n            inv_std = tl.math.rsqrt(running_var + bn_eps)   # rsqrt is faster than 1/sqrt\n            scale = bn_weight * inv_std\n            shift = bn_bias - running_mean * scale\n\n            ... then later ...\n\n            bn_out = linear_out * scale[None, :] + shift[None, :]\n\n    We also note that the model_bias is a scalar. We can precompute:\n\n            total_shift = shift + model_bias   # model_bias is scalar, so we add it to every element of shift\n            normalized = (linear_out * scale[None, :] + total_shift[None, :]) / divide_value\n\n    But note: model_bias is the same for every feature? Actually, the model_bias is a scalar (or a tensor of shape (1,)) and we are adding it to every element. \n    So we can do:\n\n            total_shift = shift + model_bias   # then broadcast total_shift[None, :] to the block.\n\n    However, we are already broadcasting shift[None, :]. Adding a scalar to a vector is cheap.\n\n    Alternatively, we can do:\n\n            normalized = (linear_out * scale[None, :] + shift[None, :] + model_bias) / divide_value\n\n    This might be more efficient because we avoid creating a new vector total_shift.\n\n    We'll do:\n\n            normalized = (linear_out * scale[None, :] + shift[None, :] + model_bias) / divide_value\n\n    This uses one extra broadcast for model_bias, but note model_bias is a scalar so it's free.\n\n    Let's code the kernel accordingly.\n\n    We also change the function to use autotune.\n\n    We'll define:\n\n        configs = [\n            triton.Config({'BLOCK_SIZE_BATCH': 16, 'BLOCK_SIZE_FEATURE': 64}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_BATCH': 32, 'BLOCK_SIZE_FEATURE': 64}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_BATCH': 64, 'BLOCK_SIZE_FEATURE': 64}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_BATCH': 16, 'BLOCK_SIZE_FEATURE': 128}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_BATCH': 32, 'BLOCK_SIZE_FEATURE': 128}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_BATCH': 64, 'BLOCK_SIZE_FEATURE': 128}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_BATCH': 16, 'BLOCK_SIZE_FEATURE': 256}, num_warps=8),\n            triton.Config({'BLOCK_SIZE_BATCH': 32, 'BLOCK_SIZE_FEATURE': 256}, num_warps=8),\n            triton.Config({'BLOCK_SIZE_BATCH': 64, 'BLOCK_SIZE_FEATURE': 256}, num_warps=8),\n        ]\n\n    And we'll set num_stages appropriately? We can leave it to Triton.\n\n    We note that the problem size is fixed: batch_size=128, out_features=512. So we can choose block sizes that divide these.\n\n    We are also going to change the kernel to use the new computation.\n\n    We must be cautious about the mask: the feature_mask and batch_mask are still computed the same way.\n\n    We'll change the kernel as follows.\n\n    Important: We are not allowed to change the function signature. We are only changing the body.\n\n    Let's rewrite the kernel accordingly.\n\n    We also note that the current kernel does not use the fast math rsqrt. We change to use tl.math.rsqrt.\n\n    We hope that these changes will improve the performance.\n\n    We'll also remove the computation of 'scale' and 'bn_out' as before and replace with the new method.\n\n    We'll leave the rest the same.\n\n    We also note that the model_bias is loaded once per block. We are doing:\n\n        model_bias = tl.load(model_bias_ptr)   # scalar\n\n    Then we use it in the computation.\n\n    We'll do:\n\n        # After loading the linear_out block and the batchnorm parameters:\n\n        inv_std = tl.math.rsqrt(running_var + bn_eps)\n        scale = bn_weight * inv_std\n        shift = bn_bias - running_mean * scale\n\n        # Then compute:\n        bn_out = linear_out * scale[None, :] + shift[None, :]\n        normalized = (bn_out + model_bias) / divide_value\n        swish_out = normalized * tl.sigmoid(normalized)\n\n    We are doing the same number of operations? Actually, we are replacing:\n\n        old: (linear_out - running_mean) * scale + bn_bias\n        with: linear_out * scale + (bn_bias - running_mean * scale)\n\n    And then the rest is the same.\n\n    We hope that the new method is faster because it avoids the subtraction and the separate broadcast of running_mean.\n\n    We are also using rsqrt which might be faster.\n\n    We also add autotuning for the block sizes.\n\n    We change the ModelNew class to pass the block sizes from the meta parameter.\n\n    We note that the grid function is defined as:\n\n        grid = lambda meta: (\n            triton.cdiv(batch_size, meta['BLOCK_SIZE_BATCH']),\n            triton.cdiv(linear_out.size(1), meta['BLOCK_SIZE_FEATURE'])\n        )\n\n    And then we pass the block sizes as kernel parameters.\n\n    We'll add the autotune decorator above the kernel.\n\n    We must be cautious: the autotune decorator must come after the @triton.jit decorator? Actually, the example in the Triton documentation puts autotune above jit.\n\n    We'll do:\n\n        @triton.autotune(\n            configs=[ ... ],\n            key=['batch_size', 'out_features']\n        )\n        @triton.jit\n        def fused_forward_kernel(...):\n            ...\n\n    We'll define the configs as above.\n\n    Given the problem size, we can also try to use num_warps=8 for larger blocks.\n\n    We'll try a few configs.\n\n    We note that the kernel might be memory bound, so increasing the block size might help by reducing the overhead.\n\n    Let's implement.\n\n    We are going to change the kernel and add autotune.\n\n    We are also going to change the way we compute the batch norm.\n\n    We hope for a speedup.\n\n    Note: We must not change the grid configuration? Actually, the grid is computed using the meta parameter and the block sizes, which is allowed.\n\n    We are also changing the block sizes to be passed from the autotuner, so the grid will change accordingly.\n\n    This is within the rules.\n\n    Let's write the code.\n\n    IMPORTANT: We are only allowed to output the code within the EVOLVE-BLOCK.\n\n    We'll output the entire block.\n\n    We'll also remove the fixed block sizes in the kernel call and use the meta parameters.\n\n    We'll change the kernel call to:\n\n        fused_forward_kernel[grid](\n            ...,\n            BLOCK_SIZE_BATCH=meta['BLOCK_SIZE_BATCH'],\n            BLOCK_SIZE_FEATURE=meta['BLOCK_SIZE_FEATURE']\n        )\n\n    And we'll define the autotune configs.\n\n    We note that the current kernel has a speedup of 1.019. We hope to get at least 1.05.\n\n    We'll try.\n\n    We also note that the previous kernel did not use autotune and used fixed block sizes (32,128). We are now tuning.\n\n    We'll also try to use a vector size for loads and stores? We leave that for a future iteration if needed.\n\n    Given the time, we do the above.\n\n    Code changes:\n\n        - Add autotune decorator with configs.\n        - Change the kernel body to precompute shift and use rsqrt.\n        - Change the kernel call to use the block sizes from meta.\n\n    We also note that the current kernel uses:\n\n        scale = bn_weight / tl.sqrt(running_var + bn_eps)\n\n    We change to:\n\n        inv_std = tl.math.rsqrt(running_var + bn_eps)\n        scale = bn_weight * inv_std\n\n    And then compute shift.\n\n    We hope this is faster.\n\n    Let's write the code.\n\n    Note: We must not change the signature of the kernel.\n\n    We are changing the internal computation only.\n\n    We also note: the previous kernel had a safety clamp? The original PyTorch did not have a clamp in the batch norm? \n    Our kernel does: tl.sqrt(running_var + bn_eps). The rsqrt is safe because we have bn_eps>0.\n\n    We'll proceed.\n\n    We output the entire block.\n\n    We note: the ModelNew class is the same except that we remove the fixed block sizes in the call and use the meta parameters.\n\n    We also note: the kernel now has an autotune decorator.\n\n    We must be cautious: the autotune decorator must be above the @triton.jit.\n\n    We do:\n\n        @triton.autotune( ... )\n        @triton.jit\n        def fused_forward_kernel(...):\n            ...\n\n    We define the configs.\n\n    We'll define 9 configs as above.\n\n    We also note that the kernel must have the same parameters as before.\n\n    We are ready.\n\n    We write the code accordingly.\n\n    IMPORTANT: We are not allowed to change the class name (ModelNew) or the function signatures.\n\n    We are only changing the kernel and the way we call it.\n\n    Let's code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_BATCH': 16, 'BLOCK_SIZE_FEATURE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_BATCH': 32, 'BLOCK_SIZE_FEATURE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_BATCH': 64, 'BLOCK_SIZE_FEATURE': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_BATCH': 16, 'BLOCK_SIZE_FEATURE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_BATCH': 32, 'BLOCK_SIZE_FEATURE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_BATCH': 64, 'BLOCK_SIZE_FEATURE': 128}, num_warps=8),\n    ],\n    key=['batch_size', 'out_features'],\n)\n@triton.jit\ndef fused_forward_kernel(\n    linear_out_ptr,\n    bn_weight_ptr,\n    bn_bias_ptr,\n    running_mean_ptr,\n    running_var_ptr,\n    model_bias_ptr,\n    divide_value,\n    output_ptr,\n    batch_size,\n    out_features,\n    bn_eps,\n    stride_lo0, stride_lo1,\n    stride_out0, stride_out1,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEATURE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_feature = tl.program_id(1)\n    \n    batch_idx = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feature_idx = pid_feature * BLOCK_SIZE_FEATURE + tl.arange(0, BLOCK_SIZE_FEATURE)\n    \n    batch_mask = batch_idx < batch_size\n    feature_mask = feature_idx < out_features\n    block_mask = batch_mask[:, None] & feature_mask[None, :]\n    \n    # Calculate offsets for linear_out access\n    lo_offsets = batch_idx[:, None] * stride_lo0 + feature_idx[None, :] * stride_lo1\n    linear_out = tl.load(linear_out_ptr + lo_offsets, mask=block_mask, other=0.0)\n    \n    # Load batchnorm parameters for the feature block\n    bn_weight = tl.load(bn_weight_ptr + feature_idx, mask=feature_mask, other=0.0)\n    bn_bias = tl.load(bn_bias_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_mean = tl.load(running_mean_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_var = tl.load(running_var_ptr + feature_idx, mask=feature_mask, other=0.0)\n    \n    # Optimized batchnorm computation\n    inv_std = tl.math.rsqrt(running_var + bn_eps)\n    scale = bn_weight * inv_std\n    shift = bn_bias - running_mean * scale\n    bn_out = linear_out * scale[None, :] + shift[None, :]\n    \n    # Add model bias and divide\n    model_bias = tl.load(model_bias_ptr)\n    normalized = (bn_out + model_bias) / divide_value\n    \n    # Swish activation\n    swish_out = normalized * tl.sigmoid(normalized)\n    \n    # Store results\n    out_offsets = batch_idx[:, None] * stride_out0 + feature_idx[None, :] * stride_out1\n    tl.store(output_ptr + out_offsets, swish_out, mask=block_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        \n    def forward(self, x):\n        linear_out = self.matmul(x)\n        \n        if self.training:\n            x = self.bn(linear_out)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            # Inference path with fused Triton kernel\n            batch_size, _ = linear_out.shape\n            output = torch.empty_like(linear_out)\n            \n            grid = lambda meta: (\n                triton.cdiv(batch_size, meta['BLOCK_SIZE_BATCH']),\n                triton.cdiv(linear_out.size(1), meta['BLOCK_SIZE_FEATURE'])\n            )\n            \n            fused_forward_kernel[grid](\n                linear_out,\n                self.bn.weight,\n                self.bn.bias,\n                self.bn.running_mean,\n                self.bn.running_var,\n                self.bias,\n                self.divide_value,\n                output,\n                batch_size,\n                linear_out.size(1),\n                self.bn.eps,\n                linear_out.stride(0), linear_out.stride(1),\n                output.stride(0), output.stride(1),\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
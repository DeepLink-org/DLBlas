{"id": "19a8b8b0-c868-46e1-8719-e87be55f4508", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_post_linear_kernel(\n    x_ptr,\n    scale_ptr,\n    shift_ptr,\n    output_ptr,\n    bias,\n    divide_value,\n    n_cols,\n    stride_x,\n    stride_scale,\n    stride_shift,\n    stride_out,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block = tl.program_id(1)\n    col_offsets = col_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    \n    # Calculate pointers\n    x_row_ptr = x_ptr + row_idx * stride_x\n    out_row_ptr = output_ptr + row_idx * stride_out\n    \n    # Load data\n    x = tl.load(x_row_ptr + col_offsets, mask=mask, other=0.0)\n    scale = tl.load(scale_ptr + col_offsets * stride_scale, mask=mask, other=0.0)\n    shift = tl.load(shift_ptr + col_offsets * stride_shift, mask=mask, other=0.0)\n    \n    # Fused operations\n    y = x * scale + shift + bias\n    y = y / divide_value\n    sig_y = tl.sigmoid(y)\n    y = y * sig_y\n    \n    # Store result\n    tl.store(out_row_ptr + col_offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        self.out_features = out_features\n        \n    def forward(self, x):\n        x = self.matmul(x)\n        \n        # Precompute BN scale/shift\n        if self.training:\n            mean = x.mean(dim=0)\n            var = x.var(dim=0, unbiased=False)\n            with torch.no_grad():\n                self.bn.running_mean = (1 - self.bn.momentum) * self.bn.running_mean + self.bn.momentum * mean\n                self.bn.running_var = (1 - self.bn.momentum) * self.bn.running_var + self.bn.momentum * var\n        else:\n            mean = self.bn.running_mean\n            var = self.bn.running_var\n            \n        inv_std = 1.0 / torch.sqrt(var + self.bn.eps)\n        scale = self.bn.weight * inv_std\n        shift = self.bn.bias - mean * scale\n        \n        # Prepare output tensor\n        output = torch.empty_like(x)\n        batch_size, n_cols = x.shape\n        \n        # Launch fused kernel\n        BLOCK_SIZE = 128\n        grid = (batch_size, triton.cdiv(n_cols, BLOCK_SIZE))\n        _fused_post_linear_kernel[grid](\n            x, scale, shift, output, \n            self.bias, self.divide_value,\n            n_cols,\n            x.stride(0), scale.stride(0), shift.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "cf1435ee-8f6b-491c-9136-8a7f56d4f465", "generation": 1, "timestamp": 1754586883.5865633, "iteration_found": 1, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_97_Matmul_BatchNorm_BiasAdd_Divide_Swish", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpct6dfr3k.py\", line 79, in forward\n    _fused_post_linear_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 30:8:\n\n    # Calculate pointers\n    x_row_ptr = x_ptr + row_idx * stride_x\n    out_row_ptr = output_ptr + row_idx * stride_out\n\n    # Load data\n    x = tl.load(x_row_ptr + col_offsets, mask=mask, other=0.0)\n    scale = tl.load(scale_ptr + col_offsets * stride_scale, mask=mask, other=0.0)\n    shift = tl.load(shift_ptr + col_offsets * stride_shift, mask=mask, other=0.0)\n\n    # Fused operations\n    y = x * scale + shift + bias\n        ^\nIncompatibleTypeErrorImpl('invalid operands of type pointer<fp32> and triton.language.float32')\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 900, in __add__\n    return add(self, other, sanitize_overflow=True, _semantic=_semantic)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2468, in add\n    return _semantic.add(x, y, sanitize_overflow)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 226, in add\n    input, other = self.binary_op_type_checking_impl(input, other, True, True)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 185, in binary_op_type_checking_impl\n    self.check_ptr_type_impl(rhs_sca_ty, lhs_sca_ty, allow_rhs_ptr)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 167, in check_ptr_type_impl\n    raise IncompatibleTypeErrorImpl(type_a, type_b)\ntriton.language.semantic.IncompatibleTypeErrorImpl: invalid operands of type pointer<fp32> and triton.language.float32\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.11, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.108, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.11, "speed_up": 1.019, "custom_timing": 0.108}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.1080\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.019, 'custom_timing': 0.108}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.1080, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.019, 'custom_timing': 0.108}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, batch normalization, bias addition, division, and Swish activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = self.bn(x)\n        x = x + self.bias\n        x = x / self.divide_value\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.1080), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.019, 'custom_timing': 0.108})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, batch normalization, bias addition, division, and Swish activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = self.bn(x)\n        x = x + self.bias\n        x = x / self.divide_value\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear (matmul) layer\n  2. BatchNorm1d\n  3. Add a bias (broadcasted)\n  4. Divide by a constant\n  5. Swish activation (x * sigmoid(x))\n\nThe current implementation uses high-level PyTorch modules (nn.Linear, nn.BatchNorm1d) and operations.\n\nWe are to rewrite this using Triton to gain performance. However, note that the entire model is a sequence of operations. We cannot replace the entire model with one Triton kernel because the operations are complex and interdependent. But we can try to fuse operations.\n\nHowever, note that the BatchNorm layer is stateful (has running mean and variance) and is typically applied per channel. We cannot easily fuse the linear layer and batchnorm without storing intermediate results.\n\nBut let's consider the operations:\n\n  x = self.matmul(x)   -> [batch_size, out_features]\n  x = self.bn(x)        -> same shape\n  x = x + self.bias     -> same shape, bias is [1] (broadcastable)\n  x = x / self.divide_value -> scalar division\n  x = x * torch.sigmoid(x) -> element-wise\n\nWe can try to write a fused kernel for the entire forward pass? However, note that BatchNorm during training uses the current batch's statistics and updates the running statistics. We must be cautious.\n\nBut the problem: the current implementation is in PyTorch and we are asked to rewrite in Triton for performance.\n\nAlternatively, we can break down:\n\nOption 1: Fuse the linear layer and the batchnorm? But note: the linear layer is a matrix multiplication. We can fuse the linear and the batchnorm by precomputing a new weight and bias for the linear, if we are in eval mode. However, during training, the batchnorm uses batch statistics and updates running stats.\n\nBut the problem does not specify if we are in training or eval. The reference code uses `nn.BatchNorm1d` which by default is in training mode if model.train() is called. We cannot assume eval.\n\nTherefore, we must preserve the batchnorm's behavior (updating running mean and variance during training). This complicates fusion.\n\nAlternatively, we can try to write a kernel that does:\n\n  - The linear layer (matrix multiplication) and then the rest of the operations?\n\nBut note: the batchnorm operation is:\n\n  y = (x - running_mean) * weight / sqrt(running_var + eps) + bias   [but wait, the BatchNorm1d in PyTorch has parameters: weight (gamma) and bias (beta). And it maintains running_mean and running_var.\n\nHowever, in the reference code, the batchnorm is defined as:\n  self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n\nAnd then in forward, we do:\n  x = self.bn(x)\n\nSo the operation is:\n\n  x_hat = (x - mean) / sqrt(var + eps)\n  x = gamma * x_hat + beta\n\nBut note: the reference code also has an additional bias and then a division and then swish.\n\nWe might be able to fuse the entire sequence from the output of the linear layer to the swish.\n\nHowever, the batchnorm requires computing the mean and variance of the current batch. So we must compute the mean and variance of the linear output for the current batch.\n\nWe can do:\n\n  Step 1: Do the matrix multiplication (this is the linear layer) for the entire batch. We can do this with a Triton kernel? But note: the linear layer is already efficient in PyTorch (using cuBLAS). So we might not gain by rewriting.\n\n  Step 2: Then we have to compute the mean and variance for each feature (over the batch) for the current batch. Then normalize.\n\n  Step 3: Then add the model's bias (which is a scalar) and then divide by a scalar and then apply swish.\n\nAlternatively, we can write a kernel that does:\n\n  - The entire forward pass from input to output? That would require doing the matrix multiplication and then the normalization and then the rest.\n\nBut note: the matrix multiplication is O(batch_size * in_features * out_features) and the rest are O(batch_size * out_features). So the matmul dominates.\n\nWe are constrained by the fact that we cannot change the function signature of the model. We are to replace the internal implementation.\n\nGiven the complexity, we might focus on fusing the operations after the linear layer? But note that the linear layer is a separate layer and we are using PyTorch's built-in. We cannot fuse without rewriting the linear.\n\nAlternatively, we can write a custom module that does the entire forward pass in one Triton kernel? That would require:\n\n  1. Storing the weight and bias for the linear layer as parameters.\n  2. Storing the parameters for batchnorm (gamma, beta, and also maintaining running mean and running var) as buffers/parameters.\n  3. Also storing the additional bias and divide_value.\n\nBut note: the reference code uses `nn.Linear` and `nn.BatchNorm1d` which have their own parameters. We cannot simply remove them.\n\nWe are allowed to change the implementation of the forward pass? The problem says: \"Generate functionally equivalent but faster Triton implementations\".\n\nWe are to rewrite the entire model? But note the `__init__` method is also given. We cannot change the parameters? We are to keep the same parameters.\n\nWe are to generate a Triton kernel that replaces the entire forward pass? But that would require a very complex kernel.\n\nAlternatively, we can break the operations and try to fuse the element-wise operations? The operations after the batchnorm: adding a scalar bias, dividing by a scalar, and swish. These are element-wise and can be fused.\n\nBut note: the batchnorm itself is a per-channel operation? Actually, batchnorm is applied per feature (each feature has its own mean, variance, gamma, beta). The subsequent bias is a scalar, and the division is scalar. Then swish is element-wise.\n\nWe can write a Triton kernel for the entire sequence from the output of the linear layer? That is:\n\n  Input: a tensor of shape [batch_size, out_features] (the result of the linear layer)\n  Then we do:\n      batchnorm (which requires computing the mean and variance for each feature over the batch) -> this is a reduction over the batch dimension.\n      then add a scalar, then divide by scalar, then swish.\n\nBut note: the batchnorm operation in training mode requires:\n\n  - Computing the mean and variance for the current batch for each feature (so we reduce over the batch dimension for each feature).\n  - Then update the running mean and running variance.\n\nWe can write a Triton kernel that:\n\n  1. Takes the linear output (let's call it `x` of shape [batch_size, out_features])\n  2. Also takes the parameters: gamma, beta (from batchnorm), the additional bias (scalar), divide_value, and the batchnorm's running_mean, running_var, and also the momentum and eps.\n  3. Computes the mean and variance for each feature (over the batch) for the current batch.\n  4. Updates the running mean and running var (if training).\n  5. Then normalizes the feature: x_hat = (x - mean) / sqrt(var + eps) for each feature.\n  6. Then: x = gamma * x_hat + beta   (per feature)\n  7. Then add the scalar bias: x = x + bias\n  8. Then divide by divide_value: x = x / divide_value\n  9. Then swish: x = x * sigmoid(x)\n\nBut note: the batchnorm in the reference code also has a momentum and updates the running statistics. We must do that atomically? Actually, the update of the running statistics is done after the kernel? Or we can do it in the kernel? We can do it in the kernel.\n\nHowever, the reduction over the batch for each feature (to compute mean and variance) is a challenge. We can do:\n\n  - We have `out_features` features. We can assign one program per feature? Then for each feature, we reduce over the batch.\n\nBut the batch size is 128, which is not too big. We can use a reduction kernel.\n\nBut note: we are to write one kernel that does everything? That might be complex.\n\nAlternatively, we can do:\n\n  Kernel 1: for the linear layer (we leave it as is, because it's efficient) and then\n  Kernel 2: a fused kernel that does batchnorm, add bias, divide, swish.\n\nBut the problem says we are to rewrite the entire model? And we are only allowed to change the Triton kernel source within the EVOLVE-BLOCK.\n\nWe are going to write a fused kernel for the operations after the linear layer? That is, we replace:\n\n  x = self.bn(x)\n  x = x + self.bias\n  x = x / self.divide_value\n  x = x * torch.sigmoid(x)\n\nwith a single Triton kernel.\n\nBut note: the batchnorm operation in training mode requires:\n\n  - Computing the mean and variance of the current batch for each channel (feature). This is a reduction over the batch dimension.\n\nWe can write a Triton kernel that:\n\n  - Each program handles one feature? Then we have `out_features` programs.\n  - For each feature, we load the entire column (batch_size elements) and compute the mean and variance.\n  - Then we update the running mean and running var (using the momentum).\n  - Then we normalize the entire column: (x - mean) / sqrt(var + eps) * gamma + beta.\n  - Then we add the scalar bias and divide by the scalar, and then apply swish.\n\nBut note: the batchnorm parameters (gamma, beta) are per feature, but the additional bias and divide_value are scalars.\n\nWe can design:\n\n  - Grid: (out_features,)\n  - For each feature index `i` (program ID), we:\n        load gamma[i], beta[i]\n        load the entire column of the linear output for feature i (from the input tensor of shape [batch_size, out_features])\n        compute the mean and variance of that column (over the batch)\n        update the running mean and running var (if training)\n        then for each row in the batch:\n            normalize: x = (x - mean) / sqrt(var + eps) * gamma[i] + beta[i]\n            then: x = x + bias\n            then: x = x / divide_value\n            then: x = x * sigmoid(x)\n        and store the result.\n\nBut note: the input tensor (linear output) is of shape [batch_size, out_features]. We are processing one column at a time? That would be non-coalesced memory access. We would do:\n\n  For each feature i, we read and write the entire column? That would be strided access (with stride = out_features). This is inefficient.\n\nAlternatively, we can have each program handle a block of features? But the reduction would be over the batch and then we have to synchronize? It becomes complicated.\n\nGiven the complexity and the fact that the batchnorm requires a reduction over the batch, and the batch size is 128, we can do:\n\n  - Use a two-pass kernel? First pass to compute the mean and variance per feature (over the batch) and then a second pass to normalize and apply the rest.\n\nBut we are to write a single kernel? We can do:\n\n  Step 1: Launch a kernel that computes the mean and variance for each feature (over the batch). This kernel would have grid (out_features,) and each program reduces over the entire batch for one feature.\n\n  Step 2: Then launch a kernel that normalizes and applies the rest? This kernel would have grid (batch_size, out_features) and each program handles one element.\n\nBut we are in the forward function and we are allowed to have multiple kernels? The problem does not say we can only have one kernel.\n\nHowever, we are to rewrite the entire forward pass? We can do:\n\n  def forward(self, x):\n        x = self.matmul(x)   # leave as is\n        # Now we want to do the rest with Triton\n        # We need to pass: \n        #   x (the linear output), \n        #   self.bn.weight, self.bn.bias, self.bn.running_mean, self.bn.running_var, self.bn.eps, self.bn.momentum\n        #   self.bias, self.divide_value\n        # and also we need to update the running mean and running var if training.\n\n        # We design two kernels: one for reduction (to compute mean and variance per feature) and one for the element-wise operations.\n\n        # But note: we can compute the variance in one pass? Actually, we can use Welford's algorithm, but for simplicity we can do two passes? Or we can do one reduction kernel that computes both.\n\n        # Alternatively, we can use PyTorch's built-in functions to compute the mean and variance per feature? But then we lose the fusion.\n\n        # We are trying to gain performance. We can write a reduction kernel for mean and variance? But PyTorch's native is already optimized.\n\n        # We might not gain by rewriting the reduction. So let's do:\n\n        #   if training:\n        #       mean = x.mean(dim=0)\n        #       var = x.var(dim=0, unbiased=False)   # note: batchnorm uses biased variance by default? Actually, the formula uses the batch size.\n        #   else:\n        #       mean = self.bn.running_mean\n        #       var = self.bn.running_var\n\n        #   x = (x - mean) * (self.bn.weight / torch.sqrt(var + self.bn.eps)) + self.bn.bias\n        #   then the rest.\n\n        # But that is what the original batchnorm does. And then we do the rest with a Triton kernel for the element-wise operations.\n\n        # We can write a Triton kernel for the entire sequence from the normalization to the swish? That is:\n\n        #   x = normalized (by batchnorm) + additional bias + divide + swish.\n\n        # This is a simple element-wise kernel.\n\n        # So we do:\n\n        #   if training:\n        #       mean = x.mean(dim=0)\n        #       var = x.var(dim=0, unbiased=False)\n        #       # update running stats: running_mean = (1-momentum)*running_mean + momentum*mean\n        #       self.bn.running_mean = (1 - self.bn.momentum) * self.bn.running_mean + self.bn.momentum * mean\n        #       self.bn.running_var = (1 - self.bn.momentum) * self.bn.running_var + self.bn.momentum * var\n        #   else:\n        #       mean = self.bn.running_mean\n        #       var = self.bn.running_var\n\n        #   x = (x - mean) * (self.bn.weight / torch.sqrt(var + self.bn.eps)) + self.bn.bias\n\n        #   Then we do the rest: \n        #       x = x + self.bias\n        #       x = x / self.divide_value\n        #       x = x * torch.sigmoid(x)\n\n        # We can fuse the last three operations? They are element-wise.\n\n        # We can write a Triton kernel for:\n        #   x = x + bias\n        #   x = x / divide_value\n        #   x = x * sigmoid(x)\n\n        # This kernel is straightforward: one element per thread.\n\n        # But note: the operations are very light. The gain might be minimal.\n\n        # Alternatively, we can fuse the normalization and the element-wise? That is, we do:\n\n        #   normalized = (x - mean) * inv_std * gamma + beta   [this is the batchnorm]\n        #   then: normalized = normalized + bias\n        #          normalized = normalized / divide_value\n        #          normalized = normalized * sigmoid(normalized)\n\n        # We can do this in one Triton kernel? We can, but we have to do it per element.\n\n        # We design a kernel for the entire post-linear part? Including the normalization and the element-wise? That would avoid writing the intermediate normalized tensor.\n\n        # How?\n\n        #   We have the linear output (x) of shape [batch_size, out_features]\n        #   We precompute the mean and variance per feature (using PyTorch, as above) -> this is O(out_features) and we do a reduction over batch_size (which is 128) -> not too heavy.\n        #   Then we launch a kernel that for each element:\n        #       reads x[i, j]\n        #       computes: \n        #           normalized = (x[i, j] - mean[j]) * (gamma[j] * inv_std[j]) + beta[j]\n        #           then: normalized = normalized + bias\n        #           then: normalized = normalized / divide_value\n        #           then: normalized = normalized * sigmoid(normalized)\n        #       and writes to output.\n\n        # This kernel would have grid (batch_size, out_features) and each thread one element.\n\n        # But note: the memory access is coalesced if we let each block handle a contiguous set of features for a set of rows.\n\n        # We can do:\n\n        #   BLOCK_SIZE = 128 (or 256, etc) in the feature dimension.\n\n        #   pid = triton.program_id(0)   # over batch_size\n        #   q = triton.program_id(1)     # over out_features, in blocks\n\n        #   Then for each element in the block, we compute.\n\n        # However, the mean, gamma, beta, inv_std are per feature. We can load them in shared memory? Or we can use a vector load.\n\n        # But note: the batch_size is 128 and out_features=512. We can have:\n\n        #   grid = (batch_size, triton.cdiv(out_features, BLOCK_SIZE))\n\n        #   Then each block processes BLOCK_SIZE features for one row.\n\n        #   We load the parameters for the BLOCK_SIZE features from global memory to registers? And then do the computation.\n\n        # This would be efficient.\n\n        # Steps:\n\n        #   Precomputation (on host, in PyTorch) for mean, inv_std (which is gamma / sqrt(var+eps)) and then add beta? Actually, we can precompute a scale and shift per feature:\n\n        #       scale = gamma * inv_std   [per feature]\n        #       shift = beta - mean * scale   [per feature]\n\n        #   Then the normalized value for an element x is: x * scale + shift\n\n        #   Then we do: (x * scale + shift) + bias -> then divide, then swish.\n\n        #   So: \n        #       y = (x * scale + shift) + bias\n        #       y = y / divide_value\n        #       y = y * sigmoid(y)\n\n        #   We can precompute the scale and shift on the host and pass them to the kernel.\n\n        #   The kernel then would do:\n\n        #       for a given row (batch index) and a block of features:\n        #           load x from the linear output for that row and the block of features.\n        #           load the scale and shift for the block of features.\n        #           compute: y = (x * scale + shift) + bias   [bias is scalar]\n        #           then: y = y / divide_value\n        #           then: y = y * sigmoid(y)\n\n        #   This is efficient.\n\n        #   But note: we also have to update the running mean and variance? We do that on the host, after we compute the mean and var.\n\n        #   So the forward function becomes:\n\n        #       x_linear = self.matmul(x)\n\n        #       if self.training:\n        #           mean = x_linear.mean(dim=0)\n        #           var = x_linear.var(dim=0, unbiased=False)\n        #           # Update running stats\n        #           self.bn.running_mean = (1 - self.bn.momentum) * self.bn.running_mean + self.bn.momentum * mean\n        #           self.bn.running_var = (1 - self.bn.momentum) * self.bn.running_var + self.bn.momentum * var\n        #       else:\n        #           mean = self.bn.running_mean\n        #           var = self.bn.running_var\n\n        #       inv_std = 1.0 / torch.sqrt(var + self.bn.eps)\n        #       scale = self.bn.weight * inv_std\n        #       shift = self.bn.bias - mean * scale\n\n        #       # Then call the Triton kernel to compute the rest.\n\n        #   We design a kernel that takes:\n        #       x_linear, scale, shift, self.bias, self.divide_value, output\n        #       and the dimensions: batch_size, out_features\n\n        #   The kernel does the element-wise operations.\n\n        #   We call it with grid (batch_size, ceil(out_features/BLOCK_SIZE))\n\n        #   We use BLOCK_SIZE as a tunable parameter.\n\n        #   We are fusing: \n        #       y = x_linear * scale + shift + bias\n        #       y = y / divide_value\n        #       y = y * sigmoid(y)\n\n        #   This is 3 element-wise operations and one activation. We can do it in one kernel.\n\n        #   This should be faster than doing it in separate PyTorch operations because we avoid multiple passes over the data.\n\n        #   Also, we are using Triton's efficient memory access.\n\n        #   We write the kernel accordingly.\n\n        #   Note: the bias is a scalar, so we broadcast it.\n\n        #   The divide_value is a scalar.\n\n        #   So in the kernel, we can precompute: total_shift = shift + self.bias\n        #   and then: y = x_linear * scale + total_shift\n        #   then: y = y / divide_value\n        #   then: y = y * sigmoid(y)\n\n        #   But note: the shift is per feature, so we cannot pre-add the scalar bias until we are in the kernel? Actually, we can pass total_shift = shift + self.bias? But wait, the shift is a vector of length out_features, and self.bias is a scalar. So total_shift would be shift + self.bias (broadcasted).\n\n        #   Alternatively, we can do in the kernel: for each feature j, shift_j = shift[j] + bias (the scalar)\n\n        #   But that would require loading the scalar bias for every element? We can pass it as a scalar and then use it.\n\n        #   We pass the scalar bias and the scalar divide_value as scalars to the kernel.\n\n        #   The kernel would look like:\n\n        #       @triton.jit\n        #       def _post_linear(\n        #           x_ptr, scale_ptr, shift_ptr, output_ptr,\n        #           bias, divide_value,\n        #           n_cols, BLOCK_SIZE: tl.constexpr,\n        #           ...\n        #       ):\n        #           row_idx = tl.program_id(0)\n        #           col_block = tl.program_id(1)\n        #           col_offsets = col_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        #           mask = col_offsets < n_cols\n        #           x_ptrs = x_ptr + row_idx * n_cols + col_offsets\n        #           x = tl.load(x_ptrs, mask=mask)\n        #           scale = tl.load(scale_ptr + col_offsets, mask=mask)\n        #           shift = tl.load(shift_ptr + col_offsets, mask=mask)\n        #           # Compute: y = x * scale + shift + bias\n        #           y = x * scale + shift + bias\n        #           y = y / divide_value\n        #           # swish: y * sigmoid(y)\n        #           sig_y = tl.sigmoid(y)\n        #           y = y * sig_y\n        #           tl.store(output_ptr + row_idx * n_cols + col_offsets, y, mask=mask)\n\n        #   Then we call this kernel.\n\n        #   We are storing the result in the output tensor.\n\n        #   We must note: we are overwriting the original x? Or we can use a new output? The reference code does in-place? We can do out-of-place.\n\n        #   We allocate an output tensor of the same shape as x_linear.\n\n        #   Then we return the output.\n\n        #   The original model returns the output.\n\n        #   This should be faster because we fuse the operations and avoid multiple passes.\n\n        #   We must be cautious of the numerical precision? But Triton uses the same precision as PyTorch for these operations.\n\n        #   Let's implement.\n\n        #   Steps in the forward function:\n\n        #       x_linear = self.matmul(x)   # [batch_size, out_features]\n\n        #       if self.training:\n        #           mean = x_linear.mean(0)\n        #           var = x_linear.var(0, unbiased=False)\n        #           with torch.no_grad():\n        #               self.bn.running_mean = (1 - self.bn.momentum) * self.bn.running_mean + self.bn.momentum * mean\n        #               self.bn.running_var = (1 - self.bn.momentum) * self.bn.running_var + self.bn.momentum * var\n        #       else:\n        #           mean = self.bn.running_mean\n        #           var = self.bn.running_var\n\n        #       inv_std = 1.0 / torch.sqrt(var + self.bn.eps)\n        #       scale = self.bn.weight * inv_std\n        #       shift = self.bn.bias - mean * scale\n\n        #       # Now call the Triton kernel\n        #       output = torch.empty_like(x_linear)\n        #       batch_size, n_cols = x_linear.shape\n        #       # Define the grid and kernel\n        #       grid = (batch_size, triton.cdiv(n_cols, BLOCK_SIZE))\n        #       _post_linear[grid](x_linear, scale, shift, output, self.bias, self.divide_value, n_cols, BLOCK_SIZE=BLOCK_SIZE)\n\n        #       return output\n\n        #   We must define the kernel and the BLOCK_SIZE (tunable).\n\n        #   We are going to write the kernel inside the class? Or outside? We are in the EVOLVE-BLOCK, so we can write it as a helper.\n\n        #   But note: we are only allowed to output the code within the EVOLVE-BLOCK.\n\n        #   We will write the kernel as a static method or outside the class.\n\n        #   However, the problem requires that we output the entire code block.\n\n        #   We'll write the kernel as a global function inside the EVOLVE-BLOCK? But note the block is for the class ModelNew.\n\n        #   We can define the kernel inside the module? Or outside? Since we are in a class definition, we cannot define a Triton kernel inside the class? We can define it outside.\n\n        #   We'll write the kernel outside the class, but inside the module? Actually, we can write it as a top-level function.\n\n        #   So we restructure:\n\n        #       import triton and tl\n\n        #       @triton.jit\n        #       def _post_linear(...): ...\n\n        #       class ModelNew(nn.Module):\n        #           ... in forward, we call _post_linear\n\n        #   But note: the kernel must be compiled for the specific device? And we are using cuda. It should be fine.\n\n        #   We'll do that.\n\n        #   We choose a BLOCK_SIZE, but we can make it tunable? We can use autotune? But for simplicity, we choose 128.\n\n        #   However, the problem says we can do adaptive tuning. We can use triton.autotune? But we are not required to.\n\n        #   Let's do without autotune for now.\n\n        #   We'll set BLOCK_SIZE = 128.\n\n        #   We also note: the kernel must be launched with the grid.\n\n        #   We are passing the scalar bias and divide_value as kernel arguments. Triton supports scalar arguments.\n\n        #   Let's write the kernel.\n\n        #   Note: the kernel must be compiled only once? We can use a constant BLOCK_SIZE.\n\n        #   We are going to write the kernel and then the ModelNew class.\n\n        #   We must be cautious: the kernel uses tl.sigmoid, which is available.\n\n        #   We must also note: the parameters (scale, shift) are of shape [out_features]. We are loading them for a block of columns.\n\n        #   The memory access: \n        #       x: contiguous in memory per row? Yes, because the linear output is [batch_size, out_features] and row-major.\n        #       scale and shift: contiguous? We can make them contiguous by storing as 1D tensors.\n\n        #   We assume they are contiguous.\n\n        #   This should be efficient.\n\n        #   We hope to gain from fusing the operations.\n\n        #   We are replacing:\n        #       x = self.bn(x_linear)   # which does: (x_linear - mean) * scale + beta? Actually, we precomputed scale and shift so that: x_bn = x_linear * scale + shift.\n        #       x = x + self.bias\n        #       x = x / self.divide_value\n        #       x = x * torch.sigmoid(x)\n\n        #   with one kernel that does all in one pass.\n\n        #   This avoids creating intermediate tensors and might be faster.\n\n        #   We try.\n\n        #   But note: the original code does the batchnorm with PyTorch's optimized batchnorm? Which is already fast. But the element-wise operations are not fused.\n\n        #   We are fusing the element-wise operations after the batchnorm.\n\n        #   We expect a speedup.\n\n        #   Let's code accordingly.\n\n        #   We must also note: the batchnorm computation on the host (mean and var) is O(batch_size * out_features) and we do it in PyTorch. This is acceptable.\n\n        #   We are not rewriting the linear layer.\n\n        #   We are only rewriting the post-linear part.\n\n        #   We hope for a speedup in the element-wise part.\n\n        #   We write the kernel and then the ModelNew class.\n\n        #   We also note: the original code had an additional bias and divide and swish. We are fusing them.\n\n        #   We must be cautious: the batchnorm in training mode requires the mean and var of the current batch. We are computing them with PyTorch. This is the same as the original.\n\n        #   So the functionality is preserved.\n\n        #   We proceed.\n\n        #   We are going to output the entire code block.\n\n        #   We remove the old forward and replace it with the new one.\n\n        #   We also add the Triton kernel.\n\n        #   Note: we must import triton and tl.\n\n        #   We add the necessary imports.\n\n        #   We are going to write:\n\n        #       import triton\n        #       import triton.language as tl\n\n        #       @triton.jit\n        #       def _post_linear_kernel(...): ...\n\n        #       class ModelNew(nn.Module):\n        #           ... same __init__ ...\n        #           def forward(self, x):\n        #               ... as described ...\n\n        #   We also note: the original code had batch_size, in_features, etc as global variables? And also get_inputs and get_init_inputs? We leave them unchanged.\n\n        #   We do not change the __init__.\n\n        #   We only change the forward.\n\n        #   We also note: the original code used `x = x + self.bias` where self.bias is a scalar. We pass it as a scalar to the kernel.\n\n        #   We are good.\n\n        #   Let's write the code.\n\n        #   We choose BLOCK_SIZE = 128.\n\n        #   We note: out_features=512, which is divisible by 128? 512/128=4, so yes.\n\n        #   We write the kernel without autotune for now.\n\n        #   We are cautious of the boundaries: we use a mask.\n\n        #   We write the kernel.\n\n        #   We call the kernel: `_fused_post_linear_kernel`\n\n        #   We are ready.\n\nHowever, note: the current program does not have Triton as a dependency? But the problem says we are to rewrite with Triton. So we assume Triton is available.\n\nLet's write the code accordingly.\n\nWe are replacing the entire EVOLVE-BLOCK.\n\nWe note: the original EVOLVE-BLOCK contained:\n  - The class ModelNew with the old forward\n  - The global variables and functions: batch_size, in_features, out_features, ... and get_inputs, get_init_inputs.\n\nWe change the forward method of ModelNew and add the kernel.\n\nWe also add the necessary imports for triton.\n\nWe do:\n\n  import math\n  import triton\n  import triton.language as tl\n\n  @triton.jit\n  def _fused_post_linear_kernel(...): ...\n\n  class ModelNew(nn.Module):\n      ... __init__ unchanged ...\n      def forward(self, x):\n          # new implementation\n\n  ... the rest (batch_size, etc) unchanged ...\n\nWe hope this passes compilation and correctness and gives a speedup.\n\nWe note: the kernel must be compiled only once? We are using a constant BLOCK_SIZE.\n\nWe use:\n\n  BLOCK_SIZE = 128\n\nBut we can make it a tunable parameter? However, we are not autotuning now.\n\nWe write the kernel.\n\nLet's go.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_post_linear_kernel(\n    x_ptr,\n    scale_ptr,\n    shift_ptr,\n    output_ptr,\n    bias,\n    divide_value,\n    n_cols,\n    stride_x,\n    stride_scale,\n    stride_shift,\n    stride_out,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block = tl.program_id(1)\n    col_offsets = col_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    \n    # Calculate pointers\n    x_row_ptr = x_ptr + row_idx * stride_x\n    out_row_ptr = output_ptr + row_idx * stride_out\n    \n    # Load data\n    x = tl.load(x_row_ptr + col_offsets, mask=mask, other=0.0)\n    scale = tl.load(scale_ptr + col_offsets * stride_scale, mask=mask, other=0.0)\n    shift = tl.load(shift_ptr + col_offsets * stride_shift, mask=mask, other=0.0)\n    \n    # Fused operations\n    y = x * scale + shift + bias\n    y = y / divide_value\n    sig_y = tl.sigmoid(y)\n    y = y * sig_y\n    \n    # Store result\n    tl.store(out_row_ptr + col_offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        self.out_features = out_features\n        \n    def forward(self, x):\n        x = self.matmul(x)\n        \n        # Precompute BN scale/shift\n        if self.training:\n            mean = x.mean(dim=0)\n            var = x.var(dim=0, unbiased=False)\n            with torch.no_grad():\n                self.bn.running_mean = (1 - self.bn.momentum) * self.bn.running_mean + self.bn.momentum * mean\n                self.bn.running_var = (1 - self.bn.momentum) * self.bn.running_var + self.bn.momentum * var\n        else:\n            mean = self.bn.running_mean\n            var = self.bn.running_var\n            \n        inv_std = 1.0 / torch.sqrt(var + self.bn.eps)\n        scale = self.bn.weight * inv_std\n        shift = self.bn.bias - mean * scale\n        \n        # Prepare output tensor\n        output = torch.empty_like(x)\n        batch_size, n_cols = x.shape\n        \n        # Launch fused kernel\n        BLOCK_SIZE = 128\n        grid = (batch_size, triton.cdiv(n_cols, BLOCK_SIZE))\n        _fused_post_linear_kernel[grid](\n            x, scale, shift, output, \n            self.bias, self.divide_value,\n            n_cols,\n            x.stride(0), scale.stride(0), shift.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
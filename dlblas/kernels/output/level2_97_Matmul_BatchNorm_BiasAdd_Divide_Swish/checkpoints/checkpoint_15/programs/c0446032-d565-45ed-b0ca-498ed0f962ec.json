{"id": "c0446032-d565-45ed-b0ca-498ed0f962ec", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_im, stride_ik,\n    stride_wn, stride_wk,\n    stride_bn,\n    stride_om, stride_on,\n    divide_value: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    input_ptrs = input_ptr + offs_m[:, None] * stride_im + offs_k[None, :] * stride_ik\n    weight_ptrs = weight_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        a = tl.load(input_ptrs, mask=k_mask[None, :] & (offs_m[:, None] < M), other=0.0)\n        b = tl.load(weight_ptrs, mask=k_mask[None, :] & (offs_n[:, None] < N), other=0.0)\n        acc += tl.dot(a, tl.trans(b))\n        input_ptrs += BLOCK_K * stride_ik\n        weight_ptrs += BLOCK_K * stride_wk\n\n    bias_vals = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += bias_vals[None, :]\n    acc = acc / divide_value\n    sig = tl.sigmoid(acc)\n    acc = acc * sig\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    output_ptrs = output_ptr + stride_om * offs_m[:, None] + stride_on * offs_n[None, :]\n    output_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(output_ptrs, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def _precompute_fused_params(self):\n        weight = self.matmul.weight.data\n        linear_bias = self.matmul.bias.data if self.matmul.bias is not None else 0\n        bn_weight = self.bn.weight.data\n        bn_bias = self.bn.bias.data\n        running_mean = self.bn.running_mean\n        running_var = self.bn.running_var\n        eps = self.bn.eps\n\n        denom = 1.0 / torch.sqrt(running_var + eps)\n        fused_w = weight * (bn_weight * denom).view(-1, 1)\n        fused_b = (linear_bias - running_mean) * (bn_weight * denom) + bn_bias + self.bias\n        return fused_w, fused_b\n\n    def forward(self, x):\n        if self.training:\n            x = self.matmul(x)\n            x = self.bn(x)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            if self.fused_weight is None or self.fused_bias is None:\n                self.fused_weight, self.fused_bias = self._precompute_fused_params()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[0]\n            output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n            \n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\n            _forward_kernel[grid](\n                x, self.fused_weight, self.fused_bias, output,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                self.fused_bias.stride(0),\n                output.stride(0), output.stride(1),\n                self.divide_value,\n                BLOCK_M=128, BLOCK_N=128, BLOCK_K=32\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "51cdbf2d-5c4e-4f0c-98fe-de99bdc9a3f7", "generation": 2, "timestamp": 1754589248.434819, "iteration_found": 14, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 26.800000000000004, "speed_up": 1.068, "runtime_ms": 0.103, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.11, "speed_up": 1.068, "custom_timing": 0.103}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 25.800000000000004, "speed_up": 1.058, "runtime_ms": 0.104, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.11, "speed_up": 1.058, "custom_timing": 0.104}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 1.0000\n- score: 25.8000\n- speed_up: 1.0580\n- runtime_ms: 0.1040\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.058, 'custom_timing': 0.104}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: score, speed_up. Consider continuing with similar changes.\n- Metrics showing regression: runtime_ms. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 21.9000, speed_up: 1.0190, runtime_ms: 0.1080, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.019, 'custom_timing': 0.108}\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 23.8000, speed_up: 1.0380, runtime_ms: 0.1060, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.038, 'custom_timing': 0.106}\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 25.8000, speed_up: 1.0580, runtime_ms: 0.1040, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.058, 'custom_timing': 0.104}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 25.8000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_im, stride_ik,\n    stride_wn, stride_wk,\n    stride_bn,\n    stride_om, stride_on,\n    divide_value: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    input_ptrs = input_ptr + offs_m[:, None] * stride_im + offs_k[None, :] * stride_ik\n    weight_ptrs = weight_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        a = tl.load(input_ptrs, mask=k_mask[None, :] & (offs_m[:, None] < M), other=0.0)\n        b = tl.load(weight_ptrs, mask=k_mask[None, :] & (offs_n[:, None] < N), other=0.0)\n        acc += tl.dot(a, tl.trans(b))\n        input_ptrs += BLOCK_K * stride_ik\n        weight_ptrs += BLOCK_K * stride_wk\n\n    bias_vals = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += bias_vals[None, :]\n    acc = acc / divide_value\n    sig = tl.sigmoid(acc)\n    acc = acc * sig\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    output_ptrs = output_ptr + stride_om * offs_m[:, None] + stride_on * offs_n[None, :]\n    output_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(output_ptrs, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def _precompute_fused_params(self):\n        weight = self.matmul.weight.data\n        linear_bias = self.matmul.bias.data if self.matmul.bias is not None else 0\n        bn_weight = self.bn.weight.data\n        bn_bias = self.bn.bias.data\n        running_mean = self.bn.running_mean\n        running_var = self.bn.running_var\n        eps = self.bn.eps\n\n        denom = 1.0 / torch.sqrt(running_var + eps)\n        fused_w = weight * (bn_weight * denom).view(-1, 1)\n        fused_b = (linear_bias - running_mean) * (bn_weight * denom) + bn_bias + self.bias\n        return fused_w, fused_b\n\n    def forward(self, x):\n        if self.training:\n            x = self.matmul(x)\n            x = self.bn(x)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            if self.fused_weight is None or self.fused_bias is None:\n                self.fused_weight, self.fused_bias = self._precompute_fused_params()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[0]\n            output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n            \n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\n            _forward_kernel[grid](\n                x, self.fused_weight, self.fused_bias, output,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                self.fused_bias.stride(0),\n                output.stride(0), output.stride(1),\n                self.divide_value,\n                BLOCK_M=64, BLOCK_N=64, BLOCK_K=32\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (25.8000), Performs well on speed_up (1.0580), Performs well on runtime_ms (0.1040), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.058, 'custom_timing': 0.104})\n\n\n### Program 2 (Score: 23.8000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_forward_kernel(\n    linear_out_ptr,\n    bn_weight_ptr,\n    bn_bias_ptr,\n    running_mean_ptr,\n    running_var_ptr,\n    model_bias_ptr,\n    divide_value,\n    output_ptr,\n    batch_size,\n    out_features,\n    bn_eps,\n    stride_lo0, stride_lo1,\n    stride_out0, stride_out1,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEATURE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_feature = tl.program_id(1)\n    \n    batch_idx = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feature_idx = pid_feature * BLOCK_SIZE_FEATURE + tl.arange(0, BLOCK_SIZE_FEATURE)\n    \n    batch_mask = batch_idx < batch_size\n    feature_mask = feature_idx < out_features\n    block_mask = batch_mask[:, None] & feature_mask[None, :]\n    \n    # Calculate offsets for linear_out access\n    lo_offsets = batch_idx[:, None] * stride_lo0 + feature_idx[None, :] * stride_lo1\n    linear_out = tl.load(linear_out_ptr + lo_offsets, mask=block_mask, other=0.0)\n    \n    # Load batchnorm parameters for the feature block\n    bn_weight = tl.load(bn_weight_ptr + feature_idx, mask=feature_mask, other=0.0)\n    bn_bias = tl.load(bn_bias_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_mean = tl.load(running_mean_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_var = tl.load(running_var_ptr + feature_idx, mask=feature_mask, other=0.0)\n    \n    # Compute batchnorm\n    scale = bn_weight / tl.sqrt(running_var + bn_eps)\n    bn_out = (linear_out - running_mean[None, :]) * scale[None, :] + bn_bias[None, :]\n    \n    # Add model bias and divide\n    model_bias = tl.load(model_bias_ptr)\n    normalized = (bn_out + model_bias) / divide_value\n    \n    # Swish activation\n    swish_out = normalized * tl.sigmoid(normalized)\n    \n    # Store results\n    out_offsets = batch_idx[:, None] * stride_out0 + feature_idx[None, :] * stride_out1\n    tl.store(output_ptr + out_offsets, swish_out, mask=block_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        \n    def forward(self, x):\n        linear_out = self.matmul(x)\n        \n        if self.training:\n            x = self.bn(linear_out)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            # Inference path with fused Triton kernel\n            batch_size, _ = linear_out.shape\n            output = torch.empty_like(linear_out)\n            \n            grid = lambda meta: (\n                triton.cdiv(batch_size, meta['BLOCK_SIZE_BATCH']),\n                triton.cdiv(linear_out.size(1), meta['BLOCK_SIZE_FEATURE'])\n            )\n            \n            fused_forward_kernel[grid](\n                linear_out,\n                self.bn.weight,\n                self.bn.bias,\n                self.bn.running_mean,\n                self.bn.running_var,\n                self.bias,\n                self.divide_value,\n                output,\n                batch_size,\n                linear_out.size(1),\n                self.bn.eps,\n                linear_out.stride(0), linear_out.stride(1),\n                output.stride(0), output.stride(1),\n                BLOCK_SIZE_BATCH=32,\n                BLOCK_SIZE_FEATURE=128\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (23.8000), Performs well on speed_up (1.0380), Performs well on runtime_ms (0.1060), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.038, 'custom_timing': 0.106})\n\n\n### Program 3 (Score: 21.9000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_forward_kernel(\n    linear_out_ptr,\n    bn_weight_ptr,\n    bn_bias_ptr,\n    running_mean_ptr,\n    running_var_ptr,\n    model_bias_ptr,\n    divide_value,\n    output_ptr,\n    batch_size,\n    out_features,\n    bn_eps,\n    stride_lo0, stride_lo1,\n    stride_out0, stride_out1,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEATURE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_feature = tl.program_id(1)\n    \n    batch_idx = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feature_idx = pid_feature * BLOCK_SIZE_FEATURE + tl.arange(0, BLOCK_SIZE_FEATURE)\n    \n    batch_mask = batch_idx < batch_size\n    feature_mask = feature_idx < out_features\n    block_mask = batch_mask[:, None] & feature_mask[None, :]\n    \n    # Calculate offsets for linear_out access\n    lo_offsets = batch_idx[:, None] * stride_lo0 + feature_idx[None, :] * stride_lo1\n    linear_out = tl.load(linear_out_ptr + lo_offsets, mask=block_mask, other=0.0)\n    \n    # Load batchnorm parameters for the feature block\n    bn_weight = tl.load(bn_weight_ptr + feature_idx, mask=feature_mask, other=0.0)\n    bn_bias = tl.load(bn_bias_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_mean = tl.load(running_mean_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_var = tl.load(running_var_ptr + feature_idx, mask=feature_mask, other=0.0)\n    \n    # Compute batchnorm\n    scale = bn_weight / tl.sqrt(running_var + bn_eps)\n    bn_out = (linear_out - running_mean[None, :]) * scale[None, :] + bn_bias[None, :]\n    \n    # Add model bias and divide\n    model_bias = tl.load(model_bias_ptr)\n    normalized = (bn_out + model_bias) / divide_value\n    \n    # Swish activation\n    swish_out = normalized * tl.sigmoid(normalized)\n    \n    # Store results\n    out_offsets = batch_idx[:, None] * stride_out0 + feature_idx[None, :] * stride_out1\n    tl.store(output_ptr + out_offsets, swish_out, mask=block_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        \n    def forward(self, x):\n        linear_out = self.matmul(x)\n        \n        if self.training:\n            x = self.bn(linear_out)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            # Inference path with fused Triton kernel\n            batch_size, _ = linear_out.shape\n            output = torch.empty_like(linear_out)\n            \n            grid = lambda meta: (\n                triton.cdiv(batch_size, meta['BLOCK_SIZE_BATCH']),\n                triton.cdiv(linear_out.size(1), meta['BLOCK_SIZE_FEATURE'])\n            )\n            \n            fused_forward_kernel[grid](\n                linear_out,\n                self.bn.weight,\n                self.bn.bias,\n                self.bn.running_mean,\n                self.bn.running_var,\n                self.bias,\n                self.divide_value,\n                output,\n                batch_size,\n                linear_out.size(1),\n                self.bn.eps,\n                linear_out.stride(0), linear_out.stride(1),\n                output.stride(0), output.stride(1),\n                BLOCK_SIZE_BATCH=32,\n                BLOCK_SIZE_FEATURE=128\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (21.9000), Performs well on speed_up (1.0190), Performs well on runtime_ms (0.1080), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.019, 'custom_timing': 0.108})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_post_linear_kernel(\n    x_ptr,\n    scale_ptr,\n    shift_ptr,\n    output_ptr,\n    bias,\n    divide_value,\n    n_cols,\n    stride_x,\n    stride_scale,\n    stride_shift,\n    stride_out,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block = tl.program_id(1)\n    col_offsets = col_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    \n    # Calculate pointers\n    x_row_ptr = x_ptr + row_idx * stride_x\n    out_row_ptr = output_ptr + row_idx * stride_out\n    \n    # Load data\n    x = tl.load(x_row_ptr + col_offsets, mask=mask, other=0.0)\n    scale = tl.load(scale_ptr + col_offsets * stride_scale, mask=mask, other=0.0)\n    shift = tl.load(shift_ptr + col_offsets * stride_shift, mask=mask, other=0.0)\n    \n    # Fused operations\n    y = x * scale + shift + bias\n    y = y / divide_value\n    sig_y = tl.sigmoid(y)\n    y = y * sig_y\n    \n    # Store result\n    tl.store(out_row_ptr + col_offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        self.out_features = out_features\n        \n    def forward(self, x):\n        x = self.matmul(x)\n        \n        # Precompute BN scale/shift\n        if self.training:\n            mean = x.mean(dim=0)\n            var = x.var(dim=0, unbiased=False)\n            with torch.no_grad():\n                self.bn.running_mean = (1 - self.bn.momentum) * self.bn.running_mean + self.bn.momentum * mean\n                self.bn.running_var = (1 - self.bn.momentum) * self.bn.running_var + self.bn.momentum * var\n        else:\n            mean = self.bn.running_mean\n            var = self.bn.running_var\n            \n        inv_std = 1.0 / torch.sqrt(var + self.bn.eps)\n        scale = self.bn.weight * inv_std\n        shift = self.bn.bias - mean * scale\n        \n        # Prepare output tensor\n        output = torch.empty_like(x)\n        batch_size, n_cols = x.shape\n        \n        # Launch fused kernel\n        BLOCK_SIZE = 128\n        grid = (batch_size, triton.cdiv(n_cols, BLOCK_SIZE))\n        _fused_post_linear_kernel[grid](\n            x, scale, shift, output, \n            self.bias, self.divide_value,\n            n_cols,\n            x.stride(0), scale.stride(0), shift.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr, \n    weight_ptr, \n    total_scale_ptr, \n    total_shift_ptr, \n    output_ptr,\n    stride_x0, stride_x1,\n    stride_w0, stride_w1,\n    stride_out0, stride_out1,\n    batch_size, \n    in_features, \n    out_features,\n    BLOCK_ROW: tl.constexpr,\n    BLOCK_FEATURE: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_feature = tl.program_id(1)\n    \n    row_start = pid_batch * BLOCK_ROW\n    feature_start = pid_feature * BLOCK_FEATURE\n    \n    row_offsets = row_start + tl.arange(0, BLOCK_ROW)\n    feature_offsets = feature_start + tl.arange(0, BLOCK_FEATURE)\n    \n    row_mask = row_offsets < batch_size\n    feature_mask = feature_offsets < out_features\n    \n    acc = tl.zeros((BLOCK_ROW, BLOCK_FEATURE), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_K):\n        k_offsets = k + tl.arange(0, BLOCK_K)\n        k_mask = k_offsets < in_features\n        \n        x_ptrs = x_ptr + row_offsets[:, None] * stride_x0 + k_offsets[None, :] * stride_x1\n        x_vals = tl.load(x_ptrs, mask=row_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        w_ptrs = weight_ptr + feature_offsets[:, None] * stride_w0 + k_offsets[None, :] * stride_w1\n        w_vals = tl.load(w_ptrs, mask=feature_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        acc += tl.dot(x_vals, w_vals, allow_tf32=True)\n    \n    ts_ptrs = total_scale_ptr + feature_offsets\n    total_scale = tl.load(ts_ptrs, mask=feature_mask, other=0.0)\n    \n    tsh_ptrs = total_shift_ptr + feature_offsets\n    total_shift = tl.load(tsh_ptrs, mask=feature_mask, other=0.0)\n    \n    output_block = acc * total_scale[None, :] + total_shift[None, :]\n    output_block = output_block * tl.sigmoid(output_block)\n    \n    out_ptrs = output_ptr + row_offsets[:, None] * stride_out0 + feature_offsets[None, :] * stride_out1\n    tl.store(out_ptrs, output_block, mask=row_mask[:, None] & feature_mask[None, :])\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n\n    def forward(self, x):\n        with torch.no_grad():\n            scale_bn = self.bn.weight / torch.sqrt(self.bn.running_var + self.bn.eps)\n            shift_bn = self.bn.bias - self.bn.running_mean * scale_bn\n            total_scale = scale_bn / self.divide_value\n            total_shift = (self.matmul.bias * scale_bn + shift_bn + self.bias) / self.divide_value\n\n        x = x.contiguous()\n        batch_size, _ = x.shape\n        output = torch.empty((batch_size, self.matmul.out_features), device=x.device, dtype=x.dtype)\n        \n        grid = lambda meta: (\n            triton.cdiv(batch_size, meta['BLOCK_ROW']), \n            triton.cdiv(self.matmul.out_features, meta['BLOCK_FEATURE'])\n        )\n        \n        fused_forward_kernel[grid](\n            x, \n            self.matmul.weight, \n            total_scale, \n            total_shift, \n            output,\n            x.stride(0), x.stride(1),\n            self.matmul.weight.stride(0), self.matmul.weight.stride(1),\n            output.stride(0), output.stride(1),\n            batch_size, \n            x.size(1), \n            self.matmul.out_features,\n            BLOCK_ROW=32, \n            BLOCK_FEATURE=32, \n            BLOCK_K=64\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.0000, Type: Exploratory)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, batch normalization, bias addition, division, and Swish activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = self.bn(x)\n        x = x + self.bias\n        x = x / self.divide_value\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nUnique approach: Excellent compiled (1.000), Excellent correctness (1.000), Alternative passed approach\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_post_linear_kernel(\n    x_ptr,\n    scale_ptr,\n    shift_ptr,\n    output_ptr,\n    bias,\n    divide_value,\n    n_cols,\n    stride_x,\n    stride_scale,\n    stride_shift,\n    stride_out,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block = tl.program_id(1)\n    col_offsets = col_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    \n    # Calculate pointers\n    x_row_ptr = x_ptr + row_idx * stride_x\n    out_row_ptr = output_ptr + row_idx * stride_out\n    \n    # Load data\n    x = tl.load(x_row_ptr + col_offsets, mask=mask, other=0.0)\n    scale = tl.load(scale_ptr + col_offsets * stride_scale, mask=mask, other=0.0)\n    shift = tl.load(shift_ptr + col_offsets * stride_shift, mask=mask, other=0.0)\n    \n    # Fused operations\n    y = x * scale + shift + bias\n    y = y / divide_value\n    sig_y = tl.sigmoid(y)\n    y = y * sig_y\n    \n    # Store result\n    tl.store(out_row_ptr + col_offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        self.out_features = out_features\n        \n    def forward(self, x):\n        x = self.matmul(x)\n        \n        # Precompute BN scale/shift\n        if self.training:\n            mean = x.mean(dim=0)\n            var = x.var(dim=0, unbiased=False)\n            with torch.no_grad():\n                self.bn.running_mean = (1 - self.bn.momentum) * self.bn.running_mean + self.bn.momentum * mean\n                self.bn.running_var = (1 - self.bn.momentum) * self.bn.running_var + self.bn.momentum * var\n        else:\n            mean = self.bn.running_mean\n            var = self.bn.running_var\n            \n        inv_std = 1.0 / torch.sqrt(var + self.bn.eps)\n        scale = self.bn.weight * inv_std\n        shift = self.bn.bias - mean * scale\n        \n        # Prepare output tensor\n        output = torch.empty_like(x)\n        batch_size, n_cols = x.shape\n        \n        # Launch fused kernel\n        BLOCK_SIZE = 128\n        grid = (batch_size, triton.cdiv(n_cols, BLOCK_SIZE))\n        _fused_post_linear_kernel[grid](\n            x, scale, shift, output, \n            self.bias, self.divide_value,\n            n_cols,\n            x.stride(0), scale.stride(0), shift.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 23.8000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_forward_kernel(\n    linear_out_ptr,\n    bn_weight_ptr,\n    bn_bias_ptr,\n    running_mean_ptr,\n    running_var_ptr,\n    model_bias_ptr,\n    divide_value,\n    output_ptr,\n    batch_size,\n    out_features,\n    bn_eps,\n    stride_lo0, stride_lo1,\n    stride_out0, stride_out1,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEATURE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_feature = tl.program_id(1)\n    \n    batch_idx = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feature_idx = pid_feature * BLOCK_SIZE_FEATURE + tl.arange(0, BLOCK_SIZE_FEATURE)\n    \n    batch_mask = batch_idx < batch_size\n    feature_mask = feature_idx < out_features\n    block_mask = batch_mask[:, None] & feature_mask[None, :]\n    \n    # Calculate offsets for linear_out access\n    lo_offsets = batch_idx[:, None] * stride_lo0 + feature_idx[None, :] * stride_lo1\n    linear_out = tl.load(linear_out_ptr + lo_offsets, mask=block_mask, other=0.0)\n    \n    # Load batchnorm parameters for the feature block\n    bn_weight = tl.load(bn_weight_ptr + feature_idx, mask=feature_mask, other=0.0)\n    bn_bias = tl.load(bn_bias_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_mean = tl.load(running_mean_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_var = tl.load(running_var_ptr + feature_idx, mask=feature_mask, other=0.0)\n    \n    # Compute batchnorm\n    scale = bn_weight / tl.sqrt(running_var + bn_eps)\n    bn_out = (linear_out - running_mean[None, :]) * scale[None, :] + bn_bias[None, :]\n    \n    # Add model bias and divide\n    model_bias = tl.load(model_bias_ptr)\n    normalized = (bn_out + model_bias) / divide_value\n    \n    # Swish activation\n    swish_out = normalized * tl.sigmoid(normalized)\n    \n    # Store results\n    out_offsets = batch_idx[:, None] * stride_out0 + feature_idx[None, :] * stride_out1\n    tl.store(output_ptr + out_offsets, swish_out, mask=block_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        \n    def forward(self, x):\n        linear_out = self.matmul(x)\n        \n        if self.training:\n            x = self.bn(linear_out)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            # Inference path with fused Triton kernel\n            batch_size, _ = linear_out.shape\n            output = torch.empty_like(linear_out)\n            \n            grid = lambda meta: (\n                triton.cdiv(batch_size, meta['BLOCK_SIZE_BATCH']),\n                triton.cdiv(linear_out.size(1), meta['BLOCK_SIZE_FEATURE'])\n            )\n            \n            fused_forward_kernel[grid](\n                linear_out,\n                self.bn.weight,\n                self.bn.bias,\n                self.bn.running_mean,\n                self.bn.running_var,\n                self.bias,\n                self.divide_value,\n                output,\n                batch_size,\n                linear_out.size(1),\n                self.bn.eps,\n                linear_out.stride(0), linear_out.stride(1),\n                output.stride(0), output.stride(1),\n                BLOCK_SIZE_BATCH=32,\n                BLOCK_SIZE_FEATURE=128\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr, \n    weight_ptr, \n    total_scale_ptr, \n    total_shift_ptr, \n    output_ptr,\n    stride_x0, stride_x1,\n    stride_w0, stride_w1,\n    stride_out0, stride_out1,\n    batch_size, \n    in_features, \n    out_features,\n    BLOCK_ROW: tl.constexpr,\n    BLOCK_FEATURE: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_feature = tl.program_id(1)\n    \n    row_start = pid_batch * BLOCK_ROW\n    feature_start = pid_feature * BLOCK_FEATURE\n    \n    row_offsets = row_start + tl.arange(0, BLOCK_ROW)\n    feature_offsets = feature_start + tl.arange(0, BLOCK_FEATURE)\n    \n    row_mask = row_offsets < batch_size\n    feature_mask = feature_offsets < out_features\n    \n    acc = tl.zeros((BLOCK_ROW, BLOCK_FEATURE), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_K):\n        k_offsets = k + tl.arange(0, BLOCK_K)\n        k_mask = k_offsets < in_features\n        \n        x_ptrs = x_ptr + row_offsets[:, None] * stride_x0 + k_offsets[None, :] * stride_x1\n        x_vals = tl.load(x_ptrs, mask=row_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        w_ptrs = weight_ptr + feature_offsets[:, None] * stride_w0 + k_offsets[None, :] * stride_w1\n        w_vals = tl.load(w_ptrs, mask=feature_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        acc += tl.dot(x_vals, w_vals, allow_tf32=True)\n    \n    ts_ptrs = total_scale_ptr + feature_offsets\n    total_scale = tl.load(ts_ptrs, mask=feature_mask, other=0.0)\n    \n    tsh_ptrs = total_shift_ptr + feature_offsets\n    total_shift = tl.load(tsh_ptrs, mask=feature_mask, other=0.0)\n    \n    output_block = acc * total_scale[None, :] + total_shift[None, :]\n    output_block = output_block * tl.sigmoid(output_block)\n    \n    out_ptrs = output_ptr + row_offsets[:, None] * stride_out0 + feature_offsets[None, :] * stride_out1\n    tl.store(out_ptrs, output_block, mask=row_mask[:, None] & feature_mask[None, :])\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n\n    def forward(self, x):\n        with torch.no_grad():\n            scale_bn = self.bn.weight / torch.sqrt(self.bn.running_var + self.bn.eps)\n            shift_bn = self.bn.bias - self.bn.running_mean * scale_bn\n            total_scale = scale_bn / self.divide_value\n            total_shift = (self.matmul.bias * scale_bn + shift_bn + self.bias) / self.divide_value\n\n        x = x.contiguous()\n        batch_size, _ = x.shape\n        output = torch.empty((batch_size, self.matmul.out_features), device=x.device, dtype=x.dtype)\n        \n        grid = lambda meta: (\n            triton.cdiv(batch_size, meta['BLOCK_ROW']), \n            triton.cdiv(self.matmul.out_features, meta['BLOCK_FEATURE'])\n        )\n        \n        fused_forward_kernel[grid](\n            x, \n            self.matmul.weight, \n            total_scale, \n            total_shift, \n            output,\n            x.stride(0), x.stride(1),\n            self.matmul.weight.stride(0), self.matmul.weight.stride(1),\n            output.stride(0), output.stride(1),\n            batch_size, \n            x.size(1), \n            self.matmul.out_features,\n            BLOCK_ROW=32, \n            BLOCK_FEATURE=32, \n            BLOCK_K=64\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 21.9000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_forward_kernel(\n    linear_out_ptr,\n    bn_weight_ptr,\n    bn_bias_ptr,\n    running_mean_ptr,\n    running_var_ptr,\n    model_bias_ptr,\n    divide_value,\n    output_ptr,\n    batch_size,\n    out_features,\n    bn_eps,\n    stride_lo0, stride_lo1,\n    stride_out0, stride_out1,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEATURE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_feature = tl.program_id(1)\n    \n    batch_idx = pid_batch * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feature_idx = pid_feature * BLOCK_SIZE_FEATURE + tl.arange(0, BLOCK_SIZE_FEATURE)\n    \n    batch_mask = batch_idx < batch_size\n    feature_mask = feature_idx < out_features\n    block_mask = batch_mask[:, None] & feature_mask[None, :]\n    \n    # Calculate offsets for linear_out access\n    lo_offsets = batch_idx[:, None] * stride_lo0 + feature_idx[None, :] * stride_lo1\n    linear_out = tl.load(linear_out_ptr + lo_offsets, mask=block_mask, other=0.0)\n    \n    # Load batchnorm parameters for the feature block\n    bn_weight = tl.load(bn_weight_ptr + feature_idx, mask=feature_mask, other=0.0)\n    bn_bias = tl.load(bn_bias_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_mean = tl.load(running_mean_ptr + feature_idx, mask=feature_mask, other=0.0)\n    running_var = tl.load(running_var_ptr + feature_idx, mask=feature_mask, other=0.0)\n    \n    # Compute batchnorm\n    scale = bn_weight / tl.sqrt(running_var + bn_eps)\n    bn_out = (linear_out - running_mean[None, :]) * scale[None, :] + bn_bias[None, :]\n    \n    # Add model bias and divide\n    model_bias = tl.load(model_bias_ptr)\n    normalized = (bn_out + model_bias) / divide_value\n    \n    # Swish activation\n    swish_out = normalized * tl.sigmoid(normalized)\n    \n    # Store results\n    out_offsets = batch_idx[:, None] * stride_out0 + feature_idx[None, :] * stride_out1\n    tl.store(output_ptr + out_offsets, swish_out, mask=block_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        \n    def forward(self, x):\n        linear_out = self.matmul(x)\n        \n        if self.training:\n            x = self.bn(linear_out)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            # Inference path with fused Triton kernel\n            batch_size, _ = linear_out.shape\n            output = torch.empty_like(linear_out)\n            \n            grid = lambda meta: (\n                triton.cdiv(batch_size, meta['BLOCK_SIZE_BATCH']),\n                triton.cdiv(linear_out.size(1), meta['BLOCK_SIZE_FEATURE'])\n            )\n            \n            fused_forward_kernel[grid](\n                linear_out,\n                self.bn.weight,\n                self.bn.bias,\n                self.bn.running_mean,\n                self.bn.running_var,\n                self.bias,\n                self.divide_value,\n                output,\n                batch_size,\n                linear_out.size(1),\n                self.bn.eps,\n                linear_out.stride(0), linear_out.stride(1),\n                output.stride(0), output.stride(1),\n                BLOCK_SIZE_BATCH=32,\n                BLOCK_SIZE_FEATURE=128\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_im, stride_ik,\n    stride_wn, stride_wk,\n    stride_bn,\n    stride_om, stride_on,\n    divide_value: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    input_ptrs = input_ptr + offs_m[:, None] * stride_im + offs_k[None, :] * stride_ik\n    weight_ptrs = weight_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        a = tl.load(input_ptrs, mask=k_mask[None, :] & (offs_m[:, None] < M), other=0.0)\n        b = tl.load(weight_ptrs, mask=k_mask[None, :] & (offs_n[:, None] < N), other=0.0)\n        acc += tl.dot(a, tl.trans(b))\n        input_ptrs += BLOCK_K * stride_ik\n        weight_ptrs += BLOCK_K * stride_wk\n\n    bias_vals = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += bias_vals[None, :]\n    acc = acc / divide_value\n    sig = tl.sigmoid(acc)\n    acc = acc * sig\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    output_ptrs = output_ptr + stride_om * offs_m[:, None] + stride_on * offs_n[None, :]\n    output_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(output_ptrs, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def _precompute_fused_params(self):\n        weight = self.matmul.weight.data\n        linear_bias = self.matmul.bias.data if self.matmul.bias is not None else 0\n        bn_weight = self.bn.weight.data\n        bn_bias = self.bn.bias.data\n        running_mean = self.bn.running_mean\n        running_var = self.bn.running_var\n        eps = self.bn.eps\n\n        denom = 1.0 / torch.sqrt(running_var + eps)\n        fused_w = weight * (bn_weight * denom).view(-1, 1)\n        fused_b = (linear_bias - running_mean) * (bn_weight * denom) + bn_bias + self.bias\n        return fused_w, fused_b\n\n    def forward(self, x):\n        if self.training:\n            x = self.matmul(x)\n            x = self.bn(x)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            if self.fused_weight is None or self.fused_bias is None:\n                self.fused_weight, self.fused_bias = self._precompute_fused_params()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[0]\n            output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n            \n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\n            _forward_kernel[grid](\n                x, self.fused_weight, self.fused_bias, output,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                self.fused_bias.stride(0),\n                output.stride(0), output.stride(1),\n                self.divide_value,\n                BLOCK_M=64, BLOCK_N=64, BLOCK_K=32\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program, which is the top-performing one (score 25.8, speed_up 1.058). However, we are to try to improve it further.\n\nThe current program uses a fused kernel that combines:\n  - Matrix multiplication (from the fused linear layer)\n  - Adding the fused bias (which includes the linear bias, batch norm, and an extra bias)\n  - Dividing by a constant\n  - Applying the Swish activation (x * sigmoid(x))\n\nThe kernel is called only during inference. During training, it uses the standard PyTorch operations.\n\nThe current kernel uses:\n  - BLOCK_M=64, BLOCK_N=64, BLOCK_K=32\n\nWe are using an H100 (SM8.9) which has Tensor Cores (FP16 32x16x16). However, note that the kernel uses tl.dot which uses Tensor Cores when the inputs are in fp16 and the block sizes are multiples of 16.\n\nThe current kernel uses:\n  - Input and weight in fp16? (The problem says the hardware has FP16 Tensor Cores. The model might be in fp16 or fp32. We should check the input dtype. However, the Triton kernel uses tl.float32 for accumulation. We are not using the tensor cores to the fullest because the accumulation is in fp32 and the block sizes are 64x64x32, which are multiples of 16, so that's good.)\n\nPossible improvements:\n\n1. Tune the block sizes: The current BLOCK_M=64, BLOCK_N=64, BLOCK_K=32. We might try different block sizes to better utilize the Tensor Cores. For example, 128x128x32 or 64x128x32 might be better. However, note that the kernel uses a 2D grid (with pid_m and pid_n) and the total number of threads is (M/BLOCK_M) * (N/BLOCK_N). We must be cautious because too many blocks might not be efficient.\n\n2. Use fp16 for the inputs and weights? The kernel currently loads as float32? Actually, the Triton kernel uses `tl.float32` for the accumulator. The inputs and weights are stored in the tensors that are passed. The model might be in fp16. We don't change the dtype of the model.\n\n3. We note that the kernel does:\n   - Matrix multiplication with accumulation in fp32.\n   - Then adds a bias (broadcast over rows) and then divides by a constant and applies the sigmoid.\n\n4. We might try to use the following:\n   - Increase the block sizes to 128 in M and/or N dimension to increase the work per thread block and reduce the grid size.\n\n5. Another idea: use the `allow_tf32` flag in the dot product? The H100 supports tf32. But note: tf32 is for inputs that are in fp32? Our inputs might be in fp16. So we cannot use tf32. We should use fp16 tensor cores.\n\n   Actually, the Triton dot operation automatically uses tensor cores when the inputs are in fp16 and the block sizes are multiples of 16. We are using multiples of 16 (64,64,32). But note: the kernel currently loads the inputs without specifying a type. The type of `a` and `b` is inferred from the pointers. We are passing `x` and `self.fused_weight` which are the same as the input. The input might be fp16? The problem says the hardware has FP16 tensor cores.\n\n   We should check: the `_forward_kernel` does not specify the dtype for the loads. It uses the dtype of the pointers. If the tensors are in fp16, then the dot product will use tensor cores.\n\n   However, the current program uses `dtype=x.dtype` for the output. The input `x` might be in fp16? The problem does not specify. We should assume that the input can be either fp16 or fp32.\n\n   We can try to force the kernel to use fp16 for the matrix multiplication if the input is fp16? But note: the kernel uses `tl.float32` for the accumulator. That is correct.\n\n   We are already using tensor cores if the input is fp16 and the block sizes are multiples of 16.\n\n6. We might try to change the block sizes to 128x128x32 or 128x64x32 to see if that improves performance.\n\n   However, note that the kernel uses shared memory. The shared memory usage is (BLOCK_M * BLOCK_K + BLOCK_K * BLOCK_N) * dtype_size. For fp16, that would be (128*32 + 32*128)*2 = (4096+4096)*2 = 16384 bytes per block. The H100 has 228 KB of shared memory per SM, so we can have up to 228*1024/16384 = 14.25 blocks per SM. This is acceptable.\n\n   Alternatively, we might try 64x128x32: (64*32+128*32)*2 = (2048+4096)*2 = 12288 bytes -> even more blocks.\n\n   But note: the grid is 1D? Actually, we have a 1D grid of size (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N)). We are mapping the 2D problem to 1D. This is standard.\n\n   We can try to use autotuning to select the best block sizes.\n\n   However, the current code does not use autotuning. We are passing fixed block sizes (64,64,32). We can introduce autotuning to select the best block sizes for the given M, N, K.\n\n   But note: the problem says to make small iterative steps (<=20 lines per revision). We are only allowed to change up to 20 lines.\n\n   Alternatively, we can try one set of block sizes that might be better.\n\n   According to the Triton documentation, for matrix multiplication, the best block sizes for H100 are often 128x128x32.\n\n7. Another idea: we are doing the entire computation (matmul, bias, scale, activation) in one kernel. But note that the Swish activation is applied after the matmul and bias. We are already fusing everything.\n\n8. We note that the current kernel does the bias addition after the matmul. Then we do:\n   acc = acc / divide_value\n   sig = tl.sigmoid(acc)\n   acc = acc * sig\n\n   We are doing two passes over the accumulator: one for the division and then the sigmoid and then the multiplication.\n\n   We might try to fuse the division into the bias? But note: the bias is added and then we divide by a constant. We could pre-divide the bias by the constant? But then the matmul result would also have to be divided. We are doing:\n\n      acc = (matmul_result + bias) / divide_value\n      then Swish.\n\n   We could pre-divide the bias by the constant and then do:\n\n      acc = matmul_result/divide_value + bias/divide_value   [but note: we are doing (matmul_result + bias)/divide_value]\n\n   However, that doesn't change the number of operations.\n\n   Alternatively, we could do:\n\n      acc = (matmul_result + bias) * (1.0/divide_value)\n\n   and then the Swish.\n\n   That is what we are already doing.\n\n   There isn't an obvious way to remove the division.\n\n9. We might try to use the `tl.sigmoid` function more efficiently? But it's already a single instruction.\n\n10. We are currently using a 2D grid for the matmul. This is standard.\n\n11. We might try to change the order of the loops? The current kernel uses a loop over K and does a dot product for each block. This is standard.\n\n12. We note that the current kernel does:\n\n        for k in range(0, tl.cdiv(K, BLOCK_K)):\n            ... load a block of A and B ...\n            acc += tl.dot(a, tl.trans(b))\n\n    We are transposing b? Actually, the weight matrix is stored in the shape [N, K]. We are loading a block of the weight matrix of size [BLOCK_N, BLOCK_K]. Then we transpose it to [BLOCK_K, BLOCK_N] for the dot product? But the dot product expects the second matrix to be of shape [BLOCK_K, BLOCK_N]? Actually, the Triton `dot` operation expects two 2D tensors and does: (BLOCK_M, BLOCK_K) dot (BLOCK_K, BLOCK_N) -> (BLOCK_M, BLOCK_N). So we don't need to transpose? \n\n    Wait, the kernel currently does:\n\n        b = tl.load(weight_ptrs, ...)   # [BLOCK_N, BLOCK_K] ? \n        then: acc += tl.dot(a, tl.trans(b))\n\n    This would be: (BLOCK_M, BLOCK_K) dot (BLOCK_K, BLOCK_N) -> (BLOCK_M, BLOCK_N). So it is correct.\n\n    But note: we could store the weight matrix in transposed form? Then we wouldn't need to transpose it in the kernel. However, the weight matrix comes from the linear layer which is [out_features, in_features]. We cannot change that.\n\n    Alternatively, we could change the way we load the weight matrix to get it in [BLOCK_K, BLOCK_N]? But that would require a different indexing.\n\n    However, the current indexing for the weight matrix is:\n\n        weight_ptrs = weight_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk\n\n    This gives a block of [BLOCK_N, BLOCK_K]. We then transpose it to [BLOCK_K, BLOCK_N] for the dot product.\n\n    We could instead load the weight matrix in a transposed way? But that would require a different stride.\n\n    We note that the stride_wn is the stride for the n dimension? Actually, the weight matrix is stored as [N, K], so stride_wn is the stride for the first dimension? Then stride_wn = weight.stride(0) and stride_wk = weight.stride(1). \n\n    We could change the kernel to load the weight matrix in a block of [BLOCK_K, BLOCK_N] by swapping the offsets? But that would require transposing the weight matrix, which we cannot do because it is provided as is.\n\n    Therefore, we leave it.\n\n13. Another idea: we are using the same BLOCK_K for both the input and weight. We might try to use a different blocking strategy? Probably not.\n\n14. We might try to use vectorized loads? The current kernel does:\n\n        a = tl.load(input_ptrs, mask=k_mask[None, :] & (offs_m[:, None] < M), other=0.0)\n        b = tl.load(weight_ptrs, mask=k_mask[None, :] & (offs_n[:, None] < N), other=0.0)\n\n    We are not using vectorized loads. We could try to load multiple elements at a time. For example, if we are loading from global memory, we can use vectorized loads to reduce the number of instructions.\n\n    We can change the kernel to use vectorized loads for the input and weight matrices. We would need to change the offsets and the block shapes accordingly.\n\n    However, note that the dot product operation requires the data to be in a 2D tensor. We might have to adjust the block shapes to be multiples of the vector size.\n\n    Let's try to use a vector size of 4 for the K dimension? We would change:\n\n        BLOCK_K -> BLOCK_K * 4? Actually, we can split the BLOCK_K dimension into two: [BLOCK_K // 4, 4] and then use vectorized load of 4 elements.\n\n    But the Triton dot operation doesn't care about the inner dimension? Actually, we would have to reshape the data.\n\n    Alternatively, we can use the `tl.make_block_ptr` and `tl.advance` to use vectorized loads? That might be complicated.\n\n    Given the constraint of 20 lines, we might not be able to do a full vectorization.\n\n15. We note that the current kernel does not use any shared memory explicitly. The `tl.dot` operation uses shared memory internally.\n\n    We are relying on the Triton compiler to optimize the memory access.\n\n16. We might try to use the following block sizes: BLOCK_M=128, BLOCK_N=64, BLOCK_K=32. This might balance the workload.\n\n    Why? Because 128x64 is 8192 elements per block, and 64x64 is 4096. We want to have enough work per thread block.\n\n    But note: the grid size will be (ceil(M/128) * ceil(N/64)). Previously it was (ceil(M/64) * ceil(N/64)). So the grid might be smaller.\n\n    We try changing the block sizes to 128 and 64.\n\n    We change the kernel call:\n\n        _forward_kernel[grid]( ... , BLOCK_M=128, BLOCK_N=64, BLOCK_K=32)\n\n    We hope that this will increase the work per thread block and reduce the grid size, which might be more efficient.\n\n    But note: the kernel uses shared memory for the dot product. The shared memory usage for the two matrices would be:\n\n        A: [BLOCK_M, BLOCK_K] -> 128*32 * sizeof(dtype) = 128*32*2 (if fp16) = 8192 bytes\n        B: [BLOCK_N, BLOCK_K] -> 64*32*2 = 4096 bytes\n        Total: 12288 bytes.\n\n    This is less than the 128x128x32 case (which was 16384) and still within the shared memory limits.\n\n17. Alternatively, we could use autotuning to select the best block sizes. But we are limited to 20 lines.\n\n    We can introduce an autotuner that selects between a few configurations.\n\n    However, the current program does not use autotuning. We are going to try one change: use 128x64x32.\n\n    We change the block sizes and see if it improves.\n\n    We note that the current performance is 0.104 ms (custom) vs 0.11 ms (reference) -> speedup 1.058.\n\n    We hope that by increasing the block sizes we can get a better speedup.\n\n18. We also note that the kernel does:\n\n        offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n        offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    and then later:\n\n        offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n        offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    We are recalculating the same offsets? We can reuse the ones from the beginning? But note: we are using them in the store part. We can save them in variables.\n\n    However, the current kernel recalculates them. It doesn't cost much.\n\nGiven the above, we will try:\n\n   Change the block sizes to BLOCK_M=128, BLOCK_N=64, BLOCK_K=32.\n\nWe change only two lines: the call to the kernel at the end of the forward pass.\n\nBut note: the kernel itself uses BLOCK_M, BLOCK_N, BLOCK_K as constexpr. We are passing them as kernel parameters.\n\nWe change:\n\n    BLOCK_M=64, BLOCK_N=64, BLOCK_K=32   -> to \n    BLOCK_M=128, BLOCK_N=64, BLOCK_K=32\n\nWe hope that this will improve the performance.\n\nWe note that the grid function is:\n\n    grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\n\nWe are using the new block sizes.\n\nLet's write the new code.\n\nWe must be cautious: the kernel uses the block sizes for the masking. The masking is done correctly.\n\nWe change only the block sizes in the kernel call.\n\nAlternatively, we might try to change the block sizes to 128 and 128? But that would double the shared memory usage? And we might run into shared memory limits? But note: 128x128x32 for fp16:\n\n    A: 128*32*2 = 8192\n    B: 128*32*2 = 8192\n    Total: 16384 bytes per block -> 16 KB. The H100 has 228 KB per SM, so we can have 14 blocks per SM. This is acceptable.\n\nBut the grid size would be smaller: (ceil(M/128) * ceil(N/128)) vs (ceil(M/64)*ceil(N/64)) which is 4 times fewer blocks? \n\nWe try 128x128x32.\n\nBut note: the problem says to change <=20 lines. We are changing one line.\n\nWe try two versions? But we can only submit one.\n\nWe choose 128x128x32 because it is a common recommendation.\n\nAlternatively, we can try both and choose the best? But we are only allowed one change.\n\nWe decide to change to 128x128x32.\n\nTherefore, in the kernel call in the forward pass, we change:\n\n    BLOCK_M=128, BLOCK_N=128, BLOCK_K=32\n\nBut note: the kernel uses:\n\n    offs_m = ... and offs_n = ... with the new block sizes.\n\n    The masking: (offs_m[:, None] < M) and (offs_n[None, :] < N) is still correct.\n\nWe change the call.\n\nWe also note that the grid function uses the new block sizes because we are passing them as parameters.\n\nTherefore, the change is:\n\n    In the forward pass (inference branch) in the ModelNew.forward method:\n\n        _forward_kernel[grid](\n            ...,\n            BLOCK_M=128, BLOCK_N=128, BLOCK_K=32   # changed from 64,64,32\n        )\n\nWe hope this will increase the speedup.\n\nBut note: if the problem size (M, N) is not divisible by 128, we might have more wasted threads? But the masking is there.\n\nWe try.\n\nWe output the entire code block with this change.\n\nWe are only changing one line (the kernel call) so we are within the 20 lines limit.\n\nLet's write the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_im, stride_ik,\n    stride_wn, stride_wk,\n    stride_bn,\n    stride_om, stride_on,\n    divide_value: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    input_ptrs = input_ptr + offs_m[:, None] * stride_im + offs_k[None, :] * stride_ik\n    weight_ptrs = weight_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        a = tl.load(input_ptrs, mask=k_mask[None, :] & (offs_m[:, None] < M), other=0.0)\n        b = tl.load(weight_ptrs, mask=k_mask[None, :] & (offs_n[:, None] < N), other=0.0)\n        acc += tl.dot(a, tl.trans(b))\n        input_ptrs += BLOCK_K * stride_ik\n        weight_ptrs += BLOCK_K * stride_wk\n\n    bias_vals = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += bias_vals[None, :]\n    acc = acc / divide_value\n    sig = tl.sigmoid(acc)\n    acc = acc * sig\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    output_ptrs = output_ptr + stride_om * offs_m[:, None] + stride_on * offs_n[None, :]\n    output_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(output_ptrs, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def _precompute_fused_params(self):\n        weight = self.matmul.weight.data\n        linear_bias = self.matmul.bias.data if self.matmul.bias is not None else 0\n        bn_weight = self.bn.weight.data\n        bn_bias = self.bn.bias.data\n        running_mean = self.bn.running_mean\n        running_var = self.bn.running_var\n        eps = self.bn.eps\n\n        denom = 1.0 / torch.sqrt(running_var + eps)\n        fused_w = weight * (bn_weight * denom).view(-1, 1)\n        fused_b = (linear_bias - running_mean) * (bn_weight * denom) + bn_bias + self.bias\n        return fused_w, fused_b\n\n    def forward(self, x):\n        if self.training:\n            x = self.matmul(x)\n            x = self.bn(x)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            if self.fused_weight is None or self.fused_bias is None:\n                self.fused_weight, self.fused_bias = self._precompute_fused_params()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[0]\n            output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n            \n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\n            _forward_kernel[grid](\n                x, self.fused_weight, self.fused_bias, output,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                self.fused_bias.stride(0),\n                output.stride(0), output.stride(1),\n                self.divide_value,\n                BLOCK_M=128, BLOCK_N=128, BLOCK_K=32\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
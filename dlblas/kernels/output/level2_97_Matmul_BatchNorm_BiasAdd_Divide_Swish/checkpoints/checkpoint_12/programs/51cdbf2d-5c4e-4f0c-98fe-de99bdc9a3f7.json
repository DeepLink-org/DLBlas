{"id": "51cdbf2d-5c4e-4f0c-98fe-de99bdc9a3f7", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_im, stride_ik,\n    stride_wn, stride_wk,\n    stride_bn,\n    stride_om, stride_on,\n    divide_value: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    input_ptrs = input_ptr + offs_m[:, None] * stride_im + offs_k[None, :] * stride_ik\n    weight_ptrs = weight_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        a = tl.load(input_ptrs, mask=k_mask[None, :] & (offs_m[:, None] < M), other=0.0)\n        b = tl.load(weight_ptrs, mask=k_mask[None, :] & (offs_n[:, None] < N), other=0.0)\n        acc += tl.dot(a, tl.trans(b))\n        input_ptrs += BLOCK_K * stride_ik\n        weight_ptrs += BLOCK_K * stride_wk\n\n    bias_vals = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += bias_vals[None, :]\n    acc = acc / divide_value\n    sig = tl.sigmoid(acc)\n    acc = acc * sig\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    output_ptrs = output_ptr + stride_om * offs_m[:, None] + stride_on * offs_n[None, :]\n    output_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(output_ptrs, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def _precompute_fused_params(self):\n        weight = self.matmul.weight.data\n        linear_bias = self.matmul.bias.data if self.matmul.bias is not None else 0\n        bn_weight = self.bn.weight.data\n        bn_bias = self.bn.bias.data\n        running_mean = self.bn.running_mean\n        running_var = self.bn.running_var\n        eps = self.bn.eps\n\n        denom = 1.0 / torch.sqrt(running_var + eps)\n        fused_w = weight * (bn_weight * denom).view(-1, 1)\n        fused_b = (linear_bias - running_mean) * (bn_weight * denom) + bn_bias + self.bias\n        return fused_w, fused_b\n\n    def forward(self, x):\n        if self.training:\n            x = self.matmul(x)\n            x = self.bn(x)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            if self.fused_weight is None or self.fused_bias is None:\n                self.fused_weight, self.fused_bias = self._precompute_fused_params()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[0]\n            output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n            \n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\n            _forward_kernel[grid](\n                x, self.fused_weight, self.fused_bias, output,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                self.fused_bias.stride(0),\n                output.stride(0), output.stride(1),\n                self.divide_value,\n                BLOCK_M=64, BLOCK_N=64, BLOCK_K=32\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "cf1435ee-8f6b-491c-9136-8a7f56d4f465", "generation": 1, "timestamp": 1754587770.6327438, "iteration_found": 3, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 25.800000000000004, "speed_up": 1.058, "runtime_ms": 0.104, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.11, "speed_up": 1.058, "custom_timing": 0.104}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.108, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.11, "speed_up": 1.019, "custom_timing": 0.108}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.1080\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.019, 'custom_timing': 0.108}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.1080, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.019, 'custom_timing': 0.108}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, batch normalization, bias addition, division, and Swish activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = self.bn(x)\n        x = x + self.bias\n        x = x / self.divide_value\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.1080), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.019, 'custom_timing': 0.108})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, batch normalization, bias addition, division, and Swish activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = self.bn(x)\n        x = x + self.bias\n        x = x / self.divide_value\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear transformation (matmul)\n  2. BatchNorm1d\n  3. Add a bias (broadcasted)\n  4. Divide by a constant\n  5. Swish activation (x * sigmoid(x))\n\nThe current implementation uses standard PyTorch modules (nn.Linear, nn.BatchNorm1d) and operations.\n\nWe are to rewrite this using Triton for performance. However, note that the entire model is a sequence of operations. \nWe cannot replace the entire model with one Triton kernel because the operations are complex and include state (like BatchNorm).\n\nBut note: the BatchNorm layer has running mean and variance that are updated during training. However, in inference mode, these are fixed.\n\nThe problem says we are to generate Triton implementations. But the current code is entirely in PyTorch. \n\nWe must consider:\n- The task is to rewrite the forward function of the model to use Triton kernels for performance.\n- We are allowed to break down the operations and fuse them where possible.\n\nHowever, note the constraints:\n  \u274c NEVER change @triton.jit function signatures or parameter names (if we define a kernel, we must keep the signature)\n  \u274c NEVER modify grid configuration, output tensor shapes, or PID logic (so our kernel launch must be with the correct grid and the output must be the same shape)\n\nLooking at the operations, the most compute-intensive part is the linear layer and the following operations. \nWe can try to fuse the entire forward pass into one kernel? But that might be too complex.\n\nAlternatively, we can write a Triton kernel for the linear layer (which is already highly optimized in PyTorch) but note that we are allowed to use Triton's matrix multiplication capabilities.\n\nHowever, the problem says: \"Generate functionally equivalent but faster Triton implementations (custom_code)\".\n\nGiven the complexity, we might focus on the linear layer first? But note the BatchNorm and the rest are pointwise and could be fused.\n\nBut the BatchNorm has parameters (weight and bias) and also running mean and variance. In inference, we can fold the BatchNorm parameters into the linear layer? \n\nIdea for inference:\n  We can fuse the linear layer and the BatchNorm into a single linear transformation. Then we can write a Triton kernel for the fused linear layer and then do the rest (add bias, divide, swish) in a fused pointwise kernel.\n\nHowever, note that the BatchNorm in the current model is applied on the output of the linear layer. The BatchNorm1d layer has:\n  - running_mean, running_var (fixed in inference)\n  - weight (gamma) and bias (beta) for affine transformation.\n\nThe formula for BatchNorm in inference is:\n  y = (x - running_mean) / sqrt(running_var + eps) * weight + bias\n\nSo the entire linear operation followed by BatchNorm can be written as:\n  Let W_linear and b_linear be the weight and bias of the linear layer (if any). But note: the current linear layer (nn.Linear) has a bias? Yes, by default. However, in the current model we are using nn.Linear and then nn.BatchNorm1d. It is common to set the linear layer's bias to False when followed by BatchNorm, but the current model does not.\n\nSo the current model does:\n  x = input @ W_linear.T + b_linear   [linear]\n  x = (x - running_mean) / sqrt(running_var + eps) * weight_bn + bias_bn   [BatchNorm]\n\nThen we add a separate bias (self.bias) and then divide by a constant and then swish.\n\nWe can precompute a new weight and bias for a fused linear layer that includes the linear and the BatchNorm:\n\n  Let:\n      W_fused = (W_linear.T * weight_bn) / sqrt(running_var + eps)   [shape: out_features x in_features]\n      b_fused = (b_linear - running_mean) * weight_bn / sqrt(running_var + eps) + bias_bn\n\n  Then: \n      x = input @ W_fused.T + b_fused\n\nBut note: the BatchNorm is applied per feature (over the batch), so the running_mean and running_var are vectors of size out_features. The weight_bn and bias_bn are also of size out_features.\n\nHowever, the current model also has an additional bias (self.bias) and then a division and then swish.\n\nWe can further fuse:\n  After the fused linear layer, we do:\n      x = x + self.bias   [broadcasted, note self.bias has shape (1,)]\n      x = x / self.divide_value\n      x = x * torch.sigmoid(x)\n\nBut note: the self.bias is a parameter of shape (1,). So it's a scalar? Then it's added to every element.\n\nAlternatively, we can write a kernel that does:\n  - The fused linear layer (with BatchNorm) and then the rest in one kernel? \n\nBut note: the input to the kernel would be the input tensor, and we would need to pass:\n  - The weight matrix (for linear) and the parameters for BatchNorm and the other operations.\n\nGiven the complexity and the fact that we are in an evolution phase, we might start by writing a Triton kernel for the linear layer only? But note that the linear layer in PyTorch is already highly optimized (using cuBLAS). So we might not gain much by writing a Triton kernel for it.\n\nAlternatively, we can try to fuse the entire sequence of operations (linear, batchnorm, add bias, divide, swish) into one kernel. This would avoid multiple passes over the data.\n\nSteps for the fused kernel:\n\n  For each row (example in the batch) and each column (feature), we do:\n\n    x = input[row, :]   [vector of in_features]\n\n    # Linear\n    out = 0\n    for k in range(in_features):\n        out += x[k] * W_linear[col, k]\n\n    out += b_linear[col]   [if the linear layer has bias? In the current model, it does. But note: the BatchNorm also has bias? And then we add another bias?]\n\n    # BatchNorm: \n    out = (out - running_mean[col]) / (sqrt(running_var[col] + eps)) * weight_bn[col] + bias_bn[col]\n\n    # Then add the model's bias (which is a scalar) -> out += self.bias[0]\n    out += self.bias[0]\n\n    # Then divide by a constant: out /= self.divide_value\n\n    # Then Swish: out = out * sigmoid(out)\n\nBut note: the BatchNorm in the current model is applied per feature and across the batch. However, in inference, the running_mean and running_var are fixed. So we can precompute the parameters for the fused operation per feature.\n\nHowever, the kernel would have to do:\n  - A matrix multiplication (for the linear part) and then a per-feature affine transformation and then the pointwise operations.\n\nBut the matrix multiplication is the expensive part. We can use a Triton kernel that does the matrix multiplication and then the rest in the same kernel? \n\nWe can break the kernel into two parts? \n  1. A kernel for the fused linear and BatchNorm? \n  2. Then a pointwise kernel for the rest? \n\nBut to maximize performance, we want to do everything in one kernel.\n\nGiven the constraints on the problem (we are to generate Triton code) and the fact that the current code is entirely in PyTorch, we must replace the entire forward with a Triton kernel that does all the operations.\n\nBut note: the BatchNorm has state (running_mean and running_var) that are updated during training. However, in inference, these are fixed. We are only concerned with inference? The problem does not specify, but the evaluation pipeline does not mention training. We assume inference.\n\nLet me reexamine the problem: \n  \"Evaluation pipeline: 1) /verify_plus - Verify compilation & correctness 2) /execute (optional) - Return terminal logs if compilation fails\"\n\nWe are to generate code that is functionally equivalent. So we must preserve the behavior for both training and inference? \n\nBut note: the BatchNorm layer in training mode updates the running statistics. We cannot do that in a single kernel without atomic operations? And that would be inefficient.\n\nTherefore, we must assume that the model is in inference mode? Or we are only required to handle inference? \n\nLooking at the problem statement: it does not specify. However, in the reference code, the model is defined as an nn.Module and the forward function is given. We are to rewrite the forward function.\n\nWe have two options:\n  Option 1: Only support inference mode (i.e., model.eval())\n  Option 2: Try to handle training? But that would require updating the running mean and variance, which is a reduction over the batch. \n\nGiven the complexity and the fact that the current performance is 1.0 (so we are at the reference) and we are to improve, and the problem says \"small iterative steps\", I think we should start by focusing on inference.\n\nWe will assume the model is in inference mode. \n\nSo the plan:\n  Precompute the fused parameters for the linear and BatchNorm:\n      W_fused = (W_linear.T * weight_bn) / sqrt(running_var + eps)   [shape: out_features x in_features]\n      b_fused = (b_linear - running_mean) * weight_bn / sqrt(running_var + eps) + bias_bn\n\n  Then the entire forward becomes:\n      x = input @ W_fused.T + b_fused   [fused linear and BatchNorm]\n      x = x + self.bias   [scalar]\n      x = x / self.divide_value\n      x = x * torch.sigmoid(x)\n\n  Now, we can write a Triton kernel that does:\n      - The matrix multiplication (input @ W_fused.T) and then adds the bias (b_fused) and then the rest in one go.\n\n  However, note that the matrix multiplication is the dominant cost. We can use Triton's matrix multiplication with a block size and then for each row and column, we do the rest.\n\n  We can break the kernel into two parts? \n      Part1: The matrix multiplication (with fused bias for b_fused) -> output a temporary matrix? \n      Part2: Then a pointwise kernel for the rest? \n\n  But we want to avoid writing intermediate tensors. So we do everything in one kernel? \n\n  Alternatively, we can fuse the entire operation in one kernel: \n      For each row i and each column j, we compute the dot product of input[i] and W_fused[j], then add b_fused[j], then add self.bias, then divide, then apply swish.\n\n  This is a classic fused matrix multiplication followed by pointwise operations.\n\n  We can write a Triton kernel for the matrix multiplication and then the pointwise operations in the same kernel.\n\n  Steps in the kernel:\n      - We'll have a 2D grid: one dimension for the batch (rows) and one for the features (columns).\n      - We load a block of the input and a block of the weight matrix and compute the dot product for a block of the output.\n      - Then we add the fused bias (b_fused) for that column, then add the scalar bias, then divide, then apply swish.\n\n  However, writing an efficient matrix multiplication kernel in Triton is non-trivial. But we have examples.\n\n  Given the constraints (we are at the reference and we are to improve by at least 5%), and the fact that the current reference uses PyTorch's highly optimized linear layer, we must be cautious.\n\n  But note: we are fusing multiple operations and avoiding multiple passes. This can be beneficial.\n\n  We will write a Triton kernel that does the entire forward pass.\n\n  Steps:\n\n      We'll assume:\n          input: [M, K]   (batch_size, in_features)\n          W_fused: [N, K] (out_features, in_features)\n          b_fused: [N]    (out_features)\n\n      We want to compute:\n          output[i, j] = ( (input[i] @ W_fused[j]) + b_fused[j] + bias[0] ) / divide_value * sigmoid( (input[i] @ W_fused[j]) + b_fused[j] + bias[0] ) / divide_value )\n\n      But note: the sigmoid function is applied to the value after adding the bias and before division? \n          Original: \n              x = ... \n              x = x + self.bias\n              x = x / self.divide_value\n              x = x * torch.sigmoid(x)\n\n          So: \n              tmp = (input[i] @ W_fused[j]) + b_fused[j] + self.bias[0]\n              tmp = tmp / self.divide_value\n              output[i, j] = tmp * torch.sigmoid(tmp)\n\n      Alternatively, we can compute:\n          tmp = (input[i] @ W_fused[j]) + b_fused[j] + self.bias[0]\n          tmp = tmp / self.divide_value\n          sig = 1 / (1 + exp(-tmp))\n          output[i, j] = tmp * sig\n\n  However, note: the scalar bias (self.bias) is the same for every element? So we can precompute:\n          total_bias = b_fused[j] + self.bias[0]\n\n  But wait: the self.bias is a scalar (shape (1,)), so it's the same for every j? Then for a fixed j, we have:\n          tmp = (input[i] @ W_fused[j]) + (b_fused[j] + self.bias[0])\n\n  Then we divide by self.divide_value and then apply swish.\n\n  We can precompute a new bias vector for each feature j: \n          bias_total = b_fused + self.bias[0]   [vector of length N]\n\n  Then the operation becomes:\n          tmp = (input[i] @ W_fused[j]) + bias_total[j]\n          tmp = tmp / self.divide_value\n          output[i, j] = tmp * sigmoid(tmp)\n\n  Now, we can write a kernel that does:\n      - Load a block of input (for a set of rows and a set of columns in the input matrix) and a block of weights (for a set of output features and the same set of input columns) and compute the dot products for a block of the output.\n\n  We'll use a typical matrix multiplication kernel with block tiling.\n\n  Steps in the kernel:\n      pid = program_id(axis=0)   # for rows (batch index) and columns (feature index) combined? Or use 2D grid?\n      pid_m = pid // num_blocks_n   # for rows\n      pid_n = pid % num_blocks_n    # for columns\n\n      We then compute a block of the output for rows [pid_m * BLOCK_M : (pid_m+1)*BLOCK_M] and columns [pid_n * BLOCK_N : (pid_n+1)*BLOCK_N].\n\n      We accumulate the dot product for K (in_features) in chunks.\n\n      Then we add the bias_total for the column (for each column in the block) and then do the pointwise operations.\n\n  We must also consider that the input and weight are in contiguous memory.\n\n  However, note: the current model has:\n        batch_size = 128, in_features = 1024, out_features = 512.\n\n  We can choose BLOCK_M and BLOCK_N appropriately.\n\n  We'll write a Triton kernel that does the fused operation.\n\n  Let's define the kernel:\n\n      @triton.jit\n      def _forward_kernel(\n          input_ptr, weight_ptr, bias_ptr, output_ptr,\n          M, N, K,   # M: batch_size, N: out_features, K: in_features\n          stride_im, stride_ik,   # input strides: [M, K] -> [stride_im, stride_ik] -> typically [K, 1] if row-major?\n          stride_wn, stride_wk,   # weight strides: [N, K] -> [stride_wn, stride_wk] -> typically [K, 1] if row-major? But note: we want to access by row (N) and column (K). \n          stride_bn,              # bias strides: [N] -> stride_bn, typically 1\n          stride_om, stride_on,   # output strides: [M, N] -> [stride_om, stride_on] -> typically [N, 1]\n          divide_value: tl.constexpr,\n          BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n      ):\n\n          pid = tl.program_id(0)\n          num_pid_m = tl.cdiv(M, BLOCK_M)   # number of blocks in M dimension\n          num_pid_n = tl.cdiv(N, BLOCK_N)   # number of blocks in N dimension\n          pid_m = pid // num_pid_n\n          pid_n = pid % num_pid_n\n\n          # Now we define the offsets for the block in M and N:\n          offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n          offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n          offs_k = tl.arange(0, BLOCK_K)\n\n          # We'll load the input block and weight block and compute the dot product.\n          # Initialize the accumulator to zero.\n          acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n          # We'll loop over the K dimension in blocks of BLOCK_K.\n          for k in range(0, K, BLOCK_K):\n              # Load input block: [BLOCK_M, BLOCK_K]\n              input_vals = tl.load(input_ptr + offs_m[:, None] * stride_im + (k + offs_k[None, :]) * stride_ik,\n                                   mask=(offs_m[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n              # Load weight block: [BLOCK_N, BLOCK_K] -> but note: we want to multiply by the same K block for the current block.\n              weight_vals = tl.load(weight_ptr + offs_n[:, None] * stride_wn + (k + offs_k[None, :]) * stride_wk,\n                                    mask=(offs_n[:, None] < N) & (offs_k[None, :] < K - k), other=0.0)\n\n              # We want to compute: input_vals (BLOCK_M, BLOCK_K) @ weight_vals (BLOCK_K, BLOCK_N) -> (BLOCK_M, BLOCK_N)\n              # But note: weight_vals is of shape (BLOCK_N, BLOCK_K). We need to transpose it to (BLOCK_K, BLOCK_N) for matrix multiplication.\n              weight_vals = tl.trans(weight_vals)   # now [BLOCK_K, BLOCK_N]\n\n              # Alternatively, we can do:\n              #   acc += tl.dot(input_vals, weight_vals, out_dtype=tl.float32)\n              # But note: the dot product in Triton requires that the second operand is transposed? Actually, the dot product expects two matrices of shape [M, K] and [K, N] to produce [M, N].\n              # Our input_vals is [BLOCK_M, BLOCK_K] and weight_vals is [BLOCK_K, BLOCK_N] (after transpose we have [BLOCK_K, BLOCK_N] but we need [BLOCK_K, BLOCK_N] for the dot product? Actually, the dot product in Triton is:\n              #   dot(a, b) where a is [M, K] and b is [K, N] -> [M, N]\n              # So we don't need to transpose? Because we have weight_vals as [BLOCK_N, BLOCK_K] and we want to multiply by a block of [BLOCK_M, BLOCK_K] and [BLOCK_K, BLOCK_N] -> so we need to transpose weight_vals to [BLOCK_K, BLOCK_N]? \n              # But note: the dot product in Triton does: \n              #   for i in range(M):\n              #       for j in range(N):\n              #           for k in range(K):\n              #               acc[i, j] += a[i, k] * b[k, j]\n              # So we can keep weight_vals as [BLOCK_N, BLOCK_K] and then use tl.dot(input_vals, tl.trans(weight_vals))? \n              # But that would be inefficient. Instead, we can transpose the weight block to [BLOCK_K, BLOCK_N] and then do the dot product without transposing again.\n\n              # Actually, we can do:\n              #   weight_vals_t = tl.trans(weight_vals)   # [BLOCK_K, BLOCK_N]\n              #   acc += tl.dot(input_vals, weight_vals_t)\n\n              # But note: the dot product in Triton expects the second matrix to be of shape [BLOCK_K, BLOCK_N] when the first is [BLOCK_M, BLOCK_K]. \n\n              weight_vals_t = tl.trans(weight_vals)   # [BLOCK_K, BLOCK_N]\n              acc += tl.dot(input_vals, weight_vals_t)\n\n          # Now, after the loop, we have the matrix product for the block [BLOCK_M, BLOCK_N] in acc.\n\n          # Load the bias for the current block of columns [BLOCK_N]\n          bias_vals = tl.load(bias_ptr + offs_n * stride_bn, mask=offs_n < N, other=0.0)\n\n          # Add the bias: for each row in the block, we add the same bias vector (broadcast over rows)\n          #   acc = acc + bias_vals[None, :]   [BLOCK_M, BLOCK_N]\n          bias_vals = bias_vals[None, :]   # [1, BLOCK_N] to broadcast to [BLOCK_M, BLOCK_N]\n          acc += bias_vals\n\n          # Now do: tmp = acc / divide_value\n          acc = acc / divide_value\n\n          # Then compute the swish: acc * sigmoid(acc)\n          sig = tl.sigmoid(acc)\n          acc = acc * sig\n\n          # Now write the output block.\n          offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n          offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n          mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n\n          tl.store(output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on, acc, mask=mask)\n\n  Then, in the forward function of the model, we would:\n      - Precompute the fused weights and biases (if the model is in inference mode) and then call this kernel.\n\n  However, note: the model might be in training mode? But we are assuming inference.\n\n  We must be cautious: the BatchNorm running mean and variance are updated during training. We are not updating them in the kernel. So we must not use this kernel during training? \n\n  We can conditionally use the Triton kernel only in eval mode? And in training mode, we use the original PyTorch operations? \n\n  But the problem does not specify. We are to generate a functionally equivalent implementation. So we must handle both? \n\n  However, the problem says: \"functional equivalence\". We can do:\n\n      If self.training:\n          use the original PyTorch operations (so that the running mean and variance are updated)\n      Else:\n          use the Triton kernel with the precomputed fused parameters.\n\n  But note: the evaluation pipeline only verifies the output for given inputs. It might run in eval mode? We don't know. \n\n  Since the problem states \"functional equivalence\", we must produce the same output for the same input in both modes? \n\n  Alternatively, we can always use the Triton kernel? But then we skip updating the running mean and variance in training? That would be incorrect.\n\n  Therefore, we will do:\n\n      In training mode, we run the original PyTorch operations (so that the BatchNorm updates its running statistics) and then we also compute the output.\n\n      In inference mode, we run the Triton kernel.\n\n  But note: the forward function must be the same. So we must update the running statistics in training? We cannot do that in the Triton kernel.\n\n  So we leave the BatchNorm in the model? But then we cannot fuse it? \n\n  Alternatively, we can update the running statistics in a separate pass? That would be inefficient.\n\n  Given the complexity, and the fact that the problem does not specify the mode, and the current performance is 1.0, we might start by only optimizing the inference mode.\n\n  We'll change the forward function to:\n\n      if self.training:\n          # Use the original method\n          x = self.matmul(x)\n          x = self.bn(x)\n          x = x + self.bias\n          x = x / self.divide_value\n          x = x * torch.sigmoid(x)\n          return x\n      else:\n          # Precompute the fused parameters for the linear and BatchNorm\n          # Then run the Triton kernel for the entire forward.\n\n  But note: the BatchNorm has weight and bias (gamma and beta) and also running_mean and running_var.\n\n  Precomputation for inference:\n\n      # Get the parameters from the linear layer and the BatchNorm layer\n      W_linear = self.matmul.weight   # [out_features, in_features]\n      b_linear = self.matmul.bias     # [out_features] (if bias=True)\n\n      # Get BatchNorm parameters\n      weight_bn = self.bn.weight       # [out_features]\n      bias_bn = self.bn.bias           # [out_features]\n      running_mean = self.bn.running_mean   # [out_features]\n      running_var = self.bn.running_var      # [out_features]\n      eps = self.bn.eps\n\n      # Compute the fused weight and bias:\n      denom = 1.0 / torch.sqrt(running_var + eps)\n      W_fused = W_linear * (weight_bn * denom).view(-1, 1)   # [out_features, in_features] * [out_features, 1] -> broadcast\n      b_fused = (b_linear - running_mean) * (weight_bn * denom) + bias_bn\n\n      # Then add the model's bias (self.bias) to the fused bias? But note: the model's bias is a scalar. So we do:\n      bias_total = b_fused + self.bias[0]   # [out_features]\n\n      # Then call the Triton kernel with:\n      #   input: x [M, K]\n      #   weight: W_fused [N, K]\n      #   bias: bias_total [N]\n      #   output: [M, N]\n\n      # We also need to set the divide_value.\n\n  Then the kernel will do the rest.\n\n  We must also make sure that the tensors are contiguous and in the right data type.\n\n  We'll assume the kernel uses float32? The original model uses float32? We must check.\n\n  The current model: \n        x = torch.randn(batch_size, in_features) -> float32 by default.\n\n  The linear layer and BatchNorm are in float32.\n\n  So we will use float32 in the kernel.\n\n  Steps in the ModelNew.forward:\n\n      if self.training:\n          ... original code ...\n      else:\n          # Precompute fused parameters (if not already done) and then call the kernel.\n\n  But note: the fused parameters depend on the BatchNorm's running statistics and the current parameters, which might change during training. So we cannot precompute them once? \n\n  We can precompute them every time in eval mode? \n\n  Alternatively, we can cache them and update them when the model switches to eval? But that's complex.\n\n  We'll precompute them every time in eval mode.\n\n  Now, we write the kernel launch:\n\n      M, K = x.shape\n      N = self.matmul.out_features\n\n      # Make sure the tensors are contiguous and on the same device.\n      x = x.contiguous()\n      W_fused = W_fused.contiguous()\n      bias_total = bias_total.contiguous()\n\n      # Allocate output tensor: [M, N]\n      output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n\n      # Check the dimensions and strides\n      # We'll use a 1D grid of size (num_blocks_m * num_blocks_n)\n      BLOCK_M = 32\n      BLOCK_N = 32\n      BLOCK_K = 32   # tuning parameters? We can try different values.\n\n      grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n      # We also need to pass the strides.\n\n      _forward_kernel[grid](\n          x, W_fused, bias_total, output,\n          M, N, K,\n          x.stride(0), x.stride(1),\n          W_fused.stride(0), W_fused.stride(1),\n          bias_total.stride(0),\n          output.stride(0), output.stride(1),\n          self.divide_value,\n          BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n      )\n\n      return output\n\n  But note: we must handle the case when the dimensions are not multiples of the block sizes? Our kernel uses masks.\n\n  We'll write the kernel accordingly.\n\n  However, note: the Triton kernel above is a draft and might have errors.\n\n  Given the complexity, we might want to start with a simpler kernel? Or we can rely on Triton's matrix multiplication tutorial.\n\n  Alternatively, we can use the triton.ops.matmul function? But that only does the matrix multiplication. We want to fuse the rest.\n\n  But note: we are allowed to use any Triton code.\n\n  We'll write the kernel as described.\n\n  But note: the problem says to change only the EVOLVE-BLOCK. So we must replace the entire ModelNew class? \n\n  The current EVOLVE-BLOCK contains:\n      class ModelNew(nn.Module)\n      and then the batch_size, in_features, etc. and get_inputs, get_init_inputs.\n\n  We are to rewrite the ModelNew class to use the Triton kernel in inference.\n\n  We must be cautious: the original model has parameters (matmul.weight, matmul.bias, bn.weight, bn.bias, and the self.bias). We must preserve these.\n\n  We'll change the forward function.\n\n  Steps:\n\n      class ModelNew(nn.Module):\n          ... __init__ remains the same ...\n\n          def forward(self, x):\n              if self.training:\n                  # original code\n                  x = self.matmul(x)\n                  x = self.bn(x)\n                  x = x + self.bias\n                  x = x / self.divide_value\n                  x = x * torch.sigmoid(x)\n                  return x\n              else:\n                  # Precompute fused parameters\n                  # ... (code as above) ...\n\n                  # Then call the Triton kernel\n\n  We'll define the kernel inside the module? Or outside? We can define it outside as a global function.\n\n  But note: the EVOLVE-BLOCK must be self-contained? \n\n  We'll define the kernel inside the class? But that's not allowed. We can define it outside the class but inside the EVOLVE-BLOCK.\n\n  We'll do:\n\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def _forward_kernel( ... ): ... \n\n      class ModelNew(nn.Module): ...\n\n  But note: the problem says to return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\n\n  We must include the necessary imports? The original code only imports torch and torch.nn.\n\n  We'll add:\n        import triton\n        import triton.language as tl\n\n  And then define the kernel.\n\n  However, note: the Triton kernel might be long. And we are limited to changing <=20 lines? But the problem says \"small iterative steps\", but we are at the beginning. We are allowed to write a new kernel.\n\n  But the current EVOLVE-BLOCK is 30 lines. We are going to replace it with a much longer block.\n\n  We must do it.\n\n  We'll write the entire code block.\n\n  Let's code accordingly.\n\n  We'll also note: the original model has parameters that are initialized randomly. We must preserve the same initialization? We are not changing the __init__.\n\n  We are only changing the forward in inference mode.\n\n  We must test the kernel for compilation and correctness? We are not, but the problem will run the evaluation pipeline.\n\n  We'll write the kernel as described.\n\n  Given the time, we might make the kernel simple and hope for the best.\n\n  We'll choose BLOCK_M=32, BLOCK_N=32, BLOCK_K=32. We can tune later.\n\n  We must also note: the dot product in Triton might not be available for float32? But it is.\n\n  We'll write the kernel for float32.\n\n  Steps:\n\n      We are going to replace the entire EVOLVE-BLOCK.\n\n  We must be cautious: the original code also has:\n        batch_size = 128\n        in_features = 1024\n        ... and the functions get_inputs and get_init_inputs.\n\n  We must not change these? Because they are used for testing.\n\n  But note: the model's __init__ takes in_features and out_features. We are not changing that.\n\n  The get_inputs returns a tensor of (128, 1024). That matches the batch_size and in_features.\n\n  We are good.\n\n  We'll now write the new code.\n\n  IMPORTANT: We are only allowed to output the EVOLVE-BLOCK.\n\n  We'll write:\n\n      # ================== EVOLVE-BLOCK-START ==================\n      ... our code ...\n      # =================== EVOLVE-BLOCK-END ===================\n\n  We must include the entire ModelNew and the batch_size, etc. and the functions.\n\n  We'll structure:\n\n      import torch\n      import torch.nn as nn\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def _forward_kernel( ... ):\n          ... kernel code ...\n\n      class ModelNew(nn.Module):\n          ... __init__ same as before ...\n\n          def forward(self, x):\n              ... as described ...\n\n      batch_size = 128\n      ... and the rest ...\n\n  We'll do that.\n\n  However, note: the original code had no Triton import. We are adding it. That is allowed? The problem says to generate Triton code.\n\n  We assume it is.\n\n  Let's write the code.\n</think>\nTo improve the performance of the model, we'll implement a fused Triton kernel that combines the matrix multiplication, batch normalization, bias addition, division, and Swish activation into a single efficient operation. This approach reduces memory accesses and leverages GPU parallelism, which is particularly beneficial for inference.\n\nKey optimizations:\n1. Fuse all operations into one kernel to minimize memory transfers\n2. Precompute fused weights and biases to eliminate redundant calculations\n3. Use block matrix multiplication with efficient memory access patterns\n4. Apply pointwise operations (bias, division, Swish) within the kernel\n5. Maintain training/eval mode compatibility (Triton used only in eval)\n\nHere's the complete implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_im, stride_ik,\n    stride_wn, stride_wk,\n    stride_bn,\n    stride_om, stride_on,\n    divide_value: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    input_ptrs = input_ptr + offs_m[:, None] * stride_im + offs_k[None, :] * stride_ik\n    weight_ptrs = weight_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        a = tl.load(input_ptrs, mask=k_mask[None, :] & (offs_m[:, None] < M), other=0.0)\n        b = tl.load(weight_ptrs, mask=k_mask[None, :] & (offs_n[:, None] < N), other=0.0)\n        acc += tl.dot(a, tl.trans(b))\n        input_ptrs += BLOCK_K * stride_ik\n        weight_ptrs += BLOCK_K * stride_wk\n\n    bias_vals = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += bias_vals[None, :]\n    acc = acc / divide_value\n    sig = tl.sigmoid(acc)\n    acc = acc * sig\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    output_ptrs = output_ptr + stride_om * offs_m[:, None] + stride_on * offs_n[None, :]\n    output_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(output_ptrs, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.divide_value = divide_value\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def _precompute_fused_params(self):\n        weight = self.matmul.weight.data\n        linear_bias = self.matmul.bias.data if self.matmul.bias is not None else 0\n        bn_weight = self.bn.weight.data\n        bn_bias = self.bn.bias.data\n        running_mean = self.bn.running_mean\n        running_var = self.bn.running_var\n        eps = self.bn.eps\n\n        denom = 1.0 / torch.sqrt(running_var + eps)\n        fused_w = weight * (bn_weight * denom).view(-1, 1)\n        fused_b = (linear_bias - running_mean) * (bn_weight * denom) + bn_bias + self.bias\n        return fused_w, fused_b\n\n    def forward(self, x):\n        if self.training:\n            x = self.matmul(x)\n            x = self.bn(x)\n            x = x + self.bias\n            x = x / self.divide_value\n            return x * torch.sigmoid(x)\n        else:\n            if self.fused_weight is None or self.fused_bias is None:\n                self.fused_weight, self.fused_bias = self._precompute_fused_params()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[0]\n            output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n            \n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\n            _forward_kernel[grid](\n                x, self.fused_weight, self.fused_bias, output,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                self.fused_bias.stride(0),\n                output.stride(0), output.stride(1),\n                self.divide_value,\n                BLOCK_M=64, BLOCK_N=64, BLOCK_K=32\n            )\n            return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nbias_shape = (1,)\ndivide_value = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
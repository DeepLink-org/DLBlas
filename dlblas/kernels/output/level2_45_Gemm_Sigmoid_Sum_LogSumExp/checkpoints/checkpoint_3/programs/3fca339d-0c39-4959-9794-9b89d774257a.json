{"id": "3fca339d-0c39-4959-9794-9b89d774257a", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n@triton.jit\ndef _per_example_sum(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x,\n    stride_weight,\n    stride_bias,\n    stride_output,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n    batch_size: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n\n    s = 0.0\n    for j in range(hidden_size):\n        b = tl.load(bias_ptr + j * stride_bias)\n        dot = b\n        for k in range(input_size):\n            x_val = tl.load(x_ptr + pid * stride_x + k)\n            w_val = tl.load(weight_ptr + j * stride_weight + k)\n            dot += x_val * w_val\n        sig = 1.0 / (1.0 + tl.exp(-dot))\n        s += sig\n    tl.store(output_ptr + pid * stride_output, s)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.bias = nn.Parameter(torch.randn(hidden_size))\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = x.contiguous()\n        output_per_example = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        grid = (triton.cdiv(batch_size, 1),)\n        _per_example_sum[grid](\n            x, self.weight, self.bias, output_per_example,\n            x.stride(0), self.weight.stride(0), 1, 1,\n            self.input_size, self.hidden_size, batch_size\n        )\n        x = torch.logsumexp(output_per_example, dim=0)\n        return x\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "d57f621a-896d-4f6a-bb46-859967d072b2", "generation": 1, "timestamp": 1754571634.99621, "iteration_found": 2, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_45_Gemm_Sigmoid_Sum_LogSumExp", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "max_difference": ["2.323922", "2.256674", "2.426967", "2.503527", "2.846488"], "avg_difference": ["2.323922", "2.256674", "2.426967", "2.503527", "2.846488"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.11, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.11, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.11, "speed_up": 1.0, "custom_timing": 0.11}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.1100\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.0, 'custom_timing': 0.11}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.1100, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.0, 'custom_timing': 0.11}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), applies Sigmoid, sums the result, and calculates the LogSumExp.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = torch.sigmoid(x)\n        x = torch.sum(x, dim=1)\n        x = torch.logsumexp(x, dim=0)\n        return x\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.1100), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.0, 'custom_timing': 0.11})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), applies Sigmoid, sums the result, and calculates the LogSumExp.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = torch.sigmoid(x)\n        x = torch.sum(x, dim=1)\n        x = torch.logsumexp(x, dim=0)\n        return x\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear transformation (input_size -> hidden_size)\n  2. Sigmoid activation\n  3. Sum over dim=1 (reducing the hidden_size dimension, so we get [batch_size] -> then we have a tensor of shape [batch_size])\n  4. LogSumExp over dim=0 (reducing the batch_size dimension, so we get a scalar)\n\nHowever, note that the current implementation in the forward pass:\n  x = self.linear1(x)  # [batch_size, hidden_size]\n  x = torch.sigmoid(x)  # [batch_size, hidden_size]\n  x = torch.sum(x, dim=1)  # [batch_size]\n  x = torch.logsumexp(x, dim=0)  # scalar\n\nBut note: the logsumexp is applied on a 1D tensor of [batch_size]. The current code does:\n  torch.logsumexp(x, dim=0)  # which is the same as x.logsumexp(0) and returns a scalar.\n\nHowever, the problem is that the current implementation is entirely in PyTorch and we are asked to rewrite it for performance.\n\nWe are allowed to use Triton. But note that the entire model is a sequence of operations. We can try to fuse some of these operations in a Triton kernel.\n\nBut note: the operations are:\n  - Linear: matrix multiplication (with bias) -> we can write a Triton kernel for this?\n  - Then sigmoid (element-wise)\n  - Then sum over columns (reduction over the hidden_size dimension)\n  - Then logsumexp over the batch (reduction over the batch dimension)\n\nHowever, the entire forward pass might not be easily fused into a single kernel because the two reductions are over different dimensions and the sigmoid is in the middle.\n\nBut note: the problem does not require that we rewrite the entire model. We are to rewrite the program to improve performance.\n\nLooking at the current program, the operations are:\n  - linear1: a nn.Linear (which is a matrix multiplication and bias add) -> this is done on the entire batch and hidden_size.\n  - sigmoid: element-wise\n  - sum: reduction over the hidden_size dimension -> output shape [batch_size]\n  - logsumexp: reduction over the batch dimension -> scalar.\n\nWe can try to fuse the linear1, sigmoid, and the first reduction (sum over hidden_size) into one kernel? Then the logsumexp is a single operation on a 1D tensor of size batch_size.\n\nAlternatively, we can write two kernels: one for the linear+sigmoid and then a reduction kernel for the sum over hidden_size? But note that the sum over hidden_size is a per-row reduction.\n\nBut note: the current program uses two linear layers? Actually, we see that there is a second linear layer (self.linear2) that is not used in the forward pass? \nWait, the forward pass does:\n  x = self.linear1(x)\n  x = torch.sigmoid(x)\n  x = torch.sum(x, dim=1)\n  x = torch.logsumexp(x, dim=0)\n\nSo the second linear layer (self.linear2) is never used. This is a mistake? Or is the model intended to have two linear layers but only one is used?\n\nLooking at the original code, the model is defined as:\n    self.linear1 = nn.Linear(input_size, hidden_size)\n    self.linear2 = nn.Linear(hidden_size, output_size)\n\nBut the forward pass does not use self.linear2. So we can remove it? However, the problem says: \"maintain the same inputs and outputs\". The output of the model is a scalar. But originally, the model had the potential to output a vector of size [output_size] if the linear2 was used. But in the current forward, it outputs a scalar.\n\nSo we have two possibilities:\n  1. The original model is intended to have two linear layers and the forward pass is incomplete? \n  2. Or the model is as described and the second linear is not used.\n\nBut note: the `get_inputs` returns a tensor of shape [batch_size, input_size] and the model returns a scalar.\n\nGiven the ambiguity, we must stick to the provided code. We are only allowed to change the implementation within the EVOLVE-BLOCK.\n\nSo we will remove the second linear layer because it is not used? But note the problem says: \"maintain the same inputs and outputs\". The model's output is a scalar and the inputs are [batch_size, input_size]. So we can remove the unused linear2.\n\nAlternatively, if we are to keep the model structure exactly as is (with two linear layers) but the forward doesn't use the second, then we are free to remove it? But note the problem says: \"maintain the same inputs and outputs\". The output is a scalar, so we don't need the second linear.\n\nBut wait, the problem says: \"Rewrite the program to improve its performance\". We are allowed to change the internal implementation.\n\nSo let's restructure the model:\n\nWe will remove the unused linear2. Then we will try to fuse the operations that are done.\n\nHowever, note that the linear1 operation is a matrix multiplication. We can write a Triton kernel for the matrix multiplication? But then we also have the sigmoid and the reduction.\n\nWe can break the forward pass into two parts:\n\n  Part1: For each example in the batch, compute the linear1 (matrix-vector product) and then apply sigmoid and then reduce (sum) over the hidden_size to get a scalar per example.\n\n  Then Part2: reduce the batch of scalars (from Part1) by logsumexp.\n\nBut note: the linear1 is a matrix multiplication of x (batch_size, input_size) with weight (hidden_size, input_size) and bias (hidden_size). The output is (batch_size, hidden_size).\n\nWe can write a Triton kernel that does:\n  - For each row (example) in the batch, do:\n      for each column in the hidden_size, compute the dot product and add bias -> then sigmoid -> then accumulate the sum for that row.\n\nSo the kernel would output a 1D tensor of length batch_size.\n\nThen we do the logsumexp on that tensor (which is a reduction over the batch) on the CPU? Or we can do it in another Triton kernel? But note: the logsumexp of a 1D tensor is very fast on the GPU, but we are only going to have one value. Alternatively, we can do the entire thing in one kernel? But the logsumexp requires the entire batch to be reduced.\n\nHowever, note that the logsumexp is done after the per-example sum. So we have:\n\n  For each example i in [0, batch_size-1]:\n      s_i = sum_{j=0}^{hidden_size-1} sigmoid( (x_i * W[:,j] + b[j]) )\n\n  Then: result = log( sum_{i=0}^{batch_size-1} exp(s_i) )\n\nBut note: the logsumexp is: log( sum_i exp(s_i) )\n\nWe can compute the per-example sum in one kernel and then do the logsumexp in a second kernel? Or we can do it in two steps.\n\nHowever, the batch_size is 128 (which is not too large) and the hidden_size is 20 (very small). So the entire computation is very small. \n\nBut the problem is that the current PyTorch implementation is already very fast (0.11 ms). We are to make it faster.\n\nGiven the small sizes, the overhead of launching a kernel might be significant. So we might want to do everything in a single kernel? \n\nAlternatively, we can write a single kernel that does:\n\n  - Each program handles one example? Then we would have batch_size programs. But then we have to do the logsumexp across programs? That requires inter-block communication and we don't have that in Triton without multiple kernels.\n\nSo let's break it:\n\n  Kernel1: for each example i, compute the linear1, sigmoid, and then the sum for that example. Output a 1D tensor of size batch_size.\n\n  Then we do a second kernel for logsumexp? But note: the logsumexp can be done by a reduction kernel that takes the 1D tensor of size batch_size and outputs a scalar.\n\nHowever, the batch_size is 128. We can do the logsumexp on the CPU? But that would require a device to host transfer and then back? That is expensive.\n\nAlternatively, we can do the entire computation in one kernel? But note that the entire computation requires:\n\n  - Reading the entire weight matrix (hidden_size, input_size) which is 20*10=200 elements. And the input matrix (batch_size, input_size)=128*10=1280 elements.\n\n  We can design a kernel that uses one block for the entire batch? But note that the batch_size is 128 and the hidden_size is 20. We can have:\n\n      Block dimensions: 1 block? But then we can have 128*20=2560 elements to process? That might be too many for one block? Actually, we can have multiple threads in one block.\n\n  Alternatively, we can have one block per example? Then 128 blocks. Each block would compute the entire hidden_size for one example? That is 20 elements per block. Then we can do the per-example sum and then we need to do a global reduction for the logsumexp? That requires inter-block synchronization which is not available in Triton.\n\nSo we can do:\n\n  Step1: Use a kernel to compute the per-example sums and store in an array of size 128.\n\n  Step2: Use a reduction kernel (that is efficient for 128 elements) to compute the logsumexp.\n\nBut note: 128 elements is very small. We can do the logsumexp on the host? But that would break the GPU flow.\n\nAlternatively, we can use a single kernel that does both the per-example sums and the logsumexp? We can do:\n\n  - We break the batch into blocks? But note that the logsumexp requires all per-example sums to be computed.\n\n  We can do:\n\n      Phase1: Each block computes the per-example sums for a set of examples. Then we write the per-example sums to global memory.\n\n      Phase2: Then we do a reduction (by a single block) of the entire array of 128 elements to compute the logsumexp.\n\n  But that requires two kernel launches? Or we can do it in one kernel with two phases? But Triton doesn't support global synchronization.\n\nTherefore, we are going to do two kernels:\n\n  Kernel1: compute the per-example sums (output: 1D tensor of batch_size)\n  Kernel2: compute the logsumexp of the 1D tensor.\n\nBut note: the current PyTorch logsumexp for a 1D tensor of 128 elements is very fast. So we might leave that in PyTorch.\n\nAlternatively, we can write a Triton kernel for the logsumexp? But for 128 elements, it might not be worth the kernel launch.\n\nGiven the small sizes, let's consider:\n\n  The current PyTorch operations are:\n\n      linear1: O(batch_size * input_size * hidden_size) = 128*10*20 = 25600 operations (for the matmul) plus 128*20 for the bias and then the sigmoid and then the sum (128*20) and then the logsumexp (128).\n\n  Total operations: about 25600 + 2560 + 2560 + 128 = about 30848 operations. This is very small.\n\nThe overhead of launching a Triton kernel for the entire linear+sigmoid+sum might be comparable to the PyTorch overhead.\n\nBut we are to try to improve performance.\n\nWe can write a Triton kernel that fuses the linear1, sigmoid, and the per-example sum. This avoids writing the intermediate matrix (batch_size, hidden_size) to memory.\n\nThe kernel for the per-example sum:\n\n  We will have batch_size program IDs. Each program (block) will handle one example.\n\n  For each example i, we need to compute:\n      s_i = 0\n      for j in range(hidden_size):\n          z = bias[j]  (if we are using bias)\n          for k in range(input_size):\n              z += x[i, k] * weight[j, k]\n          s_i += sigmoid(z)\n\n  But note: the hidden_size is 20 and input_size is 10. So each example requires 20*10=200 multiplications and 200 additions, plus 20 sigmoids and 20 additions for the sum.\n\n  We can do this in a single thread? Because 200 operations is not too heavy.\n\n  We can do:\n\n      pid = tl.program_id(0)\n      if pid < batch_size:\n          # load the row of x: x[pid, :]\n          # initialize the accumulator for this example to 0\n          s = 0.0\n          for j in range(hidden_size):\n              # load the weight row j and the bias j\n              # compute the dot product of x[pid] and weight[j]\n              dot = 0.0\n              for k in range(input_size):\n                  x_val = tl.load(x_ptr + pid * input_size + k)\n                  w_val = tl.load(weight_ptr + j * input_size + k)\n                  dot += x_val * w_val\n              dot += tl.load(bias_ptr + j)\n              # apply sigmoid\n              act = 1.0 / (1.0 + tl.exp(-dot))\n              s += act\n          tl.store(output_ptr + pid, s)\n\n  Then we have an array of batch_size elements.\n\n  Then we call torch.logsumexp on that array.\n\nBut note: the above kernel does not use any tiling or shared memory. It is a naive double loop. But for small hidden_size and input_size, it might be acceptable.\n\nHowever, we can use vector operations? We can load multiple elements of x and weight at a time? But the input_size is 10, which is not divisible by 2? We can do 10 as a vector of 2*5? But that might not help.\n\nAlternatively, we can use the fact that the hidden_size is 20 and we can unroll the inner loop? But the Triton compiler might unroll it.\n\nBut note: the current implementation in PyTorch might be using optimized GEMM for the linear, and then optimized sum. We are replacing it with a naive kernel.\n\nWe must benchmark.\n\nAlternatively, we can use a block per example and use multiple threads? But the hidden_size is only 20, so we can use 20 threads? Then each thread computes one element of the hidden_size for the example? Then we do a parallel reduction for the per-example sum? But note: the input_size is 10, so each thread would do 10 multiplications and then one sigmoid and then we reduce the 20 elements.\n\nBut then we have to do a reduction across 20 threads? That might be efficient.\n\nLet's design:\n\n  Kernel for one example:\n\n      We have 20 threads (one for each hidden unit). We can have a block of 20 threads? Then we need batch_size blocks.\n\n      Each thread j in [0,19] does:\n          dot = bias[j]\n          for k in range(input_size):   # input_size=10\n              x_val = tl.load(x_ptr + pid * input_size + k)\n              w_val = tl.load(weight_ptr + j * input_size + k)\n              dot += x_val * w_val\n          act = 1.0 / (1.0 + tl.exp(-dot))\n          # now we need to sum the 20 act values.\n\n      Then we do a shared memory reduction (within the block) to sum the 20 act values? Then store the result for the example.\n\n  Steps:\n\n      pid = tl.program_id(0)   # example index\n      j = tl.program_id(1)      # hidden index? But we are using 1D grid? Or we can use 2D grid? But note: Triton doesn't support 2D grid? Actually, we can use 1D grid and then use:\n\n          total_blocks = batch_size\n          threads_per_block = hidden_size   # 20\n\n      Then we can do:\n\n          pid = tl.program_id(0)   # which example? and then we have 20 threads in the block.\n\n          j = tl.arange(0, hidden_size)   # but we are in a block of 20, so each thread has its own j? Actually, we can use:\n\n          j_index = tl.arange(0, hidden_size)   # vector of 0..19? But we are going to use each thread to compute one j.\n\n          But note: we are going to use 20 threads per block. So we can assign each thread a j.\n\n          thread_id = tl.program_id(1)   # but wait, Triton doesn't have 2D program_id? We are using 1D grid and then 1D block? Actually, we can use:\n\n          We can use a 1D grid of size batch_size, and then a 1D block of size hidden_size? But Triton doesn't have the concept of block? We have to use the program_id and then the number of blocks? Actually, we can use:\n\n          pid = tl.program_id(0)   # this would be the example index. Then we have:\n\n          j = tl.arange(0, hidden_size)   # but we are going to process the entire hidden_size in one program? Then we don't need multiple threads per example? \n\n          Alternatively, we can use:\n\n          total_programs = batch_size * hidden_size   # 128*20=2560\n\n          Then we can have:\n\n          pid = tl.program_id(0)\n          example_id = pid // hidden_size\n          j = pid % hidden_size\n\n          Then each thread computes one hidden unit for one example.\n\n          Then we need to do a reduction per example? We can use tl.atomic_add? But that would be to global memory? We would have to initialize an output array of zeros for the per-example sums.\n\n          Steps:\n\n              output = torch.zeros(batch_size, device=x.device, dtype=torch.float32)\n\n              Then each thread (for example_id, j) does:\n\n                  # load the row of the example: x[example_id, :]  (input_size elements)\n                  # load the row of the weight: weight[j, :]\n                  # compute the dot product and add bias[j]\n                  # compute sigmoid -> act\n                  # then atomic_add to output[example_id] by act\n\n          But atomic operations are slow? And we have 20 atomic adds per example? And 128 examples? That is 2560 atomic operations.\n\n          Alternatively, we can use a reduction using tl.associative_scan? But we would have to group by example_id? That is complicated.\n\n  This might not be efficient.\n\nGiven the complexity and the small sizes, we propose:\n\n  We write a kernel that uses one program per example. Each program (block) uses one thread? But that would be inefficient? But note the work per example is only 200 multiplications and 200 additions and 20 sigmoids and 20 additions. This is acceptable.\n\n  We can do:\n\n      total_programs = batch_size\n\n      @triton.jit\n      def _per_example_sum(\n          x_ptr, weight_ptr, bias_ptr, output_ptr,\n          input_size, hidden_size,\n          stride_x, stride_weight, stride_bias, stride_output,\n          BLOCK_SIZE_H: tl.constexpr,   # we can set to hidden_size? but we don't need tiling for hidden_size? because hidden_size is 20.\n      ):\n          pid = tl.program_id(0)\n          # if pid >= batch_size, return\n\n          # We are going to loop over hidden_size and for each hidden unit, we do the inner product with the input.\n          s = 0.0\n          # We are going to load the entire x row for this example? We can load in chunks? But input_size=10, so we can load the whole row? But 10 is small.\n          # Preload the x row? We can load the whole row into registers? But we have 10 elements -> we can use a vector of 10.\n\n          # Alternatively, we can do two loops: outer loop over hidden_size, inner loop over input_size.\n\n          for j in range(hidden_size):\n              # compute the dot product of x[pid] and weight[j]\n              dot = tl.load(bias_ptr + j)   # bias for unit j\n              for k in range(input_size):\n                  x_val = tl.load(x_ptr + pid * stride_x + k)\n                  w_val = tl.load(weight_ptr + j * stride_weight + k)\n                  dot += x_val * w_val\n              # apply sigmoid\n              # We can use: 1/(1+tl.exp(-dot)) but note: if dot is large, we might get NaN? We can use the stable version?\n              # Alternatively, we can use tl.sigmoid? But Triton doesn't have a built-in sigmoid? We can use:\n              #   sig = 1.0 / (1.0 + tl.exp(-dot))\n              sig = 1.0 / (1.0 + tl.exp(-dot))\n              s += sig\n\n          tl.store(output_ptr + pid * stride_output, s)\n\n  Then we call this kernel and then do the logsumexp in PyTorch.\n\n  Steps:\n\n      output_per_example = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n      grid = (batch_size,)\n      _per_example_sum[grid](x, weight, bias, output_per_example, input_size, hidden_size, ...)\n\n      Then:\n          result = torch.logsumexp(output_per_example, dim=0)\n\n  But note: the current model returns a scalar. So we return result.\n\n  We must be cautious about the strides? We have to pass the strides.\n\n  Also, we must note that the weight in nn.Linear is stored as (hidden_size, input_size). So the stride for the weight matrix: if it's contiguous, then stride_weight = input_size? Actually, the stride for the second dimension is 1? And the first dimension is input_size? So we have:\n\n      weight_ptr + j * stride_weight + k   -> if the weight is stored in row-major, then for row j, the starting index is j * input_size, and then k.\n\n      So stride_weight = input_size? But note: the stride for the rows might be input_size, and for the columns is 1.\n\n  Similarly, the input x: we are given (batch_size, input_size). We assume contiguous? Then stride_x = input_size? Actually, the stride for the batch dimension is input_size? Then the element at [i, j] is at i * input_size + j.\n\n  So we can set:\n\n      stride_x = input_size   # because to get next row, we skip input_size elements\n      stride_weight = input_size   # same reason\n\n  But note: the input might be non-contiguous? We can make it contiguous in the forward pass.\n\n  We will require that the input is contiguous.\n\n  The output_per_example is 1D, so stride_output=1.\n\n  The bias is 1D, so stride_bias=1.\n\n  We must pass input_size and hidden_size as integers? Or we can pass them as tl.constexpr? But they are small and known at compile time.\n\n  We can set them as tl.constexpr? Then we can unroll the loops? But note: the hidden_size and input_size are fixed for the model? But the model is initialized with input_size, hidden_size. So they are fixed for the instance.\n\n  However, when we call the kernel, we can pass them as integers and then use them as tl.constexpr? Actually, we can pass them as constants.\n\n  But note: the kernel is compiled for specific values of input_size and hidden_size? So we can't use the same kernel for different sizes? But the model is fixed.\n\n  Alternatively, we can make the kernel work for any input_size and hidden_size? But then we cannot unroll.\n\n  Given the small sizes, we can unroll the loops? We can set BLOCK_SIZE_H = hidden_size and BLOCK_SIZE_I = input_size? But then we can use:\n\n      for j in range(BLOCK_SIZE_H):\n          ...\n\n      and similarly for k.\n\n  But note: the hidden_size and input_size are known at the time of kernel launch? We can pass them as tl.constexpr? And Triton will compile a specific kernel for each pair (input_size, hidden_size)? But we have only one pair.\n\n  We can do:\n\n      @triton.jit\n      def _per_example_sum(\n          x_ptr, weight_ptr, bias_ptr, output_ptr,\n          stride_x, stride_weight, stride_bias, stride_output,\n          input_size: tl.constexpr, hidden_size: tl.constexpr,\n      ):\n\n  Then when we call the kernel, we pass input_size and hidden_size as constants.\n\n  But note: the batch_size might change? We are using batch_size=128 in the example, but the model might be called with different batch sizes? The get_inputs returns a tensor of batch_size=128. But the model should handle any batch size.\n\n  So we must not hardcode batch_size in the kernel.\n\n  We can compute the grid as (x.shape[0],) for the batch size.\n\n  Now, the logsumexp: we do in PyTorch. It is a single operation on a 1D tensor of size batch_size. This is efficient.\n\nLet's write the entire model:\n\n  We remove the unused linear2.\n\n  We define the Triton kernel.\n\n  We initialize the parameters (weight and bias) for the linear layer.\n\n  In the forward pass, we:\n\n      - Check the input and make it contiguous.\n      - Launch the kernel to compute the per-example sums.\n      - Then compute the logsumexp of the resulting vector.\n\n  We return the scalar.\n\nBut note: the original model had two linear layers? We are removing the second. But the problem says: \"maintain the same inputs and outputs\". The output is a scalar and the input is a tensor of [batch_size, input_size]. So we are safe.\n\nLet's code:\n\n  Steps:\n\n      class ModelNew(nn.Module):\n          def __init__(self, input_size, hidden_size, output_size):\n              super().__init__()\n              self.input_size = input_size\n              self.hidden_size = hidden_size\n              # We don't use output_size? But the problem says the model returns a scalar. So we ignore output_size?\n              # But note: the original model had self.linear2, but it wasn't used. So we remove it.\n              self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n              self.bias = nn.Parameter(torch.randn(hidden_size))\n\n          def forward(self, x):\n              # x: [batch_size, input_size]\n              batch_size = x.shape[0]\n              x = x.contiguous()\n              # We are going to compute: for each example i, s_i = sum_{j=0}^{hidden_size-1} sigmoid( (x_i * weight[j] + bias[j]) )\n              output_per_example = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n              # grid = (batch_size,)\n              grid = (batch_size,)\n              # We call the kernel\n              _per_example_sum[grid](\n                  x, self.weight, self.bias, output_per_example,\n                  x.stride(0), self.weight.stride(0), 1, output_per_example.stride(0),\n                  self.input_size, self.hidden_size\n              )\n              # Then logsumexp over the batch dimension\n              result = torch.logsumexp(output_per_example, dim=0)\n              return result\n\n      We also need to define the Triton kernel.\n\nBut note: the kernel must be defined outside the class? And we have to import triton and tl.\n\nWe'll put the kernel definition in the same block.\n\nBut note: the EVOLVE-BLOCK must contain the entire code? Because the problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\nSo we write:\n\n  import math\n  import torch\n  import triton\n  import triton.language as tl\n\n  @triton.jit\n  def _per_example_sum(\n      x_ptr, weight_ptr, bias_ptr, output_ptr,\n      stride_x, stride_weight, stride_bias, stride_output,\n      input_size: tl.constexpr, hidden_size: tl.constexpr,\n  ):\n      pid = tl.program_id(0)\n      # Check if within bounds\n      if pid >= batch_size:   # but wait, we don't have batch_size as a kernel argument? We have the grid set to batch_size, so we don't need to check? But we can pass batch_size? Or we can not because the grid handles it? Actually, the grid might have more than needed? We should pass batch_size? Or we can get it from the grid? But we don't have it.\n\n      Let's change: we pass the batch_size? Or we can avoid because the grid is exactly batch_size? We can rely on the grid.\n\n      But note: the grid might be rounded up? We should check.\n\n      So we pass batch_size? Or we can use:\n\n          if pid >= batch_size: return\n\n      But we don't have batch_size in the kernel. So we must pass it? Or we can compute the grid as the next multiple of 1? We set the grid to (triton.cdiv(batch_size, 1),) but we are using one program per example. So we set grid=(batch_size,). Then we don't need to check? But if batch_size is not a multiple of 1, we set grid=(batch_size,). Then it's exact.\n\n      So we don't check.\n\n      Now, for the example pid, we do:\n\n          s = 0.0\n          for j in range(hidden_size):\n              # Load the bias for unit j\n              b = tl.load(bias_ptr + j * stride_bias)\n              # We are going to compute the dot product of the row pid of x and the row j of weight.\n              dot = b\n              for k in range(input_size):\n                  x_val = tl.load(x_ptr + pid * stride_x + k)\n                  w_val = tl.load(weight_ptr + j * stride_weight + k)\n                  dot += x_val * w_val\n              # Compute sigmoid\n              sig = 1.0 / (1.0 + tl.exp(-dot))\n              s += sig\n          tl.store(output_ptr + pid * stride_output, s)\n\n  But note: the stride_x is the stride for the batch dimension? For a contiguous x, stride_x = input_size. So we are loading at pid * input_size + k -> that is the element [pid, k].\n\n  Similarly, the weight: j * stride_weight + k -> if stride_weight = input_size, then it is [j, k].\n\n  The bias: j * stride_bias -> we set stride_bias=1? Then it's bias[j].\n\n  The output: pid * stride_output -> we set stride_output=1? Then it's output_per_example[pid].\n\n  This should work.\n\n  But note: the input_size and hidden_size are passed as tl.constexpr? Then the loops will be unrolled? That might be efficient.\n\n  We hope that the Triton compiler unrolls the loops because hidden_size=20 and input_size=10 are small.\n\n  Then we do the logsumexp in PyTorch.\n\n  We must also note that the original model used the nn.Linear which by default has bias? We are including the bias.\n\n  We initialize the parameters with torch.randn? The original model used nn.Linear which by default uses a uniform initialization? But the problem doesn't specify the initialization. We can keep the randn? Or we can use the same as the original? The original used nn.Linear which we are replacing. But note: the model structure is the same? The parameters are the same? We are just replacing the implementation.\n\n  We can change the __init__ to use the same initialization as nn.Linear? But the problem doesn't specify. We'll stick to the original structure? But the original used nn.Linear. So we can keep the nn.Linear? But we are replacing it with our own parameters.\n\n  Alternatively, we can keep the nn.Linear? But then we would have to extract the weight and bias? But we are not allowed to change the model structure? We are allowed to change the internal implementation.\n\n  But note: the problem says \"maintain the same inputs and outputs\". The parameters are part of the model? The original model had two linear layers? But we are removing the second. So we are changing the parameters.\n\n  We have two choices:\n\n      Option A: Keep the first linear as an nn.Linear and then use the Triton kernel to do the sigmoid and sum? But then we are not fusing the linear? The linear would be done by PyTorch and then we do the rest in Triton? That might not be faster.\n\n      Option B: Remove the nn.Linear and use our own parameters? Then we are changing the model structure? But the problem says we can change the internal implementation.\n\n  We choose Option B because we are fusing the linear, sigmoid, and sum.\n\n  But note: the original model had an unused linear2. We are removing it. That is acceptable.\n\n  Now, the code for the model:\n\n      class ModelNew(nn.Module):\n          def __init__(self, input_size, hidden_size, output_size):\n              super().__init__()\n              self.input_size = input_size\n              self.hidden_size = hidden_size\n              # We don't use output_size? But we keep it because the original had it? But we don't use it.\n              self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n              self.bias = nn.Parameter(torch.randn(hidden_size))\n\n          def forward(self, x):\n              batch_size = x.shape[0]\n              # Make sure the input is contiguous\n              x = x.contiguous()\n              # Create an output tensor for the per-example sums\n              output_per_example = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n              # Define the grid: one program per example\n              grid = (batch_size,)\n              # Launch the kernel\n              _per_example_sum[grid](\n                  x, self.weight, self.bias, output_per_example,\n                  x.stride(0), self.weight.stride(0), 1, output_per_example.stride(0),\n                  self.input_size, self.hidden_size\n              )\n              # Now compute the logsumexp over the batch dimension\n              result = torch.logsumexp(output_per_example, dim=0)\n              return result\n\n  We must define the kernel.\n\n  We also note that the original had a batch_size, input_size, hidden_size, output_size defined at the module level? We must keep them? Because the get_inputs and get_init_inputs use them.\n\n  We'll keep the same batch_size, input_size, hidden_size, output_size.\n\n  And the get_inputs returns [torch.randn(batch_size, input_size)]\n\n  And get_init_inputs returns [input_size, hidden_size, output_size]\n\n  So we don't change that.\n\n  But note: the model now does not use output_size? But we pass it to the __init__? That is okay.\n\n  Let's write the entire code block.\n\n  IMPORTANT: We must not remove the comments about the EVOLVE-BLOCK.\n\n  We write:\n\n      # ================== EVOLVE-BLOCK-START ==================\n      import math\n      import torch\n      import triton\n      import triton.language as tl\n      import torch.nn as nn\n\n      # Triton kernel for per-example sum of sigmoid(linear(x))\n      @triton.jit\n      def _per_example_sum(\n          x_ptr,\n          weight_ptr,\n          bias_ptr,\n          output_ptr,\n          stride_x,\n          stride_weight,\n          stride_bias,\n          stride_output,\n          input_size: tl.constexpr,\n          hidden_size: tl.constexpr,\n      ):\n          pid = tl.program_id(0)\n          # Initialize the accumulator for the example\n          s = 0.0\n          # Loop over hidden units\n          for j in range(hidden_size):\n              # Load the bias for this hidden unit\n              b = tl.load(bias_ptr + j * stride_bias)\n              dot = b\n              # Loop over input features\n              for k in range(input_size):\n                  x_val = tl.load(x_ptr + pid * stride_x + k)\n                  w_val = tl.load(weight_ptr + j * stride_weight + k)\n                  dot += x_val * w_val\n              # Sigmoid\n              sig = 1.0 / (1.0 + tl.exp(-dot))\n              s += sig\n          # Store the result for this example\n          tl.store(output_ptr + pid * stride_output, s)\n\n      class ModelNew(nn.Module):\n          def __init__(self, input_size, hidden_size, output_size):\n              super(ModelNew, self).__init__()\n              self.input_size = input_size\n              self.hidden_size = hidden_size\n              # Initialize parameters\n              self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n              self.bias = nn.Parameter(torch.randn(hidden_size))\n\n          def forward(self, x):\n              batch_size = x.shape[0]\n              # Make sure input is contiguous\n              x = x.contiguous()\n              # Allocate output for per-example sums\n              output_per_example = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n              # Grid of batch_size\n              grid = (batch_size,)\n              # Launch kernel\n              _per_example_sum[grid](\n                  x, self.weight, self.bias, output_per_example,\n                  x.stride(0), self.weight.stride(0), 1, output_per_example.stride(0),\n                  input_size, hidden_size\n              )\n              # Compute the logsumexp over the batch\n              x = torch.logsumexp(output_per_example, dim=0)\n              return x\n\n      batch_size = 128\n      input_size = 10\n      hidden_size = 20\n      output_size = 5\n\n      def get_inputs():\n          return [torch.randn(batch_size, input_size)]\n\n      def get_init_inputs():\n          return [input_size, hidden_size, output_size]\n      # =================== EVOLVE-BLOCK-END ===================\n\n  But note: in the kernel, we are using the variables 'input_size' and 'hidden_size' that are passed as tl.constexpr? And in the call, we pass the module's attributes? But note: in the forward, we have:\n\n          _per_example_sum[grid]( ... , input_size, hidden_size)\n\n      But the module has self.input_size and self.hidden_size? We are passing the variables from the outer scope? Actually, the outer scope has input_size=10 and hidden_size=20? Because we set them at the module level? But the model might be created with different sizes? \n\n      How are we creating the model? In the __init__, we use the passed input_size and hidden_size? And then in the kernel call, we pass:\n\n          input_size, hidden_size   -> these are the ones from the outer scope? That is wrong.\n\n      We must pass the instance's input_size and hidden_size? We have:\n\n          self.input_size and self.hidden_size.\n\n      So we change:\n\n          _per_example_sum[grid](\n              ...,\n              self.input_size, self.hidden_size\n          )\n\n  Also, note that the kernel uses the variable names without the 'self'. So we are passing the values.\n\n  We also note that the kernel uses the same variable names as the module-level ones? But they are in a different scope.\n\n  So we fix the call:\n\n          _per_example_sum[grid](\n              x, self.weight, self.bias, output_per_example,\n              x.stride(0), self.weight.stride(0), 1, output_per_example.stride(0),\n              self.input_size, self.hidden_size\n          )\n\n  Now, the model-level variables (batch_size, input_size, hidden_size, output_size) are used in get_inputs and get_init_inputs? And in the model __init__ we use the passed arguments? So it should be consistent.\n\n  We hope that the kernel will be compiled for the specific input_size and hidden_size of the model.\n\n  But note: the model is initialized with the values from get_init_inputs? Which are the module-level ones? So when we create the model, we do:\n\n      model = ModelNew(input_size, hidden_size, output_size)\n\n  Then in the forward, self.input_size is 10 and self.hidden_size is 20.\n\n  So it matches.\n\n  We are done.\n\n  Let's hope it compiles and runs faster.\n\n  Note: we are not doing any tuning? We are using the simplest kernel.\n\n  But the problem says: if the previous compile failed, fix it. We are starting from a working PyTorch code.\n\n  We are replacing the PyTorch operations with a Triton kernel. We hope it is faster.\n\n  If not, we might have to optimize further.\n\n  But note: the current PyTorch implementation does:\n\n      x = self.linear1(x)   # this is a highly optimized GEMM? But it's small: 128x10 * 10x20 -> 128x20.\n      x = torch.sigmoid(x)  # element-wise\n      x = torch.sum(x, dim=1) # reduction\n\n  We are fusing these three steps into one kernel. This avoids writing the intermediate 128x20 matrix to memory. That might be the performance gain.\n\n  We'll see.\n\n  We output the code as required.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n@triton.jit\ndef _per_example_sum(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x,\n    stride_weight,\n    stride_bias,\n    stride_output,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n    batch_size: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n\n    s = 0.0\n    for j in range(hidden_size):\n        b = tl.load(bias_ptr + j * stride_bias)\n        dot = b\n        for k in range(input_size):\n            x_val = tl.load(x_ptr + pid * stride_x + k)\n            w_val = tl.load(weight_ptr + j * stride_weight + k)\n            dot += x_val * w_val\n        sig = 1.0 / (1.0 + tl.exp(-dot))\n        s += sig\n    tl.store(output_ptr + pid * stride_output, s)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.bias = nn.Parameter(torch.randn(hidden_size))\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = x.contiguous()\n        output_per_example = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        grid = (triton.cdiv(batch_size, 1),)\n        _per_example_sum[grid](\n            x, self.weight, self.bias, output_per_example,\n            x.stride(0), self.weight.stride(0), 1, 1,\n            self.input_size, self.hidden_size, batch_size\n        )\n        x = torch.logsumexp(output_per_example, dim=0)\n        return x\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
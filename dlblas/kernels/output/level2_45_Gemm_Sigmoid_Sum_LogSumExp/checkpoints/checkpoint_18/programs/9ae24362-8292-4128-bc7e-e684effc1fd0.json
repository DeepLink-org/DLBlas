{"id": "9ae24362-8292-4128-bc7e-e684effc1fd0", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n@triton.jit\ndef _per_example_sum(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x,\n    stride_weight,\n    stride_bias,\n    stride_output,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n    batch_size: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n    \n    # Precompute input feature offsets\n    k_offsets = tl.arange(0, input_size)\n    mask_k = k_offsets < input_size\n    \n    # Load entire input vector for current example\n    x_vals = tl.load(x_ptr + pid * stride_x + k_offsets, mask=mask_k, other=0.0)\n    \n    s = 0.0\n    for j in range(hidden_size):\n        # Load entire weight row for current hidden unit\n        w_ptrs = weight_ptr + j * stride_weight + k_offsets\n        w_vals = tl.load(w_ptrs, mask=mask_k, other=0.0)\n        \n        # Compute vectorized dot product\n        dot = tl.sum(x_vals * w_vals)\n        dot += tl.load(bias_ptr + j * stride_bias)\n        \n        # Apply sigmoid and accumulate\n        sig = tl.sigmoid(dot)\n        s += sig\n    \n    # Store final sum for this example\n    tl.store(output_ptr + pid * stride_output, s)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = x.contiguous()\n        output_per_example = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        \n        # Prepare parameters\n        weight = self.linear1.weight.contiguous()\n        bias = self.linear1.bias.contiguous()\n        \n        # Launch kernel\n        grid = (batch_size,)\n        _per_example_sum[grid](\n            x, weight, bias, output_per_example,\n            x.stride(0), weight.stride(0), 1, 1,\n            self.input_size, self.hidden_size, batch_size\n        )\n        \n        # Final reduction\n        x = torch.logsumexp(output_per_example, dim=0)\n        return x\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "f18f9fdd-fae8-431c-ad4d-0057f0f6f306", "generation": 3, "timestamp": 1754575402.1750455, "iteration_found": 20, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_45_Gemm_Sigmoid_Sum_LogSumExp", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\n    raise ValueError(\"arange's range must be a power of 2\")\nValueError: arange's range must be a power of 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpcnqf8_ot.py\", line 69, in forward\n    _per_example_sum[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 19:16:\n    stride_bias,\n    stride_output,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n    batch_size: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n\n    # Precompute input feature offsets\n    k_offsets = tl.arange(0, input_size)\n                ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\n    raise ValueError(\"arange's range must be a power of 2\")\nValueError: arange's range must be a power of 2\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.11, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 30.000000000000007, "speed_up": 1.1, "runtime_ms": 0.1, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.11, "speed_up": 1.1, "custom_timing": 0.1}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 1.0000\n- score: 30.0000\n- speed_up: 1.1000\n- runtime_ms: 0.1000\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.1, 'custom_timing': 0.1}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: score, speed_up, runtime_ms. Consider continuing with similar changes.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_45_Gemm_Sigmoid_Sum_LogSumExp', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpv5tug287.py\", line 76, in forward\\n    _per_example_sum[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 28:4:\\n    if pid >= batch_size:\\n        return\\n\\n    # Vectorized indices\\n    j = tl.arange(0, BLOCK_HIDDEN)\\n    active = j < hidden_size\\n    bias_vals = tl.load(bias_ptr + j * stride_bias, mask=active, other=0.0)\\n    dot_vals = bias_vals\\n\\n    # Process input in chunks\\n    num_chunks = tl.cdiv(input_size, BLOCK_INPUT)\\n    for chunk in tl.static_range(num_chunks):\\n    ^\\nAssertionError(\\'int32[] used as tl.static_range start value is not a constexpr\\')\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 3120, in __init__\\n    assert isinstance(arg1, constexpr), f\"{arg1} used as tl.static_range start value is not a constexpr\"\\nAssertionError: int32[] used as tl.static_range start value is not a constexpr\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.11, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_45_Gemm_Sigmoid_Sum_LogSumExp', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmp03qb3ndt.py\", line 69, in forward\\n    _logsumexp[grid2](output_stage1, output_stage2, batch_size, BLOCK_SIZE=BLOCK_SIZE)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\\n    dependencies_finder.visit(self.parse())\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\\n    return visitor(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\\n    self.visit(item)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\\n    return visitor(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\\n    self.generic_visit(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\\n    self.visit(item)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\\n    return visitor(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\\n    self.generic_visit(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\\n    self.visit(value)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\\n    return visitor(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\\n    self.visit(item)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\\n    return visitor(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\\n    self.visit(value)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\\n    return visitor(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\\n    self.visit(value)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\\n    return visitor(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\\n    ret = getattr(lhs, node.attr)\\nAttributeError: module \\'triton.language.math\\' has no attribute \\'inf\\'\\n', 'runtime_error_name': 'builtins.AttributeError', 'reference_timing_ms': 0.11, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 30.0000, speed_up: 1.1000, runtime_ms: 0.1000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.1, 'custom_timing': 0.1}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 30.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n@triton.jit\ndef _per_example_sum(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x,\n    stride_weight,\n    stride_bias,\n    stride_output,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n    batch_size: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n    \n    s = 0.0\n    for j in range(hidden_size):\n        # Load bias value for current hidden unit\n        b = tl.load(bias_ptr + j * stride_bias)\n        dot = b\n        \n        # Compute dot product between input and weight vectors\n        for k in range(input_size):\n            x_val = tl.load(x_ptr + pid * stride_x + k)\n            w_val = tl.load(weight_ptr + j * stride_weight + k)\n            dot += x_val * w_val\n        \n        # Apply sigmoid and accumulate\n        sig = tl.sigmoid(dot)\n        s += sig\n    \n    # Store final sum for this example\n    tl.store(output_ptr + pid * stride_output, s)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = x.contiguous()\n        output_per_example = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        \n        # Prepare parameters\n        weight = self.linear1.weight.contiguous()\n        bias = self.linear1.bias.contiguous()\n        \n        # Launch kernel\n        grid = (batch_size,)\n        _per_example_sum[grid](\n            x, weight, bias, output_per_example,\n            x.stride(0), weight.stride(0), 1, 1,\n            self.input_size, self.hidden_size, batch_size\n        )\n        \n        # Final reduction\n        x = torch.logsumexp(output_per_example, dim=0)\n        return x\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (30.0000), Performs well on speed_up (1.1000), Performs well on runtime_ms (0.1000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.11, 'speed_up': 1.1, 'custom_timing': 0.1})\n\n\n### Program 2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_linear_sigmoid_sum(x_ptr, w_ptr, b_ptr, output_ptr, batch_size, input_size, hidden_size, BLOCK_SIZE_INPUT: tl.constexpr):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n    \n    row_sum = 0.0\n    for j in range(0, hidden_size):\n        dot = 0.0\n        for k in range(0, input_size, BLOCK_SIZE_INPUT):\n            offset = k + tl.arange(0, BLOCK_SIZE_INPUT)\n            mask = offset < input_size\n            x_i = tl.load(x_ptr + pid * input_size + offset, mask=mask, other=0.0)\n            w_j = tl.load(w_ptr + j * input_size + offset, mask=mask, other=0.0)\n            dot += tl.sum(x_i * w_j)\n        bias_j = tl.load(b_ptr + j)\n        dot += bias_j\n        s = 1.0 / (1.0 + tl.exp(-dot))\n        row_sum += s\n    \n    tl.store(output_ptr + pid, row_sum)\n\n@triton.jit\ndef _logsumexp(input_ptr, output_ptr, n, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    if pid >= 1:\n        return\n    \n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n\n    x = tl.load(input_ptr + offsets, mask=mask, other=-tl.math.inf)\n    \n    max_val = tl.max(x, axis=0)\n    exp_x = tl.exp(x - max_val)\n    sum_exp = tl.sum(exp_x, axis=0)\n    log_sum_exp = tl.log(sum_exp) + max_val\n    \n    tl.store(output_ptr, log_sum_exp)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        batch_size, input_size = x.shape\n        hidden_size = self.linear1.weight.shape[0]\n        \n        x = x.contiguous()\n        w = self.linear1.weight.contiguous()\n        b = self.linear1.bias.contiguous()\n        \n        output_stage1 = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        grid1 = (batch_size,)\n        BLOCK_SIZE_INPUT = 128\n        _fused_linear_sigmoid_sum[grid1](x, w, b, output_stage1, batch_size, input_size, hidden_size, BLOCK_SIZE_INPUT)\n        \n        output_stage2 = torch.empty(1, device=x.device, dtype=torch.float32)\n        grid2 = (1,)\n        BLOCK_SIZE = triton.next_power_of_2(batch_size)\n        _logsumexp[grid2](output_stage1, output_stage2, batch_size, BLOCK_SIZE=BLOCK_SIZE)\n        \n        return output_stage2[0]\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_45_Gemm_Sigmoid_Sum_LogSumExp', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmp03qb3ndt.py\", line 69, in forward\\n    _logsumexp[grid2](output_stage1, output_stage2, batch_size, BLOCK_SIZE=BLOCK_SIZE)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\\n    dependencies_finder.visit(self.parse())\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\\n    return visitor(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\\n    self.visit(item)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\\n    return visitor(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\\n    self.generic_visit(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\\n    self.visit(item)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\\n    return visitor(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\\n    self.generic_visit(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\\n    self.visit(value)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\\n    return visitor(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\\n    self.visit(item)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\\n    return visitor(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\\n    self.visit(value)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\\n    return visitor(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\\n    self.visit(value)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\\n    return visitor(node)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\\n    ret = getattr(lhs, node.attr)\\nAttributeError: module \\'triton.language.math\\' has no attribute \\'inf\\'\\n', 'runtime_error_name': 'builtins.AttributeError', 'reference_timing_ms': 0.11, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n### Program 3 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n@triton.jit\ndef _per_example_sum(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x,\n    stride_weight,\n    stride_bias,\n    stride_output,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n    batch_size: tl.constexpr,\n    BLOCK_INPUT: tl.constexpr,\n    BLOCK_HIDDEN: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n    \n    # Vectorized indices\n    j = tl.arange(0, BLOCK_HIDDEN)\n    active = j < hidden_size\n    bias_vals = tl.load(bias_ptr + j * stride_bias, mask=active, other=0.0)\n    dot_vals = bias_vals\n    \n    # Process input in chunks\n    num_chunks = tl.cdiv(input_size, BLOCK_INPUT)\n    for chunk in tl.static_range(num_chunks):\n        offs_k = chunk * BLOCK_INPUT + tl.arange(0, BLOCK_INPUT)\n        mask_k = offs_k < input_size\n        \n        # Load input vector chunk\n        x_vals = tl.load(x_ptr + pid * stride_x + offs_k, mask=mask_k, other=0.0)\n        \n        # Load weight matrix chunk\n        w_ptrs = weight_ptr + j[:, None] * stride_weight + offs_k[None, :]\n        w_vals = tl.load(w_ptrs, mask=active[:, None] & mask_k[None, :], other=0.0)\n        \n        # Compute partial dot products\n        partial_dot = tl.sum(x_vals[None, :] * w_vals, axis=1)\n        dot_vals += partial_dot\n    \n    # Apply sigmoid and reduce\n    sig_vals = tl.sigmoid(dot_vals)\n    s = tl.sum(tl.where(active, sig_vals, 0.0), axis=0)\n    \n    # Store result\n    tl.store(output_ptr + pid * stride_output, s)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.bias = nn.Parameter(torch.randn(hidden_size))\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = x.contiguous()\n        output_per_example = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        \n        # Tuning parameters\n        BLOCK_INPUT = 128\n        BLOCK_HIDDEN = min(128, triton.next_power_of_2(self.hidden_size))\n        \n        grid = (batch_size,)\n        _per_example_sum[grid](\n            x, self.weight, self.bias, output_per_example,\n            x.stride(0), self.weight.stride(0), 1, 1,\n            self.input_size, self.hidden_size, batch_size,\n            BLOCK_INPUT, BLOCK_HIDDEN\n        )\n        x = torch.logsumexp(output_per_example, dim=0)\n        return x\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_45_Gemm_Sigmoid_Sum_LogSumExp', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpv5tug287.py\", line 76, in forward\\n    _per_example_sum[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 28:4:\\n    if pid >= batch_size:\\n        return\\n\\n    # Vectorized indices\\n    j = tl.arange(0, BLOCK_HIDDEN)\\n    active = j < hidden_size\\n    bias_vals = tl.load(bias_ptr + j * stride_bias, mask=active, other=0.0)\\n    dot_vals = bias_vals\\n\\n    # Process input in chunks\\n    num_chunks = tl.cdiv(input_size, BLOCK_INPUT)\\n    for chunk in tl.static_range(num_chunks):\\n    ^\\nAssertionError(\\'int32[] used as tl.static_range start value is not a constexpr\\')\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 3120, in __init__\\n    assert isinstance(arg1, constexpr), f\"{arg1} used as tl.static_range start value is not a constexpr\"\\nAssertionError: int32[] used as tl.static_range start value is not a constexpr\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.11, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n@triton.jit\ndef _per_example_sum(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x,\n    stride_weight,\n    stride_bias,\n    stride_output,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n    batch_size: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n\n    s = 0.0\n    for j in range(hidden_size):\n        b = tl.load(bias_ptr + j * stride_bias)\n        dot = b\n        for k in range(input_size):\n            x_val = tl.load(x_ptr + pid * stride_x + k)\n            w_val = tl.load(weight_ptr + j * stride_weight + k)\n            dot += x_val * w_val\n        sig = 1.0 / (1.0 + tl.exp(-dot))\n        s += sig\n    tl.store(output_ptr + pid * stride_output, s)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.bias = nn.Parameter(torch.randn(hidden_size))\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = x.contiguous()\n        output_per_example = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        grid = (triton.cdiv(batch_size, 1),)\n        _per_example_sum[grid](\n            x, self.weight, self.bias, output_per_example,\n            x.stride(0), self.weight.stride(0), 1, 1,\n            self.input_size, self.hidden_size, batch_size\n        )\n        x = torch.logsumexp(output_per_example, dim=0)\n        return x\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), applies Sigmoid, sums the result, and calculates the LogSumExp.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = torch.sigmoid(x)\n        x = torch.sum(x, dim=1)\n        x = torch.logsumexp(x, dim=0)\n        return x\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n@triton.jit\ndef _per_example_sum(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x,\n    stride_weight,\n    stride_bias,\n    stride_output,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n    batch_size: tl.constexpr,\n    BLOCK_INPUT: tl.constexpr,\n    BLOCK_HIDDEN: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n    \n    # Vectorized indices\n    j = tl.arange(0, BLOCK_HIDDEN)\n    active = j < hidden_size\n    bias_vals = tl.load(bias_ptr + j * stride_bias, mask=active, other=0.0)\n    dot_vals = bias_vals\n    \n    # Process input in chunks\n    num_chunks = tl.cdiv(input_size, BLOCK_INPUT)\n    for chunk in tl.static_range(num_chunks):\n        offs_k = chunk * BLOCK_INPUT + tl.arange(0, BLOCK_INPUT)\n        mask_k = offs_k < input_size\n        \n        # Load input vector chunk\n        x_vals = tl.load(x_ptr + pid * stride_x + offs_k, mask=mask_k, other=0.0)\n        \n        # Load weight matrix chunk\n        w_ptrs = weight_ptr + j[:, None] * stride_weight + offs_k[None, :]\n        w_vals = tl.load(w_ptrs, mask=active[:, None] & mask_k[None, :], other=0.0)\n        \n        # Compute partial dot products\n        partial_dot = tl.sum(x_vals[None, :] * w_vals, axis=1)\n        dot_vals += partial_dot\n    \n    # Apply sigmoid and reduce\n    sig_vals = tl.sigmoid(dot_vals)\n    s = tl.sum(tl.where(active, sig_vals, 0.0), axis=0)\n    \n    # Store result\n    tl.store(output_ptr + pid * stride_output, s)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.bias = nn.Parameter(torch.randn(hidden_size))\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = x.contiguous()\n        output_per_example = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        \n        # Tuning parameters\n        BLOCK_INPUT = 128\n        BLOCK_HIDDEN = min(128, triton.next_power_of_2(self.hidden_size))\n        \n        grid = (batch_size,)\n        _per_example_sum[grid](\n            x, self.weight, self.bias, output_per_example,\n            x.stride(0), self.weight.stride(0), 1, 1,\n            self.input_size, self.hidden_size, batch_size,\n            BLOCK_INPUT, BLOCK_HIDDEN\n        )\n        x = torch.logsumexp(output_per_example, dim=0)\n        return x\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n@triton.jit\ndef _per_example_sum(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x,\n    stride_weight,\n    stride_bias,\n    stride_output,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n    batch_size: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n\n    s = 0.0\n    for j in range(hidden_size):\n        b = tl.load(bias_ptr + j * stride_bias)\n        dot = b\n        for k in range(input_size):\n            x_val = tl.load(x_ptr + pid * stride_x + k)\n            w_val = tl.load(weight_ptr + j * stride_weight + k)\n            dot += x_val * w_val\n        sig = 1.0 / (1.0 + tl.exp(-dot))\n        s += sig\n    tl.store(output_ptr + pid * stride_output, s)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.bias = nn.Parameter(torch.randn(hidden_size))\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = x.contiguous()\n        output_per_example = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        grid = (triton.cdiv(batch_size, 1),)\n        _per_example_sum[grid](\n            x, self.weight, self.bias, output_per_example,\n            x.stride(0), self.weight.stride(0), 1, 1,\n            self.input_size, self.hidden_size, batch_size\n        )\n        x = torch.logsumexp(output_per_example, dim=0)\n        return x\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_linear_sigmoid_sum(x_ptr, w_ptr, b_ptr, output_ptr, batch_size, input_size, hidden_size, BLOCK_SIZE_INPUT: tl.constexpr):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n    \n    row_sum = 0.0\n    for j in range(0, hidden_size):\n        dot = 0.0\n        for k in range(0, input_size, BLOCK_SIZE_INPUT):\n            offset = k + tl.arange(0, BLOCK_SIZE_INPUT)\n            mask = offset < input_size\n            x_i = tl.load(x_ptr + pid * input_size + offset, mask=mask, other=0.0)\n            w_j = tl.load(w_ptr + j * input_size + offset, mask=mask, other=0.0)\n            dot += tl.sum(x_i * w_j)\n        bias_j = tl.load(b_ptr + j)\n        dot += bias_j\n        s = 1.0 / (1.0 + tl.exp(-dot))\n        row_sum += s\n    \n    tl.store(output_ptr + pid, row_sum)\n\n@triton.jit\ndef _logsumexp(input_ptr, output_ptr, n, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    if pid >= 1:\n        return\n    \n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n\n    x = tl.load(input_ptr + offsets, mask=mask, other=-tl.math.inf)\n    \n    max_val = tl.max(x, axis=0)\n    exp_x = tl.exp(x - max_val)\n    sum_exp = tl.sum(exp_x, axis=0)\n    log_sum_exp = tl.log(sum_exp) + max_val\n    \n    tl.store(output_ptr, log_sum_exp)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        batch_size, input_size = x.shape\n        hidden_size = self.linear1.weight.shape[0]\n        \n        x = x.contiguous()\n        w = self.linear1.weight.contiguous()\n        b = self.linear1.bias.contiguous()\n        \n        output_stage1 = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        grid1 = (batch_size,)\n        BLOCK_SIZE_INPUT = 128\n        _fused_linear_sigmoid_sum[grid1](x, w, b, output_stage1, batch_size, input_size, hidden_size, BLOCK_SIZE_INPUT)\n        \n        output_stage2 = torch.empty(1, device=x.device, dtype=torch.float32)\n        grid2 = (1,)\n        BLOCK_SIZE = triton.next_power_of_2(batch_size)\n        _logsumexp[grid2](output_stage1, output_stage2, batch_size, BLOCK_SIZE=BLOCK_SIZE)\n        \n        return output_stage2[0]\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: -100.0000, Type: Exploratory)\n```python\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)   # This is defined but not used in forward\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = torch.sigmoid(x)\n        x = torch.sum(x, dim=1)\n        x = torch.logsumexp(x, dim=0)\n        return x\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), applies Sigmoid, sums the result, and calculates the LogSumExp.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = torch.sigmoid(x)\n        x = torch.sum(x, dim=1)\n        x = torch.logsumexp(x, dim=0)\n        return x\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nUnique approach: Excellent compiled (1.000), Excellent correctness (1.000), Alternative passed approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n@triton.jit\ndef _per_example_sum(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x,\n    stride_weight,\n    stride_bias,\n    stride_output,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n    batch_size: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n    \n    s = 0.0\n    for j in range(hidden_size):\n        # Load bias value for current hidden unit\n        b = tl.load(bias_ptr + j * stride_bias)\n        dot = b\n        \n        # Compute dot product between input and weight vectors\n        for k in range(input_size):\n            x_val = tl.load(x_ptr + pid * stride_x + k)\n            w_val = tl.load(weight_ptr + j * stride_weight + k)\n            dot += x_val * w_val\n        \n        # Apply sigmoid and accumulate\n        sig = tl.sigmoid(dot)\n        s += sig\n    \n    # Store final sum for this example\n    tl.store(output_ptr + pid * stride_output, s)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = x.contiguous()\n        output_per_example = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        \n        # Prepare parameters\n        weight = self.linear1.weight.contiguous()\n        bias = self.linear1.bias.contiguous()\n        \n        # Launch kernel\n        grid = (batch_size,)\n        _per_example_sum[grid](\n            x, weight, bias, output_per_example,\n            x.stride(0), weight.stride(0), 1, 1,\n            self.input_size, self.hidden_size, batch_size\n        )\n        \n        # Final reduction\n        x = torch.logsumexp(output_per_example, dim=0)\n        return x\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program, which has a Triton kernel `_per_example_sum` that computes for each example in the batch:\n  1. Linear transformation (with weights and bias) for a hidden layer.\n  2. Applies sigmoid to each element of the hidden layer.\n  3. Sums the sigmoid outputs for the example.\n\nThen, in the model's forward, we do a logsumexp over the per-example sums.\n\nThe current implementation uses nested loops in Triton: an outer loop over hidden units and an inner loop over input features.\n\nWe are achieving a speedup of 1.1x (10% improvement) but we want to improve further.\n\nAreas for improvement:\n1. The current kernel is memory bound and does not leverage vectorization or block operations.\n2. We are doing a lot of small loads (each weight and input element individually) which is inefficient.\n\nPlan:\nWe can restructure the kernel to process blocks of input features and hidden units. This will allow us to:\n  - Load contiguous blocks of weights and inputs into registers or shared memory.\n  - Use vectorized operations and better cache utilization.\n\nHowever, note that the current top-performing program (Program 1) uses a similar nested loop and achieves 1.1x speedup.\nWe also see that attempts to use tiling (like in Program 3) led to compilation errors because of the use of non-constexpr in static_range.\n\nLet's try to rewrite the kernel with tiling without using `tl.static_range` (which requires a constexpr) by using a regular loop and blocking.\n\nWe will:\n  - Block the input dimension (K) and the hidden dimension (J) to process in chunks.\n  - For each example (batch element), we will compute a row of the hidden layer by processing blocks of input features and accumulating in a tile of hidden units.\n\nConstraints:\n  - We cannot change the function signature of the Triton kernel arbitrarily (but we can add new constexpr block sizes).\n\nWe note that the hidden_size (20) and input_size (10) are small. Therefore, we might not need to tile the hidden dimension. Instead, we can process the entire hidden dimension per program and tile the input dimension.\n\nProposed kernel structure:\n  - Each program handles one batch element (pid).\n  - We use a 1D block for the hidden dimension? Actually, we can process the entire hidden dimension in one go by having a block of hidden_size (but hidden_size is 20, which is small).\n\nAlternatively, we can use vectorization in the inner loop. We can load a block of input features and a corresponding block of weights for multiple hidden units at once.\n\nGiven the small sizes, we can try to load the entire weight row for a hidden unit in one go? But input_size=10, so we can load 10 elements. However, 10 is not a power of two and might not be vectorized optimally.\n\nWe can do:\n  - For each hidden unit, we load the entire weight row and the entire input row? But that would be 10*20=200 loads per batch element. Currently, we are doing 20*10=200 loads as well.\n\nBut note: the current kernel does:\n  for j in range(hidden_size):   # 20 times\n      load bias (1 load)\n      for k in range(input_size): # 10 times\n          load x (1 load) and weight (1 load) -> 20*10*2 = 400 loads per example.\n\nWe can reduce by loading the input once and then reusing it for all hidden units? That would be 10 (for x) + 20*10 (for weights) = 210 loads.\n\nAlternatively, we can use tiling in the input dimension. Let's set a block size for the input dimension (BLOCK_INPUT). We will:\n  - Load a block of input features (say 4, 8, or 10 if we can do the whole thing at once).\n  - For each hidden unit, we load a block of weights and compute a partial dot product.\n\nBut note: the hidden_size is only 20, so we can process all hidden units in one program without tiling the hidden dimension.\n\nWe can do:\n\n  pid = program_id(0)  # batch element index\n\n  # Preload the entire input for this example? But we have to be careful about the size (10 is small).\n  # Let's preload the input into registers.\n\n  # Preload input for this example\n  k_offsets = tl.arange(0, input_size)\n  x_vals = tl.load(x_ptr + pid * stride_x + k_offsets)\n\n  s = 0.0\n  for j in range(hidden_size):\n      # Load the entire weight row for hidden unit j\n      w_ptrs = weight_ptr + j * stride_weight + k_offsets\n      w_vals = tl.load(w_ptrs)   # This is a vector of input_size elements\n\n      # Compute dot product\n      dot = tl.sum(x_vals * w_vals)   # This is efficient? Triton can do a vector dot product.\n\n      # Add bias\n      b = tl.load(bias_ptr + j * stride_bias)\n      dot += b\n\n      # Sigmoid and accumulate\n      sig = tl.sigmoid(dot)\n      s += sig\n\n  tl.store(output_ptr + pid * stride_output, s)\n\nBut note: the above preloads the input once and then for each j, loads the entire weight row. This reduces the number of loads from 400 to 10 (for x) + 20*10 (for weights) = 210.\n\nHowever, we are still doing 20*10 = 200 loads for weights. We might be able to block the hidden dimension? But 20 is small.\n\nAlternatively, we can load the entire weight matrix? That would be 20*10=200 elements. But we cannot because of register pressure? We have 20*10=200 elements and each thread would need 200 registers? That is too high.\n\nTherefore, we stick with the above.\n\nBut note: the current kernel has two nested loops and we are replacing it with a single loop over j and a vectorized dot product. This should be faster because the inner loop is replaced by a hardware-accelerated vector operation.\n\nLet's write the kernel accordingly.\n\nWe must be cautious about the mask for the input preload because input_size might not be a multiple of the block size? Actually, we are using the exact input_size. But we can set the block size to input_size and then mask if necessary? However, we are iterating exactly the input_size.\n\nWe can do:\n\n  k_offsets = tl.arange(0, input_size)\n  mask = k_offsets < input_size   # This will be all true, but we use it for safety.\n\nBut note: input_size is a constexpr, so we know it at compile time. We can avoid masking if we are sure that we are not going out of bounds? Actually, we are loading exactly input_size elements.\n\nWe can also consider that the input_size might be small (10) so we can set BLOCK_INPUT = input_size.\n\nSimilarly, we can set BLOCK_HIDDEN = hidden_size? But we are not tiling the hidden dimension.\n\nWe'll try to write the kernel without tiling the hidden dimension and with a single preload of the input.\n\nHowever, note that the input_size and hidden_size are provided as constexpr, so we can use them.\n\nBut what if they are large? We are only dealing with 10 and 20 in this example, but the kernel should be general.\n\nWe can set a maximum block size? But 10 and 20 are small.\n\nAlternatively, we can use a fixed block size for the input dimension and loop over chunks? But for small sizes, we can do the whole input at once.\n\nWe'll do:\n\n  BLOCK_K = input_size   # We are going to load the entire input at once.\n\nBut we can also make it a constexpr that we set from the caller.\n\nBut note: the kernel signature must not change arbitrarily? We are allowed to add new constexpr parameters.\n\nWe'll add two constexpr parameters: BLOCK_INPUT and BLOCK_HIDDEN, but we will set BLOCK_INPUT = input_size and BLOCK_HIDDEN = hidden_size? Actually, we don't need to tile the hidden dimension in this kernel.\n\nBut we cannot have BLOCK_INPUT as a constexpr that exceeds the input_size? We can set it to min(input_size, some_max) but we are going to set it to input_size.\n\nAlternatively, we can avoid the constexpr for block size and just use input_size and hidden_size. Since they are constexpr, we can use them in the tl.arange.\n\nWe'll write:\n\n  k_offsets = tl.arange(0, input_size)\n  mask_k = k_offsets < input_size   # always true, but for correctness\n\n  x_vals = tl.load(x_ptr + pid * stride_x + k_offsets, mask=mask_k, other=0.0)\n\n  for j in range(hidden_size):\n      w_ptrs = weight_ptr + j * stride_weight + k_offsets\n      w_vals = tl.load(w_ptrs, mask=mask_k, other=0.0)\n\n      dot = tl.sum(x_vals * w_vals)   # dot product of two vectors of length input_size\n\n      ... \n\nThis should work.\n\nBut note: the current kernel does not use vectorized loads. This version uses vectorized loads for the entire input and weight row.\n\nWe expect this to be faster.\n\nHowever, we must check the maximum block size for the vector operation. The Triton compiler might break it down if input_size is too large? But 10 is small.\n\nLet's code accordingly.\n\nWe'll change the kernel to:\n\n  pid = tl.program_id(0)\n  if pid >= batch_size:\n      return\n\n  # Offsets for the input features\n  k_offsets = tl.arange(0, input_size)\n  mask_k = k_offsets < input_size   # This mask is for safety if input_size is not a multiple of 1? Actually, we are using the whole input.\n\n  # Load the entire input for this example\n  x_vals = tl.load(x_ptr + pid * stride_x + k_offsets, mask=mask_k, other=0.0)\n\n  s = 0.0\n  for j in range(hidden_size):\n      # Load the weight row for this hidden unit\n      w_ptrs = weight_ptr + j * stride_weight + k_offsets\n      w_vals = tl.load(w_ptrs, mask=mask_k, other=0.0)\n\n      # Compute dot product\n      dot = tl.sum(x_vals * w_vals)\n\n      # Load bias and add\n      b = tl.load(bias_ptr + j * stride_bias)\n      dot += b\n\n      # Sigmoid and accumulate\n      sig = tl.sigmoid(dot)\n      s += sig\n\n  tl.store(output_ptr + pid * stride_output, s)\n\nThis kernel should be more efficient because it uses vectorized loads and a vector dot product.\n\nBut note: the inner loop over j (hidden_size) is still there. Since hidden_size=20 is small, it should be acceptable.\n\nWe'll run and see.\n\nWe must update the call site to not change the grid and other parameters. We are only changing the kernel.\n\nWe are adding two new constexpr parameters? Actually, no, we are using input_size and hidden_size which are already provided.\n\nBut note: the current kernel does not have a loop over k, so we are not breaking the inner loop into tiles. We are replacing the inner loop with a vector operation.\n\nWe'll write the kernel accordingly.\n\nHowever, we must note that the current kernel has been verified for correctness and we are changing the order of operations? The arithmetic should be the same.\n\nLet's generate the code accordingly.\n\nWe are going to replace the kernel with the new one.\n\nWe note that the current program is:\n\n# ================== EVOLVE-BLOCK-START ==================\n... [current code] ...\n# =================== EVOLVE-BLOCK-END ===================\n\nWe will replace the kernel but leave the rest unchanged.\n\nLet's write the new kernel inside the EVOLVE-BLOCK.\n\nImportant: We must not change the class name (ModelNew) or the function signatures.\n\nWe'll write:\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n@triton.jit\ndef _per_example_sum(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x,\n    stride_weight,\n    stride_bias,\n    stride_output,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n    batch_size: tl.constexpr\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n    \n    # Precompute input feature offsets\n    k_offsets = tl.arange(0, input_size)\n    mask_k = k_offsets < input_size\n    \n    # Load entire input vector for current example\n    x_vals = tl.load(x_ptr + pid * stride_x + k_offsets, mask=mask_k, other=0.0)\n    \n    s = 0.0\n    for j in range(hidden_size):\n        # Load entire weight row for current hidden unit\n        w_ptrs = weight_ptr + j * stride_weight + k_offsets\n        w_vals = tl.load(w_ptrs, mask=mask_k, other=0.0)\n        \n        # Compute vectorized dot product\n        dot = tl.sum(x_vals * w_vals)\n        dot += tl.load(bias_ptr + j * stride_bias)\n        \n        # Apply sigmoid and accumulate\n        sig = tl.sigmoid(dot)\n        s += sig\n    \n    # Store final sum for this example\n    tl.store(output_ptr + pid * stride_output, s)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = x.contiguous()\n        output_per_example = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        \n        # Prepare parameters\n        weight = self.linear1.weight.contiguous()\n        bias = self.linear1.bias.contiguous()\n        \n        # Launch kernel\n        grid = (batch_size,)\n        _per_example_sum[grid](\n            x, weight, bias, output_per_example,\n            x.stride(0), weight.stride(0), 1, 1,\n            self.input_size, self.hidden_size, batch_size\n        )\n        \n        # Final reduction\n        x = torch.logsumexp(output_per_example, dim=0)\n        return x\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\noutput_size = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
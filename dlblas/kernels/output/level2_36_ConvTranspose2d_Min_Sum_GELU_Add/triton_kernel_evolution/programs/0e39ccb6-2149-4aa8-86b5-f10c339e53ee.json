{"id": "0e39ccb6-2149-4aa8-86b5-f10c339e53ee", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_reduction(\n    x_ptr, \n    bias_ptr,\n    out_ptr,\n    height, \n    width, \n    channels,\n    x_batch_stride, \n    x_channel_stride, \n    x_height_stride, \n    x_width_stride,\n    out_batch_stride,\n    out_channel_stride,\n    out_height_stride,\n    out_width_stride,\n    BLOCK_C: tl.constexpr,\n    BLOCK_H: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_width = tl.program_id(1)\n    \n    # Initialize accumulator\n    accumulator = 0.0\n    \n    # Process height in blocks\n    for h_block in range(0, height, BLOCK_H):\n        h_offsets = h_block + tl.arange(0, BLOCK_H)\n        h_mask = h_offsets < height\n        \n        # Initialize min values for current height block\n        min_val_block = tl.full((BLOCK_H,), float('inf'), dtype=tl.float32)\n        \n        # Process channels in blocks\n        for c_block in range(0, channels, BLOCK_C):\n            c_offsets = c_block + tl.arange(0, BLOCK_C)\n            c_mask = c_offsets < channels\n            \n            # Calculate memory offsets and load data block\n            offs = pid_batch * x_batch_stride + \\\n                   c_offsets[:, None] * x_channel_stride + \\\n                   h_offsets[None, :] * x_height_stride + \\\n                   pid_width * x_width_stride\n            vals = tl.load(x_ptr + offs, \n                          mask=c_mask[:, None] & h_mask[None, :], \n                          other=float('inf'))\n            \n            # Reduce along channel dimension\n            block_min = tl.min(vals, axis=0)\n            min_val_block = tl.minimum(min_val_block, block_min)\n        \n        # Accumulate min values for valid heights\n        accumulator += tl.sum(tl.where(h_mask, min_val_block, 0.0))\n    \n    # Compute GELU activation\n    GELU_SCALE = 0.7071067811865476\n    gelu_val = 0.5 * accumulator * (1.0 + tl.erf(accumulator * GELU_SCALE))\n    \n    # Vectorized bias addition and output storage\n    for c_block in range(0, channels, BLOCK_C):\n        c_offsets = c_block + tl.arange(0, BLOCK_C)\n        c_mask = c_offsets < channels\n        \n        # Load bias block\n        bias_vals = tl.load(bias_ptr + c_offsets, mask=c_mask)\n        \n        # Compute output values\n        out_vals = gelu_val + bias_vals\n        \n        # Calculate output offsets and store\n        out_offsets = pid_batch * out_batch_stride + \\\n                      c_offsets * out_channel_stride + \\\n                      pid_width * out_width_stride\n        tl.store(out_ptr + out_offsets, out_vals, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        batch, channels, height, width = x.shape\n        x = x.contiguous()\n        bias_flat = self.bias.view(-1)\n        \n        output = torch.empty(batch, channels, 1, width, device=x.device, dtype=x.dtype)\n        grid = (batch, width)\n        \n        x_stride_b, x_stride_c, x_stride_h, x_stride_w = x.stride()\n        out_stride_b, out_stride_c, out_stride_h, out_stride_w = output.stride()\n        \n        # Tuned block sizes for H100 architecture\n        BLOCK_C = 16\n        BLOCK_H = 64\n        \n        _fused_reduction[grid](\n            x, bias_flat, output,\n            height, width, channels,\n            x_stride_b, x_stride_c, x_stride_h, x_stride_w,\n            out_stride_b, out_stride_c, out_stride_h, out_stride_w,\n            BLOCK_C=BLOCK_C, \n            BLOCK_H=BLOCK_H\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "e2c51bd1-7670-42b4-bda7-440d7184d5f1", "generation": 3, "timestamp": 1754570819.9960446, "iteration_found": 25, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.842, "runtime_ms": 0.228, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.192, "speed_up": 0.842, "custom_timing": 0.228}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.356, "runtime_ms": 0.54, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.192, "speed_up": 0.356, "custom_timing": 0.54}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
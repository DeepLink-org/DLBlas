{"id": "dee3fe90-240d-4394-b3a2-7e5ad95af2de", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_act_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride parameters\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    # Operation parameters\n    scaling_factor: tl.constexpr,\n    hardtanh_min: tl.constexpr,\n    hardtanh_max: tl.constexpr,\n    # Blocking\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_n[None, :] < N), other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    # Add bias\n    bias_ptrs = bias_ptr + offs_n\n    bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n    accumulator += bias[None, :]\n    \n    # Apply scaling\n    accumulator = accumulator * scaling_factor\n    \n    # Apply hardtanh\n    accumulator = tl.minimum(tl.maximum(accumulator, hardtanh_min), hardtanh_max)\n    \n    # Apply GELU approximation\n    accumulator = accumulator * 0.5 * (1.0 + tl.erf(accumulator * 0.7071067811865475))\n    \n    # Write back\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n        \n        # Initialize weights\n        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = torch.nn.Parameter(torch.randn(out_features))\n        \n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        # Preallocate output\n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        \n        # Launch kernel\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        fused_gemm_act_kernel[grid](\n            x, self.weight, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            self.scaling_factor,\n            self.hardtanh_min,\n            self.hardtanh_max,\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "2024c81a-b1d3-443f-a949-4bc1a1e66dca", "generation": 1, "timestamp": 1754572902.0258868, "iteration_found": 3, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_53_Gemm_Scaling_Hardtanh_GELU", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "max_difference": ["2.124471", "2.124471", "2.124471", "2.124470", "2.124471"], "avg_difference": ["0.944960", "0.949327", "0.949207", "0.959383", "0.947962"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.098, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0653, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.098, "speed_up": 1.501, "custom_timing": 0.0653}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0653\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.098, 'speed_up': 1.501, 'custom_timing': 0.0653}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0653, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.098, 'speed_up': 1.501, 'custom_timing': 0.0653}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, scaling, hardtanh, and GELU activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n        self.gelu = nn.GELU()\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scaling_factor\n        x = self.hardtanh(x)\n        x = self.gelu(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0653), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.098, 'speed_up': 1.501, 'custom_timing': 0.0653})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, scaling, hardtanh, and GELU activation.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n        self.gelu = nn.GELU()\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scaling_factor\n        x = self.hardtanh(x)\n        x = self.gelu(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear layer (GEMM)\n  2. Scaling (multiplication by a scalar)\n  3. Hardtanh (clamp between min and max)\n  4. GELU activation\n\nThe goal is to rewrite this model using Triton to accelerate the operations, especially the GEMM and the pointwise operations.\n\nHowever, note that the current implementation is entirely in PyTorch and uses `nn.Linear` which is already optimized (using cuBLAS for GEMM). To get a speedup, we must replace the GEMM and the subsequent operations with a Triton kernel that fuses these operations.\n\nPlan:\n  - We will write a Triton kernel that fuses the entire forward pass: GEMM, scaling, hardtanh, and GELU.\n  - The GEMM part will be implemented using block matrix multiplication with Tensor Cores (for FP16).\n  - We assume the input is in FP16 (or we can convert to FP16 for Tensor Core efficiency). But note: the original model does not specify, so we must be cautious.\n\nConstraints:\n  - The input tensor is of shape (batch_size, in_features) = (128, 1024)\n  - The weight matrix is of shape (out_features, in_features) = (512, 1024)\n\nSteps in the kernel:\n  1. We will split the batch dimension and the feature dimensions into blocks.\n  2. Each program will compute a block of the output matrix (for example, BLOCK_SIZE_M x BLOCK_SIZE_N).\n  3. We will use a block for the reduction dimension (K) and iterate over the in_features.\n\nAdditionally, we will fuse the scaling, hardtanh, and GELU operations so that we do not write the intermediate GEMM result to global memory until the very end.\n\nWe note:\n  - The current model uses `nn.Linear` which is FP32 by default? We must check the input. But the problem says we can use FP16 for Tensor Cores. We'll assume we can use FP16.\n\nApproach for the kernel:\n  - We'll have two main dimensions: batch (M) and output features (N). The reduction is over the input features (K).\n  - We'll precompute pointers and strides.\n  - We'll load blocks of the input and weight matrices.\n  - We'll accumulate the matrix product in a local register.\n  - Then we scale, apply hardtanh, and then GELU.\n\nHowever, note that the problem says: \n  - We cannot change the function signatures of the @triton.jit function? But in this case, we are writing a new kernel.\n\nBut note: the original code structure is:\n  class ModelNew(nn.Module):\n      ... with a forward method.\n\nWe are going to change the forward method to call our Triton kernel. We must also initialize the weights appropriately.\n\nSteps in the module:\n  1. We will store the weight matrix and the parameters (scaling_factor, hardtanh_min, hardtanh_max) as before.\n  2. In the forward pass, we will call the Triton kernel.\n\nWe must write:\n  - A Triton kernel for the entire fused operation.\n\nImportant: We must handle the bias? The current nn.Linear has a bias by default. But in the original code, we did not see a bias. Let me check: `nn.Linear` has bias=True by default. However, in the problem statement, the original code did not show the Linear layer having a bias? We must be cautious.\n\nLooking at the original ModelNew:\n  self.gemm = nn.Linear(...)   -> this includes a bias.\n\nBut note: the original forward pass does:\n  x = self.gemm(x)   -> which is (batch, in) -> (batch, out) and adds a bias.\n\nTherefore, our Triton kernel must support bias.\n\nSo the operations are:\n  x = x @ weight.t() + bias   [GEMM with bias]\n  x = x * scaling_factor\n  x = hardtanh(x, min_val, max_val)\n  x = gelu(x)\n\nWe will write a kernel that does all of these.\n\nKernel design:\n\n  We'll have:\n    input: x (batch_size, in_features)\n    weight: (out_features, in_features)\n    bias: (out_features,)\n\n  We want to compute:\n    out = gelu( hardtanh( (x @ weight.t() + bias) * scaling_factor ) )\n\n  But note: the scaling factor is a scalar, and hardtanh has min and max.\n\n  We can break down the pointwise operations:\n\n    y = (x @ weight.t() + bias) * scaling_factor\n    y = y.clamp(min=hardtanh_min, max=hardtanh_max)\n    y = gelu(y)\n\n  We can do the clamp and gelu in the same kernel.\n\nImplementation:\n\n  We'll use a 2D grid:\n    grid = (triton.cdiv(out_features, BLOCK_SIZE_N), batch_size)\n\n  But note: the batch dimension is 128, which is small. Alternatively, we can use:\n\n    grid = (triton.cdiv(batch_size, BLOCK_SIZE_M), triton.cdiv(out_features, BLOCK_SIZE_N))\n\n  Then each block computes a tile of [BLOCK_SIZE_M, BLOCK_SIZE_N] in the output.\n\n  Steps:\n\n    pid_m = tl.program_id(0)   # batch dimension\n    pid_n = tl.program_id(1)    # output feature dimension\n\n    We then have offsets in the batch and output feature.\n\n    We need to iterate over the input features (K) to compute the dot product.\n\n    We'll load a block of x: [BLOCK_SIZE_M, BLOCK_SIZE_K] and a block of weight: [BLOCK_SIZE_N, BLOCK_SIZE_K] and compute the dot product.\n\n    Then we add the bias (for the entire column, but we only have a block in the column) and then do the pointwise operations.\n\n  However, note that the bias is a vector of length out_features. We can load a block of the bias for the current BLOCK_SIZE_N.\n\n  Then we do:\n    acc = acc + bias[offsets_n]   # but note: bias is 1D of size out_features, so we load a block of it for the current pid_n\n\n  Then:\n    acc = acc * scaling_factor\n    acc = tl.minimum(tl.maximum(acc, hardtanh_min), hardtanh_max)\n    acc = gelu(acc)   # we can use tl.gelu from triton.language\n\n  Then store.\n\nBut note: the input x is (batch, in_features) and weight is (out_features, in_features). So the matrix multiplication is:\n\n    out = x @ weight.t()   -> (batch, out_features)\n\nWe are using the typical GEMM formulation: row-major.\n\nWe must be cautious about the memory layout.\n\nWe'll use:\n  x_ptr: pointer to x, which is (batch, in_features), row-major.\n  w_ptr: pointer to weight, which is (out_features, in_features), row-major? Actually, the Linear layer stores weight as (out_features, in_features) and we access it row by row.\n\nIn the kernel, we will:\n\n  offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n  offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n  offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n  x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n  w_ptrs = w_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk   # note: weight is stored as (out_features, in_features) so the first stride is stride_wn (for output feature) and then stride_wk (for input feature).\n\nBut note: the weight matrix is stored in row-major, so the stride between two consecutive output features is in_features (stride_wn = in_features? actually, the stride for the output feature dimension is in_features, and the stride for the input feature dimension is 1?).\n\nActually, the weight tensor is 2D: (out_features, in_features). The stride for the first dimension (out_features) is in_features, and the second dimension is 1.\n\nSimilarly, the input tensor x is (batch, in_features): the stride for batch is in_features and for in_features is 1.\n\nSo we can define:\n  stride_xm = in_features   # because moving to next batch row, we skip in_features elements\n  stride_xk = 1\n  stride_wn = in_features   # because moving to next output feature row, we skip in_features elements\n  stride_wk = 1\n\nBut note: the input tensor x might be non-contiguous? We can require it to be contiguous.\n\nWe'll also load the bias:\n  bias_ptrs = bias_ptr + offs_n\n\nWe must mask for the boundaries in M, N, and K.\n\nWe will set:\n  BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K.\n\nWe'll choose these to be powers of two and suitable for Tensor Cores (e.g., 64, 128, 32) and for the hardware (H100 has 128x256x32 for FP16).\n\nBut note: the problem says we have FP16 Tensor Core 32x16x16. So we should use at least 16 for the inner dimension? Actually, the Tensor Core operation is 32x16x16. We can set BLOCK_SIZE_K=32? and then BLOCK_SIZE_M=32, BLOCK_SIZE_N=32? But we can also use larger blocks.\n\nWe'll use tunable parameters for BLOCK_SIZE_M, BLOCK_SIZE_N, and BLOCK_SIZE_K.\n\nWe'll use:\n  @triton.autotune(...) for these.\n\nBut note: the problem says we can modify the block dimensions.\n\nWe'll write the kernel and then set up the autotune.\n\nHowever, note the problem constraints:\n  - We cannot change the grid configuration? Actually, we are defining the grid. The grid is determined by the batch size and output features.\n\n  We'll set:\n    grid = (triton.cdiv(batch_size, BLOCK_SIZE_M), triton.cdiv(out_features, BLOCK_SIZE_N))\n\n  We must not change the output tensor shapes.\n\nWe are going to replace the entire forward function with a Triton kernel call.\n\nSteps in the module:\n\n  We will initialize:\n    self.weight = nn.Parameter(torch.randn(out_features, in_features))\n    self.bias = nn.Parameter(torch.randn(out_features))   # note: by default, Linear has bias. So we must have one.\n\n  But in the original, the Linear layer is initialized with bias. So we must keep the bias.\n\n  We also have the parameters: scaling_factor, hardtanh_min, hardtanh_max. These are scalars and fixed.\n\n  In the forward pass:\n    We call the kernel.\n\nWe must handle data types: we'll use FP16 for the computation to use Tensor Cores? But note: the original model uses FP32? The input is generated by `torch.randn` which is FP32. We can convert to FP16? But the problem does not specify. We can try to use FP16 for faster computation.\n\nBut note: the GELU and hardtanh are pointwise and can be done in FP16. Also, the GEMM in FP16 is faster.\n\nWe'll do:\n  x = x.to(torch.float16)\n  weight = weight.to(torch.float16)\n  bias = bias.to(torch.float16)\n\nBut the output? The original code returns FP32? We can return FP16 and then convert to FP32 if needed? But the problem says we must be numerically equivalent. We can do the entire computation in FP16 and then convert the output to FP32? Or we can do the accumulation in FP32? \n\nThe Triton kernel can accumulate in FP32 even if the inputs are FP16. We can do:\n\n  acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\nThen we load the x and weight in FP16, convert to FP32, do the dot product, then at the end we do the pointwise operations in FP32? But then we store as FP16? Or as FP32? The original model returns FP32? We don't know. The original model: the Linear layer returns FP32, then multiplied by a scalar (which is float) -> FP32, then hardtanh and GELU in FP32.\n\nTo be safe, we can return FP32. But note: the kernel might be faster in FP16.\n\nAlternatively, we can let the user choose. But the problem says we must match the reference. The reference uses FP32? The input is FP32. We can do:\n\n  Option 1: Do everything in FP32. Then we don't use Tensor Cores? But Tensor Cores support FP32 via tf32? But we are not guaranteed.\n\n  Option 2: Convert input to FP16, do the computation in FP16 (with accumulation in FP32 for GEMM) and then convert the result to FP32 at the end? That might be acceptable.\n\nBut note: the problem says numerical correctness within float32 tolerance. So if we do FP16, we might lose precision. However, the GEMM in FP16 with accumulation in FP32 is standard and should be acceptable.\n\nWe'll do:\n  - Convert the input, weight, and bias to FP16.\n  - Do the GEMM in FP16 (with FP32 accumulation) and the pointwise operations in FP32? Actually, we can do the pointwise operations in FP16 as well? But GELU and hardtanh are nonlinear and might have precision issues.\n\n  Alternatively, we can do the entire computation in FP32? But then we lose the Tensor Core acceleration.\n\nConsidering the hardware has FP16 Tensor Cores, we want to use FP16.\n\nWe'll design the kernel for FP16 inputs and weights, and FP32 accumulation for the GEMM. Then we do the pointwise operations in FP32 and then convert the output to FP32? Or we can output FP16? The original model returns FP32? We don't know. The problem says we must match the reference. The reference returns the same as the original model. The original model returns the output of GELU, which is FP32? Because the input was FP32 and the Linear layer is FP32.\n\nBut note: the original model does:\n\n  x = self.gemm(x)   -> FP32\n  then multiplied by a scalar (FP32) -> FP32\n  then hardtanh and GELU in FP32.\n\nSo the output is FP32.\n\nTherefore, we must return FP32.\n\nWe can do:\n\n  - Load the input and weight in FP16, but convert to FP32 for the accumulation.\n  - Then do the pointwise operations in FP32 and store as FP32.\n\nBut that doesn't use Tensor Cores? Tensor Cores require the inputs to be FP16 and the output to be FP16? Actually, they can accumulate in FP32. But the Tensor Core operation itself is on FP16.\n\nWe can do:\n\n  acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n  for k in range(0, in_features, BLOCK_SIZE_K):\n      # Load a block of x and weight\n      x_block = tl.load(x_ptrs, mask=mask_x, other=0.0).to(tl.float16)\n      w_block = tl.load(w_ptrs, mask=mask_w, other=0.0).to(tl.float16)\n      # Convert to FP32 for the accumulation? But we want to use Tensor Cores.\n\nActually, Triton has a built-in for matrix multiplication that uses Tensor Cores: `tl.dot`. And it automatically uses Tensor Cores when the inputs are FP16 and the accumulation is in FP32? We can use:\n\n  acc += tl.dot(x_block, w_block, allow_tf32=False, out_dtype=tl.float32)\n\nBut note: the problem says FP16 Tensor Core 32x16x16. We must set the block sizes appropriately.\n\nAlternatively, we can use the new `tl.dot` that automatically uses Tensor Cores when the block sizes are multiples of 16x16x16? We'll set:\n\n  BLOCK_SIZE_M = 64\n  BLOCK_SIZE_N = 64\n  BLOCK_SIZE_K = 32   # because the inner dimension for Tensor Core is 16? But 32 is also supported.\n\nWe'll use tunable parameters.\n\nNow, let's write the kernel.\n\nWe'll define the kernel with many tunable configurations.\n\nWe note that the batch size is 128 and the output features is 512. The input features is 1024.\n\nWe'll set the grid as:\n\n  grid = (triton.cdiv(batch_size, BLOCK_SIZE_M), triton.cdiv(out_features, BLOCK_SIZE_N))\n\nWe must also pass the pointers for x, weight, bias, and the output.\n\nWe'll pass:\n  x_ptr, w_ptr, bias_ptr, out_ptr\n  and the strides and constants: batch_size, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max.\n\nWe also pass the stride variables.\n\nWe'll write the kernel in Triton.\n\nNote: We must do boundary checks for the input and weight.\n\nWe'll write the kernel and then the wrapper.\n\nWe'll replace the forward method of ModelNew to use the kernel.\n\nBut note: the problem says we must not change the grid configuration? Actually, we are defining the grid. The grid is determined by the block sizes and the problem size. We are allowed to change the block sizes.\n\nAlso, we are allowed to change the internal implementation.\n\nLet's write the code.\n\nWe note: the original model also has a bias. We must include it.\n\nWe'll create a new class ModelNew that uses the Triton kernel.\n\nWe'll also note: the kernel must be compiled, so we need to handle the case when the input dimensions are not powers of two? We'll use masking.\n\nSteps:\n\n  1. In the __init__, we initialize the parameters as before, but we do not use nn.Linear. Instead, we initialize the weight and bias ourselves.\n\n  2. We will create a function that calls the Triton kernel.\n\nBut note: the original code had:\n    self.gemm = nn.Linear(...)\n\nWe are replacing that with our own weight and bias.\n\nSo we change:\n\n  class ModelNew(nn.Module):\n      def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n          super().__init__()\n          self.in_features = in_features\n          self.out_features = out_features\n          self.scaling_factor = scaling_factor\n          self.hardtanh_min = hardtanh_min\n          self.hardtanh_max = hardtanh_max\n          # Initialize weight and bias\n          self.weight = nn.Parameter(torch.randn(out_features, in_features))\n          self.bias = nn.Parameter(torch.randn(out_features))\n\n      def forward(self, x):\n          # Check dimensions: x is (batch, in_features)\n          batch_size = x.shape[0]\n          # Allocate output: (batch, out_features) in FP32\n          output = torch.empty((batch_size, self.out_features), dtype=torch.float32, device=x.device)\n          # Call the Triton kernel\n          # We need to pass: x, self.weight, self.bias, output, and the parameters\n          # We also need the grid and block sizes.\n\n          # We'll write a helper function to call the kernel.\n\n          # We must set the grid and the kernel configuration.\n\n          # We'll define the kernel with autotune, so we need to pass the config.\n\n          # For now, let's assume we have a kernel function called `fused_gemm_act`\n\n          # We'll write the kernel below.\n\n          # Note: we must convert x, weight, bias to FP16? But the kernel expects FP16? We can design the kernel to take FP16 for x and weight, and FP16 for bias? Then convert to FP32 for accumulation and then the pointwise operations.\n\n          # We'll convert the tensors to FP16 for the kernel.\n\n          x_fp16 = x.to(torch.float16)\n          weight_fp16 = self.weight.to(torch.float16)\n          bias_fp16 = self.bias.to(torch.float16)\n\n          # Call the kernel\n          fused_gemm_act(\n              x_fp16, weight_fp16, bias_fp16, output,\n              self.scaling_factor, self.hardtanh_min, self.hardtanh_max,\n              batch_size, self.in_features, self.out_features\n          )\n\n          return output\n\nBut note: we must also handle the case when the dimensions are not divisible by the block sizes? The kernel must use masking.\n\nNow, we write the kernel.\n\nWe'll define:\n\n  @triton.autotune(...)\n  @triton.jit\n  def fused_gemm_act_kernel(\n      x_ptr, w_ptr, bias_ptr, out_ptr,\n      scaling_factor, hardtanh_min, hardtanh_max,\n      batch_size, in_features, out_features,\n      stride_xb, stride_xk,\n      stride_wo, stride_wi,   # for weight: stride_wo is the stride along the output feature, stride_wi along the input feature\n      stride_ob, stride_oo,   # for output: batch and output feature\n      BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n  ):\n      ...\n\nBut note: we are passing the strides. We can compute these in the wrapper.\n\nAlternatively, we can get the strides by:\n\n  stride_xb = x.stride(0)\n  stride_xk = x.stride(1)\n  stride_wo = w.stride(0)\n  stride_wi = w.stride(1)\n  stride_ob = output.stride(0)\n  stride_oo = output.stride(1)\n\nBut note: the input x is (batch, in_features) -> contiguous: so stride(0)=in_features, stride(1)=1.\n  Similarly, weight is (out_features, in_features): contiguous: stride(0)=in_features, stride(1)=1? Actually, no: in row-major, the first dimension stride is in_features and the second is 1? But in PyTorch, the tensor might be transposed? We assume contiguous.\n\nWe'll require the input and weight to be contiguous.\n\nWe'll write the kernel and then the wrapper.\n\nDue to the complexity, we'll only write the kernel for the case where the inputs are contiguous.\n\nWe'll set:\n\n  pid_m = tl.program_id(0)   # batch block index\n  pid_n = tl.program_id(1)   # output feature block index\n\n  # Compute the starting indices for the block in M (batch) and N (output feature)\n  offs_b = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n  offs_o = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n  # The reduction is over the input features (K)\n  offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n  # Initialize the accumulator\n  acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n  # Loop over the input features in blocks of BLOCK_SIZE_K\n  for k in range(0, in_features, BLOCK_SIZE_K):\n      # The current block in K\n      k_remaining = in_features - k\n      k_current = min(BLOCK_SIZE_K, k_remaining)\n\n      # Load a block of x: [BLOCK_SIZE_M, BLOCK_SIZE_K]\n      x_ptrs = x_ptr + offs_b[:, None] * stride_xb + (k + offs_k[None, :]) * stride_xk\n      mask_x = (offs_b[:, None] < batch_size) & (offs_k[None, :] < k_current)\n      x_block = tl.load(x_ptrs, mask=mask_x, other=0.0)\n\n      # Load a block of weight: [BLOCK_SIZE_N, BLOCK_SIZE_K]\n      w_ptrs = w_ptr + offs_o[:, None] * stride_wo + (k + offs_k[None, :]) * stride_wi\n      mask_w = (offs_o[:, None] < out_features) & (offs_k[None, :] < k_current)\n      w_block = tl.load(w_ptrs, mask=mask_w, other=0.0)\n\n      # Convert to FP32? Actually, the dot product will be done in FP32? But we want to use Tensor Cores? We can use tl.dot that supports FP16.\n      # We can keep as FP16 and use tl.dot with FP16 inputs and FP32 output.\n      # Note: x_block and w_block are in FP16.\n\n      # Compute the dot product and accumulate\n      acc += tl.dot(x_block, w_block, allow_tf32=False, out_dtype=tl.float32)\n\n  # Now, we have the GEMM result for this block. Add bias.\n  # The bias is a vector of size out_features. We load a block of the bias for the current output feature block.\n  bias_ptrs = bias_ptr + offs_o\n  mask_bias = offs_o < out_features\n  bias_block = tl.load(bias_ptrs, mask=mask_bias, other=0.0)\n  # bias_block is in FP16? We convert to FP32.\n  bias_block = bias_block.to(tl.float32)\n  # Now, we need to add bias to each row of the block? Actually, the bias is per output feature, so we add to every row in the batch dimension.\n  # We can do: acc = acc + bias_block[None, :]\n  acc += bias_block[None, :]\n\n  # Then scale\n  acc = acc * scaling_factor\n\n  # Then hardtanh: clamp between hardtanh_min and hardtanh_max\n  acc = tl.minimum(tl.maximum(acc, hardtanh_min), hardtanh_max)\n\n  # Then GELU: use tl.gelu, which is available in Triton 2.1+?\n  # If not, we can approximate. But let's assume we have tl.gelu.\n  acc = tl.gelu(acc)\n\n  # Now, we write the output. The output is in FP32.\n  out_ptrs = out_ptr + offs_b[:, None] * stride_ob + offs_o[None, :] * stride_oo\n  mask_out = (offs_b[:, None] < batch_size) & (offs_o[None, :] < out_features)\n  tl.store(out_ptrs, acc, mask=mask_out)\n\nWe then call this kernel with the grid and the block sizes.\n\nWe must set the grid:\n\n  grid = (triton.cdiv(batch_size, BLOCK_SIZE_M), triton.cdiv(out_features, BLOCK_SIZE_N))\n\nAnd we must pass the strides.\n\nBut note: the input tensor x might be non-contiguous? We'll require it to be contiguous.\n\nWe'll also note: the kernel has many parameters. We must define the autotune configurations.\n\nWe'll define:\n\n  configs = []\n  for BLOCK_SIZE_M in [32, 64]:\n      for BLOCK_SIZE_N in [32, 64]:\n          for BLOCK_SIZE_K in [32, 64]:\n              # But note: the reduction dimension must be at least 16? We choose multiples of 16.\n              if BLOCK_SIZE_K < 16: continue\n              configs.append(triton.Config({'BLOCK_SIZE_M': BLOCK_SIZE_M, 'BLOCK_SIZE_N': BLOCK_SIZE_N, 'BLOCK_SIZE_K': BLOCK_SIZE_K}, num_warps=4, num_stages=4))\n  ... and also try different num_warps and num_stages.\n\nBut we have to be careful: the block sizes must be such that the grid is not too small? The batch size is 128 and output features is 512.\n\n  For BLOCK_SIZE_M=64: grid_m = ceil(128/64)=2\n  For BLOCK_SIZE_N=64: grid_n = ceil(512/64)=8 -> total grid 16.\n\nWe can also try larger blocks? But 64 is already large.\n\nWe'll write the autotune with a few configurations.\n\nWe'll put the kernel definition inside the module? Or outside? We'll put it outside the class.\n\nBut note: the problem says to return the entire code block.\n\nWe'll write:\n\n  import triton\n  import triton.language as tl\n\n  @triton.autotune(...)\n  @triton.jit\n  def fused_gemm_act_kernel(...):\n      ...\n\nThen in the forward method, we call this kernel.\n\nBut note: the kernel must be compiled only once? We can define it at the top level.\n\nNow, let's write the entire code.\n\nWe'll also note: the original code had batch_size, in_features, etc. as global variables? But we are replacing the model.\n\nWe'll keep the global variables for the problem: batch_size, in_features, out_features, etc.\n\nBut note: the problem says the inputs are generated by `get_inputs` and `get_init_inputs`. We must not change the interface.\n\nWe'll keep:\n\n  batch_size = 128\n  in_features = 1024\n  out_features = 512\n  scaling_factor = 0.5\n  hardtanh_min = -2\n  hardtanh_max = 2\n\nBut the model class now uses these values? Actually, the model class is initialized with these values. So we are safe.\n\nWe'll write the code accordingly.\n\nDue to the complexity and the constraints, we'll write the entire code block.\n\nNote: We are replacing the entire EVOLVE-BLOCK.\n\nWe must be cautious: the previous code was a PyTorch model. Now we are replacing it with a Triton model.\n\nLet's write the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_act_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride parameters\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    # Operation parameters\n    scaling_factor: tl.constexpr,\n    hardtanh_min: tl.constexpr,\n    hardtanh_max: tl.constexpr,\n    # Blocking\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_n[None, :] < N), other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    # Add bias\n    bias_ptrs = bias_ptr + offs_n\n    bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n    accumulator += bias[None, :]\n    \n    # Apply scaling\n    accumulator = accumulator * scaling_factor\n    \n    # Apply hardtanh\n    accumulator = tl.minimum(tl.maximum(accumulator, hardtanh_min), hardtanh_max)\n    \n    # Apply GELU approximation\n    accumulator = accumulator * 0.5 * (1.0 + tl.erf(accumulator * 0.7071067811865475))\n    \n    # Write back\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n        \n        # Initialize weights\n        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = torch.nn.Parameter(torch.randn(out_features))\n        \n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        # Preallocate output\n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        \n        # Launch kernel\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        fused_gemm_act_kernel[grid](\n            x, self.weight, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            self.scaling_factor,\n            self.hardtanh_min,\n            self.hardtanh_max,\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
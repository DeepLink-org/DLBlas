{"id": "2d4f4f1c-8b42-4c56-bcb4-21b756874154", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_act_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    scaling_factor: tl.constexpr,\n    hardtanh_min: tl.constexpr,\n    hardtanh_max: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr = 8,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    mask_a = offs_am[:, None] < M\n    mask_b = offs_bn[None, :] < N\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        mask_k = offs_k < k_remaining\n        \n        a = tl.load(a_ptrs, mask=mask_a & mask_k[None, :], other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b & mask_k[:, None], other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    accumulator = accumulator * scaling_factor\n    accumulator = tl.minimum(tl.maximum(accumulator, hardtanh_min), hardtanh_max)\n    \n    gelu = 0.5 * accumulator * (1.0 + tl.erf(accumulator * 0.7071067811865475))\n    \n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, gelu, mask=c_mask)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n        \n        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = torch.nn.Parameter(torch.randn(out_features))\n        \n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        weight_t = self.weight.t().contiguous()\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        \n        fused_gemm_act_kernel[grid](\n            x, weight_t, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n            self.scaling_factor,\n            self.hardtanh_min,\n            self.hardtanh_max,\n            GROUP_M=8\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "dee3fe90-240d-4394-b3a2-7e5ad95af2de", "generation": 2, "timestamp": 1754577982.2599902, "iteration_found": 25, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_53_Gemm_Scaling_Hardtanh_GELU", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "max_difference": ["2.124471", "2.124471", "2.124471", "2.124471", "2.124471"], "avg_difference": ["1.003505", "1.002429", "1.005118", "1.004470", "1.003335"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.098, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_53_Gemm_Scaling_Hardtanh_GELU", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "max_difference": ["2.124471", "2.124471", "2.124471", "2.124470", "2.124471"], "avg_difference": ["0.944960", "0.949327", "0.949207", "0.959383", "0.947962"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.098, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 0.0000\n- score: 10.0000\n- stage: wrong_answer\n- verify_payload: {'id': 'level2_53_Gemm_Scaling_Hardtanh_GELU', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'max_difference': ['2.124471', '2.124471', '2.124471', '2.124470', '2.124471'], 'avg_difference': ['0.944960', '0.949327', '0.949207', '0.959383', '0.947962'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.098, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_53_Gemm_Scaling_Hardtanh_GELU', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'max_difference': ['1.198273', '1.174833', '1.185630', '1.188589', '1.162814'], 'avg_difference': ['0.162430', '0.162119', '0.162723', '0.163059', '0.163781'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.098, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_53_Gemm_Scaling_Hardtanh_GELU', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'max_difference': ['2.124471', '2.124471', '2.124471', '2.124470', '2.124471'], 'avg_difference': ['0.944960', '0.949327', '0.949207', '0.959383', '0.947962'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.098, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 36.5000, speed_up: 1.1650, runtime_ms: 0.0841, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.098, 'speed_up': 1.165, 'custom_timing': 0.0841}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 36.5000)\n```python\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Fused kernel for GEMM + scaling + Hardtanh + GELU\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_scale_act_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride info\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    # Activation parameters\n    scale, min_val, max_val,\n    # Meta-parameters\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr = 8,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    # Precompute constant masks outside the loop\n    mask_a_constant = offs_am[:, None] < M\n    mask_b_constant = offs_bn[None, :] < N\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        mask_k = offs_k < k_remaining\n        \n        a = tl.load(a_ptrs, mask=mask_a_constant & mask_k[None, :], other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b_constant & mask_k[:, None], other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    # Apply scaling\n    accumulator = accumulator * scale\n    \n    # Apply Hardtanh\n    accumulator = tl.minimum(tl.maximum(accumulator, min_val), max_val)\n    \n    # Apply GELU using erf-based exact formula\n    gelu = 0.5 * accumulator * (1.0 + tl.erf(accumulator * 0.7071067811865475))\n    \n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, gelu, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n\n    def forward(self, x):\n        x = x.contiguous()\n        M, K = x.shape\n        N = self.linear.weight.shape[0]\n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        \n        bias_ptr = self.linear.bias\n        if bias_ptr is not None:\n            bias_ptr = bias_ptr.data\n        else:\n            bias_ptr = None\n        \n        fused_gemm_scale_act_kernel[grid](\n            x, self.linear.weight.t(), output, bias_ptr,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.linear.weight.stride(1), self.linear.weight.stride(0),\n            output.stride(0), output.stride(1),\n            self.scaling_factor, self.hardtanh_min, self.hardtanh_max,\n            GROUP_M=8\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (36.5000), Performs well on speed_up (1.1650), Performs well on runtime_ms (0.0841), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.098, 'speed_up': 1.165, 'custom_timing': 0.0841})\n\n\n### Program 2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_act_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride parameters\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    # Operation parameters\n    scaling_factor: tl.constexpr,\n    hardtanh_min: tl.constexpr,\n    hardtanh_max: tl.constexpr,\n    # Blocking\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_n[None, :] < N), other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    # Add bias\n    bias_ptrs = bias_ptr + offs_n\n    bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n    accumulator += bias[None, :]\n    \n    # Apply scaling\n    accumulator = accumulator * scaling_factor\n    \n    # Apply hardtanh\n    accumulator = tl.minimum(tl.maximum(accumulator, hardtanh_min), hardtanh_max)\n    \n    # Apply GELU approximation\n    accumulator = accumulator * 0.5 * (1.0 + tl.erf(accumulator * 0.7071067811865475))\n    \n    # Write back\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n        \n        # Initialize weights\n        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = torch.nn.Parameter(torch.randn(out_features))\n        \n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        # Preallocate output\n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        \n        # Launch kernel\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        fused_gemm_act_kernel[grid](\n            x, self.weight, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            self.scaling_factor,\n            self.hardtanh_min,\n            self.hardtanh_max,\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_53_Gemm_Scaling_Hardtanh_GELU', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'max_difference': ['2.124471', '2.124471', '2.124471', '2.124470', '2.124471'], 'avg_difference': ['0.944960', '0.949327', '0.949207', '0.959383', '0.947962'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.098, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n### Program 3 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 64}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_scale_act_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    scale, min_val, max_val,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr = 8\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < k_remaining)\n        b_mask = (offs_k[:, None] < k_remaining) & (offs_bn[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    # Apply scaling and activation functions\n    accumulator = accumulator * scale\n    accumulator = tl.minimum(tl.maximum(accumulator, min_val), max_val)\n    \n    # GELU using erf approximation\n    one_over_sqrt2 = 0.7071067811865476\n    x = accumulator * one_over_sqrt2\n    erf = tl.erf(x)\n    gelu = 0.5 * accumulator * (1.0 + erf)\n    \n    # Store results\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, gelu, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n\n    def forward(self, x):\n        x = x.contiguous()\n        M, K = x.shape\n        N = self.linear.weight.shape[0]\n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        \n        bias_ptr = self.linear.bias.data if self.linear.bias is not None else None\n        \n        fused_gemm_scale_act_kernel[grid](\n            x, \n            self.linear.weight.t().contiguous(), \n            output, \n            bias_ptr,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.linear.weight.stride(1), self.linear.weight.stride(0),\n            output.stride(0), output.stride(1),\n            self.scaling_factor, \n            self.hardtanh_min, \n            self.hardtanh_max,\n            GROUP_M=8\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_53_Gemm_Scaling_Hardtanh_GELU', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'max_difference': ['1.198273', '1.174833', '1.185630', '1.188589', '1.162814'], 'avg_difference': ['0.162430', '0.162119', '0.162723', '0.163059', '0.163781'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.098, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _fused_gemm_act_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    scale: tl.constexpr,\n    min_val: tl.constexpr,\n    max_val: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        \n        a_mask = (offs_m[:, None] < M) & k_mask[None, :]\n        b_mask = (offs_n[None, :] < N) & k_mask[:, None]\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Add bias if present\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += bias[None, :]\n\n    # Apply scaling and activation functions\n    acc = acc * scale\n    acc = tl.minimum(tl.maximum(acc, min_val), max_val)\n    \n    # Replace tanh with erf-based GELU\n    one_over_sqrt2 = 0.7071067811865476\n    x = acc * one_over_sqrt2\n    erf = tl.erf(x)\n    gelu = 0.5 * acc * (1.0 + erf)\n    acc = gelu\n\n    # Store output with boundary checks\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.zeros(out_features))  # Added bias parameter\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n        self.in_features = in_features\n        self.out_features = out_features\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        \n        # Handle bias pointer (None if bias disabled)\n        bias_ptr = self.bias if self.bias is not None else None\n        \n        _fused_gemm_act_kernel[grid](\n            x, self.weight, output, bias_ptr,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            self.scaling_factor, self.hardtanh_min, self.hardtanh_max,\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Fused kernel for GEMM + scaling + Hardtanh + GELU\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_scale_act_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride info\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    # Activation parameters\n    scale, min_val, max_val,\n    # Meta-parameters\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr = 8,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    # Precompute masks for rows and columns\n    row_mask = offs_am[:, None] < M\n    col_mask = offs_bn[None, :] < N\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    num_k_blocks = tl.cdiv(K, BLOCK_K)\n    \n    for k_block in range(num_k_blocks):\n        k_remaining = K - k_block * BLOCK_K\n        k_mask = offs_k < k_remaining\n        \n        a_mask = k_mask[None, :] & row_mask\n        b_mask = k_mask[:, None] & col_mask\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        \n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    # Apply scaling\n    accumulator = accumulator * scale\n    \n    # Apply Hardtanh\n    accumulator = tl.minimum(tl.maximum(accumulator, min_val), max_val)\n    \n    # Apply GELU using optimized erf calculation\n    SQRT_2_OVER_PI = 0.7978845608028654\n    gelu = 0.5 * accumulator * (1.0 + tl.erf(accumulator * SQRT_2_OVER_PI))\n    \n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, gelu, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n\n    def forward(self, x):\n        x = x.contiguous()\n        M, K = x.shape\n        N = self.linear.weight.shape[0]\n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        \n        bias_ptr = self.linear.bias.data if self.linear.bias is not None else None\n        \n        fused_gemm_scale_act_kernel[grid](\n            x, self.linear.weight.t(), output, bias_ptr,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.linear.weight.stride(1), self.linear.weight.stride(0),\n            output.stride(0), output.stride(1),\n            self.scaling_factor, self.hardtanh_min, self.hardtanh_max,\n            GROUP_M=8\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 36.5000, Type: High-Performer)\n```python\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Fused kernel for GEMM + scaling + Hardtanh + GELU\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_scale_act_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride info\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    # Activation parameters\n    scale, min_val, max_val,\n    # Meta-parameters\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr = 8,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    # Precompute constant masks outside the loop\n    mask_a_constant = offs_am[:, None] < M\n    mask_b_constant = offs_bn[None, :] < N\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        mask_k = offs_k < k_remaining\n        \n        a = tl.load(a_ptrs, mask=mask_a_constant & mask_k[None, :], other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b_constant & mask_k[:, None], other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    # Apply scaling\n    accumulator = accumulator * scale\n    \n    # Apply Hardtanh\n    accumulator = tl.minimum(tl.maximum(accumulator, min_val), max_val)\n    \n    # Apply GELU using erf-based exact formula\n    gelu = 0.5 * accumulator * (1.0 + tl.erf(accumulator * 0.7071067811865475))\n    \n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, gelu, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n\n    def forward(self, x):\n        x = x.contiguous()\n        M, K = x.shape\n        N = self.linear.weight.shape[0]\n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        \n        bias_ptr = self.linear.bias\n        if bias_ptr is not None:\n            bias_ptr = bias_ptr.data\n        else:\n            bias_ptr = None\n        \n        fused_gemm_scale_act_kernel[grid](\n            x, self.linear.weight.t(), output, bias_ptr,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.linear.weight.stride(1), self.linear.weight.stride(0),\n            output.stride(0), output.stride(1),\n            self.scaling_factor, self.hardtanh_min, self.hardtanh_max,\n            GROUP_M=8\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _fused_gemm_act_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    scale: tl.constexpr,\n    min_val: tl.constexpr,\n    max_val: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        \n        a_mask = (offs_m[:, None] < M) & k_mask[None, :]\n        b_mask = (offs_n[None, :] < N) & k_mask[:, None]\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Add bias if present\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += bias[None, :]\n\n    # Apply scaling and activation functions\n    acc = acc * scale\n    acc = tl.minimum(tl.maximum(acc, min_val), max_val)\n    \n    # Replace tanh with erf-based GELU\n    one_over_sqrt2 = 0.7071067811865476\n    x = acc * one_over_sqrt2\n    erf = tl.erf(x)\n    gelu = 0.5 * acc * (1.0 + erf)\n    acc = gelu\n\n    # Store output with boundary checks\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.zeros(out_features))  # Added bias parameter\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n        self.in_features = in_features\n        self.out_features = out_features\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        \n        # Handle bias pointer (None if bias disabled)\n        bias_ptr = self.bias if self.bias is not None else None\n        \n        _fused_gemm_act_kernel[grid](\n            x, self.weight, output, bias_ptr,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            self.scaling_factor, self.hardtanh_min, self.hardtanh_max,\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _fused_gemm_act_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    scale: tl.constexpr,\n    min_val: tl.constexpr,\n    max_val: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        \n        # Mask for rows and columns with boundary checks\n        a_mask = (offs_m[:, None] < M) & k_mask[None, :]\n        b_mask = (offs_n[None, :] < N) & k_mask[:, None]\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Apply scaling and activation functions\n    acc = acc * scale\n    acc = tl.minimum(tl.maximum(acc, min_val), max_val)\n    \n    # Replace tanh with erf-based GELU\n    one_over_sqrt2 = 0.7071067811865476\n    x = acc * one_over_sqrt2\n    erf = tl.erf(x)\n    gelu = 0.5 * acc * (1.0 + erf)\n    acc = gelu\n\n    # Store output with boundary checks\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n        self.in_features = in_features\n        self.out_features = out_features\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        \n        _fused_gemm_act_kernel[grid](\n            x, self.weight, output,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            self.scaling_factor, self.hardtanh_min, self.hardtanh_max,\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Fused kernel for GEMM + scaling + Hardtanh + GELU\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_scale_act_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride info\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    # Activation parameters\n    scale, min_val, max_val,\n    # Meta-parameters\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr = 8,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    # Precompute masks for rows and columns\n    row_mask = offs_am[:, None] < M\n    col_mask = offs_bn[None, :] < N\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    num_k_blocks = tl.cdiv(K, BLOCK_K)\n    \n    for k_block in range(num_k_blocks):\n        k_remaining = K - k_block * BLOCK_K\n        k_mask = offs_k < k_remaining\n        \n        a_mask = k_mask[None, :] & row_mask\n        b_mask = k_mask[:, None] & col_mask\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        \n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    # Apply scaling\n    accumulator = accumulator * scale\n    \n    # Apply Hardtanh\n    accumulator = tl.minimum(tl.maximum(accumulator, min_val), max_val)\n    \n    # Apply GELU using optimized erf calculation\n    SQRT_2_OVER_PI = 0.7978845608028654\n    gelu = 0.5 * accumulator * (1.0 + tl.erf(accumulator * SQRT_2_OVER_PI))\n    \n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, gelu, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n\n    def forward(self, x):\n        x = x.contiguous()\n        M, K = x.shape\n        N = self.linear.weight.shape[0]\n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        \n        bias_ptr = self.linear.bias.data if self.linear.bias is not None else None\n        \n        fused_gemm_scale_act_kernel[grid](\n            x, self.linear.weight.t(), output, bias_ptr,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.linear.weight.stride(1), self.linear.weight.stride(0),\n            output.stride(0), output.stride(1),\n            self.scaling_factor, self.hardtanh_min, self.hardtanh_max,\n            GROUP_M=8\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 64}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_scale_act_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    scale, min_val, max_val,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr = 8\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < k_remaining)\n        b_mask = (offs_k[:, None] < k_remaining) & (offs_bn[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    # Apply scaling and activation functions\n    accumulator = accumulator * scale\n    accumulator = tl.minimum(tl.maximum(accumulator, min_val), max_val)\n    \n    # GELU using erf approximation\n    one_over_sqrt2 = 0.7071067811865476\n    x = accumulator * one_over_sqrt2\n    erf = tl.erf(x)\n    gelu = 0.5 * accumulator * (1.0 + erf)\n    \n    # Store results\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, gelu, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n\n    def forward(self, x):\n        x = x.contiguous()\n        M, K = x.shape\n        N = self.linear.weight.shape[0]\n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        \n        bias_ptr = self.linear.bias.data if self.linear.bias is not None else None\n        \n        fused_gemm_scale_act_kernel[grid](\n            x, \n            self.linear.weight.t().contiguous(), \n            output, \n            bias_ptr,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.linear.weight.stride(1), self.linear.weight.stride(0),\n            output.stride(0), output.stride(1),\n            self.scaling_factor, \n            self.hardtanh_min, \n            self.hardtanh_max,\n            GROUP_M=8\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_act_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride parameters\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    # Operation parameters\n    scaling_factor: tl.constexpr,\n    hardtanh_min: tl.constexpr,\n    hardtanh_max: tl.constexpr,\n    # Blocking\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_n[None, :] < N), other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    # Add bias\n    bias_ptrs = bias_ptr + offs_n\n    bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n    accumulator += bias[None, :]\n    \n    # Apply scaling\n    accumulator = accumulator * scaling_factor\n    \n    # Apply hardtanh\n    accumulator = tl.minimum(tl.maximum(accumulator, hardtanh_min), hardtanh_max)\n    \n    # Apply GELU approximation\n    accumulator = accumulator * 0.5 * (1.0 + tl.erf(accumulator * 0.7071067811865475))\n    \n    # Write back\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n        \n        # Initialize weights\n        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = torch.nn.Parameter(torch.randn(out_features))\n        \n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        # Preallocate output\n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        \n        # Launch kernel\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        fused_gemm_act_kernel[grid](\n            x, self.weight, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            self.scaling_factor,\n            self.hardtanh_min,\n            self.hardtanh_max,\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program that has correctness issues (correctness: 0.0000) and a stage of 'wrong_answer'.\n The current program is an autotuned Triton kernel for a fused GEMM operation followed by scaling, Hardtanh, and GELU activation.\n\n The problem: The current kernel produces an output mismatch.\n\n Let's compare with the top-performing program (Program 1) that had correctness 1.0000 and score 36.5000.\n\n In the current program (the one we are to rewrite), note the following:\n\n 1. The kernel signature and the way it is called in the model are similar to the top-performing program.\n 2. The top-performing program (Program 1) uses a different grouping strategy (GROUP_M) for the program ID mapping and has a different loop structure.\n\n However, the main issue in the current program might be the GELU implementation or the way the bias is added.\n\n Specifically, in the current program we have:\n\n    # Apply GELU approximation\n    accumulator = accumulator * 0.5 * (1.0 + tl.erf(accumulator * 0.7071067811865475))\n\n But note: the GELU formula is: 0.5 * x * (1 + erf(x / sqrt(2)))\n Here we are multiplying the accumulator by 0.5 and then by (1+erf(...)), but note that the argument to erf is `accumulator * 0.7071067811865475` (which is x/sqrt(2)).\n\n However, the top-performing program (Program 1) does:\n\n    gelu = 0.5 * accumulator * (1.0 + tl.erf(accumulator * 0.7071067811865475))\n\n This is the same formula.\n\n So what could be the issue?\n\n Let's look at the bias addition:\n\n Current program:\n    bias_ptrs = bias_ptr + offs_n\n    bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n    accumulator += bias[None, :]\n\n This is adding the bias to the accumulator after the GEMM. Then we scale, apply hardtanh, and then GELU.\n\n The top program does the same order: GEMM, then bias, then scaling, then hardtanh, then GELU.\n\n Another possibility: the top program uses a different mask for the GEMM loop.\n\n In the current program, the mask for a and b in the GEMM loop is computed per iteration as:\n\n    a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n    b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_n[None, :] < N), other=0.0)\n\n But note: the top program precomputes the row and column masks and then combines with the k-bound mask.\n\n However, the top program actually does:\n\n    mask_a_constant = offs_am[:, None] < M\n    mask_b_constant = offs_bn[None, :] < N\n\n    and then in the loop:\n\n        mask_k = offs_k < k_remaining\n        a = tl.load(a_ptrs, mask=mask_a_constant & mask_k[None, :], other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b_constant & mask_k[:, None], other=0.0)\n\n The current program does:\n\n    mask for a: (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n    mask for b: (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n\n The order of the conditions is different but the meaning is the same.\n\n However, note that the top program uses the same mask for the entire row (mask_a_constant) and entire column (mask_b_constant) and then just the k-bound changes per block.\n\n The current program recalculates the row and column bounds every time. This might be less efficient but should be correct.\n\n Another difference: the top program uses a different program ID mapping (with GROUP_M) to improve L2 cache hit rate.\n\n But the correctness issue is more likely due to:\n\n  - The bias loading: in the current program, we are loading the bias for the entire row of the output (for each column in the current block). However, note that the bias is of size (out_features) and we are loading it for the columns in the current block (offs_n). This should be correct.\n\n  - The GELU: the formula is the same.\n\n  - The scaling and hardtanh: same.\n\n Another possibility: the current program does not handle the case when the matrix dimensions are not multiples of the block size? But the masks are applied.\n\n After comparing, note that the top program (Program 1) has the following in the kernel for the bias:\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n\n In the current program, we do:\n\n    bias_ptrs = bias_ptr + offs_n\n    bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n    accumulator += bias[None, :]\n\n This is equivalent.\n\n However, note that the top program uses `offs_bn` and we use `offs_n` for the same thing (the column indices in the current block).\n\n Let me check the variable names: in the current program, we have:\n\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n and in the top program:\n\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n So same.\n\n What else? The top program uses `tl.dot` with `allow_tf32=True` and the current program does too.\n\n Another possibility: the top program uses a different order of the activation functions? No, the same.\n\n But wait: the current program applies:\n\n    accumulator = accumulator * scaling_factor\n    accumulator = tl.minimum(tl.maximum(accumulator, hardtanh_min), hardtanh_max)\n    accumulator = accumulator * 0.5 * (1.0 + tl.erf(accumulator * 0.7071067811865475))\n\n However, note that the GELU is applied after the Hardtanh. But the original PyTorch reference does:\n\n    x = x @ self.weight.t() + self.bias\n    x = x * self.scaling_factor\n    x = torch.nn.functional.hardtanh(x, min_val=self.hardtanh_min, max_val=self.hardtanh_max)\n    x = torch.nn.functional.gelu(x)\n\n So the order is: linear, then scaling, then hardtanh, then gelu.\n\n The current program and the top program both follow this order.\n\n However, the top program had correctness and the current does not.\n\n Let me check the way the grid is launched:\n\n Current program:\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n\n Top program:\n    grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n\n This is the same.\n\n Another possibility: the top program uses a grouped program ID mapping (with GROUP_M) which might change the order of computation, but that should not affect correctness.\n\n After careful thought, note that the top program (which is correct) has:\n\n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n and then the store:\n\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n In the current program, we have:\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n and then the store:\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n So same.\n\n The only difference I can spot is in the loop mask for the GEMM:\n\n    In the current program, for the a matrix we do:\n        mask = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n\n    But note: the condition for the k dimension: `offs_k[None, :] < K - k`\n    This is a vector of length BLOCK_SIZE_K, and we are checking if the k index is within the remaining elements.\n\n    However, the top program does:\n        mask_k = offs_k < k_remaining   # k_remaining = K - k\n        a = tl.load(a_ptrs, mask=mask_a_constant & mask_k[None, :], other=0.0)\n\n    This is equivalent.\n\n But wait: in the current program, for the b matrix we do:\n\n        mask = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n\n    The top program does:\n\n        b = tl.load(b_ptrs, mask=mask_b_constant & mask_k[:, None], other=0.0)\n\n    Note: the top program uses `mask_k[:, None]` which is of shape (BLOCK_SIZE_K, 1) and then broadcasted to (BLOCK_SIZE_K, BLOCK_SIZE_N). In the current program, we have (offs_k[:, None] < ...) which is (BLOCK_SIZE_K, 1) and then we do an `&` with (offs_n[None, :] < N) which is (1, BLOCK_SIZE_N). This gives (BLOCK_SIZE_K, BLOCK_SIZE_N). So same.\n\n However, note that the current program uses `K - k` for the remaining k, but the top program uses `k_remaining = K - k` and then `mask_k = offs_k < k_remaining`. This is the same.\n\n After this, I suspect the issue might be in the way the bias is loaded? The current program does:\n\n    bias_ptrs = bias_ptr + offs_n\n    bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n\n But note: the bias is a 1D tensor of size N (out_features). We are loading a vector of length BLOCK_SIZE_N at offsets `offs_n` (which are the column indices for this block). Then we add it to the accumulator (which is of shape (BLOCK_SIZE_M, BLOCK_SIZE_N)) by broadcasting.\n\n However, the top program does the same.\n\n What if the current program does not handle the case when the block goes out of bounds for the bias? We have a mask: `mask=offs_n < N`. So that should be safe.\n\n But wait: the top program uses the same mask: `mask=offs_bn < N`.\n\n There is one more difference: the top program uses `tl.zeros` for the accumulator and then accumulates. The current program does the same.\n\n I notice that the current program does not precompute the mask for the entire row and column, but recalculates it at every iteration. This might be a performance issue but not correctness.\n\n However, let me look at the top program that passed correctness (Program 1) and the current program that fails. The top program that passed had:\n\n    class ModelNew(nn.Module):\n        def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n            super(ModelNew, self).__init__()\n            self.linear = nn.Linear(in_features, out_features)\n            ...\n\n    and in the forward:\n        N = self.linear.weight.shape[0]\n        ...\n        bias_ptr = self.linear.bias\n        if bias_ptr is not None:\n            bias_ptr = bias_ptr.data\n        else:\n            bias_ptr = None\n\n        ... and then passed to the kernel.\n\n In the current program, we have:\n\n    class ModelNew(torch.nn.Module):\n        def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n            ...\n            self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n            self.bias = torch.nn.Parameter(torch.randn(out_features))\n\n        ... and in forward:\n            bias_ptr = self.bias   # and passed to the kernel.\n\n But note: the reference PyTorch model uses `nn.Linear` which by default has a bias. The current program does not use `nn.Linear` but defines its own weight and bias.\n\n However, the test might be using a model that has the same structure as the reference? The top program used `nn.Linear` and then extracted the weight and bias.\n\n The problem: the test might be comparing against a model that uses `nn.Linear` and the current program uses a different weight initialization? But that should not matter because the test runs the same input through both.\n\n But wait: the test passes the same input to both the reference and the custom model. The reference model is implemented in PyTorch as:\n\n    class ReferenceModel(nn.Module):\n        def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n            super(ReferenceModel, self).__init__()\n            self.linear = nn.Linear(in_features, out_features)\n            self.scaling_factor = scaling_factor\n            self.hardtanh_min = hardtanh_min\n            self.hardtanh_max = hardtanh_max\n\n        def forward(self, x):\n            x = self.linear(x)\n            x = x * self.scaling_factor\n            x = torch.nn.functional.hardtanh(x, min_val=self.hardtanh_min, max_val=self.hardtanh_max)\n            x = torch.nn.functional.gelu(x)\n            return x\n\n The current program's model:\n\n    class ModelNew(torch.nn.Module):\n        def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n            ...\n            self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n            self.bias = torch.nn.Parameter(torch.randn(out_features))\n\n        def forward(self, x):\n            ... (uses the Triton kernel)\n\n The reference model uses `nn.Linear` which initializes the weight and bias in a specific way (with Kaiming uniform). The current model initializes with `torch.randn` (standard normal). This would cause the outputs to be different.\n\n But note: the test might initialize the models with the same weights? The test code is not shown, but typically in such benchmarks, the models are initialized with the same weights for comparison.\n\n However, the top program (Program 1) uses `nn.Linear` and then passes the weight and bias from that linear layer to the kernel. The current program does not use `nn.Linear` but defines its own parameters.\n\n How does the test work? It runs:\n\n    reference_model = ReferenceModel(...)\n    custom_model = ModelNew(...)\n\n    # Then it must set the same weights and bias for both?\n\n    # Or maybe the test builds the reference model and the custom model and then sets the same weights?\n\n Looking at the provided context, we don't have the test code. But the top program (Program 1) uses `nn.Linear` and then extracts the weight and bias, so it would be compatible with the reference.\n\n The current program does not use `nn.Linear` and so if the test copies the weights from the reference model to the custom model, it might not work because the current model has two parameters (weight and bias) but the reference model's linear layer has two parameters (weight and bias) as well. So it might work.\n\n However, the top program (Program 1) had correctness and the current program does not. So the issue must be in the kernel.\n\n Given the complexity, let's try to refactor the current kernel to be more similar to the top program that passed (Program 1).\n\n Steps:\n\n  1. Change the program ID mapping to use the grouped approach (GROUP_M) to improve performance and also to match the top program. This might not be necessary for correctness but it is used in the top program.\n\n  2. Precompute the row and column masks outside the loop.\n\n  3. Change the loop to use a variable `k_remaining` and a mask for k.\n\n  4. Also, note that the top program uses `allow_tf32=True` in `tl.dot` and the current program does too.\n\n  5. The GELU formula is the same.\n\n Alternatively, we can take the entire kernel from the top program (Program 1) and insert it into the current program's model. But note: the current program's model structure is different (it has its own weight and bias as parameters, not via an `nn.Linear`). However, the kernel call is similar.\n\n But note: the top program (Program 1) had the following kernel call:\n\n        fused_gemm_scale_act_kernel[grid](\n            x, self.linear.weight.t(), output, bias_ptr,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.linear.weight.stride(1), self.linear.weight.stride(0),\n            output.stride(0), output.stride(1),\n            self.scaling_factor, self.hardtanh_min, self.hardtanh_max,\n            GROUP_M=8\n        )\n\n In the current program, we are passing:\n\n        fused_gemm_act_kernel[grid](\n            x, self.weight, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            self.scaling_factor,\n            self.hardtanh_min,\n            self.hardtanh_max,\n        )\n\n Note: the top program passes the weight as `self.linear.weight.t()` and the strides accordingly: \n        weight stride: (stride(1), stride(0)) -> which is the strides of the transposed weight.\n\n In the current program, we are passing `self.weight` (which is of shape [out_features, in_features]) and then the strides: \n        self.weight.stride(0) -> stride along the out_features dimension (say, stride0)\n        self.weight.stride(1) -> stride along the in_features dimension (say, stride1)\n\n But in the kernel, the a matrix is the input (x) of shape [M, K] and the b matrix is the weight of shape [out_features, in_features] = [N, K]? \n\n However, note: the GEMM is: x (M, K) @ weight (K, N)  -> (M, N). But our weight is stored as (N, K). So we need to transpose it to (K, N). \n\n The top program does: `self.linear.weight.t()` which transposes the weight to [in_features, out_features] = [K, N]. Then the stride for the weight in the kernel for the k dimension becomes stride(0) (the original stride(1) for the non-transposed) and for n becomes stride(0) (the original stride(0) for the non-transposed, which becomes stride(1) in the transposed).\n\n But in the current program, we are not transposing the weight. We are passing the weight as [N, K] and then in the kernel we are using:\n\n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak   # for x: [M, K]\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn    # for weight: [N, K]? \n\n But note: the kernel expects the b matrix to be in row-major with dimensions [K, N]? Actually, the kernel is written for:\n\n    a: [M, K] (row-major: strides (stride_am, stride_ak))\n    b: [K, N] (row-major: strides (stride_bk, stride_bn))\n\n But our weight is [N, K] (row-major: then the stride for the first dimension (N) is K, and for the second dimension (K) is 1).\n\n How do we represent a matrix of shape [N, K] as [K, N]? We cannot without transposing.\n\n The current kernel is written for:\n\n    a: [M, K] and b: [K, N] to produce [M, N].\n\n But we are passing the weight of shape [N, K] as b_ptr. This is incorrect because the kernel will interpret the weight matrix as [K, N] without transposing.\n\n This is the likely cause of the correctness issue.\n\n How to fix?\n\n We have two options:\n\n   Option 1: Transpose the weight before passing to the kernel, and adjust the strides accordingly.\n\n   Option 2: Change the kernel to accept the weight in the shape [N, K] (so we are effectively doing x @ weight.T).\n\n The current kernel does:\n\n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak   # [M, K]\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn    # here, stride_bk is the stride for the k dimension in the b matrix, and stride_bn for the n dimension.\n\n If the weight is stored as [N, K] (row-major), then to access an element at [k, n] we would do:\n\n    base + k * stride_bk + n * stride_bn\n\n But note: the weight matrix of [N, K] has stride (K, 1). So to get the element at (k, n) we need to do:\n\n    base + n * (stride for the first dimension) + k * (stride for the second dimension) = base + n * K + k\n\n But in the kernel, we are doing:\n\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n If we set:\n    stride_bk = 1   (because k is the inner dimension in the weight [N, K])\n    stride_bn = K   (because n is the outer dimension)\n\n then it would be correct.\n\n However, in the current program, we are passing:\n\n    self.weight.stride(0)  # stride for dimension 0 (n) -> K\n    self.weight.stride(1)  # stride for dimension 1 (k) -> 1\n\n and then in the kernel we are using:\n\n    stride_bk = self.weight.stride(0) -> K\n    stride_bn = self.weight.stride(1) -> 1\n\n This is swapped! We are setting:\n\n    stride_bk = K   # for the k index, we step by K\n    stride_bn = 1   # for the n index, we step by 1\n\n But in the kernel, we are using:\n\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n which becomes: base + (offs_k * K) + (offs_n * 1). This would access:\n\n    base + offs_k * K + offs_n\n\n This corresponds to the element at [index_k, index_n] in a matrix that is stored in row-major with dimensions [?, K]? Actually, this is the element at row offs_k and column offs_n? But wait, the weight is stored as [N, K]. We are indexing by k (which is the column in the weight) and n (which is the row in the weight). This is transposed.\n\n We are effectively accessing the weight as if it were stored in column-major? But we want to use the weight as [K, N] (row-major). The current access pattern with stride_bk=K and stride_bn=1 would give a matrix of dimensions [K, N] but stored in memory in a way that the first K elements are the first row? Actually, no: the memory layout would be:\n\n    For a row-major matrix of shape [K, N], the element (i, j) is at base + i * N + j.\n\n But we are computing base + i * K + j. This is not the same.\n\n Therefore, the kernel expects the b matrix to be in row-major [K, N] (with stride_bk = N, stride_bn = 1). But we are passing a weight that is [N, K] and then using strides (K, 1) which is the strides for a [K, N] matrix in column-major? Actually, it's the strides for a [K, N] matrix in row-major if we set stride_bk = 1 and stride_bn = K? No.\n\n Let me clarify:\n\n   Matrix of shape [K, N] in row-major: \n        element (i, j) at base + i * N + j\n        so stride for i (the first dimension) is N, and for j (the second) is 1.\n\n   Matrix of shape [N, K] in row-major:\n        element (i, j) at base + i * K + j\n        stride for i is K, for j is 1.\n\n In the kernel, we have:\n\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk) + (offs_n[None, :] * stride_bn)\n\n We want to access the element at (k, n) in the matrix of shape [K, N]. So we set:\n\n    stride_bk = N   (the stride for the first dimension, which is K)\n    stride_bn = 1   (the stride for the second dimension, which is N)\n\n But we are passing:\n\n    stride_bk = self.weight.stride(0) = K   (for the weight [N, K], the stride for the first dimension is K)\n    stride_bn = self.weight.stride(1) = 1\n\n This is not the same as (N, 1). So we are not representing the weight as [K, N].\n\n How did the top program do it? They did:\n\n    self.linear.weight.t()   # this gives a tensor of shape [K, N] (because the original was [N, K])\n    and then the strides:\n        stride_bk = self.linear.weight.stride(1)   # which was the stride for the original second dimension (K) -> which is 1? \n        stride_bn = self.linear.weight.stride(0)   # which was the stride for the original first dimension (N) -> which is K?\n\n    But note: the transposed tensor [K, N] has strides swapped. So the stride for the first dimension (K) of the transposed tensor is the original stride(1) = 1, and the stride for the second dimension (N) is the original stride(0) = K.\n\n    So they passed:\n        stride_bk = 1\n        stride_bn = K\n\n    This is correct for a [K, N] matrix in row-major? No, because a [K, N] matrix in row-major should have stride for the first dimension = N, and for the second = 1.\n\n    But wait: the tensor is contiguous after transpose? Not necessarily. The transpose of a row-major [N, K] is a column-major [K, N]? Actually, it's not contiguous. The stride for the first dimension (K) of the transposed tensor is 1 (the inner dimension) and the second dimension (N) is K (the outer dimension). This is column-major for the [K, N] matrix.\n\n    How does the kernel access it? It does:\n\n        b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    If we set stride_bk=1 and stride_bn=K, then we are doing:\n\n        base + offs_k * 1 + offs_n * K\n\n    This is the element at (offs_k, offs_n) in a column-major matrix? Actually, it's row-major if we consider the matrix to have dimensions [N, K] in row-major? It's confusing.\n\n    Alternatively, we can think of the weight matrix we want to use as the transpose of the stored weight. We want to do x @ W.T. So we can pass the weight as is and then in the kernel treat it as if it were stored in column-major for the [K, N] representation? \n\n    But the kernel is written for a row-major b matrix of [K, N]. So we need to pass a contiguous tensor that is the transpose of the weight? \n\n    The top program did: `self.linear.weight.t().contiguous()`. Then the transposed tensor would be contiguous and have the correct row-major layout for [K, N]? Not exactly: the contiguous tensor of the transpose would be row-major for the new shape [K, N]. Then the strides would be:\n\n        stride for dimension 0 (K) = N\n        stride for dimension 1 (N) = 1\n\n    So then they would pass:\n\n        stride_bk = N   # the stride for the first dimension of the transposed tensor (K) is N\n        stride_bn = 1   # the stride for the second dimension (N) is 1\n\n    But in the top program, they passed:\n\n        stride_bk = self.linear.weight.stride(1)   -> which is 1 (for the original weight [N, K] the stride for the last dimension is 1)\n        stride_bn = self.linear.weight.stride(0)   -> which is K\n\n    This does not match. \n\n    Actually, the top program did:\n\n        a_ptr, b_ptr, c_ptr, bias_ptr,\n        M, N, K,\n        stride_am, stride_ak,   # for x: [M, K] -> strides (stride(0), stride(1)) = (K, 1) if contiguous\n        stride_bk, stride_bn,   # for the weight after transpose: [K, N] -> strides: for the new dimension K, it was the old dimension K (which was the inner dimension in the original) -> so stride_bk = original stride(1) = 1; stride_bn = original stride(0) = K.\n\n        But note: the contiguous tensor after transpose might not be contiguous? They did not call .contiguous() in the top program. \n\n    However, the top program passed the weight as `self.linear.weight.t()`, which might be non-contiguous. The Triton kernel can handle non-contiguous tensors by using the strides.\n\n    In the kernel, for the weight [K, N] (non-contiguous, because it's the transpose of [N, K] which is contiguous), the stride for the first dimension (K) is 1, and for the second dimension (N) is K.\n\n    Then in the kernel, we do:\n\n        b_ptrs = b_ptr + offs_k[:, None] * 1 + offs_n[None, :] * K\n\n    This is the correct element at (k, n) in the weight matrix? \n\n        base + k * 1 + n * K\n\n    This is the element at position [n, k] in the original weight (which is stored in row-major [N, K]: the element at row n, column k is at base + n * K + k). \n\n    But wait, we want to do x (M, K) @ W_original (N, K)^T = x (M, K) @ W_original^T (K, N). The element at (k, n) in W_original^T is the element at (n, k) in W_original.\n\n    So the kernel is correctly accessing W_original^T.\n\n    Therefore, the top program is correct.\n\n    In the current program, we are passing the weight as [N, K] and then using strides (stride(0)=K, stride(1)=1). The kernel is using:\n\n        b_ptrs = b_ptr + offs_k[:, None] * stride_bk (K) + offs_n[None, :] * stride_bn (1)\n\n        = base + offs_k * K + offs_n\n\n    This is the element at (n=?, k=?) in the original weight? \n\n        base + offs_k * K + offs_n\n\n    This would be the element at row offs_k and column offs_n in the weight matrix [N, K]? But wait, the weight matrix [N, K] has:\n\n        element [n, k] = base + n * K + k\n\n    Here, we have base + offs_k * K + offs_n = base + k * K + n.\n\n    This is the element at row k and column n? But k is in the range [0, K) and n in [0, BLOCK_SIZE_N). This would be out of bounds for the weight matrix which has only N rows (N is the number of out_features) and K columns.\n\n    We intended to access the element [n, k] in the weight, but we are accessing [k, n] which is not valid.\n\n    So the current program has the weight matrix accessed in a transposed way without actually transposing the weight.\n\n    How to fix in the current program?\n\n        Option 1: Transpose the weight and use the same strides as the top program.\n\n        Option 2: Change the kernel to access the weight in the original layout.\n\n    Since we want to match the top program that passed, we choose Option 1.\n\n    We will change the current program to:\n\n        In the forward pass:\n            weight_t = self.weight.t()   # now it is [K, N]\n            # and then pass weight_t to the kernel, and for strides use:\n            stride_bk = weight_t.stride(0)   # for the first dimension (K) -> which should be N? Not necessarily: if non-contiguous, it might be 1? But we want the kernel to use the correct strides for the transposed weight.\n\n        However, note: the kernel expects strides for the b matrix (which is now [K, N]) as:\n\n            stride for k dimension (first dimension) -> stride(0) of weight_t\n            stride for n dimension (second dimension) -> stride(1) of weight_t\n\n        For a contiguous tensor of shape [K, N], the strides would be (N, 1). But the transposed tensor from a contiguous [N, K] is not contiguous. Its strides are (1, N) for [K, N]? Actually, the original weight [N, K] has strides (K, 1). Then the transposed tensor has strides (1, K) for the new shape [K, N].\n\n        So we would pass:\n\n            stride_bk = 1\n            stride_bn = K   # because the stride for the n dimension in the transposed tensor is K.\n\n        But note: in the kernel, we are doing:\n\n            b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n        with stride_bk=1 and stride_bn=K, we get:\n\n            base + offs_k * 1 + offs_n * K\n\n        which is the element at (k, n) in the new tensor [K, N]? But the new tensor is the transpose of the original, so this element is the element at (n, k) in the original.\n\n        This is correct.\n\n    Alternatively, we can avoid the transpose by changing the kernel to:\n\n        b_ptrs = b_ptr + offs_n[:, None] * stride_bn + offs_k[None, :] * stride_bk   # note: swapped offs_n and offs_k\n\n    but then we would be loading a row of the weight matrix for the given offs_n, and then we would need to transpose it in the kernel? That would be inefficient.\n\n Given the above, I will change the current program to:\n\n    In the forward method:\n\n        weight_t = self.weight.t()   # shape [K, N]\n        stride_bk = weight_t.stride(0)   # 1\n        stride_bn = weight_t.stride(1)   # K\n\n        Then call the kernel with:\n            x, weight_t, ...,\n            ...,\n            stride_bk, stride_bn,   # 1 and K\n\n    But note: the kernel expects the b matrix to be of size [K, N]. We are now passing weight_t which is [K, N].\n\n    Alternatively, we can change the kernel to accept the weight in the original layout by swapping the roles of the strides and indices, but that would require a kernel change.\n\n    However, the top program passed the weight as transposed and then used strides (stride(1), stride(0)) from the original weight (which become (1, K) for the transposed weight).\n\n    Let's do it like the top program.\n\n    We will:\n\n        - Change the model to use an `nn.Linear` layer? But the current program does not. We can keep the custom parameters.\n\n        - In the forward, we will pass the transposed weight and the strides accordingly.\n\n    Alternatively, we can change the kernel to work with the weight in the original shape [N, K] without transposing. How?\n\n        We want to compute x (M, K) @ weight (N, K)^T -> (M, N).\n\n        In the kernel, we can change the b matrix access to:\n\n            b_ptrs = b_ptr + offs_n[:, None] * stride_bn + offs_k[None, :] * stride_bk\n\n        and then we would be loading a row of the weight matrix (for the given offs_n) and then we would need to transpose it for the dot product? But the dot product requires the vector from a and the vector from b to be of length K.\n\n        Alternatively, we can load the weight matrix in a way that we get a [BLOCK_SIZE_N, BLOCK_SIZE_K] block in column-major order? \n\n        But the Triton dot operation expects the b matrix to be in row-major for the block? Actually, the dot product in Triton is flexible with the layout as long as the inner dimension is contiguous.\n\n        However, the easiest is to transpose the weight outside the kernel.\n\n    I choose to transpose the weight and adjust the strides as in the top program.\n\n    Steps for the current program:\n\n        In the forward:\n\n            weight_t = self.weight.t()   # [K, N]\n            # But note: we must make sure it's contiguous? The top program did not, but let's check the strides.\n\n            # If the weight is originally contiguous (row-major [N, K]), then the transposed weight will have strides (1, N) for [K, N].\n\n            stride_bk = weight_t.stride(0)   # 1\n            stride_bn = weight_t.stride(1)   # N? but wait: the original weight was [N, K] with strides (K, 1). The transposed weight has strides (1, K) because the first dimension (K) was the inner dimension in the original? \n\n            Actually, the transposed tensor of a contiguous tensor [N, K] is not contiguous. Its strides are (1, N) for the new shape [K, N]. But the stride for the first dimension (K) is 1, and for the second dimension (N) is the original stride for the first dimension, which is K (if the original was contiguous row-major, then the first dimension stride is K).\n\n            So we have:\n                stride_bk = 1\n                stride_bn = K   (not N)\n\n        So we pass:\n\n            b_ptr = weight_t\n            stride_bk = 1\n            stride_bn = K   # note: K is the in_features (the second dimension of the original weight)\n\n        But note: in the kernel, the b matrix is expected to be of size [K, N] and we are using strides (1, K). This is correct as explained.\n\n    We also note that the top program passed the weight as `self.linear.weight.t()` and then used:\n\n        stride_bk = self.linear.weight.stride(1)   # 1\n        stride_bn = self.linear.weight.stride(0)   # K\n\n    This is exactly what we have.\n\n    Therefore, we will change the current program's forward method to:\n\n        weight_t = self.weight.t()\n        stride_bk = weight_t.stride(0)   # 1\n        stride_bn = weight_t.stride(1)   # K\n\n        and then call the kernel with weight_t, stride_bk, stride_bn.\n\n    But note: the kernel expects the b matrix to be of size [K, N] and we are now providing that.\n\n    We must also change the kernel to handle the new program ID mapping with GROUP_M for performance? Not for correctness, but we want to match the top program.\n\n    Given the time, let's try to fix the correctness first by transposing the weight and adjusting the strides.\n\n    We'll change the kernel to be exactly the same as the top program for the program ID mapping and loop structure? Not necessary for correctness, but we want to maximize performance.\n\n    However, the current kernel does not have the GROUP_M optimization and the top program does.\n\n    We are allowed to change up to 20 lines.\n\n    Given the complexity, I will do:\n\n        Option: Change the kernel to the one from the top program (Program 1) and then adjust the model to pass the transposed weight and the strides accordingly.\n\n    Steps:\n\n        1. Replace the kernel with the one from Program 1.\n\n        2. In the model's forward, transpose the weight and use the strides as in the top program.\n\n    But note: the top program used an `nn.Linear` and then extracted the weight and bias. We are not using `nn.Linear` but we can still do:\n\n        weight_t = self.weight.t()\n        # then the strides for the transposed weight are:\n        #   stride_bk = 1\n        #   stride_bn = self.weight.stride(0)   # which is the original stride for the first dimension (N) -> which is K (in_features) if the weight is contiguous.\n\n        But if the weight is not contiguous, we should use the strides of the transposed tensor.\n\n        So we do:\n            stride_bk = weight_t.stride(0)\n            stride_bn = weight_t.stride(1)\n\n    Let's write the code accordingly.\n\n    We are going to replace the kernel and then adjust the call.\n\n    We will take the kernel from Program 1 and insert it.\n\n    And then in the forward:\n\n        weight_t = self.weight.t()\n        bias_ptr = self.bias if self.bias is not None else None   # but note: the top program used None for no bias, but our model always has bias? The test might have bias.\n\n        Then call the kernel with:\n            x, weight_t, output, bias_ptr,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),   # stride_bk and stride_bn\n            output.stride(0), output.stride(1),\n            self.scaling_factor, self.hardtanh_min, self.hardtanh_max,\n            GROUP_M=8   # note: the kernel has a GROUP_M argument\n\n    But note: the kernel in Program 1 has an extra argument at the end: GROUP_M=8.\n\n    Also, the kernel in Program 1 is called `fused_gemm_scale_act_kernel`, so we must change the name.\n\n    Alternatively, we can keep the kernel name as `fused_gemm_act_kernel` and change the top program's kernel to that name? But let's not change the kernel name to avoid confusion.\n\n    We'll change the kernel to the one from Program 1 and change its name to the current kernel's name? Or leave the name as in the top program.\n\n    Since the current program's kernel is named `fused_gemm_act_kernel`, and we are replacing it, we can change the name of the top program's kernel to `fused_gemm_act_kernel`.\n\n    But note: the autotune and jit decorators: the top program's kernel has autotune with different configs and key.\n\n    We are going to replace the entire kernel.\n\n    Given the constraints, we will do:\n\n        kernel code from Program 1, but change the kernel name to `fused_gemm_act_kernel` to match the call site.\n\n    But the current program's kernel has different config parameters (BLOCK_SIZE_M vs BLOCK_M, etc.). We can adjust the config to match the kernel or leave the kernel as in Program 1.\n\n    The top program's kernel uses BLOCK_M, BLOCK_N, BLOCK_K.\n\n    The current program's autotune config uses BLOCK_SIZE_M, etc.\n\n    We will change the autotune config in the new kernel to use the same block size arguments as the current program's kernel? Or change the call site to use the new block size arguments.\n\n    Alternatively, we can change the new kernel to use the same block size argument names as the current program's kernel (BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K) and then in the kernel body use these.\n\n    This might be safer because the call site in the model uses:\n\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n\n    and the new kernel uses BLOCK_M, BLOCK_N.\n\n    We have two options:\n\n        Option A: Change the new kernel to use BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K.\n\n        Option B: Change the grid function in the model to use BLOCK_M, BLOCK_N.\n\n    I choose Option A to minimize changes in the model.\n\n    We will modify the kernel from Program 1 to use the block size argument names: BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K.\n\n    And then in the kernel body, replace every occurrence of BLOCK_M with BLOCK_SIZE_M, etc.\n\n    But note: the kernel also has a GROUP_M. We will leave that as is.\n\n    Alternatively, we can change the grid function to use the new names. Since the kernel is being replaced, the model's grid function should be adjusted to the new kernel's block size names.\n\n    The current grid function uses 'BLOCK_SIZE_M' and 'BLOCK_SIZE_N'. The new kernel uses 'BLOCK_M' and 'BLOCK_N'. So we must change the grid function to use the new names.\n\n    We will change the grid function to:\n\n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n\n    as in the top program.\n\n    Given the above, let's do the following in the current program:\n\n        Replace the kernel with the one from Program 1, and change its name to `fused_gemm_act_kernel`.\n\n        Change the grid function in the forward to use 'BLOCK_M' and 'BLOCK_N'.\n\n        In the forward, transpose the weight and use the strides of the transposed weight.\n\n        Also, pass the GROUP_M=8.\n\n    But wait: the current program's kernel had an extra argument at the end for the blocking sizes? The new kernel has:\n\n        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n        GROUP_M: tl.constexpr = 8,\n\n    In the call site, we are passing the blocking sizes via the autotune, and then GROUP_M=8.\n\n    We will do.\n\n    However, note that the new kernel has a different program ID mapping. The grid function should be the same because it is based on the number of blocks in M and N.\n\n    The grid function in the new kernel is:\n\n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n\n    This is the same as the current grid function but with different block size names.\n\n    We will change the current grid function to use 'BLOCK_M' and 'BLOCK_N'.\n\n    Let's write the code.\n\n    Due to the complexity and the fact that we are limited to 20 lines, we might have to replace the entire kernel and then adjust the model's forward.\n\n    But note: the problem says to change only the Triton kernel source within the block? And the model class is also within the block.\n\n    We are allowed to change the entire block.\n\n    We will:\n\n        - Replace the kernel with the one from Program 1, but change the kernel name to `fused_gemm_act_kernel` to match the call site.\n\n        - In the kernel, change the block size argument names to BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K? Actually, let's not change the kernel's block size names. Instead, change the grid function in the model to use the new names.\n\n        - In the model's forward, change the grid function to:\n\n                grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n\n          and then when calling the kernel, pass the weight.t() and its strides.\n\n        - Also, pass GROUP_M=8.\n\n    Let's do it.\n\n    However, the current program's kernel is autotuned with a different set of configs. We will use the autotune configs from Program 1.\n\n    We are replacing the entire kernel, so we include the autotune configs from Program 1.\n\n    Given the above, here is the rewritten program.\n</think>\nTo address the correctness issue and improve performance, I'll implement the following changes based on the analysis:\n\n1. Fix weight matrix representation by transposing it before kernel execution\n2. Use grouped program ID mapping (GROUP_M) to improve L2 cache locality\n3. Precompute row/column masks outside GEMM loop for efficiency\n4. Maintain proper tensor core alignment with blocking parameters\n5. Ensure correct GELU implementation with erf-based formula\n\nHere's the complete solution:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_act_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    scaling_factor: tl.constexpr,\n    hardtanh_min: tl.constexpr,\n    hardtanh_max: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr = 8,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    mask_a = offs_am[:, None] < M\n    mask_b = offs_bn[None, :] < N\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        mask_k = offs_k < k_remaining\n        \n        a = tl.load(a_ptrs, mask=mask_a & mask_k[None, :], other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b & mask_k[:, None], other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    accumulator = accumulator * scaling_factor\n    accumulator = tl.minimum(tl.maximum(accumulator, hardtanh_min), hardtanh_max)\n    \n    gelu = 0.5 * accumulator * (1.0 + tl.erf(accumulator * 0.7071067811865475))\n    \n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, gelu, mask=c_mask)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scaling_factor = scaling_factor\n        self.hardtanh_min = hardtanh_min\n        self.hardtanh_max = hardtanh_max\n        \n        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = torch.nn.Parameter(torch.randn(out_features))\n        \n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        weight_t = self.weight.t().contiguous()\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        \n        fused_gemm_act_kernel[grid](\n            x, weight_t, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n            self.scaling_factor,\n            self.hardtanh_min,\n            self.hardtanh_max,\n            GROUP_M=8\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 0.5\nhardtanh_min = -2\nhardtanh_max = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "d4c83998-9e77-4806-b0ea-c68bd81ee5cd", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef max_pool3d_kernel(\n    input_ptr,\n    output_ptr,\n    # Tensor dimensions\n    B, C, D, H, W,\n    D_out, H_out, W_out,\n    # Input tensor strides\n    input_stride_b, input_stride_c, input_stride_d, input_stride_h, input_stride_w,\n    # Output tensor strides\n    output_stride_b, output_stride_c, output_stride_d, output_stride_h, output_stride_w,\n    # Pooling parameters\n    kernel_d: tl.constexpr, kernel_h: tl.constexpr, kernel_w: tl.constexpr,\n    stride_d, stride_h, stride_w,\n    padding_d, padding_h, padding_w,\n    dilation_d, dilation_h, dilation_w,\n    # Block size\n    BLOCK_SIZE: tl.constexpr\n):\n    # Program indices\n    pid_b = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    pid_s = tl.program_id(2)\n    \n    # Decompose spatial index\n    hw_size = H_out * W_out\n    d_out = pid_s // hw_size\n    hw = pid_s % hw_size\n    h_out = hw // W_out\n    w_out = hw % W_out\n\n    # Check bounds\n    if pid_b >= B or pid_c >= C or d_out >= D_out or h_out >= H_out or w_out >= W_out:\n        return\n\n    # Calculate starting indices\n    start_d = d_out * stride_d - padding_d\n    start_h = h_out * stride_h - padding_h\n    start_w = w_out * stride_w - padding_w\n    \n    # Precompute base offset for batch and channel\n    base_offset = pid_b * input_stride_b + pid_c * input_stride_c\n    max_val = tl.full((), float('-inf'), dtype=tl.float32)\n    \n    # Optimized for common kernel size 3x3x3\n    if kernel_d == 3 and kernel_h == 3 and kernel_w == 3:\n        # Unroll loops for 3x3x3 case\n        for kd in tl.static_range(0, 3):\n            d_in = start_d + kd * dilation_d\n            d_in_bound = (d_in >= 0) & (d_in < D)\n            for kh in tl.static_range(0, 3):\n                h_in = start_h + kh * dilation_h\n                h_in_bound = d_in_bound & (h_in >= 0) & (h_in < H)\n                for kw in tl.static_range(0, 3):\n                    w_in = start_w + kw * dilation_w\n                    in_bounds = h_in_bound & (w_in >= 0) & (w_in < W)\n                    \n                    # Calculate input offset\n                    input_offset = base_offset + \\\n                                  d_in * input_stride_d + \\\n                                  h_in * input_stride_h + \\\n                                  w_in * input_stride_w\n                    \n                    # Load value with boundary mask\n                    val = tl.load(input_ptr + input_offset, mask=in_bounds, other=float('-inf'))\n                    max_val = tl.maximum(max_val, val)\n    else:\n        # Generic kernel size handling\n        for kd in range(0, kernel_d):\n            d_in = start_d + kd * dilation_d\n            d_in_bound = (d_in >= 0) & (d_in < D)\n            for kh in range(0, kernel_h):\n                h_in = start_h + kh * dilation_h\n                h_in_bound = d_in_bound & (h_in >= 0) & (h_in < H)\n                for kw in range(0, kernel_w):\n                    w_in = start_w + kw * dilation_w\n                    in_bounds = h_in_bound & (w_in >= 0) & (w_in < W)\n                    \n                    # Calculate input offset\n                    input_offset = base_offset + \\\n                                  d_in * input_stride_d + \\\n                                  h_in * input_stride_h + \\\n                                  w_in * input_stride_w\n                    \n                    # Load value with boundary mask\n                    val = tl.load(input_ptr + input_offset, mask=in_bounds, other=float('-inf'))\n                    max_val = tl.maximum(max_val, val)\n    \n    # Calculate output offset\n    out_offset = (\n        pid_b * output_stride_b + \n        pid_c * output_stride_c + \n        d_out * output_stride_d + \n        h_out * output_stride_h + \n        w_out * output_stride_w\n    )\n    \n    # Store result\n    tl.store(output_ptr + out_offset, max_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, \n                 dilation: int = 1, return_indices: bool = False, ceil_mode: bool = False):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n        self.dilation = dilation\n        self.return_indices = return_indices\n        self.ceil_mode = ceil_mode\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Fallback to PyTorch for special cases\n        if self.return_indices or self.ceil_mode:\n            return torch.nn.functional.max_pool3d(\n                x, self.kernel_size, self.stride, self.padding, \n                self.dilation, self.ceil_mode, self.return_indices\n            )\n            \n        # Ensure input is contiguous\n        x = x.contiguous()\n        \n        # Get input dimensions\n        B, C, D, H, W = x.shape\n        \n        # Convert parameters to tuples\n        kernel_size = (self.kernel_size, self.kernel_size, self.kernel_size)\n        stride = (self.stride, self.stride, self.stride)\n        padding = (self.padding, self.padding, self.padding)\n        dilation = (self.dilation, self.dilation, self.dilation)\n        \n        # Calculate output dimensions\n        def output_dim(dim, k, p, d, s):\n            numerator = dim + 2 * p - d * (k - 1) - 1\n            return (numerator // s) + 1\n            \n        D_out = output_dim(D, kernel_size[0], padding[0], dilation[0], stride[0])\n        H_out = output_dim(H, kernel_size[1], padding[1], dilation[1], stride[1])\n        W_out = output_dim(W, kernel_size[2], padding[2], dilation[2], stride[2])\n        \n        # Create output tensor\n        output = torch.empty((B, C, D_out, H_out, W_out), \n                             device=x.device, dtype=x.dtype)\n        \n        # Grid configuration - 3D grid for better parallelism\n        grid = (B, C, D_out * H_out * W_out)\n        \n        # Launch kernel\n        max_pool3d_kernel[grid](\n            x, output,\n            B, C, D, H, W,\n            D_out, H_out, W_out,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4),\n            kernel_size[0], kernel_size[1], kernel_size[2],\n            stride[0], stride[1], stride[2],\n            padding[0], padding[1], padding[2],\n            dilation[0], dilation[1], dilation[2],\n            BLOCK_SIZE=1\n        )\n        \n        return output\n\nbatch_size = 16\nchannels = 32\ndim1 = 64\ndim2 = 64\ndim3 = 64\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, dim1, dim2, dim3)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "2faf64de-8ae3-451d-8037-aa35ca9845c0", "generation": 4, "timestamp": 1754646039.2041087, "iteration_found": 24, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.019, "runtime_ms": 25.0, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.487, "speed_up": 0.019, "custom_timing": 25.0}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.018, "runtime_ms": 26.6, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.487, "speed_up": 0.018, "custom_timing": 26.6}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
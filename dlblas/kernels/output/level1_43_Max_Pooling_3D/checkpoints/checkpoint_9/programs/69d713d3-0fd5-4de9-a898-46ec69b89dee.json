{"id": "69d713d3-0fd5-4de9-a898-46ec69b89dee", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _max_pool3d_kernel(\n    x_ptr,\n    output_ptr,\n    batch,\n    channels,\n    D, H, W,\n    out_d, out_h, out_w,\n    stride_d, stride_h, stride_w,\n    pad_d, pad_h, pad_w,\n    dilation_d, dilation_h, dilation_w,\n    kernel_d, kernel_h, kernel_w,\n    stride_n, stride_c, stride_d_in, stride_h_in, stride_w_in,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid1 = tl.program_id(0)\n    pid2 = tl.program_id(1)\n    \n    total_spatial = out_d * out_h * out_w\n    n = pid1 // channels\n    c = pid1 % channels\n    idx_spatial = pid2\n    pd = idx_spatial // (out_h * out_w)\n    ph = (idx_spatial % (out_h * out_w)) // out_w\n    pw = idx_spatial % out_w\n\n    start_d = pd * stride_d - pad_d\n    start_h = ph * stride_h - pad_h\n    start_w = pw * stride_w - pad_w\n\n    max_val = -tl.math.inf\n\n    for kd in range(0, kernel_d):\n        d_index = start_d + kd * dilation_d\n        if d_index < 0 or d_index >= D:\n            continue\n        for kh in range(0, kernel_h):\n            h_index = start_h + kh * dilation_h\n            if h_index < 0 or h_index >= H:\n                continue\n            for kw in range(0, kernel_w):\n                w_index = start_w + kw * dilation_w\n                if w_index < 0 or w_index >= W:\n                    continue\n\n                offset = (n * stride_n + c * stride_c + \n                          d_index * stride_d_in + \n                          h_index * stride_h_in + \n                          w_index * stride_w_in)\n                val = tl.load(x_ptr + offset)\n                if val > max_val:\n                    max_val = val\n\n    output_offset = pid1 * total_spatial + pid2\n    tl.store(output_ptr + output_offset, max_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, \n                 dilation: int = 1, return_indices: bool = False, ceil_mode: bool = False):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n        self.dilation = dilation\n        self.return_indices = return_indices\n        self.ceil_mode = ceil_mode\n\n        if isinstance(self.kernel_size, int):\n            self.kernel_size = (self.kernel_size,)*3\n        if isinstance(self.stride, int):\n            self.stride = (self.stride,)*3\n        if isinstance(self.padding, int):\n            self.padding = (self.padding,)*3\n        if isinstance(self.dilation, int):\n            self.dilation = (self.dilation,)*3\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.return_indices or self.ceil_mode:\n            return torch.nn.functional.max_pool3d(\n                x, self.kernel_size, self.stride, self.padding, \n                self.dilation, self.ceil_mode, self.return_indices\n            )\n\n        x_contiguous = x.contiguous()\n        batch, channels, D, H, W = x_contiguous.shape\n\n        def _output_size(dim, k, s, p, d):\n            return (dim + 2 * p - d * (k - 1) - 1) // s + 1\n\n        out_d = _output_size(D, self.kernel_size[0], self.stride[0], self.padding[0], self.dilation[0])\n        out_h = _output_size(H, self.kernel_size[1], self.stride[1], self.padding[1], self.dilation[1])\n        out_w = _output_size(W, self.kernel_size[2], self.stride[2], self.padding[2], self.dilation[2])\n\n        output = torch.empty((batch, channels, out_d, out_h, out_w), device=x.device, dtype=x.dtype)\n        total_batch_channels = batch * channels\n        total_spatial = out_d * out_h * out_w\n        grid = (total_batch_channels, total_spatial)\n\n        stride_n = x_contiguous.stride(0)\n        stride_c = x_contiguous.stride(1)\n        stride_d_in = x_contiguous.stride(2)\n        stride_h_in = x_contiguous.stride(3)\n        stride_w_in = x_contiguous.stride(4)\n\n        _max_pool3d_kernel[grid](\n            x_contiguous, output,\n            batch, channels, D, H, W,\n            out_d, out_h, out_w,\n            self.stride[0], self.stride[1], self.stride[2],\n            self.padding[0], self.padding[1], self.padding[2],\n            self.dilation[0], self.dilation[1], self.dilation[2],\n            self.kernel_size[0], self.kernel_size[1], self.kernel_size[2],\n            stride_n, stride_c, stride_d_in, stride_h_in, stride_w_in,\n            BLOCK_SIZE=32\n        )\n        return output\n\nbatch_size = 16\nchannels = 32\ndim1 = 64\ndim2 = 64\ndim3 = 64\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, dim1, dim2, dim3)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "363f0b65-65ae-4b46-a7a7-78a97af66e97", "generation": 1, "timestamp": 1754641280.3812754, "iteration_found": 1, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_43_Max_Pooling_3D", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpvh8m9z64.py\", line 111, in forward\n    _max_pool3d_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language.math' has no attribute 'inf'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 0.487, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.473, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.487, "speed_up": 1.03, "custom_timing": 0.473}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.4730\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.487, 'speed_up': 1.03, 'custom_timing': 0.473}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.4730, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.487, 'speed_up': 1.03, 'custom_timing': 0.473}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 3D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False, ceil_mode: bool = False):\n        \"\"\"\n        Initializes the Max Pooling 3D layer.\n\n        Args:\n            kernel_size (int): Size of the kernel for the max pooling operation.\n            stride (int, optional): Stride of the pooling operation. Defaults to None, which means stride is equal to kernel_size.\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            return_indices (bool, optional): Whether to return indices of the maximum values. Defaults to False.\n            ceil_mode (bool, optional): When True, the output size is ceil(input_size / stride) instead of floor. Defaults to False.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.maxpool = nn.MaxPool3d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices, ceil_mode=ceil_mode)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 3D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, dim1, dim2, dim3).\n\n        Returns:\n            torch.Tensor: Output tensor with Max Pooling 3D applied.\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 16\nchannels = 32\ndim1 = 64\ndim2 = 64\ndim3 = 64\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, dim1, dim2, dim3)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.4730), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.487, 'speed_up': 1.03, 'custom_timing': 0.473})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs Max Pooling 3D.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False, ceil_mode: bool = False):\n        \"\"\"\n        Initializes the Max Pooling 3D layer.\n\n        Args:\n            kernel_size (int): Size of the kernel for the max pooling operation.\n            stride (int, optional): Stride of the pooling operation. Defaults to None, which means stride is equal to kernel_size.\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            return_indices (bool, optional): Whether to return indices of the maximum values. Defaults to False.\n            ceil_mode (bool, optional): When True, the output size is ceil(input_size / stride) instead of floor. Defaults to False.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.maxpool = nn.MaxPool3d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices, ceil_mode=ceil_mode)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies Max Pooling 3D to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, dim1, dim2, dim3).\n\n        Returns:\n            torch.Tensor: Output tensor with Max Pooling 3D applied.\n        \"\"\"\n        return self.maxpool(x)\n\nbatch_size = 16\nchannels = 32\ndim1 = 64\ndim2 = 64\ndim3 = 64\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, dim1, dim2, dim3)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch reference code for a 3D max pooling operation. The current implementation uses `nn.MaxPool3d` which is efficient but we are to replace it with a Triton kernel for potentially better performance.\n\nHowever, note that the problem states that the current program is at the reference code stage (using PyTorch's built-in) and we are to rewrite it using Triton.\n\nThe constraints:\n- We must not change the function signatures (the forward method and the class structure).\n- We must maintain the same inputs and outputs.\n\nPlan:\n1. We will remove the `nn.MaxPool3d` layer and replace it with a custom Triton kernel in the forward pass.\n2. We need to write a Triton kernel for 3D max pooling. The kernel will:\n   - Iterate over each output position (which is determined by the input dimensions, kernel size, stride, padding, and dilation).\n   - For each output position, we will iterate over the 3D kernel to find the maximum value.\n3. We must handle the parameters: kernel_size, stride, padding, dilation, return_indices, ceil_mode.\n   - Note: The original problem states that the current program does not use `return_indices` and `ceil_mode` in the example, but we must account for them as per the `__init__` method.\n\nImportant: The Triton kernel must be designed for 3D max pooling.\n\nSteps for the Triton kernel:\n   a. Determine the output dimensions based on the input dimensions and pooling parameters (using the same formula as PyTorch).\n   b. The kernel will be launched with a grid that covers all output positions (for batch, channel, and spatial dimensions).\n   c. Each program will compute the max over the kernel's 3D window.\n\nHowever, note that the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" But in this case, the entire block is the model definition and the test setup. Since we are replacing the PyTorch implementation with a Triton one, we have to change the forward method and also define the Triton kernel.\n\nBut note: The example provided in the problem (the quantize example) shows that we can define the Triton kernel and the function that calls it. So we are allowed to change the entire block.\n\nWe are going to:\n   - Remove the `nn.MaxPool3d` layer.\n   - Define a Triton kernel for 3D max pooling (with the option to return indices if needed, but the problem says return_indices is False by default, so we ignore for now? Actually, the problem says we must maintain the same functionality. So we have to handle return_indices? But the current example does not use it. We are going to assume that we are only required to return the output tensor for now, and if return_indices is True, we must return both. However, the problem says: \"maintain the same inputs and outputs\". So if `return_indices` is set to True in the initialization, we must return a tuple.\n\nBut the current reference code returns only the tensor because `return_indices` is False. We are going to assume that in this benchmark, `return_indices` is always False? The `get_inputs` does not set `return_indices` to True. So we can ignore the indices for now? However, to be safe, we should implement the indices if `return_indices` is True.\n\nBut note: The problem says to rewrite the program to improve performance. We are allowed to change the internal implementation. We are going to focus on the case when `return_indices` is False because that is the default.\n\nHowever, the problem says: \"maintain the same inputs and outputs\". So if the model is initialized with `return_indices=True`, we must return two tensors. We are going to design:\n\n   - If `return_indices` is True, then we return (output, indices) just like PyTorch.\n   - Else, we return output.\n\nBut implementing the indices in Triton might be complex and the problem says we can do small iterative steps. Since the current state does not use indices (the test doesn't set it to True), we can first implement without indices and then in a future step add indices? But the problem says we must maintain the same functionality.\n\nGiven the complexity, we will implement without indices for now and note that if `return_indices` is True, we must use the PyTorch fallback? Or we must implement it. But the problem says we cannot change the signature. We have to return the same as the original.\n\nBut the original returns only the tensor when `return_indices=False` and a tuple when `return_indices=True`. We cannot change that.\n\nWe have two options:\n   Option 1: Implement both the output and the indices in Triton. This is complex and might be out of scope for a first step.\n   Option 2: For the case when `return_indices` is True, we use the PyTorch max_pool3d function. We can do:\n\n        if self.return_indices:\n            # Use PyTorch's implementation for now because we haven't implemented indices in Triton\n            return F.max_pool3d(x, self.kernel_size, self.stride, self.padding, self.dilation, self.ceil_mode, self.return_indices)\n\n   But then we are not improving that case. However, the benchmark might not test that case.\n\nLooking at the test setup:\n\n    def get_init_inputs():\n        return [kernel_size, stride, padding, dilation]\n\nIt does not pass `return_indices` or `ceil_mode`. So they are set to their defaults (False). So we can assume that in the benchmark, `return_indices` is False and `ceil_mode` is False.\n\nTherefore, we can ignore the indices for now and focus on the case when `return_indices=False`.\n\nSimilarly, we ignore `ceil_mode` for now and assume floor division.\n\nImplementation:\n\n   We will write a Triton kernel for 3D max pooling without returning indices.\n\n   Steps for the kernel:\n\n      Let:\n        input shape: (batch, channels, D, H, W)\n        kernel: (k_d, k_h, k_w)  [we assume kernel_size is either an int (then same for all) or a tuple of 3 ints]\n\n      We assume the parameters:\n        stride: (s_d, s_h, s_w) [if None, then set to kernel_size]\n        padding: (pad_d, pad_h, pad_w)\n        dilation: (d_d, d_h, d_w)\n\n      Output dimensions:\n        out_d = (D + 2 * pad_d - dilation[0] * (kernel_size[0] - 1) - 1) // stride[0] + 1\n        similarly for H and W.\n\n      The kernel will be launched with 3D grid: (batch * channels, out_d, out_h, out_w) but note: Triton grid can be 1D or 2D. We can use a 1D grid by flattening the batch and channels and the spatial dimensions? Or we can use a 3D grid: (out_d, out_h, out_w) and then for each batch and channel we do in a loop? But that would be inefficient.\n\n      Alternatively, we can use a 1D grid that covers all output elements (batch * channels * out_d * out_h * out_w). But that might be too many threads? We can also use a 2D grid: (batch * channels, out_d * out_h * out_w) or (batch, channels * out_d * out_h * out_w). We choose a 1D grid for simplicity.\n\n      However, note that the input tensor is 5D. We can flatten the batch and channels together? Because the pooling is independent per channel and per batch.\n\n      So we can do:\n\n          total_output_elements = batch * channels * out_d * out_h * out_w\n\n      and launch with grid = (total_output_elements,)\n\n      Each program will compute one output element.\n\n      Steps in the kernel:\n\n        pid = program_id(0)\n        # Compute indices: batch, channel, d, h, w from pid\n        # We assume order: [batch, channels, D, H, W] for input and output.\n\n        # First, we can compute:\n        n = pid // (channels * out_d * out_h * out_w)\n        remainder = pid % (channels * out_d * out_h * out_w)\n        c = remainder // (out_d * out_h * out_w)\n        remainder2 = remainder % (out_d * out_h * out_w)\n        pd = remainder2 // (out_h * out_w)\n        ph = (remainder2 % (out_h * out_w)) // out_w\n        pw = remainder2 % out_w\n\n        But note: This might be expensive. Alternatively, we can use a 3D grid? But Triton only supports 1D or 2D grid.\n\n        We can do 2D grid: \n            grid = (batch * channels, out_d * out_h * out_w)\n\n        Then in the kernel:\n            pid1 = program_id(0)  # which batch and channel\n            pid2 = program_id(1)  # which spatial position in the output\n\n            n = pid1 // channels\n            c = pid1 % channels\n            pd = pid2 // (out_h * out_w)\n            ph = (pid2 % (out_h * out_w)) // out_w\n            pw = pid2 % out_w\n\n        This is better because we avoid a big division.\n\n      Then, for the output position (n, c, pd, ph, pw), we compute:\n\n        start_d = pd * stride_d - pad_d\n        start_h = ph * stride_h - pad_h\n        start_w = pw * stride_w - pad_w\n\n        Then we iterate over the kernel:\n\n        max_val = -infinity\n        for kd in range(0, kernel_d):\n            for kh in range(0, kernel_h):\n                for kw in range(0, kernel_w):\n                    d_index = start_d + kd * dilation_d\n                    h_index = start_h + kh * dilation_h\n                    w_index = start_w + kw * dilation_w\n                    if d_index >=0 and d_index < D and h_index>=0 and h_index<H and w_index>=0 and w_index<W:\n                        val = input[n, c, d_index, h_index, w_index]\n                        if val > max_val:\n                            max_val = val\n\n        Then set output[n, c, pd, ph, pw] = max_val\n\n      However, note: the window might be entirely out of bounds? Then we set to 0? But in PyTorch, if the window is entirely in negative padding, it returns -inf? Actually, the max over an empty set is -inf in PyTorch.\n\n      We can initialize max_val to -inf.\n\n      But note: the Triton kernel does not have a direct -inf for the data type. We can use `tl.min` for the type? Or we can use a very small number. Alternatively, we can use `tl.where` and a flag.\n\n      We can do:\n\n          max_val = -float('inf')\n          # But in Triton, we can use `tl.min` for the type? Actually, we can use `-tl.math.inf` for float32.\n\n      However, the input tensor might be of any dtype? The example uses float? But the test input is `torch.randn` which is float32.\n\n      We'll assume float32.\n\n      Steps:\n\n        We load the input tensor. We must use `tl.load` with the correct pointer and mask.\n\n      How to compute the base pointer for the input?\n\n        input_ptr + n * stride_n + c * stride_c\n\n        Then we add d_index * stride_d + h_index * stride_h + w_index * stride_w.\n\n      We precompute the strides.\n\n      However, note: the input tensor might not be contiguous. We should make it contiguous in the function call.\n\n      We'll do in the host function:\n\n          x = x.contiguous()\n\n      Then we can pass the strides.\n\n      Alternatively, we can use `triton.as_tensor` and then get the strides? Actually, we can pass the strides as kernel parameters.\n\n      We'll pass the following to the kernel:\n\n          input_ptr, output_ptr, \n          batch, channels, D, H, W,\n          out_d, out_h, out_w,\n          stride_d, stride_h, stride_w, \n          pad_d, pad_h, pad_w,\n          dilation_d, dilation_h, dilation_w,\n          kernel_d, kernel_h, kernel_w,\n          stride_n, stride_c, stride_d_in, stride_h_in, stride_w_in\n\n      But note: the kernel_size, stride, padding, dilation might be integers or tuples. In the __init__ of the model, we convert them to tuples of 3.\n\n      We'll do in the model:\n\n          if isinstance(kernel_size, int):\n              kernel_size = (kernel_size, kernel_size, kernel_size)\n          similarly for stride, padding, dilation.\n\n      Then we can compute the output shape.\n\n      How to compute output shape? We can use the formula from PyTorch:\n\n          out_d = floor((D + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n\n      But note: if ceil_mode is True, we use ceil. We are not handling ceil_mode for now? The test uses default (False). So we do floor.\n\n      We'll compute the output shape in the forward function.\n\n      Steps in the forward function:\n\n          if self.return_indices:\n              # For now, use PyTorch if return_indices is True\n              return F.max_pool3d(x, self.kernel_size, self.stride, self.padding, self.dilation, self.ceil_mode, self.return_indices)\n\n          # Else, use Triton\n          x_contiguous = x.contiguous()\n          batch, channels, D, H, W = x_contiguous.shape\n          # Compute output shape\n          def _output_size(dim, k, s, p, d):\n              return (dim + 2 * p - d * (k - 1) - 1) // s + 1\n\n          out_d = _output_size(D, self.kernel_size[0], self.stride[0], self.padding[0], self.dilation[0])\n          out_h = _output_size(H, self.kernel_size[1], self.stride[1], self.padding[1], self.dilation[1])\n          out_w = _output_size(W, self.kernel_size[2], self.stride[2], self.padding[2], self.dilation[2])\n\n          output = torch.empty((batch, channels, out_d, out_h, out_w), device=x.device, dtype=x.dtype)\n\n          # Flatten the batch and channels for the grid\n          total_batch_channels = batch * channels\n          total_spatial = out_d * out_h * out_w\n\n          # We use a 2D grid: (total_batch_channels, total_spatial)\n          grid = (total_batch_channels, total_spatial)\n\n          # We pass the kernel parameters as constants\n          # Note: the kernel expects the strides for the input tensor (in memory) and the pooling parameters.\n\n          # We get the strides of the contiguous tensor\n          stride_n = x_contiguous.stride(0)\n          stride_c = x_contiguous.stride(1)\n          stride_d_in = x_contiguous.stride(2)\n          stride_h_in = x_contiguous.stride(3)\n          stride_w_in = x_contiguous.stride(4)\n\n          # Launch the kernel\n          _max_pool3d_kernel[grid](\n              x_contiguous, output,\n              batch, channels, D, H, W,\n              out_d, out_h, out_w,\n              self.stride[0], self.stride[1], self.stride[2],\n              self.padding[0], self.padding[1], self.padding[2],\n              self.dilation[0], self.dilation[1], self.dilation[2],\n              self.kernel_size[0], self.kernel_size[1], self.kernel_size[2],\n              stride_n, stride_c, stride_d_in, stride_h_in, stride_w_in,\n          )\n\n          return output\n\n      Now, the Triton kernel:\n\n        @triton.jit\n        def _max_pool3d_kernel(\n            x_ptr,\n            output_ptr,\n            batch,\n            channels,\n            D, H, W,\n            out_d, out_h, out_w,\n            stride_d, stride_h, stride_w,\n            pad_d, pad_h, pad_w,\n            dilation_d, dilation_h, dilation_w,\n            kernel_d, kernel_h, kernel_w,\n            stride_n, stride_c, stride_d_in, stride_h_in, stride_w_in,\n            BLOCK_SIZE: tl.constexpr,   # We can use a block size for the inner loop? Actually, we are not using blocks, we are iterating over the kernel window. But the kernel window is small (e.g., 3x3x3=27). So we can do without tiling.\n        ):\n            pid1 = tl.program_id(0)   # batch and channel index\n            pid2 = tl.program_id(1)   # spatial index\n\n            # Reconstruct indices:\n            n = pid1 // channels\n            c = pid1 % channels\n            idx_spatial = pid2\n            pd = idx_spatial // (out_h * out_w)\n            ph = (idx_spatial % (out_h * out_w)) // out_w\n            pw = idx_spatial % out_w\n\n            # Compute the start of the window in the input space:\n            start_d = pd * stride_d - pad_d\n            start_h = ph * stride_h - pad_h\n            start_w = pw * stride_w - pad_w\n\n            # Initialize max_val to -inf\n            max_val = -tl.math.inf\n\n            # We iterate over the kernel\n            for kd in range(0, kernel_d):\n                d_index = start_d + kd * dilation_d\n                if d_index < 0 or d_index >= D:\n                    continue\n                for kh in range(0, kernel_h):\n                    h_index = start_h + kh * dilation_h\n                    if h_index < 0 or h_index >= H:\n                        continue\n                    for kw in range(0, kernel_w):\n                        w_index = start_w + kw * dilation_w\n                        if w_index < 0 or w_index >= W:\n                            continue\n\n                        # Compute the memory offset\n                        offset = (n * stride_n + c * stride_c + \n                                  d_index * stride_d_in + \n                                  h_index * stride_h_in + \n                                  w_index * stride_w_in)\n                        val = tl.load(x_ptr + offset)\n\n                        if val > max_val:\n                            max_val = val\n\n            # Store the result\n            output_offset = (n * stride_n + c * stride_c + \n                             pd * stride_d_in * ...)   # Wait, the output tensor is contiguous? We created it as empty with the same shape and on the same device? But we didn't set strides. It is contiguous by default.\n\n            Actually, we created the output as contiguous. So we can compute the offset for the output similarly.\n\n            But note: the output tensor has shape (batch, channels, out_d, out_h, out_w). We can compute:\n\n            output_offset = n * (channels * out_d * out_h * out_w) + c * (out_d * out_h * out_w) + pd * (out_h * out_w) + ph * out_w + pw\n\n            Alternatively, we can precompute the strides for the output? But we didn't pass them. We can also assume the output is contiguous? It is.\n\n            So:\n\n            output_index = n * channels * out_d * out_h * out_w + c * out_d * out_h * out_w + pd * out_h * out_w + ph * out_w + pw\n\n            But note: we have a 2D grid, so we are storing by the same pid1 and pid2? Actually, the output tensor is contiguous and we can compute:\n\n            output_offset = pid1 * (out_d * out_h * out_w) + pid2\n\n            Because the grid is (batch*channels, out_d*out_h*out_w). So the output element at (pid1, pid2) is at the linear index: pid1 * total_spatial + pid2.\n\n            But note: the output tensor is 5D. We created it as (batch, channels, out_d, out_h, out_w). Then we can view it as (batch*channels, out_d, out_h, out_w) and then flatten the spatial part? Actually, we don't need to. We can compute:\n\n            output_linear_index = n * channels * out_d * out_h * out_w + c * out_d * out_h * out_w + pd * out_h * out_w + ph * out_w + pw\n\n            But note: pd, ph, pw are computed from pid2, and we have:\n\n                total_spatial = out_d * out_h * out_w\n                linear_index = pid1 * total_spatial + pid2\n\n            And that matches:\n\n                pid1 = n * channels + c\n                pid2 = pd * (out_h * out_w) + ph * out_w + pw\n\n            So:\n\n                linear_index = (n * channels + c) * total_spatial + (pd * (out_h*out_w) + ph * out_w + pw)\n\n            But the output tensor we created is contiguous and has shape (batch, channels, out_d, out_h, out_w). The element at (n, c, pd, ph, pw) is at:\n\n                n * (channels * out_d * out_h * out_w) + c * (out_d * out_h * out_w) + pd * (out_h * out_w) + ph * out_w + pw\n\n            which is the same as (n * channels + c) * (out_d*out_h*out_w) + (pd*out_h*out_w + ph*out_w + pw) only if we interpret:\n\n                pid1 = n * channels + c   -> then we have:\n\n                linear_index = pid1 * total_spatial + pid2\n\n            So we can simply do:\n\n                output_linear_index = pid1 * (out_d * out_h * out_w) + pid2\n\n            Then we store:\n\n                tl.store(output_ptr + output_linear_index, max_val)\n\n      But note: the output tensor is contiguous? We created it with `torch.empty` and the same shape, so it should be contiguous.\n\n      However, we must check the strides? The contiguous tensor in PyTorch has strides in decreasing order. So the stride for the first dimension is (channels * out_d * out_h * out_w) and then for channels is (out_d * out_h * out_w), etc. So the linear index we computed is the same as the memory offset? Yes, for a contiguous tensor.\n\n      Therefore, we can store at:\n\n          output_ptr + pid1 * total_spatial + pid2\n\n      But note: total_spatial = out_d * out_h * out_w.\n\n      We pass total_spatial? Actually, we don't pass it. We can compute it in the kernel? We pass out_d, out_h, out_w, so we can compute:\n\n          total_spatial = out_d * out_h * out_w\n\n      Then:\n\n          output_offset = pid1 * total_spatial + pid2\n\n      Then:\n\n          tl.store(output_ptr + output_offset, max_val)\n\n      But note: the output is of the same dtype as the input? We assume float32.\n\n      We are done.\n\n      However, note: the kernel window iteration might be inefficient because it's nested loops. But the kernel window is small (e.g., 3x3x3=27), so it's acceptable.\n\n      We must also note that we are not using any shared memory or tensor cores. This is a straightforward implementation.\n\n      We are going to try to compile and run.\n\n      But note: the problem says to use Triton. We are writing the kernel in Triton.\n\n      We must also note that the kernel might be slow because of the nested loops? But the window is small.\n\n      We can try to unroll the inner loops? We can use `tl.static_range` for the kernel dimensions? But the kernel size is passed as a variable. We cannot unroll if it's dynamic.\n\n      We assume the kernel sizes are small (like 2,3,5) so the loops are short.\n\n      We are going to write the kernel accordingly.\n\n      We must also note that the input tensor is in row-major and we are loading one element at a time? This might not be coalesced. But each thread is accessing arbitrary locations? So it might be slow.\n\n      However, we note that the window for adjacent output positions overlap? And we are not sharing the data between threads. This is the same as a naive implementation.\n\n      But we are hoping that the kernel size is small and the number of threads is large enough to hide the latency.\n\n      We are going to write the kernel and hope it compiles and runs.\n\n      We also note that we are not using any advanced features like tensor cores because max pooling doesn't involve matrix multiplications.\n\n      Finally, we set the grid to (total_batch_channels, total_spatial). The total number of threads is total_batch_channels * total_spatial, which is the total number of output elements. This is acceptable for modern GPUs if the output is not too big.\n\n      In the test, the input is (16, 32, 64,64,64) and kernel=3, stride=2, padding=1, dilation=3.\n\n        Output spatial: \n          out_d = (64 + 2*1 - 3*(3-1) -1)//2 + 1 = (64+2-6-1)//2+1 = (59)//2+1 = 29+1? Actually, 59//2=29, then 30? But let me compute:\n\n          (64 + 2 - 3*2 - 1) = 64+2-6-1=59, then 59//2=29 (floor) then +1 -> 30.\n\n          So output is (16,32,30,30,30) -> total output elements: 16*32*30*30*30 = 13,824,000. This is acceptable for a grid.\n\n      We are going to write the kernel.\n\n      We must also note: the kernel parameters (like kernel_d, kernel_h, kernel_w) must be passed as constants? Not necessarily, they can be variables.\n\n      But note: the Triton compiler might unroll the loops if the kernel sizes are known at compile time? But they are passed as parameters. We can use `tl.constexpr` for them? But the kernel size might change per call? We cannot.\n\n      Alternatively, we can pass them as regular arguments and use dynamic loops.\n\n      We'll do dynamic loops.\n\n      Now, we write the code.\n\n      Important: We must not change the class name and the functions `get_inputs` and `get_init_inputs`.\n\n      We are going to replace the entire EVOLVE-BLOCK.\n\n      We note: the current model uses `nn.MaxPool3d`. We are replacing it with our own implementation.\n\n      We also note: the test has a fixed batch_size, channels, dims, and parameters. We are going to use the same.\n\n      We'll write the Triton kernel and the model class.\n\n      We are going to do:\n\n          import triton\n          import triton.language as tl\n\n      But note: the problem says we are to rewrite the entire block. We are allowed to change the entire block.\n\n      We'll write:\n\n          import torch\n          import torch.nn as nn\n          import triton\n          import triton.language as tl\n\n          class ModelNew(nn.Module):\n              ... [our implementation] ...\n\n          batch_size = ... [same as before] ...\n\n      We are going to keep the same batch_size, etc.\n\n      Now, the code.\n\n      Note: We are ignoring ceil_mode and return_indices for now? We are going to handle return_indices by conditionally using PyTorch. For ceil_mode, we are using the floor formula. But the test uses ceil_mode=False, so it's safe.\n\n      We'll add a TODO for ceil_mode and return_indices.\n\n      Let's code.\n\n      IMPORTANT: We must not break the test. The test passes the parameters: kernel_size, stride, padding, dilation. We are going to convert them to tuples.\n\n      We'll do in __init__:\n\n          self.kernel_size = kernel_size\n          self.stride = stride if stride is not None else kernel_size\n          self.padding = padding\n          self.dilation = dilation\n          self.return_indices = return_indices\n          self.ceil_mode = ceil_mode\n\n          # Convert to tuples if they are ints\n          if isinstance(self.kernel_size, int):\n              self.kernel_size = (self.kernel_size, self.kernel_size, self.kernel_size)\n          if isinstance(self.stride, int):\n              self.stride = (self.stride, self.stride, self.stride)\n          if isinstance(self.padding, int):\n              self.padding = (self.padding, self.padding, self.padding)\n          if isinstance(self.dilation, int):\n              self.dilation = (self.dilation, self.dilation, self.dilation)\n\n      Then in forward, we use these tuples.\n\n      We are ready.\n\n      We'll write the Triton kernel and the model.\n\n      Note: The kernel must be jit-compiled. We use `@triton.jit`.\n\n      We are going to write the kernel without autotune because the kernel size is fixed per call? But we can try to use autotune? The problem says we can use adaptive tuning for block sizes? But in this kernel, we are not using any blocks. We are processing one output element per thread.\n\n      We are not using any shared memory, so we don't have a block size. So we don't need autotune.\n\n      We are going to write a simple kernel.\n\n      Let's write the code.\n\n      We hope for a speedup? The PyTorch implementation is highly optimized. But we are writing a naive version. We might not get a speedup. However, the problem says we are to rewrite for performance. We are taking a first step.\n\n      We are going to run and see.\n\n      If it doesn't perform well, we can optimize in the next steps.\n\n      We are going to write the entire block.\n\n      Note: We are replacing the reference code with a Triton kernel.\n\n      We must not change the test parameters (batch_size, etc.) and the functions get_inputs and get_init_inputs.\n\n      We are going to keep them the same.\n\n      Now, the code.\n\n      IMPORTANT: The problem says to output the entire block. We are going to write:\n\n          # ================== EVOLVE-BLOCK-START ==================\n          ... [our code] ...\n          # =================== EVOLVE-BLOCK-END ===================\n\n      We are going to write the entire block.\n\n      Let's code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _max_pool3d_kernel(\n    x_ptr,\n    output_ptr,\n    batch,\n    channels,\n    D, H, W,\n    out_d, out_h, out_w,\n    stride_d, stride_h, stride_w,\n    pad_d, pad_h, pad_w,\n    dilation_d, dilation_h, dilation_w,\n    kernel_d, kernel_h, kernel_w,\n    stride_n, stride_c, stride_d_in, stride_h_in, stride_w_in,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid1 = tl.program_id(0)\n    pid2 = tl.program_id(1)\n    \n    total_spatial = out_d * out_h * out_w\n    n = pid1 // channels\n    c = pid1 % channels\n    idx_spatial = pid2\n    pd = idx_spatial // (out_h * out_w)\n    ph = (idx_spatial % (out_h * out_w)) // out_w\n    pw = idx_spatial % out_w\n\n    start_d = pd * stride_d - pad_d\n    start_h = ph * stride_h - pad_h\n    start_w = pw * stride_w - pad_w\n\n    max_val = -tl.math.inf\n\n    for kd in range(0, kernel_d):\n        d_index = start_d + kd * dilation_d\n        if d_index < 0 or d_index >= D:\n            continue\n        for kh in range(0, kernel_h):\n            h_index = start_h + kh * dilation_h\n            if h_index < 0 or h_index >= H:\n                continue\n            for kw in range(0, kernel_w):\n                w_index = start_w + kw * dilation_w\n                if w_index < 0 or w_index >= W:\n                    continue\n\n                offset = (n * stride_n + c * stride_c + \n                          d_index * stride_d_in + \n                          h_index * stride_h_in + \n                          w_index * stride_w_in)\n                val = tl.load(x_ptr + offset)\n                if val > max_val:\n                    max_val = val\n\n    output_offset = pid1 * total_spatial + pid2\n    tl.store(output_ptr + output_offset, max_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, \n                 dilation: int = 1, return_indices: bool = False, ceil_mode: bool = False):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n        self.dilation = dilation\n        self.return_indices = return_indices\n        self.ceil_mode = ceil_mode\n\n        if isinstance(self.kernel_size, int):\n            self.kernel_size = (self.kernel_size,)*3\n        if isinstance(self.stride, int):\n            self.stride = (self.stride,)*3\n        if isinstance(self.padding, int):\n            self.padding = (self.padding,)*3\n        if isinstance(self.dilation, int):\n            self.dilation = (self.dilation,)*3\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.return_indices or self.ceil_mode:\n            return torch.nn.functional.max_pool3d(\n                x, self.kernel_size, self.stride, self.padding, \n                self.dilation, self.ceil_mode, self.return_indices\n            )\n\n        x_contiguous = x.contiguous()\n        batch, channels, D, H, W = x_contiguous.shape\n\n        def _output_size(dim, k, s, p, d):\n            return (dim + 2 * p - d * (k - 1) - 1) // s + 1\n\n        out_d = _output_size(D, self.kernel_size[0], self.stride[0], self.padding[0], self.dilation[0])\n        out_h = _output_size(H, self.kernel_size[1], self.stride[1], self.padding[1], self.dilation[1])\n        out_w = _output_size(W, self.kernel_size[2], self.stride[2], self.padding[2], self.dilation[2])\n\n        output = torch.empty((batch, channels, out_d, out_h, out_w), device=x.device, dtype=x.dtype)\n        total_batch_channels = batch * channels\n        total_spatial = out_d * out_h * out_w\n        grid = (total_batch_channels, total_spatial)\n\n        stride_n = x_contiguous.stride(0)\n        stride_c = x_contiguous.stride(1)\n        stride_d_in = x_contiguous.stride(2)\n        stride_h_in = x_contiguous.stride(3)\n        stride_w_in = x_contiguous.stride(4)\n\n        _max_pool3d_kernel[grid](\n            x_contiguous, output,\n            batch, channels, D, H, W,\n            out_d, out_h, out_w,\n            self.stride[0], self.stride[1], self.stride[2],\n            self.padding[0], self.padding[1], self.padding[2],\n            self.dilation[0], self.dilation[1], self.dilation[2],\n            self.kernel_size[0], self.kernel_size[1], self.kernel_size[2],\n            stride_n, stride_c, stride_d_in, stride_h_in, stride_w_in,\n            BLOCK_SIZE=32\n        )\n        return output\n\nbatch_size = 16\nchannels = 32\ndim1 = 64\ndim2 = 64\ndim3 = 64\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, dim1, dim2, dim3)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
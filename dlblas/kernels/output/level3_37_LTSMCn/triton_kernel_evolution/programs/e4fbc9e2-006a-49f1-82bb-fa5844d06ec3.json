{"id": "e4fbc9e2-006a-49f1-82bb-fa5844d06ec3", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_lstm_cell(\n    # Input tensors\n    x_ptr, h_ptr, c_ptr,\n    # Weight matrices and biases\n    w_ih_ptr, w_hh_ptr, b_ih_ptr, b_hh_ptr,\n    # Output tensors\n    next_h_ptr, next_c_ptr,\n    # Dimensions\n    input_size, hidden_size, batch_size,\n    # Tensor strides\n    x_batch_stride, x_in_stride,\n    h_batch_stride, h_hid_stride,\n    c_batch_stride, c_hid_stride,\n    w_ih_hid_stride, w_ih_in_stride,\n    w_hh_hid_stride, w_hh_in_stride,\n    # Block configurations\n    BLOCK_IN: tl.constexpr, \n    BLOCK_HID: tl.constexpr, \n    BLOCK_BATCH: tl.constexpr,\n):\n    # Parallelize over batch and hidden dimensions\n    pid_m = tl.program_id(0)\n    pid_b = tl.program_id(1)\n    \n    # Create masks for safe memory access\n    hid_mask = pid_m * BLOCK_HID + tl.arange(0, BLOCK_HID) < (4 * hidden_size)\n    batch_mask = pid_b * BLOCK_BATCH + tl.arange(0, BLOCK_BATCH) < batch_size\n    full_mask = hid_mask[:, None] & batch_mask[None, :]\n    \n    # Compute offsets for batch and hidden dimensions\n    hid_offsets = pid_m * BLOCK_HID + tl.arange(0, BLOCK_HID)\n    batch_offsets = pid_b * BLOCK_BATCH + tl.arange(0, BLOCK_BATCH)\n    \n    # Initialize accumulator for input and hidden projections\n    gates_ih = tl.zeros((BLOCK_HID, BLOCK_BATCH), dtype=tl.float32)\n    gates_hh = tl.zeros((BLOCK_HID, BLOCK_BATCH), dtype=tl.float32)\n    \n    # Compute input projection using tensor cores\n    for block_in in range(0, input_size, BLOCK_IN):\n        in_offsets = block_in + tl.arange(0, BLOCK_IN)\n        in_mask = in_offsets < input_size\n        \n        # Load input block\n        x_vals = tl.load(\n            x_ptr + batch_offsets[None, :] * x_batch_stride + in_offsets[:, None] * x_in_stride,\n            mask=in_mask[:, None] & batch_mask[None, :],\n            other=0.0\n        )\n        \n        # Load weights block\n        w_ih_vals = tl.load(\n            w_ih_ptr + hid_offsets[:, None] * w_ih_hid_stride + in_offsets[None, :] * w_ih_in_stride,\n            mask=hid_mask[:, None] & in_mask[None, :],\n            other=0.0\n        )\n        \n        # Accumulate matrix product\n        gates_ih += tl.dot(w_ih_vals, x_vals, allow_tf32=True)\n    \n    # Compute hidden projection using tensor cores\n    for block_hid in range(0, hidden_size, BLOCK_IN):\n        h_offsets = block_hid + tl.arange(0, BLOCK_IN)\n        h_mask = h_offsets < hidden_size\n        \n        # Load hidden state block\n        h_vals = tl.load(\n            h_ptr + batch_offsets[None, :] * h_batch_stride + h_offsets[:, None] * h_hid_stride,\n            mask=h_mask[:, None] & batch_mask[None, :],\n            other=0.0\n        )\n        \n        # Load weights block\n        w_hh_vals = tl.load(\n            w_hh_ptr + hid_offsets[:, None] * w_hh_hid_stride + h_offsets[None, :] * w_hh_in_stride,\n            mask=hid_mask[:, None] & h_mask[None, :],\n            other=0.0\n        )\n        \n        # Accumulate matrix product\n        gates_hh += tl.dot(w_hh_vals, h_vals, allow_tf32=True)\n    \n    # Load and add biases\n    b_ih = tl.load(b_ih_ptr + hid_offsets, mask=hid_mask, other=0.0)\n    b_hh = tl.load(b_hh_ptr + hid_offsets, mask=hid_mask, other=0.0)\n    gates = gates_ih + gates_hh + b_ih[:, None] + b_hh[:, None]\n    \n    # Split into gates (i, f, g, o)\n    chunk_size = BLOCK_HID // 4\n    i_gate = gates[0:chunk_size, :]\n    f_gate = gates[chunk_size:2*chunk_size, :]\n    g_gate = gates[2*chunk_size:3*chunk_size, :]\n    o_gate = gates[3*chunk_size:4*chunk_size, :]\n    \n    # Compute base hidden offsets for cell state\n    base_hid_offsets = pid_m * chunk_size + tl.arange(0, chunk_size)\n    base_hid_mask = base_hid_offsets < hidden_size\n    \n    # Load current cell state\n    c_vals = tl.load(\n        c_ptr + batch_offsets[None, :] * c_batch_stride + base_hid_offsets[:, None] * c_hid_stride,\n        mask=base_hid_mask[:, None] & batch_mask[None, :],\n        other=0.0\n    )\n    \n    # Compute activations\n    i = tl.sigmoid(i_gate)\n    f = tl.sigmoid(f_gate)\n    g = tl.tanh(g_gate)\n    o = tl.sigmoid(o_gate)\n    \n    # Compute next cell state\n    next_c = f * c_vals + i * g\n    \n    # Compute next hidden state\n    next_h = o * tl.tanh(next_c)\n    \n    # Store results\n    tl.store(\n        next_c_ptr + batch_offsets[None, :] * c_batch_stride + base_hid_offsets[:, None] * c_hid_stride,\n        next_c,\n        mask=base_hid_mask[:, None] & batch_mask[None, :]\n    )\n    tl.store(\n        next_h_ptr + batch_offsets[None, :] * h_batch_stride + base_hid_offsets[:, None] * h_hid_stride,\n        next_h,\n        mask=base_hid_mask[:, None] & batch_mask[None, :]\n    )\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        super(ModelNew, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        \n        # Persistent buffers for hidden states\n        self.register_buffer('h0', torch.randn(num_layers, 1, hidden_size))\n        self.register_buffer('c0', torch.randn(num_layers, 1, hidden_size))\n        \n        # Weight and bias parameters\n        self.weight_ih = nn.ParameterList()\n        self.weight_hh = nn.ParameterList()\n        self.bias_ih = nn.ParameterList()\n        self.bias_hh = nn.ParameterList()\n        \n        for i in range(num_layers):\n            layer_input_size = input_size if i == 0 else hidden_size\n            self.weight_ih.append(nn.Parameter(torch.randn(4 * hidden_size, layer_input_size)))\n            self.weight_hh.append(nn.Parameter(torch.randn(4 * hidden_size, hidden_size)))\n            self.bias_ih.append(nn.Parameter(torch.zeros(4 * hidden_size)))\n            self.bias_hh.append(nn.Parameter(torch.zeros(4 * hidden_size)))\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        seq_len = x.size(1)\n        \n        # Initialize hidden and cell states\n        h = self.h0.expand(-1, batch_size, -1).contiguous()\n        c = self.c0.expand(-1, batch_size, -1).contiguous()\n        \n        # Process through each layer\n        for layer in range(self.num_layers):\n            layer_h = h[layer]\n            layer_c = c[layer]\n            weight_ih = self.weight_ih[layer]\n            weight_hh = self.weight_hh[layer]\n            bias_ih = self.bias_ih[layer]\n            bias_hh = self.bias_hh[layer]\n            \n            # Process each timestep\n            for t in range(seq_len):\n                x_t = x[:, t, :] if layer == 0 else layer_h\n                \n                # Configure grid\n                grid = (\n                    triton.cdiv(4 * self.hidden_size, 32),\n                    triton.cdiv(batch_size, 32),\n                )\n                \n                # Prepare outputs\n                next_h = torch.empty_like(layer_h)\n                next_c = torch.empty_like(layer_c)\n                \n                # Execute fused LSTM kernel\n                fused_lstm_cell[grid](\n                    x_t, layer_h, layer_c,\n                    weight_ih, weight_hh, bias_ih, bias_hh,\n                    next_h, next_c,\n                    x_t.size(1), self.hidden_size, batch_size,\n                    x_t.stride(0), x_t.stride(1),\n                    layer_h.stride(0), layer_h.stride(1),\n                    layer_c.stride(0), layer_c.stride(1),\n                    weight_ih.stride(0), weight_ih.stride(1),\n                    weight_hh.stride(0), weight_hh.stride(1),\n                    BLOCK_IN=32, BLOCK_HID=32, BLOCK_BATCH=32\n                )\n                \n                layer_h, layer_c = next_h, next_c\n            \n            # Update layer states\n            h[layer] = layer_h\n            c[layer] = layer_c\n        \n        return c\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, sequence_length, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "4f9beb42-fa1a-46d9-9c42-537e8c230516", "generation": 4, "timestamp": 1754595368.66674, "iteration_found": 23, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_37_LTSMCn", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpbnyml02a.py\", line 192, in forward\n    fused_lstm_cell[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 25.4, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_37_LTSMCn", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "max_difference": ["1.106112", "1.175641", "1.141564", "1.161014", "1.142271"], "avg_difference": ["0.086596", "0.085834", "0.085436", "0.084983", "0.086138"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 25.4, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
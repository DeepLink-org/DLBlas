{"id": "55686dab-40d2-4268-8d1e-1aba8a2ebf60", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef min_reduction_kernel(\n    x_ptr,\n    output_ptr,\n    stride_x0, stride_x1, stride_x2,\n    stride_out0, stride_out1,\n    reduction_dim_size,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    base_ptr = x_ptr + pid0 * stride_x0 + pid1 * stride_x1\n    offsets = tl.arange(0, BLOCK_SIZE)\n    ptrs = base_ptr + offsets * stride_x2\n    mask = offsets < reduction_dim_size\n    data = tl.load(ptrs, mask=mask, other=float('inf'))\n    min_val = tl.min(data, axis=0)\n    tl.store(output_ptr + pid0 * stride_out0 + pid1 * stride_out1, min_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.contiguous()\n        if self.dim != 2:\n            perm = [i for i in range(3) if i != self.dim] + [self.dim]\n            x = x.permute(perm).contiguous()\n        \n        reduction_size = x.shape[2]\n        output_shape = x.shape[:2]\n        output = torch.empty(output_shape, device=x.device, dtype=x.dtype)\n        \n        stride0 = x.stride(0)\n        stride1 = x.stride(1)\n        out_stride0 = output.stride(0)\n        out_stride1 = output.stride(1)\n        \n        grid = (output_shape[0], output_shape[1])\n        \n        # Dynamically determine optimal block size and warps\n        block_size = 1\n        while block_size < reduction_size and block_size < 1024:\n            block_size *= 2\n        if block_size < 32:\n            block_size = 32\n        num_warps = block_size // 32\n        \n        min_reduction_kernel[grid, num_warps=num_warps](\n            x, output, \n            stride0, stride1, 1,\n            out_stride0, out_stride1,\n            reduction_size,\n            BLOCK_SIZE=block_size\n        )\n        return output\n\nbatch_size = 16\ndim1 = 256\ndim2 = 256\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "68d5ad3f-989a-4b3c-a4f4-69d1c22e0a93", "generation": 2, "timestamp": 1754639587.6612859, "iteration_found": 8, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "", "exec_log": "[Child] \u7ed1\u5b9a GPU 5, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 57\n    min_reduction_kernel[grid, num_warps=num_warps](\n                               ^^^^^^^^^^^^^^^^^^^\nSyntaxError: invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.42, "runtime_ms": 0.0571, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.024, "speed_up": 0.42, "custom_timing": 0.0571}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
{"id": "7206e3db-cc35-4ed6-b667-0bd763bbc19c", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)  # Batch index\n    j = tl.arange(0, BLOCK_SIZE)  # Output feature indices\n    mask = j < out_features\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Loop over input features\n    for i in range(0, in_features):\n        # Load input element (scalar)\n        x_val = tl.load(x_ptr + pid * in_features + i)\n        # Load weight elements (vector)\n        w_vals = tl.load(w_ptr + j * in_features + i, mask=mask, other=0)\n        acc += x_val * w_vals\n\n    # Load bias and add to accumulator\n    b_vals = tl.load(b_ptr + j, mask=mask, other=0)\n    acc += b_vals\n\n    # Apply first Mish activation\n    softplus1 = tl.log(1 + tl.exp(acc))\n    tanh1 = tl.tanh(softplus1)\n    mish1 = acc * tanh1\n\n    # Apply second Mish activation\n    softplus2 = tl.log(1 + tl.exp(mish1))\n    tanh2 = tl.tanh(softplus2)\n    mish2 = mish1 * tanh2\n\n    # Store final result\n    output_ptr_base = output_ptr + pid * out_features\n    tl.store(output_ptr_base + j, mish2, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        # Ensure contiguous memory access\n        x = x.contiguous()\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        batch_size = x.shape[0]\n        output = torch.empty((batch_size, self.linear.out_features), \n                             device=x.device, dtype=x.dtype)\n        \n        # Configure kernel launch\n        grid = (batch_size,)\n        BLOCK_SIZE = triton.next_power_of_2(self.linear.out_features)\n        \n        # Launch fused kernel\n        _forward_kernel[grid](\n            x, weight, bias, output,\n            self.linear.in_features, \n            self.linear.out_features,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\nin_features = 10\nout_features = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "51a7ac27-9fae-499e-b942-bf80f2820940", "generation": 1, "timestamp": 1754565940.8286078, "iteration_found": 1, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_29_Matmul_Mish_Mish", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpnji71b2k.py\", line 70, in forward\n    _forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 0.0501, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0565, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0501, "speed_up": 0.887, "custom_timing": 0.0565}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0565\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0501, 'speed_up': 0.887, 'custom_timing': 0.0565}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0565, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0501, 'speed_up': 0.887, 'custom_timing': 0.0565}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies Mish, and applies Mish again.\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.nn.functional.mish(x)\n        x = torch.nn.functional.mish(x)\n        return x\n\nbatch_size = 128\nin_features = 10\nout_features = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0565), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0501, 'speed_up': 0.887, 'custom_timing': 0.0565})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies Mish, and applies Mish again.\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.nn.functional.mish(x)\n        x = torch.nn.functional.mish(x)\n        return x\n\nbatch_size = 128\nin_features = 10\nout_features = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a simple model that does:\n  1. Linear layer (matrix multiplication + bias)\n  2. Mish activation\n  3. Mish activation again\n\nThe current implementation uses PyTorch's built-in functions. We are to rewrite it in Triton to gain speed.\n\nHowever, note that the current program is a reference PyTorch code. We are allowed to rewrite the entire EVOLVE-BLOCK.\n\nBut note the constraints:\n  - We cannot change the function signatures (like the forward method) or the class name (ModelNew).\n  - We must maintain the same inputs and outputs.\n\nThe problem: The current implementation (using PyTorch) is slow. We want to speed up the linear layer and the Mish activations.\n\nPlan:\n  1. We will write a Triton kernel for the linear layer (matrix multiplication with bias).\n  2. We will write a Triton kernel for the Mish activation (which is: x * tanh(softplus(x))).\n  3. We will replace the current forward with a sequence that uses our Triton kernels.\n\nHowever, note that the model does:\n  x = linear(x)   -> [matrix multiplication: (batch, in_features) * (in_features, out_features) + bias]\n  then two Mish activations.\n\nWe can break the forward into:\n  - Step 1: Linear transformation (with bias)\n  - Step 2: Mish activation (applied twice)\n\nWe can write:\n  - A Triton kernel for the linear layer (with bias) for the case of (batch, in_features) * (in_features, out_features).\n  - A Triton kernel for the Mish activation that is applied element-wise.\n\nBut note: The linear layer in Triton requires a 2D matrix multiplication. We can write a kernel that does:\n  output = input @ weight.t() + bias\n\nWe are going to write two kernels:\n  1. `linear_kernel`: For the linear transformation.\n  2. `mish_kernel`: For the Mish activation.\n\nWe note that the input is of shape (batch_size, in_features) and weight is (out_features, in_features).\n\nWe can design the linear kernel as follows:\n  - Each program will compute one row of the output (i.e., one batch element and all out_features? but note: batch is the first dimension). Actually, we want to parallelize over batch and over features? \n  - Alternatively, we can have a 2D grid: \n        pid_b = program_id(0) over batch\n        pid_f = program_id(1) over out_features\n  But that might be too many blocks.\n\n  Instead, we can do:\n      pid = program_id(0)  # we can have batch_size * out_features blocks? but that might be too many for large batch and large out_features.\n\n  Alternatively, we can do:\n      Let the grid be (batch_size,)\n      Each block computes one row of the output (so one batch element) and iterates over the out_features in chunks.\n\n  However, note that the out_features might be large. We can have two-dimensional grid: (batch_size, num_blocks_required_for_out_features). But that complicates.\n\n  Another common approach: use a 1D grid of size batch_size, and then each block computes a contiguous segment of the output row (i.e., a block of out_features). We then use a loop over the in_features to compute the dot product.\n\n  Steps for linear kernel (without bias for now, then add bias at the end):\n\n  Let:\n      input: [batch, in_features]\n      weight: [out_features, in_features]   (so we need to transpose to [in_features, out_features] for efficient access? Actually, we want to load a row of input and a row of weight? But note: we are computing: for each batch element and each output feature, we do a dot product over in_features.\n\n  We can have:\n      pid = program_id(0)   # we are going to have batch_size * (ceil(out_features / BLOCK_SIZE)) blocks? Actually, we can have a 2D grid: (batch_size, ceil(out_features / BLOCK_SIZE))\n\n  However, the problem size is small: batch_size=128, in_features=10, out_features=20. So we can have a 1D grid of size batch_size, and each block computes the entire row (all out_features) for one batch element.\n\n  But note: 20 is small. We can have a block of 20. But 20 is less than 1024 (max block size) so it's fine.\n\n  Proposed kernel for linear:\n\n      grid = (batch_size,)\n      BLOCK_SIZE = out_features   (but we can also set it to a power of two, say 32, and then mask the extra)\n\n  Steps in kernel:\n      pid = program_id(0)   # batch index\n      We load the entire row of input (pid-th row) into a register (or shared memory? but small: 10 elements). \n      Then we loop over the output features? Actually, we want to compute for each output feature j: \n          out[pid, j] = bias[j] + sum_{i} input[pid, i] * weight[j, i]\n\n      We can do:\n          for j in range(0, out_features):\n              acc = 0\n              for i in range(0, in_features):\n                  acc += input[pid, i] * weight[j, i]\n              out[pid, j] = acc + bias[j]\n\n      But that would be inefficient in Triton because we are not using the parallel capabilities.\n\n  Alternatively, we can vectorize over the output features? We can have each thread in the block compute one output feature. We set BLOCK_SIZE = out_features, and if out_features is 20, then we have 20 threads per block.\n\n  Steps:\n      pid = program_id(0)   # batch index\n      feature_index = tl.arange(0, BLOCK_SIZE)   # [0,1,...,19] for out_features=20\n      mask = feature_index < out_features   # always true in this case, but for generality\n\n      We need to load the input row: we can load the entire row (if in_features is small) into a register? But we can also load it in a loop over in_features.\n\n      We can do:\n          acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n          for i in range(0, in_features):\n              # Load the input element at (pid, i)\n              x_val = tl.load(x_ptr + pid * in_features + i)   # if x is contiguous\n              # Load the weight vector for all output features at weight row i (but note: weight is [out_features, in_features] -> stored row-major: so row j has the weights for output j)\n              # Actually, we want the entire column i of weight? But stored as row-major, then column i is at [0:out_features, i] -> so we can load a contiguous block of out_features at weight_ptr + i * out_features? \n              # But wait: the weight matrix is stored as (out_features, in_features). The element at (j,i) is at weight_ptr + j * in_features + i? \n              # Actually, no: because row-major: the (j,i) element is at j * in_features + i.\n\n              # So to load the entire column i for all j in [0, out_features-1]: we need to load from weight_ptr + i + j*in_features? but that is non-contiguous.\n\n          This would be inefficient.\n\n  We can change the weight layout? But we cannot change the input because the PyTorch linear layer uses (out_features, in_features). We are going to precompute the weight matrix in the transposed form? Actually, in the kernel we can precompute the weight matrix in a transposed and then use it? But note: we are going to pass the weight as a tensor. We can pre-transpose the weight and store it in the model? That would be acceptable.\n\n  Alternatively, we can reorganize the kernel to have two-dimensional blocks? We have:\n      batch index and feature index.\n\n  Given the small sizes, we can do:\n\n      grid = (batch_size, out_features)   -> 128 * 20 = 2560 blocks, which is acceptable.\n\n      Then each block has one thread? Actually, we can set the block to be 1x1? But that is inefficient.\n\n      Alternatively, we can have 1D blocks and use a grid of (batch_size * out_features) and then each thread computes one element. Then we loop over in_features? That would be:\n\n          pid = program_id(0)   # from 0 to batch_size*out_features-1\n          batch_idx = pid // out_features\n          feature_idx = pid % out_features\n\n          acc = bias[feature_idx]\n          for i in range(0, in_features):\n              x_val = tl.load(x_ptr + batch_idx * in_features + i)\n              w_val = tl.load(w_ptr + feature_idx * in_features + i)\n              acc += x_val * w_val\n          tl.store(output_ptr + pid, acc)\n\n      But note: this is a lot of global memory loads: each thread does in_features (10) loads from x and 10 loads from w. And the same x element is loaded by out_features (20) threads in the same row -> so we are loading the same x element 20 times.\n\n  We can use shared memory? We can load the entire input row for one batch element into shared memory, then all the threads in the same row (same batch element) can reuse it. But note: we have one thread per output feature per batch. We can have a block of threads that are for the same batch element? Then we can have:\n\n      grid = (batch_size,)\n      block = (out_features,)   -> 20, which is acceptable.\n\n      Steps:\n          pid = program_id(0)   # batch index\n          feature_idx = tl.arange(0, out_features)   # [0,1,...,19] for the block\n\n          # Load the entire input row for this batch into shared memory? Actually, we can use registers? But we can also use shared memory for the input row and the weight columns? But note: we have 10 elements for the input row. We can load the input row once.\n\n          # All threads in the block load the same input row? Actually, we can have each thread load one element? But we have 20 threads and 10 elements -> we can assign two elements per thread? Or we can have the first 10 threads load one element.\n\n          # Alternatively, we can have the entire block load the input row in a coalesced way? We can have:\n          input_row = tl.zeros((in_features,), dtype=tl.float32)\n          # We can load the input row by having each thread load multiple elements? Actually, we can have the first min(in_features, num_threads) threads load one element.\n\n          # Since in_features=10 and we have 20 threads, we can assign the first 10 threads to load one element each.\n\n          # Let's do:\n          input_shared = tl.zeros((in_features,), dtype=tl.float32)\n          idx = tl.arange(0, in_features)\n          mask = idx < in_features\n          input_vals = tl.load(x_ptr + pid * in_features + idx, mask=mask, other=0)\n          # But note: all threads are loading the same input_vals? Actually, we want to broadcast? Or we can have thread 0 to 9 load the elements? We can use:\n\n          # Actually, we can use one thread to load the entire row? But that would be inefficient. Alternatively, we can use the entire block to load the row: we can have each thread load one element if in_features <= num_threads? We have 20 threads and 10 elements -> so we can assign two elements per thread? Not necessary.\n\n          # We can use:\n          #   input_shared = tl.zeros((in_features,), dtype=tl.float32)\n          #   for i in range(0, in_features):\n          #       if tl.program_id(1) == 0:   # but we don't have program_id(1) in 1D grid? We are using 1D grid and 1D block.\n\n          # Actually, we are using a 1D grid and 1D block of size out_features (20). We can load the input row by having the first min(in_features, out_features) threads load one element.\n\n          # Let:\n          tid = tl.arange(0, BLOCK_SIZE)   # BLOCK_SIZE=out_features? but we set BLOCK_SIZE to out_features? Actually, we set BLOCK_SIZE to the number of threads per block (which is out_features, 20).\n\n          # We can do:\n          input_shared = tl.zeros((in_features,), dtype=tl.float32)\n          # We want to load the input row. We can use:\n          #   for offset in range(0, in_features, BLOCK_SIZE):\n          #       idx = offset + tid\n          #       mask = idx < in_features\n          #       if mask:\n          #           input_shared[idx] = tl.load(x_ptr + pid * in_features + idx, mask=mask)\n\n          # But note: we cannot have shared memory in Triton? Actually, we can use `tl.static_print` and `tl.static_range`? We can use the fact that in_features is small and known at compile time? Actually, we can set it as a constant.\n\n          # Alternatively, we can avoid shared memory and just have each thread load the entire input row? That would be 10 loads per thread -> 10*20=200 loads for the same row? That's bad.\n\n      Given the small sizes, we can do:\n\n          Let the kernel be for a fixed in_features (10) and out_features (20). We can unroll the loop.\n\n          We can precompute the weight matrix as a contiguous array? But we are passing the weight as a tensor.\n\n      Actually, we are going to write a kernel that works for any in_features and out_features, but we assume they are small.\n\n      We can do:\n\n          acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)   # BLOCK_SIZE = out_features? but we are using the block to cover out_features? Actually, we set the block to the out_features, but if out_features is 20 and we set BLOCK_SIZE=32, then we have 32 threads, and we mask the extra 12.\n\n          # We are going to loop over the in_features in chunks? But 10 is small, we can do one chunk.\n\n          for i in range(0, in_features):\n              # Each thread j (for j in [0, BLOCK_SIZE-1]) will load:\n              #   x_val = input[pid, i]   -> same for all threads in the block? so we can broadcast?\n              #   w_val = weight[j, i]   -> each thread loads one w_val.\n\n              # How to load x_val? We can have the entire block load the same x_val? We can have thread 0 load it and then broadcast? But Triton doesn't have shared memory? We can use:\n\n              #   x_ptr_i = x_ptr + pid * in_features + i\n              #   x_val = tl.load(x_ptr_i)   # but note: if we do this, every thread loads the same address -> that's one broadcast load? which is efficient.\n\n              # Then each thread j loads w_val from w_ptr + j * in_features + i.\n\n              # Then do: acc_j = acc_j + x_val * w_val\n\n          # Then add bias: acc_j += bias[j]\n\n      So the kernel:\n\n          pid = program_id(0)   # batch index\n          j = tl.arange(0, BLOCK_SIZE)   # thread index in the block, which corresponds to output feature index\n          mask = j < out_features\n\n          acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n          for i in range(0, in_features):\n              # Load the input element for this batch and feature i\n              x_val = tl.load(x_ptr + pid * in_features + i)   # scalar\n              # Load the weight vector for this output feature j and input feature i: for each j, we load w[j, i]\n              w_ptr_i = w_ptr + i   # base for column i? but note: weight is stored as (out_features, in_features) -> row-major: element (j,i) is at j * in_features + i.\n              w_vals = tl.load(w_ptr_i + j * in_features, mask=mask, other=0)   # vector of length BLOCK_SIZE? but note: we are loading for each j: w_ptr + j * in_features + i -> same as w_ptr_i + j * in_features.\n\n              acc += x_val * w_vals\n\n          # Load bias: for each j\n          b_vals = tl.load(b_ptr + j, mask=mask, other=0)\n          acc += b_vals\n\n          # Write the output: at (pid, j)\n          output_ptr_j = output_ptr + pid * out_features + j\n          tl.store(output_ptr_j, acc, mask=mask)\n\n      We set BLOCK_SIZE to the next power of two above out_features? But we know out_features=20 -> so 32.\n\n      We can set BLOCK_SIZE = 32.\n\n      Grid: (batch_size,)\n\n  This kernel should work.\n\n  For the Mish activation, we write an element-wise kernel:\n\n      grid = (batch_size * out_features,)\n      We can set BLOCK_SIZE = 1024? But note: total elements = batch_size * out_features = 128*20=2560, so we can set BLOCK_SIZE=2560? But that is too big (max 1024). So we can have multiple blocks.\n\n      Alternatively, we can use a 1D grid and 1D block, and each block processes a chunk.\n\n      Steps:\n\n          pid = program_id(0)\n          idx = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n          mask = idx < n_elements   # n_elements = batch_size * out_features\n\n          x = tl.load(x_ptr + idx, mask=mask)\n          # Mish: x * tanh(softplus(x))\n          softplus = tl.log(1 + tl.exp(x))\n          tanh_softplus = tl.tanh(softplus)\n          mish = x * tanh_softplus\n\n          tl.store(output_ptr + idx, mish, mask=mask)\n\n      We can set BLOCK_SIZE=1024? Then we need 3 blocks for 2560 elements.\n\n  But note: we are applying Mish twice. We can write a kernel that does two Mish in one? Or we can call the same kernel twice.\n\n  We will write one kernel for Mish and then call it twice.\n\nHowever, note: the model structure is:\n\n  x = linear(x)\n  x = mish(x)\n  x = mish(x)\n\nWe can fuse the two Mish? Actually, we can write a kernel that does Mish twice? But that would be:\n\n      mish1 = x * tanh(softplus(x))\n      mish2 = mish1 * tanh(softplus(mish1))\n\nBut we don't know if that is numerically stable? And it's two separate operations.\n\nAlternatively, we can just call the same kernel twice.\n\nBut note: we are going to have two global memory passes for the two Mish. We can try to fuse the two Mish? But the problem is that the output of the first Mish is the input to the second Mish. We can do:\n\n      Write a kernel that does two Mish in a row? But that would require storing the intermediate? We cannot avoid the intermediate because the second Mish depends on the entire first Mish.\n\n  Actually, we can do:\n\n      kernel_mish_twice:\n          ... \n          x = load\n          temp = x * tanh(softplus(x))\n          y = temp * tanh(softplus(temp))\n          store y\n\n  This would save one global memory round trip.\n\n  So we write two kernels:\n      kernel_linear\n      kernel_mish_twice   (or we write kernel_mish and call it twice? but then we have two kernels and two launches)\n\n  We want to minimize kernel launch overhead? For small tensors, the overhead might be significant. So fusing the two Mish might be beneficial.\n\n  Alternatively, we can write a kernel that does the linear and then the two Mish? That would be even better? But note: the linear kernel writes to global memory and then the Mish kernel reads it. If we fuse, we avoid that.\n\n  However, the linear kernel uses a block per batch element (with 32 threads) and the Mish kernel is element-wise. They are different. Fusing them would be complex.\n\n  Given the small size, we can do:\n\n      Option 1: \n          linear_kernel (grid=(batch_size,), block=(32,)) -> output tensor A\n          mish_kernel (grid=(ceil(2560/1024),), block=(1024,)) applied to A -> output tensor B\n          mish_kernel (grid=(ceil(2560/1024),), block=(1024,)) applied to B -> output tensor C\n\n      Option 2: \n          Write a kernel that does linear and then the two Mish for one element? But note the linear kernel is per row and the Mish is per element. We can do:\n\n          We can extend the linear kernel to do the two Mish after computing the linear? But then we would do the Mish in the same block? But the linear kernel block has 32 threads and we have 20 elements per row? Then we can do:\n\n          After computing the linear result for the row (a vector of 20 elements in the block), we can do the Mish on that vector? But note: the Mish is element-wise. So we can do:\n\n              for j in range(0, out_features):\n                  x = acc[j]   # which is in register\n                  temp = x * tanh(softplus(x))\n                  temp2 = temp * tanh(softplus(temp))\n                  acc[j] = temp2\n\n          Then store.\n\n      This would avoid global memory for the intermediate.\n\n      So we can do:\n\n          In the linear kernel, after we have computed the linear result (acc) for the row, we do:\n\n              # Convert acc to float32? It already is.\n              # Apply two Mish:\n              #   mish1 = x * tanh(softplus(x))\n              #   mish2 = mish1 * tanh(softplus(mish1))\n\n          How to compute softplus? We can use: softplus(x) = log(1+exp(x))\n\n          We can use:\n              softplus_x = tl.log(1 + tl.exp(x))\n              tanh_sp = tl.tanh(softplus_x)\n              mish1 = x * tanh_sp\n\n              softplus_mish1 = tl.log(1 + tl.exp(mish1))\n              tanh_sp2 = tl.tanh(softplus_mish1)\n              mish2 = mish1 * tanh_sp2\n\n          Then store mish2.\n\n      This would be efficient.\n\n  Therefore, we can write one kernel that does:\n      - Linear transformation for one batch row (with bias)\n      - Then applies Mish twice (element-wise to the entire row)\n\n  And then we call this kernel for each batch row.\n\n  Steps for the combined kernel:\n\n      pid = program_id(0)   # batch index\n      j = tl.arange(0, BLOCK_SIZE)   # for output features, BLOCK_SIZE = 32 (next power of two above max out_features? but we know it's 20)\n      mask = j < out_features\n\n      # Load the input row for this batch\n      #   x_ptr: pointer to input tensor of shape [batch, in_features]\n      #   w_ptr: pointer to weight tensor of shape [out_features, in_features]\n      #   b_ptr: pointer to bias tensor of shape [out_features]\n\n      acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n      for i in range(0, in_features):\n          x_val = tl.load(x_ptr + pid * in_features + i)   # scalar\n          w_vals = tl.load(w_ptr + j * in_features + i, mask=mask, other=0)   # vector of BLOCK_SIZE\n          acc += x_val * w_vals\n\n      # Add bias\n      b_vals = tl.load(b_ptr + j, mask=mask, other=0)\n      acc += b_vals\n\n      # Now apply two Mish activations to each element in acc (which is the linear output for the row)\n      # We do element-wise for each j in the block.\n\n      # First Mish:\n      #   For each element in acc (we have a vector of BLOCK_SIZE), we compute:\n      x = acc   # vector\n      #   Note: we are using the same variable name, but we can do:\n      #   softplus_x = tl.log(1 + tl.exp(x))\n      #   But note: tl.exp might be expensive? But BLOCK_SIZE is 32, so it's fine.\n\n      #   However, we must be cautious of numerical stability? We can use the stable softplus: \n      #       softplus(x) = log(1+exp(x)) but if x is large, exp(x) might overflow.\n      #   Triton's tl.exp might handle it? But we can use:\n      #       softplus_x = tl.where(x > 20, x, tl.log(1+tl.exp(x)))\n      #   However, we are in a kernel and we care about performance. For now, we use the direct formula.\n\n      #   We do:\n      softplus_x = tl.log(1 + tl.exp(x))\n      tanh_sp = tl.tanh(softplus_x)\n      mish1 = x * tanh_sp\n\n      # Second Mish:\n      softplus_mish1 = tl.log(1 + tl.exp(mish1))\n      tanh_sp2 = tl.tanh(softplus_mish1)\n      mish2 = mish1 * tanh_sp2\n\n      # Now store mish2 to output\n      output_ptr_base = output_ptr + pid * out_features\n      tl.store(output_ptr_base + j, mish2, mask=mask)\n\n  We assume the output tensor is of shape [batch, out_features].\n\n  We must set the grid and block:\n      grid = (batch_size,)\n      block = (BLOCK_SIZE,)   # which is 32\n\n  We note: in_features and out_features are known at kernel launch? We can pass them as constants.\n\n  But note: the kernel must be generic? We can pass them as arguments.\n\n  We will write:\n\n      @triton.jit\n      def _forward_kernel(\n          x_ptr,\n          w_ptr,\n          b_ptr,\n          output_ptr,\n          in_features,\n          out_features,\n          BLOCK_SIZE: tl.constexpr,\n      ):\n          pid = tl.program_id(0)   # batch index\n          j = tl.arange(0, BLOCK_SIZE)\n          mask = j < out_features\n\n          # ... rest as above\n\n  Then in the model:\n\n      We will replace the forward with:\n\n          def forward(self, x):\n              # We are going to call the kernel\n              # x is [batch, in_features]\n              # self.linear.weight is [out_features, in_features]\n              # self.linear.bias is [out_features]\n\n              # We need to make sure the tensors are contiguous and on the same device.\n\n              batch_size = x.shape[0]\n              output = torch.empty((batch_size, self.linear.out_features), device=x.device, dtype=x.dtype)\n\n              # We set the grid and block\n              grid = (batch_size,)\n              BLOCK_SIZE = 32   # next power of two above out_features? but we know it's 20 -> 32 is enough.\n\n              _forward_kernel[grid](x, self.linear.weight, self.linear.bias, output, \n                                     in_features=self.linear.in_features, \n                                     out_features=self.linear.out_features,\n                                     BLOCK_SIZE=BLOCK_SIZE)\n\n              return output\n\n  But note: the kernel uses tl.exp and tl.log, which are available in triton.language.\n\n  However, we must note: the kernel uses float32? The model might be using float16? The original model uses the same dtype as input.\n\n  We can check: the input is from `get_inputs` -> torch.randn(batch_size, in_features) -> float32.\n\n  So we are safe.\n\n  But if we want to support float16, we might need to convert to float32 for the computation? Or we can use float16? The softplus and tanh in float16 might be unstable? But Triton supports float16 math.\n\n  However, the original model uses PyTorch's mish which is in float32? Actually, the reference code uses the same dtype as input. So we should do the same.\n\n  We can do:\n\n      We will write the kernel in the same dtype as the input? But note: Triton might not support all operations in float16? Actually, it does.\n\n  But let's stick to float32 for the kernel? We can load the input as float32, then do the computation in float32, then store as float32? But the input might be float16? We can convert.\n\n  Alternatively, we can make the kernel generic? We can use the dtype of the pointers.\n\n  We can do:\n\n      We assume the input, weight, bias, and output are of the same dtype? But the weight and bias in PyTorch linear are float32 by default? And the input is float32? So we are safe.\n\n  We note: the original model uses the same dtype throughout.\n\n  Let's code accordingly.\n\n  Steps for the ModelNew:\n\n      We will remove the two mish calls and the linear call and replace with the kernel.\n\n      We need to import triton and write the kernel.\n\n  But note: the EVOLVE-BLOCK must contain the entire class and the batch_size, in_features, out_features, and the get_inputs, get_init_inputs.\n\n  We are going to write:\n\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def _forward_kernel( ... ): ...\n\n      class ModelNew(nn.Module):\n          def __init__(self, in_features, out_features):\n              super(ModelNew, self).__init__()\n              self.linear = nn.Linear(in_features, out_features)\n\n          def forward(self, x):\n              ...   # call the kernel\n\n      batch_size = 128\n      in_features = 10\n      out_features = 20\n\n      def get_inputs(): ...\n\n      def get_init_inputs(): ...\n\n  We must note: the kernel might be compiled at the first run? So we should not worry about autotuning? We can autotune? But the problem size is fixed? We can set the BLOCK_SIZE to 32 and that's fixed.\n\n  We are not autotuning because the problem size is small and fixed.\n\n  Let's code accordingly.\n\n  However, note: the in_features and out_features are passed at runtime? They are 10 and 20? But the kernel must be generic? Actually, we can pass them as arguments.\n\n  We are going to write:\n\n      @triton.jit\n      def _forward_kernel(\n          x_ptr,\n          w_ptr,\n          b_ptr,\n          output_ptr,\n          in_features,\n          out_features,\n          BLOCK_SIZE: tl.constexpr,\n      ):\n          pid = tl.program_id(0)   # batch index\n          j = tl.arange(0, BLOCK_SIZE)\n          mask = j < out_features\n\n          # Initialize accumulator\n          acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n          # Loop over in_features\n          for i in range(0, in_features):\n              # Load x[pid, i]\n              x_val = tl.load(x_ptr + pid * in_features + i)\n              # Load a block of weights: for each j, w[j, i] = w_ptr + j * in_features + i\n              w_vals = tl.load(w_ptr + j * in_features + i, mask=mask, other=0)\n              acc += x_val * w_vals\n\n          # Add bias\n          b_vals = tl.load(b_ptr + j, mask=mask, other=0)\n          acc += b_vals\n\n          # Apply two Mish activations\n          # First Mish\n          softplus_x = tl.log(1 + tl.exp(acc))\n          tanh_sp = tl.tanh(softplus_x)\n          mish1 = acc * tanh_sp\n\n          # Second Mish\n          softplus_mish1 = tl.log(1 + tl.exp(mish1))\n          tanh_sp2 = tl.tanh(softplus_mish1)\n          mish2 = mish1 * tanh_sp2\n\n          # Store output\n          output_ptr_base = output_ptr + pid * out_features\n          tl.store(output_ptr_base + j, mish2, mask=mask)\n\n  Then in the forward:\n\n      def forward(self, x):\n          # Check contiguous\n          x = x.contiguous()\n          weight = self.linear.weight.contiguous()\n          bias = self.linear.bias.contiguous()\n          batch_size = x.shape[0]\n          output = torch.empty((batch_size, self.linear.out_features), device=x.device, dtype=x.dtype)\n\n          # We set the grid and block\n          grid = (batch_size,)\n          # We choose BLOCK_SIZE as the next power of two of out_features? but we know it's 20 -> 32.\n          BLOCK_SIZE = 32\n          # But we must pass the out_features and in_features as integers.\n          _forward_kernel[grid](x, weight, bias, output, self.linear.in_features, self.linear.out_features, BLOCK_SIZE)\n\n          return output\n\n  But note: the kernel uses tl.exp and tl.log, which might be in float32? And the input might be float32? So it's consistent.\n\n  However, we are doing the entire computation in float32? The original model uses float32? Yes, because the input is float32.\n\n  We are good.\n\n  Let's write the code.\n\n  But note: the kernel uses tl.zeros and then accumulates in float32? The input is float32? Yes.\n\n  We must note: the weight and bias are stored as float32 by default in nn.Linear.\n\n  We are safe.\n\n  We will now write the EVOLVE-BLOCK.\n\n  IMPORTANT: We are replacing the entire EVOLVE-BLOCK.\n\n  We must include the kernel and the class.\n\n  We note: the kernel is inside the EVOLVE-BLOCK? Yes.\n\n  We are going to write:\n\n      import math\n      import torch\n      import torch.nn as nn\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def _forward_kernel(...): ...   # as above\n\n      class ModelNew(nn.Module):\n          ...\n\n      batch_size = 128\n      in_features = 10\n      out_features = 20\n\n      def get_inputs(): ...\n\n      def get_init_inputs(): ...\n\n  But note: the get_inputs returns a tensor of shape (batch_size, in_features) -> which is (128,10) and dtype=float32.\n\n  We are good.\n\n  Let's code accordingly.\n\n  However, note: the kernel uses the absolute indices? We assume the tensors are contiguous. We made them contiguous in the forward.\n\n  We are done.\n\n  We hope this will be faster because:\n      - The linear layer is done in a fused kernel with the two Mish.\n      - We are avoiding global memory for the intermediate results.\n\n  But note: the original reference did:\n\n        x = self.linear(x)   # which is a highly optimized cuBLAS call? \n        x = torch.nn.functional.mish(x)   # which is optimized in PyTorch?\n        x = torch.nn.functional.mish(x)\n\n  Our kernel might be faster because we fuse the operations and avoid intermediate round trips to global memory.\n\n  Let's hope.\n\n  We output the entire EVOLVE-BLOCK.\n\n  Note: We are not allowed to change the class name (ModelNew) or the function names (get_inputs, get_init_inputs) or the batch_size, in_features, out_features.\n\n  We are going to write the code accordingly.\n\n  One more note: the kernel uses a for-loop over in_features (10 iterations). This is unrolled by Triton? We hope so.\n\n  We are ready.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)  # Batch index\n    j = tl.arange(0, BLOCK_SIZE)  # Output feature indices\n    mask = j < out_features\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Loop over input features\n    for i in range(0, in_features):\n        # Load input element (scalar)\n        x_val = tl.load(x_ptr + pid * in_features + i)\n        # Load weight elements (vector)\n        w_vals = tl.load(w_ptr + j * in_features + i, mask=mask, other=0)\n        acc += x_val * w_vals\n\n    # Load bias and add to accumulator\n    b_vals = tl.load(b_ptr + j, mask=mask, other=0)\n    acc += b_vals\n\n    # Apply first Mish activation\n    softplus1 = tl.log(1 + tl.exp(acc))\n    tanh1 = tl.tanh(softplus1)\n    mish1 = acc * tanh1\n\n    # Apply second Mish activation\n    softplus2 = tl.log(1 + tl.exp(mish1))\n    tanh2 = tl.tanh(softplus2)\n    mish2 = mish1 * tanh2\n\n    # Store final result\n    output_ptr_base = output_ptr + pid * out_features\n    tl.store(output_ptr_base + j, mish2, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        # Ensure contiguous memory access\n        x = x.contiguous()\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        batch_size = x.shape[0]\n        output = torch.empty((batch_size, self.linear.out_features), \n                             device=x.device, dtype=x.dtype)\n        \n        # Configure kernel launch\n        grid = (batch_size,)\n        BLOCK_SIZE = triton.next_power_of_2(self.linear.out_features)\n        \n        # Launch fused kernel\n        _forward_kernel[grid](\n            x, weight, bias, output,\n            self.linear.in_features, \n            self.linear.out_features,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\nin_features = 10\nout_features = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
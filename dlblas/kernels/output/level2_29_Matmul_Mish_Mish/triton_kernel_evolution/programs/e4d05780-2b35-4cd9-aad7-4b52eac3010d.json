{"id": "e4d05780-2b35-4cd9-aad7-4b52eac3010d", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_K': 32}, num_warps=1),\n        triton.Config({'BLOCK_K': 64}, num_warps=1),\n        triton.Config({'BLOCK_K': 128}, num_warps=2),\n        triton.Config({'BLOCK_K': 256}, num_warps=4),\n        triton.Config({'BLOCK_K': 512}, num_warps=8),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef fused_linear_mish_mish_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    stride_x,\n    stride_out,\n    BLOCK_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    row_id = pid // out_features\n    feature_id = pid % out_features\n    \n    # Precompute base pointers\n    x_base = x_ptr + row_id * stride_x\n    w_base = weight_ptr + feature_id * in_features\n    \n    # Initialize accumulator\n    acc = 0.0\n    \n    # Process input features in vectorized blocks\n    for block_start in range(0, in_features, BLOCK_K):\n        offsets = block_start + tl.arange(0, BLOCK_K)\n        mask = offsets < in_features\n        \n        # Vectorized loads\n        x_vec = tl.load(x_base + offsets, mask=mask, other=0.0)\n        w_vec = tl.load(w_base + offsets, mask=mask, other=0.0)\n        \n        # Accumulate partial dot product\n        acc += tl.sum(x_vec * w_vec)\n    \n    # Add bias\n    bias_val = tl.load(bias_ptr + feature_id)\n    acc += bias_val\n    \n    # Optimized Mish activation (twice)\n    def mish(x):\n        sp = tl.log(1.0 + tl.exp(-tl.abs(x))) + tl.maximum(x, 0.0)\n        tanh_sp = (1.0 - tl.exp(-2 * sp)) / (1.0 + tl.exp(-2 * sp))\n        return x * tanh_sp\n    \n    # Apply Mish twice\n    result = mish(mish(acc))\n    \n    # Store result\n    output_offset = row_id * stride_out + feature_id\n    tl.store(output_ptr + output_offset, result)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.in_features = in_features\n        self.out_features = out_features\n        \n    def forward(self, x):\n        output = torch.empty(x.shape[0], self.out_features, \n                            device=x.device, dtype=x.dtype)\n        if x.is_cuda:\n            # Ensure contiguous tensors\n            x_contig = x.contiguous()\n            weight_contig = self.linear.weight.contiguous()\n            bias_contig = self.linear.bias.contiguous()\n            \n            # Configure kernel launch\n            grid = (x.shape[0] * self.out_features,)\n            fused_linear_mish_mish_kernel[grid](\n                x_contig, \n                weight_contig, \n                bias_contig,\n                output,\n                self.in_features,\n                self.out_features,\n                x_contig.stride(0),\n                output.stride(0),\n            )\n        else:\n            # Fallback for CPU\n            output = self.linear(x)\n            output = torch.nn.functional.mish(output)\n            output = torch.nn.functional.mish(output)\n        return output\n\nbatch_size = 128\nin_features = 10\nout_features = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "2dd18249-0c37-4ed4-807b-2416106f8624", "generation": 4, "timestamp": 1754569119.0217576, "iteration_found": 23, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_29_Matmul_Mish_Mish", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp993c3gnr.py\", line 87, in forward\n    fused_linear_mish_mish_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 160, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 146, in kernel_call\n    self.fn.run(\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.UnsupportedLanguageConstruct: at 40:4:\n        x_vec = tl.load(x_base + offsets, mask=mask, other=0.0)\n        w_vec = tl.load(w_base + offsets, mask=mask, other=0.0)\n\n        # Accumulate partial dot product\n        acc += tl.sum(x_vec * w_vec)\n\n    # Add bias\n    bias_val = tl.load(bias_ptr + feature_id)\n    acc += bias_val\n\n    # Optimized Mish activation (twice)\n    def mish(x):\n    ^\nnested function definition is not supported.\n", "runtime_error_name": "triton.compiler.errors.UnsupportedLanguageConstruct", "reference_timing_ms": 0.0501, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.984, "runtime_ms": 0.0509, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0501, "speed_up": 0.984, "custom_timing": 0.0509}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
{"id": "4da639fe-42b6-4b43-b96d-ee602cb450eb", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _linear_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, B,\n    stride_x_b, stride_x_m,\n    stride_weight_n, stride_weight_m,\n    stride_bias_n,\n    stride_output_b, stride_output_n,\n    BLOCK_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    b_idx = pid // N\n    n_idx = pid % N\n    \n    if b_idx >= B or n_idx >= N:\n        return\n    \n    bias_val = tl.load(bias_ptr + n_idx * stride_bias_n)\n    acc = 0.0\n    \n    for m_block in range(0, M, BLOCK_SIZE_M):\n        m_offsets = tl.arange(0, BLOCK_SIZE_M)\n        m_mask = m_block + m_offsets < M\n        \n        x_offset = b_idx * stride_x_b + (m_block + m_offsets) * stride_x_m\n        weight_offset = n_idx * stride_weight_n + (m_block + m_offsets) * stride_weight_m\n        \n        x_val = tl.load(x_ptr + x_offset, mask=m_mask, other=0.0).to(tl.float16)\n        weight_val = tl.load(weight_ptr + weight_offset, mask=m_mask, other=0.0).to(tl.float16)\n        \n        a = tl.reshape(x_val, (1, BLOCK_SIZE_M))\n        b = tl.reshape(weight_val, (BLOCK_SIZE_M, 1))\n        acc += tl.dot(a, b).to(tl.float32)\n    \n    output_offset = b_idx * stride_output_b + n_idx * stride_output_n\n    tl.store(output_ptr + output_offset, acc + bias_val)\n\ndef linear_triton(x, weight, bias):\n    B, M = x.shape\n    N, _ = weight.shape\n    output = torch.empty((B, N), device=x.device, dtype=x.dtype)\n    \n    grid = (B * N,)\n    BLOCK_SIZE_M = 128\n    \n    _linear_kernel[grid](\n        x, weight, bias, output,\n        M, N, B,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        bias.stride(0),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE_M\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n        \n        self.fc1_weight = nn.Parameter(torch.empty(120, 400))\n        self.fc1_bias = nn.Parameter(torch.empty(120))\n        self.fc2_weight = nn.Parameter(torch.empty(84, 120))\n        self.fc2_bias = nn.Parameter(torch.empty(84))\n        self.fc3_weight = nn.Parameter(torch.empty(num_classes, 84))\n        self.fc3_bias = nn.Parameter(torch.empty(num_classes))\n        \n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        nn.init.kaiming_uniform_(self.fc1_weight, a=math.sqrt(5))\n        nn.init.kaiming_uniform_(self.fc2_weight, a=math.sqrt(5))\n        nn.init.kaiming_uniform_(self.fc3_weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.fc1_weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.fc1_bias, -bound, bound)\n        nn.init.uniform_(self.fc2_bias, -bound, bound)\n        nn.init.uniform_(self.fc3_bias, -bound, bound)\n    \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 16*5*5)\n        \n        x = linear_triton(x, self.fc1_weight, self.fc1_bias)\n        x = F.relu(x)\n        x = linear_triton(x, self.fc2_weight, self.fc2_bias)\n        x = F.relu(x)\n        x = linear_triton(x, self.fc3_weight, self.fc3_bias)\n        return x\n\nbatch_size = 1\nnum_classes = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, 1, 32, 32)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "6edbf63e-f4fb-4e6e-8b1f-cfa30c1b7401", "generation": 1, "timestamp": 1754594674.106344, "iteration_found": 3, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_4_LeNet5", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2045, in dot\n    return _semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1504, in dot\n    and rhs.shape[-1].value >= min_dot_size[1], \\\nAssertionError: Input shapes should have M >= 16, N >= 16 and K >= 16\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp96vjiinj.py\", line 97, in forward\n    x = linear_triton(x, self.fc1_weight, self.fc1_bias)\n  File \"/tmp/tmp96vjiinj.py\", line 54, in linear_triton\n    _linear_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 32:15:\n        m_offsets = tl.arange(0, BLOCK_SIZE_M)\n        m_mask = m_block + m_offsets < M\n\n        x_offset = b_idx * stride_x_b + (m_block + m_offsets) * stride_x_m\n        weight_offset = n_idx * stride_weight_n + (m_block + m_offsets) * stride_weight_m\n\n        x_val = tl.load(x_ptr + x_offset, mask=m_mask, other=0.0).to(tl.float16)\n        weight_val = tl.load(weight_ptr + weight_offset, mask=m_mask, other=0.0).to(tl.float16)\n\n        a = tl.reshape(x_val, (1, BLOCK_SIZE_M))\n        b = tl.reshape(weight_val, (BLOCK_SIZE_M, 1))\n        acc += tl.dot(a, b).to(tl.float32)\n               ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2045, in dot\n    return _semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1504, in dot\n    and rhs.shape[-1].value >= min_dot_size[1], \\\nAssertionError: Input shapes should have M >= 16, N >= 16 and K >= 16\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.206, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.177, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.206, "speed_up": 1.164, "custom_timing": 0.177}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.1770\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.206, 'speed_up': 1.164, 'custom_timing': 0.177}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.1770, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.206, 'speed_up': 1.164, 'custom_timing': 0.177}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes):\n        \"\"\"\n        LeNet-5 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n        self.fc2 = nn.Linear(in_features=120, out_features=84)\n        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the LeNet-5 model.\n\n        :param x: The input tensor, shape (batch_size, 1, 32, 32)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        # First convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Second convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Flatten the output for the fully connected layers\n        x = x.view(-1, 16*5*5)\n        \n        # First fully connected layer with ReLU activation\n        x = F.relu(self.fc1(x))\n        \n        # Second fully connected layer with ReLU activation\n        x = F.relu(self.fc2(x))\n        \n        # Final fully connected layer\n        x = self.fc3(x)\n        \n        return x\n\n# Test code for the LeNet-5 model\nbatch_size = 1\nnum_classes = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, 1, 32, 32)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.1770), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.206, 'speed_up': 1.164, 'custom_timing': 0.177})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes):\n        \"\"\"\n        LeNet-5 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n        self.fc2 = nn.Linear(in_features=120, out_features=84)\n        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the LeNet-5 model.\n\n        :param x: The input tensor, shape (batch_size, 1, 32, 32)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        # First convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Second convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Flatten the output for the fully connected layers\n        x = x.view(-1, 16*5*5)\n        \n        # First fully connected layer with ReLU activation\n        x = F.relu(self.fc1(x))\n        \n        # Second fully connected layer with ReLU activation\n        x = F.relu(self.fc2(x))\n        \n        # Final fully connected layer\n        x = self.fc3(x)\n        \n        return x\n\n# Test code for the LeNet-5 model\nbatch_size = 1\nnum_classes = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, 1, 32, 32)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch implementation of LeNet-5 (a CNN for image classification) and are asked to rewrite it for performance improvement.\n However, note that the current program is a reference PyTorch implementation and the task is to rewrite it using Triton for GPU acceleration.\n\n But wait: the example provided earlier was about quantizing a tensor, but here we have a complete CNN model.\n\n Important: The task is to rewrite the entire model? Actually, the problem context says that we are given a dataset of kernelbench files (each containing a reference_code and we are to write custom_code in Triton). However, the current code block is a complete model (LeNet-5) implemented in PyTorch.\n\n How to approach?\n We are to replace the PyTorch operations with Triton kernels? But note that Triton is typically used for writing individual kernels, not entire models.\n\n However, the problem states: \"Generate functionally equivalent but faster Triton implementations (custom_code)\".\n\n But the example only showed one function (per_row_quantize_int8) being replaced by a Triton kernel. So we must identify which part of the model is the bottleneck and replace that with a Triton kernel.\n\n But the current model is a standard LeNet-5. The operations are convolutions, ReLU, max pooling, and linear layers. We are not given any performance bottleneck, but the meta says:\n   speed_up: 1.164 (so the current PyTorch version is already 1.164x faster than the reference? But note: the current program is the reference code)\n\n Actually, the current program is the reference code. We are to write a custom (Triton) implementation to replace it.\n\n However, note the constraints:\n   - We are only allowed to change the Triton kernel source within the EVOLVE-BLOCK.\n   - The grid configuration, output tensor shapes, and PID logic are locked.\n\n But the current code doesn't have any Triton kernel. So we must introduce Triton kernels for some operations.\n\n Steps:\n\n 1. Identify which operations in the model are worth accelerating with Triton? \n    - The model is small (LeNet-5) and runs in 0.177 ms. This is very fast. But we are required to get at least 5% speedup.\n\n 2. We cannot replace the entire model with one Triton kernel? Because the model has multiple layers.\n\n 3. We can focus on the most expensive operations. In CNNs, convolutions are usually the bottleneck.\n\n 4. We are going to write a Triton kernel for the convolution operations? But note that the current convolutions are using PyTorch's `nn.Conv2d` which is already optimized (with cuDNN).\n\n 5. However, the problem says we are on H100 and we can use Tensor Cores (FP16). The current model uses FP32? We see the input is `torch.randn(batch_size, 1, 32, 32)` which is FP32.\n\n 6. We can try to convert the model to use FP16? But note: Triton kernels can be written in FP16. However, the problem does not require changing the model's data type.\n\n 7. Alternatively, we can write a Triton kernel for the entire forward pass? That would be very complex.\n\n 8. But note: the problem says \"small iterative steps: Modify \u226420 lines per revision\". So we cannot rewrite the entire model.\n\n 9. The problem says: \"Modify ONLY the Triton kernel source within this block\". So we are allowed to write one or more Triton kernels and replace some of the operations.\n\n 10. The current model has two convolution layers and three linear layers. The linear layers are also matrix multiplications.\n\n 11. We can try to replace the linear layers with Triton kernels? Because they are matrix multiplications and Triton has a good example for matrix multiplication.\n\n 12. However, note that the linear layers are after the convolutions and the input to the first linear layer is 16*5*5=400. The layers are 400->120, 120->84, 84->10. These are small matrices.\n\n 13. Alternatively, we can replace the convolution layers? But writing a convolution in Triton is complex.\n\n 14. Another idea: the max-pooling layers? They are 2x2 with stride 2. We can write a Triton kernel for max-pooling? But it's already optimized in PyTorch.\n\n 15. Given the constraints, we might not get a significant speedup by replacing one operation. But we are required at least 5% speedup.\n\n 16. The meta says: 'reference_timing_ms': 0.206, 'speed_up': 1.164, 'custom_timing': 0.177. This means that the current custom_timing (which is the time of the reference code) is 0.177 ms? And the reference_timing_ms is 0.206? And the speed_up is 1.164? This is confusing.\n\n 17. Actually, the current program is the reference code. The custom_timing (which would be the Triton version) is not present because we haven't written one.\n\n 18. Let me re-read: \n      \"Current performance metrics: ... speed_up: 1.0000 ... meta: ... 'speed_up': 1.164\"\n    This seems inconsistent.\n\n 19. The problem says: \"Scoring: Speedup > 1 required for points; Score = (speed_up - 1) \u00d7 100\"\n\n 20. The current program (the reference) has speed_up=1.0000. The meta says 'speed_up': 1.164 and 'custom_timing': 0.177. This suggests that the reference code runs in 0.206 ms and the custom code (if we write one) should run in less than 0.206 ms to get a speedup.\n\n 21. Actually, the meta says: \n        'reference_timing_ms': 0.206,   # time of the reference code (PyTorch)\n        'custom_timing': 0.177,          # time of the custom code (Triton) -> which we haven't written? But it's showing 0.177? \n        'speed_up': 1.164                # 0.206 / 0.177 \u2248 1.164\n\n    So it appears that there was a previous Triton version that ran in 0.177 ms? But the current program is the reference.\n\n 22. The history says: \"Previous Attempts: Attempt 1: Unknown changes\" and then we have the current program as the reference.\n\n 23. Given the confusion, we must focus on the task: rewrite the program to improve performance.\n\n 24. We are to replace the reference PyTorch code with a Triton implementation? But the entire model? That would be a huge Triton kernel and is not typical.\n\n 25. Alternatively, we can replace individual layers? But note the constraints: we cannot change the function signatures of the forward pass? Actually, we are replacing the entire module.\n\n 26. How about we write a Triton kernel for the entire forward pass? That would be a monolithic kernel and very complex.\n\n 27. Given the complexity, let's reconsider: the problem says we are given a dataset of kernelbench files. Each file contains a reference_code (a PyTorch function) and we are to write a custom_code (a Triton kernel). But here the reference_code is a complete model.\n\n 28. We are allowed to restructure the model? We are allowed to write multiple Triton kernels? The example only had one.\n\n 29. The problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\" and the code must be named as `ModelNew`.\n\n 30. We are going to write a Triton kernel for the convolution operation? But note: the convolution in LeNet-5 is small: \n        conv1: input [1, 1, 32, 32], weight [6,1,5,5] -> output [1,6,28,28]\n        conv2: input [1,6,14,14] (after pooling), weight [16,6,5,5] -> output [1,16,10,10]\n\n 31. We can write a Triton kernel for the convolution and then use it in the model.\n\n 32. Steps for the convolution kernel:\n      - We'll write a Triton kernel for 2D convolution.\n      - We must support the same parameters: kernel_size=5, stride=1, padding=0.\n      - The kernel will be for FP32? or FP16? We can try FP16 to use Tensor Cores? But the model is in FP32.\n\n 33. We can change the model to use FP16? But the problem does not specify. We must maintain functionality.\n\n 34. Alternatively, we can write the convolution in FP32 without Tensor Cores? Then we might not get the best performance.\n\n 35. Given the time, we decide to write a Triton kernel for the convolution that uses Tensor Cores in FP16. We will convert the input and weights to FP16, do the convolution, and convert back to FP32? But that adds overhead.\n\n 36. We must be cautious: the problem says \"functionally equivalent\". So we must maintain the same output within tolerance.\n\n 37. Alternatively, we can write the convolution in FP32 without Tensor Cores? But then we won't get the Tensor Core speed.\n\n 38. We are going to write a convolution kernel using the Triton example as a base.\n\n 39. However, note: the problem says \"small iterative steps: Modify \u226420 lines per revision\". We cannot write a 200-line kernel in one step.\n\n 40. Given the complexity and the small size of the model, and the fact that the current runtime is 0.177 ms, it might be very difficult to get a 5% speedup.\n\n 41. Alternative approach: replace the linear layers? They are small, but we can write a Triton kernel for matrix multiplication and use it for the linear layers.\n\n 42. The first linear layer: input [1, 400] (after flattening) and weight [400, 120]. This is a matrix multiplication of [1,400] x [400,120] -> [1,120].\n\n 43. We can write a Triton kernel for this matrix multiplication? We can use the Triton matrix multiplication example and adjust for the sizes.\n\n 44. Steps for the linear layer kernel:\n      - We'll write a kernel for `matmul` that uses tiling and shared memory.\n      - We can use FP16 for Tensor Cores? But the model is in FP32.\n\n 45. We decide to write a kernel for the linear layers in FP32. We can still use the Tensor Cores with tf32? But H100 supports tf32.\n\n 46. However, the problem says \"FP16 Tensor Core 32\u00d716\u00d716\". So we should use FP16.\n\n 47. We can change the linear layers to use FP16? But then the entire model must be in FP16? We are only replacing the linear layers? The rest of the model is in FP32.\n\n 48. We must be cautious of the data type. We can try to do the linear layers in FP16 and then convert the output to FP32? But that might break the model.\n\n 49. Alternatively, we can write the kernel in FP32 and not use Tensor Cores? Then we might not get the best speed.\n\n 50. Given the constraints, we decide to replace the linear layers with a Triton kernel that does the matrix multiplication in FP16 (with Tensor Cores) and accumulates in FP32, then we cast the result to FP32.\n\n 51. We will write one Triton kernel for the linear layer and use it for all three linear layers.\n\n 52. We are going to change the model to use our custom linear layer for `fc1`, `fc2`, and `fc3`.\n\n 53. Steps:\n      - Remove the `nn.Linear` layers and replace them with calls to our Triton kernel.\n      - We must write the kernel and the wrapper.\n\n 54. We note: the input to the linear layer is 2D: [batch_size, in_features]. The weight matrix is [out_features, in_features] (but stored as [in_features, out_features] in PyTorch? Actually, PyTorch linear layer: `y = x @ W.t() + bias`? But we don't have bias in the kernel? The current model does have bias because `nn.Linear` has bias by default.\n\n 55. We must check: the current model uses `nn.Linear` which includes a bias. Our Triton kernel must also add a bias.\n\n 56. We are going to write a kernel for `linear = (x @ weight.t()) + bias`.\n\n 57. We design the kernel for the linear layer:\n\n      Inputs:\n        x: tensor of shape [B, M]  (M = in_features)\n        weight: tensor of shape [N, M]  (N = out_features)\n        bias: tensor of shape [N]\n\n      Output:\n        y: tensor of shape [B, N]\n\n 58. We can write a kernel that processes one row of x at a time? Or multiple rows? We can use a grid of B blocks? Or split the batch and features.\n\n 59. We note that the batch size is 1 in the test. So we can have a grid of (B, N) or (B, ceil(N/BLOCK_SIZE)).\n\n 60. Given the small size, we can have one program per output feature? But then we would have N programs. For the first linear layer, N=120.\n\n 61. We'll write a kernel that uses tiling: each block computes a block of output.\n\n 62. We are going to write a Triton kernel for `linear_layer` that does:\n        y = (x @ weight.t()) + bias\n\n 63. We will then replace the current `F.relu(self.fc1(x))` with:\n        x = linear_layer_triton(x, self.fc1_weight, self.fc1_bias)\n        x = F.relu(x)\n\n    and similarly for the others.\n\n 64. We must also remove the `nn.Linear` and replace with parameters.\n\n 65. We change the model:\n\n        self.fc1_weight = nn.Parameter(torch.Tensor(120, 400))\n        self.fc1_bias = nn.Parameter(torch.Tensor(120))\n\n        and similarly for fc2 and fc3.\n\n 66. Then in the forward pass, we call our Triton kernel for the linear operation.\n\n 67. But note: we are not allowed to change the output tensor shapes? We are not changing, we are replacing with an equivalent operation.\n\n 68. We must initialize the weights and biases appropriately.\n\n 69. We can do:\n\n        # Instead of self.fc1 = nn.Linear(400, 120)\n        self.fc1_weight = nn.Parameter(torch.Tensor(120, 400))\n        self.fc1_bias = nn.Parameter(torch.Tensor(120))\n\n        and then initialize them.\n\n 70. We must initialize the parameters. We can copy the initialization from `nn.Linear`? But we don't have the code.\n\n 71. Alternatively, we can keep the `nn.Linear` layers but use their weights and biases in our kernel? But then we are not saving the time of the PyTorch linear.\n\n 72. We decide to replace the `nn.Linear` layers with our own parameters and then use our kernel.\n\n 73. Steps for the rewrite:\n\n        In __init__:\n          Remove:\n            self.fc1 = nn.Linear(...)\n            self.fc2 = nn.Linear(...)\n            self.fc3 = nn.Linear(...)\n\n          Replace with:\n            self.fc1_weight = nn.Parameter(torch.Tensor(120, 400))\n            self.fc1_bias = nn.Parameter(torch.Tensor(120))\n            ... similarly for fc2 and fc3.\n\n          Then initialize these parameters (we can use the same initialization as PyTorch).\n\n        In forward:\n          Replace:\n            x = F.relu(self.fc1(x))\n          with:\n            x = linear_triton(x, self.fc1_weight, self.fc1_bias)\n            x = F.relu(x)\n\n 74. We must write the `linear_triton` function that calls the Triton kernel.\n\n 75. We design the Triton kernel for linear:\n\n      @triton.autotune(...)\n      @triton.jit\n      def _linear_kernel(...)\n\n      Then a wrapper function that sets up the grid and calls the kernel.\n\n 76. However, note the constraints: we cannot change the grid configuration? Actually, we are writing the grid, so it's allowed.\n\n 77. We are going to write a kernel that uses a 2D grid: \n        grid = (triton.cdiv(N, BLOCK_SIZE_N), B)\n      where each block computes a block of outputs of size [BLOCK_SIZE_N] for one batch.\n\n 78. But note: the batch size is 1 in the test. So we can also use a 1D grid of size (ceil(N/BLOCK_SIZE_N),).\n\n 79. We'll write the kernel for the linear layer without tiling in the M dimension? Because M is 400, which is small.\n\n 80. We can have each program compute one output element? That would be:\n\n        pid = tl.program_id(0)\n        n = pid\n        if n < N:\n            acc = 0.0\n            for m in range(M):\n                acc += x_ptr[b, m] * weight_ptr[n, m]\n            acc += bias_ptr[n]\n            output_ptr[b, n] = acc\n\n      But that is not efficient.\n\n 81. We'll tile in the M dimension? We can use a block of size BLOCK_SIZE_M for M.\n\n 82. Given the complexity and the small sizes, we might not get a speedup. But we try.\n\n 83. Alternatively, we can use the Triton matrix multiplication example and adapt it for the case where one matrix is a vector (if batch=1) or small.\n\n 84. Given the time, we decide to use a simple kernel without tiling for the entire feature dimension? But that would be inefficient.\n\n 85. We write a kernel that uses tiling in the M dimension and each block computes one output element? But then we have to reduce over M.\n\n 86. We write a kernel that uses a 1D grid of size (B * N) and each block computes one output element.\n\n        pid = tl.program_id(0)\n        b = pid // N\n        n = pid % N\n        if b < B and n < N:\n            acc = 0.0\n            for m in range(0, M, BLOCK_SIZE_M):\n                offs_m = m + tl.arange(0, BLOCK_SIZE_M)\n                mask_m = offs_m < M\n                x_val = tl.load(x_ptr + b*M + offs_m, mask=mask_m, other=0.0)\n                w_val = tl.load(weight_ptr + n*M + offs_m, mask=mask_m, other=0.0)\n                acc += tl.sum(x_val * w_val)\n            acc += tl.load(bias_ptr + n)\n            tl.store(output_ptr + b*N + n, acc)\n\n 87. But note: we are not using Tensor Cores. We want to use Tensor Cores.\n\n 88. To use Tensor Cores, we need to use the `tl.dot` instruction and have the data in FP16. We'll do:\n\n        Convert the input and weight to FP16, do the dot product in FP16, and accumulate in FP32.\n\n 89. We change the kernel to:\n\n        x_val = tl.load(...)   # load as FP32, then convert to FP16?\n        w_val = tl.load(...)   # same\n\n        But we can also store the weights and input in FP16? We are going to change the model to use FP16 for the linear layer parameters and input? But the input to the linear layer is the output of the previous layer (which is FP32).\n\n 90. We decide to do the conversion in the kernel:\n\n        x_val = tl.load(...).to(tl.float16)\n        w_val = tl.load(...).to(tl.float16)\n\n        Then do a dot product? But the dot product in Triton for FP16 is done with `tl.dot` and it uses Tensor Cores if the block size is appropriate.\n\n 91. We need to use a block size that is a multiple of 16? The Triton documentation says for Tensor Cores we should have block sizes of 64x64 or 128x128.\n\n 92. Given M=400, we can pad to 416 (multiple of 16) and use a block size of 16? But then we would have 26 blocks per row? That is not enough parallelism.\n\n 93. We are going to use a block size of 128 for M? Then each program would compute one output element by iterating over 4 blocks (400/128=3.125 -> 4 blocks).\n\n 94. We design the kernel for one output element per program? Then the grid is (B * N).\n\n 95. We are also going to use a block size of 128 for M, and use the dot product for a vector of size 128? But the dot product in Triton is for two vectors? We can do:\n\n        acc = 0.0\n        for m in range(0, M, BLOCK_SIZE_M):\n            offs = m + tl.arange(0, BLOCK_SIZE_M)\n            mask = offs < M\n            a = tl.load(x_ptr + b*M + offs, mask=mask, other=0.0).to(tl.float16)\n            b = tl.load(weight_ptr + n*M + offs, mask=mask, other=0.0).to(tl.float16)\n            acc += tl.sum(a * b)   # but this is not using dot product efficiently.\n\n        We want to use `tl.dot`? But `tl.dot` is for matrix multiplication.\n\n 96. We can do:\n\n        # We are loading two vectors, so we can do a dot product by reshaping?\n        # We can use tl.dot(a, b) but a and b must be of shape [1, BLOCK_SIZE_M] and [BLOCK_SIZE_M, 1]? Then the result is a scalar.\n\n        a = tl.reshape(a, (1, BLOCK_SIZE_M))\n        b = tl.reshape(b, (BLOCK_SIZE_M, 1))\n        acc_fp16 = tl.dot(a, b)\n        acc += acc_fp16.to(tl.float32)\n\n 97. This would use one Tensor Core operation per block.\n\n 98. We set BLOCK_SIZE_M to 128? But 128 is not a multiple of 16? It is (128 % 16 == 0). So it's acceptable.\n\n 99. We write the kernel accordingly.\n\n 100. However, note: the model is very small and the overhead of launching many small kernels might dominate. We are launching B*N kernels. For the first linear layer, B=1, N=120 -> 120 kernels. This might be inefficient.\n\n 101. We might want to combine multiple output elements per kernel? But then we are increasing the complexity.\n\n 102. Given the time, we decide to try the simple approach.\n\n 103. We write the kernel and hope that the Tensor Core usage will give us speed.\n\n 104. Steps for the kernel:\n\n        @triton.jit\n        def _linear_kernel(\n            x_ptr, weight_ptr, bias_ptr, output_ptr,\n            M, N, B,\n            stride_x_b, stride_x_m,\n            stride_weight_n, stride_weight_m,\n            stride_bias_n,\n            stride_output_b, stride_output_n,\n            BLOCK_SIZE_M: tl.constexpr,\n        ):\n            pid = tl.program_id(0)\n            b = pid // N\n            n = pid % N\n            if b >= B or n >= N:\n                return\n\n            bias = tl.load(bias_ptr + n * stride_bias_n)\n\n            output_offset = b * stride_output_b + n * stride_output_n\n            acc = 0.0\n            for m in range(0, M, BLOCK_SIZE_M):\n                offs_m = tl.arange(0, BLOCK_SIZE_M)\n                x_offs = b * stride_x_b + (m + offs_m) * stride_x_m\n                weight_offs = n * stride_weight_n + (m + offs_m) * stride_weight_m\n\n                mask = (m + offs_m) < M\n\n                x_val = tl.load(x_ptr + x_offs, mask=mask, other=0.0)\n                weight_val = tl.load(weight_ptr + weight_offs, mask=mask, other=0.0)\n\n                # Convert to FP16 for Tensor Core\n                x_val_fp16 = x_val.to(tl.float16)\n                weight_val_fp16 = weight_val.to(tl.float16)\n\n                # Reshape to [1, BLOCK_SIZE_M] and [BLOCK_SIZE_M, 1] for matrix multiplication\n                a = tl.reshape(x_val_fp16, (1, BLOCK_SIZE_M))\n                b = tl.reshape(weight_val_fp16, (BLOCK_SIZE_M, 1))\n                # Dot product returns a scalar in FP16\n                product = tl.dot(a, b)\n                acc += product.to(tl.float32)\n\n            acc += bias\n            tl.store(output_ptr + output_offset, acc)\n\n 105. We then call this kernel with grid = (B * N,). And BLOCK_SIZE_M=128.\n\n 106. We must also have an autotuner for BLOCK_SIZE_M? But we set it to 128.\n\n 107. However, note: the problem says \"Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions\". We can make it tunable.\n\n 108. We'll use an autotuner for BLOCK_SIZE_M? But the kernel is very simple.\n\n 109. Given the time, we skip autotuning for now.\n\n 110. We then replace the linear layers in the model with this kernel.\n\n 111. But note: the kernel does not support transposed weight? In our kernel, we are loading the weight as [n, m] and the weight in PyTorch is stored as [out_features, in_features] which is [N, M]. And in the linear layer, we do `x @ weight.t()`? Actually, no: the standard linear layer is `x @ W.t() + b` where W is of shape [out_features, in_features]. But in our kernel, we are storing the weight in the same layout: [N, M]. And the kernel does:\n\n        for each n, m: we multiply x[b, m] * weight[n, m]\n\n        which is equivalent to: x[b] (row vector) times weight[n] (row vector) -> so we are doing: \n            output[b, n] = dot(x[b], weight[n])\n\n        which is: output = x @ weight^T\n\n        so we are correct.\n\n 112. We must also handle the case where the batch size is not 1? But the test uses batch_size=1.\n\n 113. We write the wrapper function for the linear layer:\n\n        def linear_triton(x, weight, bias):\n            assert x.dim() == 2, \"x must be 2D\"\n            B, M = x.shape\n            N = weight.shape[0]\n            assert weight.shape[1] == M, \"Incompatible dimensions\"\n            assert bias.shape[0] == N, \"Incompatible bias shape\"\n            output = torch.empty((B, N), device=x.device, dtype=x.dtype)\n\n            # We use a 1D grid of B*N\n            grid = (B * N,)\n            # We set BLOCK_SIZE_M to 128\n            BLOCK_SIZE_M = 128\n\n            # Strides\n            stride_x_b, stride_x_m = x.stride()\n            stride_weight_n, stride_weight_m = weight.stride()\n            stride_bias_n = bias.stride()[0]\n            stride_output_b, stride_output_n = output.stride()\n\n            _linear_kernel[grid](\n                x, weight, bias, output,\n                M, N, B,\n                stride_x_b, stride_x_m,\n                stride_weight_n, stride_weight_m,\n                stride_bias_n,\n                stride_output_b, stride_output_n,\n                BLOCK_SIZE_M\n            )\n            return output\n\n 114. We then change the model to use this.\n\n 115. We also must initialize the weights and biases. We can initialize the weights with the same method as PyTorch? We don't have the current weights, but we can use the same initialization.\n\n 116. We can use `nn.init.kaiming_uniform_` for the weights and `nn.init.zeros_` for the biases? But the original model was using the default initialization of `nn.Linear`.\n\n 117. We do in __init__:\n\n        # For fc1\n        self.fc1_weight = nn.Parameter(torch.empty(120, 400))\n        self.fc1_bias = nn.Parameter(torch.empty(120))\n        nn.init.kaiming_uniform_(self.fc1_weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.fc1_weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.fc1_bias, -bound, bound)\n\n        and similarly for fc2 and fc3.\n\n 118. But note: the original model had `fc1 = nn.Linear(400, 120)`. The initialization of `nn.Linear` is:\n\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            init.uniform_(self.bias, -bound, bound)\n\n 119. So we are doing the same.\n\n 120. We change the model accordingly.\n\n 121. But note: the original model had three linear layers. We must replace all three.\n\n 122. We are going to write the kernel and hope that it works.\n\n 123. However, note the small size of the matrices: the first linear layer is 1x400 times 400x120 -> 1x120. The kernel will launch 120 programs. The second linear layer is 1x120 times 120x84 -> 84 programs. The third is 1x84 times 84x10 -> 10 programs. This is a total of 214 programs. The overhead of launching 214 Triton kernels might be high.\n\n 124. We might not get a speedup. But we try.\n\n 125. Alternatively, we could write a kernel that processes the entire batch and output features in a more efficient manner? But that would be more complex.\n\n 126. Given the constraints of 20 lines per revision, we cannot write a more complex kernel.\n\n 127. We decide to try this approach.\n\n 128. We also note that the kernel might be inefficient because of the loop over M with step BLOCK_SIZE_M, and the small BLOCK_SIZE_M (128) for M=400 -> 4 iterations.\n\n 129. We hope that the Tensor Core dot product will make it fast.\n\n 130. Let's write the code.\n\n 131. We must also include the Triton imports and the kernel definition.\n\n 132. We are only allowed to change the EVOLVE-BLOCK? So we put everything inside.\n\n 133. We are going to write the entire ModelNew class with the changes.\n\n 134. We also note: the test code uses `get_inputs` and `get_init_inputs`. We must not change them.\n\n 135. We write the code accordingly.\n\n 136. We note that the current runtime is 0.177 ms. We hope to reduce it.\n\n 137. We also note that the kernel might be numerically different because of FP16 accumulation? But we accumulate in FP32.\n\n 138. We hope it passes correctness.\n\n Given the complexity and the line limit, we write the code below.\n</think>\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _linear_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, B,\n    stride_x_b, stride_x_m,\n    stride_weight_n, stride_weight_m,\n    stride_bias_n,\n    stride_output_b, stride_output_n,\n    BLOCK_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    b_idx = pid // N\n    n_idx = pid % N\n    \n    if b_idx >= B or n_idx >= N:\n        return\n    \n    bias_val = tl.load(bias_ptr + n_idx * stride_bias_n)\n    acc = 0.0\n    \n    for m_block in range(0, M, BLOCK_SIZE_M):\n        m_offsets = tl.arange(0, BLOCK_SIZE_M)\n        m_mask = m_block + m_offsets < M\n        \n        x_offset = b_idx * stride_x_b + (m_block + m_offsets) * stride_x_m\n        weight_offset = n_idx * stride_weight_n + (m_block + m_offsets) * stride_weight_m\n        \n        x_val = tl.load(x_ptr + x_offset, mask=m_mask, other=0.0).to(tl.float16)\n        weight_val = tl.load(weight_ptr + weight_offset, mask=m_mask, other=0.0).to(tl.float16)\n        \n        a = tl.reshape(x_val, (1, BLOCK_SIZE_M))\n        b = tl.reshape(weight_val, (BLOCK_SIZE_M, 1))\n        acc += tl.dot(a, b).to(tl.float32)\n    \n    output_offset = b_idx * stride_output_b + n_idx * stride_output_n\n    tl.store(output_ptr + output_offset, acc + bias_val)\n\ndef linear_triton(x, weight, bias):\n    B, M = x.shape\n    N, _ = weight.shape\n    output = torch.empty((B, N), device=x.device, dtype=x.dtype)\n    \n    grid = (B * N,)\n    BLOCK_SIZE_M = 128\n    \n    _linear_kernel[grid](\n        x, weight, bias, output,\n        M, N, B,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        bias.stride(0),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE_M\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n        \n        self.fc1_weight = nn.Parameter(torch.empty(120, 400))\n        self.fc1_bias = nn.Parameter(torch.empty(120))\n        self.fc2_weight = nn.Parameter(torch.empty(84, 120))\n        self.fc2_bias = nn.Parameter(torch.empty(84))\n        self.fc3_weight = nn.Parameter(torch.empty(num_classes, 84))\n        self.fc3_bias = nn.Parameter(torch.empty(num_classes))\n        \n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        nn.init.kaiming_uniform_(self.fc1_weight, a=math.sqrt(5))\n        nn.init.kaiming_uniform_(self.fc2_weight, a=math.sqrt(5))\n        nn.init.kaiming_uniform_(self.fc3_weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.fc1_weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.fc1_bias, -bound, bound)\n        nn.init.uniform_(self.fc2_bias, -bound, bound)\n        nn.init.uniform_(self.fc3_bias, -bound, bound)\n    \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 16*5*5)\n        \n        x = linear_triton(x, self.fc1_weight, self.fc1_bias)\n        x = F.relu(x)\n        x = linear_triton(x, self.fc2_weight, self.fc2_bias)\n        x = F.relu(x)\n        x = linear_triton(x, self.fc3_weight, self.fc3_bias)\n        return x\n\nbatch_size = 1\nnum_classes = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, 1, 32, 32)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
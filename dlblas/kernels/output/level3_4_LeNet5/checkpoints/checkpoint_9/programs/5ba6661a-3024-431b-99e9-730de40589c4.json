{"id": "5ba6661a-3024-431b-99e9-730de40589c4", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef linear_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    m, n, k,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_K: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    x_row_ptr = x_ptr + pid_m * stride_xm\n    weight_col_ptr = weight_ptr + pid_n * stride_wn\n    \n    acc = tl.zeros((1,), dtype=tl.float32)\n    for block_idx in range(0, k, BLOCK_K):\n        k_offs = block_idx + tl.arange(0, BLOCK_K)\n        mask_k = k_offs < k\n        \n        x_val = tl.load(x_row_ptr + k_offs, mask=mask_k, other=0.0)\n        weight_val = tl.load(weight_col_ptr + k_offs, mask=mask_k, other=0.0)\n        \n        acc += tl.sum(x_val * weight_val)\n    \n    bias_val = tl.load(bias_ptr + pid_n)\n    acc += bias_val\n    \n    output_ptr = output_ptr + pid_m * stride_om + pid_n * stride_on\n    tl.store(output_ptr, acc)\n\ndef triton_linear(x, weight, bias):\n    m, k = x.shape\n    n, k2 = weight.shape\n    assert k == k2, \"Incompatible dimensions\"\n    \n    output = torch.empty((m, n), device=x.device, dtype=x.dtype)\n    \n    BLOCK_K = 32\n    BLOCK_N = 128\n    grid = (m, triton.cdiv(n, BLOCK_N))\n    \n    linear_kernel[grid](\n        x, weight, bias, output,\n        m, n, k,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        output.stride(0), output.stride(1),\n        BLOCK_K=BLOCK_K, BLOCK_N=BLOCK_N\n    )\n    \n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n        self.fc2 = nn.Linear(in_features=120, out_features=84)\n        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n    \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        x = x.view(-1, 16*5*5)\n        \n        # Replace fc1 with Triton implementation\n        x = F.relu(triton_linear(x, self.fc1.weight, self.fc1.bias))\n        \n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# Test code for the LeNet-5 model\nbatch_size = 1\nnum_classes = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, 1, 32, 32)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "6edbf63e-f4fb-4e6e-8b1f-cfa30c1b7401", "generation": 1, "timestamp": 1754594042.8546016, "iteration_found": 2, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_4_LeNet5", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2216, in store\n    return _semantic.store(pointer, value, mask, boundary_check, cache_modifier, eviction_policy)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1292, in store\n    return self._store_legacy(ptr, val, mask, boundary_check, cache, eviction)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1248, in _store_legacy\n    raise ValueError(\"Value argument cannot be block type if pointer argument is not a block\")\nValueError: Value argument cannot be block type if pointer argument is not a block\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpev1ih0xf.py\", line 79, in forward\n    x = F.relu(triton_linear(x, self.fc1.weight, self.fc1.bias))\n  File \"/tmp/tmpev1ih0xf.py\", line 51, in triton_linear\n    linear_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 30:4:\n        mask_k = k_offs < k\n\n        x_val = tl.load(x_row_ptr + k_offs, mask=mask_k, other=0.0)\n        weight_val = tl.load(weight_col_ptr + k_offs, mask=mask_k, other=0.0)\n\n        acc += tl.sum(x_val * weight_val)\n\n    bias_val = tl.load(bias_ptr + pid_n)\n    acc += bias_val\n\n    output_ptr = output_ptr + pid_m * stride_om + pid_n * stride_on\n    tl.store(output_ptr, acc)\n    ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2216, in store\n    return _semantic.store(pointer, value, mask, boundary_check, cache_modifier, eviction_policy)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1292, in store\n    return self._store_legacy(ptr, val, mask, boundary_check, cache, eviction)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1248, in _store_legacy\n    raise ValueError(\"Value argument cannot be block type if pointer argument is not a block\")\nValueError: Value argument cannot be block type if pointer argument is not a block\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.206, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.177, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.206, "speed_up": 1.164, "custom_timing": 0.177}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.1770\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.206, 'speed_up': 1.164, 'custom_timing': 0.177}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.1770, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.206, 'speed_up': 1.164, 'custom_timing': 0.177}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes):\n        \"\"\"\n        LeNet-5 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n        self.fc2 = nn.Linear(in_features=120, out_features=84)\n        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the LeNet-5 model.\n\n        :param x: The input tensor, shape (batch_size, 1, 32, 32)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        # First convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Second convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Flatten the output for the fully connected layers\n        x = x.view(-1, 16*5*5)\n        \n        # First fully connected layer with ReLU activation\n        x = F.relu(self.fc1(x))\n        \n        # Second fully connected layer with ReLU activation\n        x = F.relu(self.fc2(x))\n        \n        # Final fully connected layer\n        x = self.fc3(x)\n        \n        return x\n\n# Test code for the LeNet-5 model\nbatch_size = 1\nnum_classes = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, 1, 32, 32)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.1770), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.206, 'speed_up': 1.164, 'custom_timing': 0.177})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes):\n        \"\"\"\n        LeNet-5 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n        self.fc2 = nn.Linear(in_features=120, out_features=84)\n        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the LeNet-5 model.\n\n        :param x: The input tensor, shape (batch_size, 1, 32, 32)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        # First convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Second convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Flatten the output for the fully connected layers\n        x = x.view(-1, 16*5*5)\n        \n        # First fully connected layer with ReLU activation\n        x = F.relu(self.fc1(x))\n        \n        # Second fully connected layer with ReLU activation\n        x = F.relu(self.fc2(x))\n        \n        # Final fully connected layer\n        x = self.fc3(x)\n        \n        return x\n\n# Test code for the LeNet-5 model\nbatch_size = 1\nnum_classes = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, 1, 32, 32)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch implementation of LeNet-5 and we are to rewrite it using Triton for performance.\n However, note that the current program is a reference PyTorch model (LeNet-5) and the task is to generate a Triton implementation for a kernel.\n But wait: the example provided in the context was about a quantization kernel. The current program is a full model.\n\n After re-examining the task, it appears there might be a misunderstanding. The context states:\n   \"Dataset: 250 kernelbench/level1-level5 *.py files\"\n   \"Each file contains PyTorch reference (reference_code)\"\n   \"Goal: Generate functionally equivalent but faster Triton implementations (custom_code)\"\n\n However, the current program is a full LeNet-5 model, which is not a single kernel.\n\n But note: the evaluation pipeline is for kernels (verify_plus, execute, scoring by speedup).\n\n How to proceed?\n We must look for the kernel that is the hotspot in the model and replace it with a Triton kernel.\n\n In the LeNet-5 model, the forward pass consists of:\n   - Two convolutional layers\n   - Two max-pooling layers\n   - Three linear layers\n\n The most compute-intensive parts are the convolutions and the linear layers.\n\n However, the problem states that we are only allowed to change the Triton kernel source within the EVOLVE-BLOCK.\n\n Since the current block is a full model, we must identify which operation we are targeting.\n\n Let's note the constraints:\n   - We are not allowed to change the grid configuration, output tensor shapes, etc.\n\n Given the complexity, it is likely that the task is to rewrite one of the operations (like a convolution or linear layer) in Triton.\n\n But the provided example was a quantization kernel. And the current model does not have a quantization step.\n\n How to interpret the task?\n\n The problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\"\n\n In the current EVOLVE-BLOCK, there is no Triton kernel. So we are in Phase 1: we must create a Triton kernel that replaces one of the operations.\n\n However, the model is a reference implementation. We are allowed to change the internal implementation of the model as long as the inputs and outputs remain the same.\n\n But note: the test code at the bottom (get_inputs and get_init_inputs) must remain.\n\n We are allowed to replace parts of the model with Triton kernels. We have to choose which part.\n\n Considering the performance, we note that the reference timing is 0.206 ms and the custom timing is 0.177 ms (from meta) giving a speedup of 1.164.\n\n However, the current program is the reference. We are to write a custom version that uses Triton.\n\n Steps:\n\n 1. We must decide which operation to replace with a Triton kernel.\n\n 2. The most time-consuming operation in LeNet-5 for small input (32x32) is likely the first convolution? But note the model is small.\n\n 3. We are constrained by the fact that we must not change the function signatures of the model.\n\n 4. We are allowed to change the forward method as long as we produce the same output.\n\n 5. We can replace one of the layers (like conv1, conv2, fc1, etc.) with a custom Triton kernel.\n\n 6. However, note that the convolution and linear operations are already highly optimized in PyTorch (using cuDNN and cuBLAS). Therefore, it might be challenging to beat them.\n\n 7. Alternatively, we might consider replacing an entire sequence (like conv + relu + maxpool) with a fused Triton kernel? But that would be complex.\n\n 8. Given the constraints of the problem (we are only allowed to change the EVOLVE-BLOCK) and that we must return the entire code block, we must create a Triton kernel for one operation and then use it in the model.\n\n 9. We have to be cautious: the model is small and the input is 1x1x32x32. The overhead of launching a kernel might be significant.\n\n 10. Since the problem says \"small iterative steps\", we might start by replacing the first fully connected layer (fc1) with a Triton kernel? Because matrix multiplication is a well-known kernel and we can try to optimize it for the specific size (16*5*5 = 400 to 120).\n\n 11. The matrix size: input is [batch_size, 400] and weight is [400, 120]. This is a small matrix. But we can try.\n\n 12. Alternatively, we could replace the ReLU activation? But ReLU is very simple and unlikely to be a bottleneck.\n\n 13. We note that the current model is implemented in pure PyTorch and we are to introduce Triton for one part.\n\n 14. Let's plan: we will replace the first fully connected layer (self.fc1) with a Triton matrix multiplication kernel.\n\n 15. We must write a Triton kernel for matrix multiplication that takes an input matrix A [m, k] and weight matrix B [k, n] and produces C [m, n].\n\n 16. We note that the current model uses:\n        x = x.view(-1, 16*5*5)   # [batch_size, 400]\n        x = F.relu(self.fc1(x))   # [batch_size, 120]\n\n 17. We can replace the fc1 with a custom linear layer that uses Triton.\n\n 18. Steps for the linear layer in Triton:\n      - We will write a kernel for the forward pass of the linear layer (matrix multiplication plus bias? note: the original has bias because nn.Linear has bias by default).\n      - We have to handle the bias: if the original has bias, we add it.\n\n 19. How to check: the original model uses nn.Linear, which has bias. So we must include bias.\n\n 20. We will create a new class for the linear layer that uses Triton.\n\n 21. We must also consider that the weights and biases are parameters that require gradients. However, in the forward pass we are only doing inference? The problem does not specify training. But the model is defined for both.\n\n 22. The problem says: the test code uses `torch.randn` and the model is called. We are not told about backward. However, the problem says \"functionally equivalent\", so we must also support backward if the original does? But note the context says \"reference_code\" and then we are to write \"custom_code\" for the kernel. The evaluation is by numerical correctness and speed in forward.\n\n 23. Since the problem does not specify, we will assume that we are only optimizing the forward pass and that backward is not required. However, the model is defined as an nn.Module and might be used in training. But the problem says \"functional equivalence\" in the forward pass.\n\n 24. We are not required to implement the backward in Triton? The problem does not specify. However, the example (quantization) did not have backward.\n\n 25. Considering the complexity and the constraints (only forward is tested for correctness and speed), we will implement only the forward with Triton.\n\n 26. We will create a class `TritonLinear` that subclasses `nn.Module` and replaces `self.fc1`.\n\n 27. We must initialize it with the same weights and bias.\n\n 28. However, note: we are allowed to change the model structure? The problem says: \"all other code is LOCKED\" except the Triton kernel source. But we are in the EVOLVE-BLOCK and we are allowed to change the entire block? Yes, because the block is the entire model and the test code.\n\n 29. We are going to:\n        - Keep the same __init__ parameters (num_classes) and the same layers, except replace the fc1 with our TritonLinear.\n        - The TritonLinear will have the same input and output features.\n\n 30. We must copy the weights and biases from the original fc1 to the TritonLinear? Actually, we are replacing the layer. So in the __init__, we will create the TritonLinear with the same in_features and out_features, and then we can delete the original fc1? But note the model is already defined.\n\n 31. Alternatively, we can change the forward to use our Triton kernel for the fc1 step and leave the rest. But we cannot remove the original fc1 because it holds the parameters.\n\n 32. We can do: in the forward, when we get to fc1, we use our Triton kernel to compute the linear transformation using the same weights and bias.\n\n 33. We will write a Triton kernel that does: C = A @ B + bias, where A is [m, k], B is [k, n], and bias is [n].\n\n 34. We note that the input to fc1 is [batch_size, 400] and the weight is [120, 400] (but stored as [out_features, in_features]) and we do: x @ weight.t() + bias.\n\n 35. So in the kernel, we are multiplying A [m, k] with B [k, n] where B is the weight matrix (of shape [400, 120])? Actually, we can transpose the weight and then use the kernel for A @ W without transposing? Or we can write the kernel to handle the transposed weight.\n\n 36. We can pre-transpose the weight when initializing the TritonLinear? Then we store the weight as [400, 120] and then our kernel multiplies [m,400] * [400,120] -> [m,120]. This is efficient.\n\n 37. However, note: the original weight in nn.Linear is of shape [out_features, in_features]. So if we want to avoid the transpose, we can store the weight in the transposed form (as [in_features, out_features]) and then the multiplication becomes: x (m, in) * weight (in, out) -> (m, out).\n\n 38. We can do that. Then we don't need to transpose in the kernel.\n\n 39. Steps in the model:\n        In __init__:\n            self.fc1_weight = nn.Parameter(torch.Tensor(400, 120))   # but originally it was (120,400)\n            self.fc1_bias = nn.Parameter(torch.Tensor(120))\n\n        Then we initialize them with the original fc1's weight transposed and the same bias.\n\n        But wait, we are replacing the original fc1. So we can remove the original fc1 and use our own parameters.\n\n 40. However, the problem says: \"maintain the same inputs and outputs\". We are not changing the parameters of the model? We are just replacing the implementation.\n\n 41. We must be cautious: the model is defined with self.fc1, self.fc2, self.fc3. We cannot change the names? Actually, we are allowed to change the entire block. We can restructure the model as long as the input and output are the same.\n\n 42. We will do:\n\n        class TritonLinear(nn.Module):\n            def __init__(self, in_features, out_features):\n                super().__init__()\n                self.weight = nn.Parameter(torch.empty(in_features, out_features))\n                self.bias = nn.Parameter(torch.empty(out_features))\n                # initialize properly? We will leave that to the caller.\n\n            def forward(self, x):\n                # We'll write a Triton kernel for: x @ self.weight + self.bias\n                return triton_linear(x, self.weight, self.bias)\n\n        Then in the main model, we replace:\n            self.fc1 = nn.Linear(...)   -> becomes:\n            self.fc1 = TritonLinear(400, 120)\n\n        But note: the original model uses nn.Linear which has weight of shape [120,400]. We are storing [400,120]. So we must transpose the weight when we initialize? Actually, we can load the state_dict from the original model? But the problem does not say anything about state_dict.\n\n        However, the test code does not load any pre-trained weights. It just runs with random weights.\n\n 43. We can initialize the TritonLinear with the same initialization as nn.Linear? We can copy the initialization from nn.Linear.\n\n 44. Alternatively, we can keep the original nn.Linear and in the forward, we use our kernel with the weight transposed? That would be:\n\n        x = triton_linear(x, self.fc1.weight.t(), self.fc1.bias)\n\n 45. This avoids having to change the parameters. We can leave self.fc1 as is and just use its parameters in our custom kernel.\n\n 46. So in the forward of the model, we change:\n\n        # Instead of: x = F.relu(self.fc1(x))\n        x = F.relu(triton_linear(x, self.fc1.weight, self.fc1.bias))\n\n 47. But note: the original self.fc1(x) does: F.linear(x, self.fc1.weight, self.fc1.bias) which is: x @ weight.t() + bias.\n\n 48. So if we write a kernel that does: x @ (weight.t()) + bias, then we can avoid transposing the weight by passing the weight as is and then in the kernel we do the multiplication without explicit transpose? Actually, we can precompute the transposed weight and pass it? Or we can pass the weight and then the kernel will do the transpose? That would be inefficient.\n\n 49. We can pre-transpose the weight and store it? But the weight might be updated during training. So we would have to transpose every forward? That would add overhead.\n\n 50. Alternatively, we can design the kernel to handle the transposed weight by reading it in a transposed manner? That is, we can consider the weight matrix as stored in row-major but we want to access it as if it were column-major? This is equivalent to transposing the access.\n\n 51. We can write the kernel to multiply A (x) [m, k] by B (weight) [n, k] (because it is stored as [out_features, in_features]) and produce C [m, n]. This is standard for matrix multiplication without transposing B.\n\n 52. The operation: C = A @ B.t()   is equivalent to: C[i, j] = sum_k A[i, k] * B[j, k]\n\n 53. So in the kernel, we can load B in a transposed way by swapping the indices? Actually, we can use the same kernel as for A @ B but by passing the strides appropriately.\n\n 54. We can use the same kernel for both by specifying the strides. We can have a kernel that does:\n\n        C = A @ B   if B is [k, n]   (column-major for B, meaning each column is contiguous)\n        but if we want C = A @ B^T, then we can pass B as [n, k] and then use the stride to read it row by row? Actually, we can pass the stride for B as (1, k) if it is row-major? \n\n 55. However, the problem says: \"\u274c NEVER change @triton.jit function signatures or parameter names\" but we are writing a new kernel. So we design the kernel to take the weight in the original form (without transposing).\n\n 56. We design the kernel for the operation: C = A @ B^T + bias\n\n        where A: [m, k] (row-major: stride0 = k, stride1 = 1)\n        B: [n, k] (row-major: stride0 = k, stride1 = 1)   -> because the original weight is [n, k] (n=120, k=400)\n        Then C: [m, n]\n\n 57. We can write a kernel that uses the standard matrix multiplication algorithm for A and B^T (which is equivalent to B being stored in row-major and we want to use it as if it were column-major?).\n\n 58. Alternatively, we can write the kernel to do:\n\n        for i in range(m):\n            for j in range(n):\n                c = 0\n                for l in range(k):\n                    a = A[i, l]\n                    b = B[j, l]   # because B is stored row-major and we are reading along the row -> that is the same as the l-th element of the j-th row of B, which is the same as the j-th column of B^T.\n                C[i, j] = c + bias[j]\n\n 59. So we can use a kernel that loads the row of A and the row of B (which is the same as the column of B^T).\n\n 60. We will write a Triton kernel for this.\n\n 61. We note the matrix sizes: m (batch size) is 1 in the test, k=400, n=120. This is a very small matrix. We can use a single block? Or multiple blocks.\n\n 62. We can assign one program per row of A? Then we would have m programs. Each program computes one row of C.\n\n 63. For each row i, we compute the entire row of C (n=120 elements). We can use a block of 120 threads? But 120 is less than the maximum block size (1024). So we can do:\n\n        pid = tl.program_id(0)   # over m\n        if pid >= m: return\n\n        for j in range(0, n, BLOCK_SIZE_N):\n            ... vectorized ...\n\n 64. We can also parallelize over j.\n\n 65. We design:\n\n        kernel_linear(C, A, B, bias, m, n, k, ...)\n\n        where:\n            C: output [m, n]\n            A: [m, k]\n            B: [n, k]   (the weight matrix in original form: [out_features, in_features])\n            bias: [n]\n\n        Steps:\n            pid = tl.program_id(0)  # over m\n            if pid >= m: return\n\n            # We are going to compute one row of C: C[pid, :]\n            # We will iterate over the k dimension in blocks.\n\n        Alternatively, we can do a block that computes a tile of [BLOCK_M, BLOCK_N] for C? But m is 1 in the test, so we only have one row.\n\n        Since the batch size is 1 in the test, we can use a grid of (ceil(m / BLOCK_M), ceil(n / BLOCK_N)) and then each block computes a tile.\n\n        But the problem says: \"NEVER modify grid configuration\" -> but we are writing the kernel and the grid? Actually, the problem says we cannot change the grid configuration that is passed? We are free to design the grid.\n\n        However, note: the constraint says: \"\u274c NEVER modify grid configuration, output tensor shapes, or PID logic\" in the context of the Triton kernel. But we are writing the kernel from scratch.\n\n        Actually, the constraint is for the existing Triton kernel that we are modifying. Since there isn't one, we are free to design.\n\n        But the problem says: \"Modify ONLY the Triton kernel source within this block\". So we are to write a Triton kernel and the necessary wrapper.\n\n 66. We write a kernel that uses a 2D grid: \n        grid = (triton.cdiv(m, BLOCK_M), triton.cdiv(n, BLOCK_N))\n\n        and each block computes a tile of size [BLOCK_M, BLOCK_N] of C.\n\n 67. We'll use the standard matrix multiplication kernel.\n\n 68. We note: the kernel must be efficient for small matrices? But the problem says the input is 1x400 and weight is 120x400.\n\n 69. We choose BLOCK_M and BLOCK_N appropriately. Since m is 1, we set BLOCK_M=1. Then we only have one row. Then we can set BLOCK_N=128 (which is >=120) so that we cover the entire row in one block.\n\n 70. Alternatively, we can set BLOCK_M=1 and BLOCK_N=32, then we have 4 blocks (120/32=3.75 -> 4). But that might be inefficient.\n\n 71. We try to cover the entire row in one block.\n\n 72. We write the kernel for the operation:\n\n        c = 0\n        for l in range(0, k, BLOCK_K):\n            a = load A[pid, l:l+BLOCK_K]   # [BLOCK_K]\n            b = load B[:, l:l+BLOCK_K]   but we are only loading a block of columns? and we are computing a tile.\n\n        Actually, we are computing a tile of C: [BLOCK_M, BLOCK_N] by accumulating over blocks of K.\n\n 73. Since BLOCK_M=1 and we are doing the entire row, we can do:\n\n        row = pid   # which row of A and C we are computing.\n\n        for j in range(0, n, BLOCK_N):\n            # j: starting column index for the current tile\n            # We are going to compute C[row, j:j+BLOCK_N]\n\n        But note: we are accumulating over k. We can break k into blocks and for each block, we load a block of A and a block of B.\n\n 74. We design:\n\n        pid = tl.program_id(0)   # over the rows of C: m\n        pid_n = tl.program_id(1)   # over the columns of C: n, in steps of BLOCK_N\n\n        offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n        mask_n = offs_n < n\n\n        c = tl.zeros((BLOCK_N,), dtype=tl.float32)\n\n        for block_idx in range(0, k, BLOCK_K):\n            offs_k = block_idx + tl.arange(0, BLOCK_K)\n            mask_k = offs_k < k\n\n            a = tl.load(A + pid * stride_am + offs_k, mask=mask_k, other=0.0)\n            # For B: we need to load a block of rows? Actually, we are at a block of columns (offs_k) and we want to load the entire block of B for the columns offs_k and rows offs_n? But that would be a [BLOCK_N, BLOCK_K] block. We don't have enough memory.\n\n        Alternatively, we can load one row of A (which is of length k) and then we want to multiply by the corresponding block of the rows of B? Actually, we are computing the dot product of the row of A with each column of B (which is a row of B^T). \n\n        We can do:\n\n            b = tl.load(B + offs_n[:, None] * stride_bn + offs_k[None, :] * stride_bk, mask=mask_n[:,None] & mask_k[None,:], other=0.0)\n\n            Then we do: \n                c += tl.sum(a[None, :] * b, axis=1)\n\n        But note: the stride for B: \n            B is [n, k] -> row-major: stride0 = k, stride1 = 1.\n            So: \n                B[offs_n, offs_k] = B + offs_n * k + offs_k\n\n        Then we add the bias: at the end, we do:\n            c = c + tl.load(bias + offs_n, mask=mask_n, other=0.0)\n\n        Then we store C[pid, offs_n] = c\n\n 75. But note: the entire row of A is reused for every column in the tile? Yes, so we can load the row of A once per block of columns? Actually, we are breaking k into blocks. We load a block of the row of A and then the corresponding block of each row of B? Then we accumulate.\n\n 76. We can do:\n\n        c = 0\n        for block_idx in range(0, k, BLOCK_K):\n            offs_k = block_idx + tl.arange(0, BLOCK_K)\n            mask_k = offs_k < k\n\n            a = tl.load(A + pid * stride_am + offs_k, mask=mask_k, other=0.0)   # [BLOCK_K]\n\n            # Now we want to load a block of B: [BLOCK_N, BLOCK_K] at the current block of columns and the current block of k.\n            b = tl.load(B + offs_n[:, None] * stride_bn + offs_k[None, :] * stride_bk, mask=mask_n[:,None] & mask_k[None,:], other=0.0)   # [BLOCK_N, BLOCK_K]\n\n            # Then we compute: for each j in BLOCK_N, the dot product of a and b[j, :] is the sum of a * b[j, :]\n            # We can do:\n            c_block = tl.sum(a[None, :] * b, axis=1)   # [BLOCK_N]\n            c += c_block\n\n        Then add bias.\n\n 77. However, this might not be efficient because we are doing a reduction along the k dimension in the inner loop. And we are using a double loop: one over k blocks and one over the BLOCK_N.\n\n 78. We can try to use tiling in shared memory? But the problem does not require a huge speedup and the matrix is small.\n\n 79. We choose BLOCK_K = 32, BLOCK_N = 128 (if n<=128, we can set BLOCK_N=128 and then only one block in the n dimension).\n\n 80. Since n=120, we can set BLOCK_N=128 and then we have one block in the n dimension per row.\n\n 81. Then the grid would be (m, 1). For m=1, we have one block.\n\n 82. We write the kernel accordingly.\n\n 83. Steps for the kernel:\n\n        pid = tl.program_id(0)   # row index in C (which is the same as the row index in A)\n        pid_n = tl.program_id(1)   # column block index for C\n\n        # We are going to compute the entire row of C? Not exactly, but we set BLOCK_N to cover the entire n.\n\n        offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)   # [0,1,...,BLOCK_N-1] -> then we mask\n        mask_n = offs_n < n\n\n        # Pointers for A and B\n        A_ptr = A + pid * stride_am   # points to the beginning of the row of A\n        # We'll accumulate in a register\n        acc = tl.zeros((BLOCK_N,), dtype=tl.float32)\n\n        # Loop over k in blocks of BLOCK_K\n        for k_idx in range(0, k, BLOCK_K):\n            offs_k = k_idx + tl.arange(0, BLOCK_K)\n            mask_k = offs_k < k\n\n            # Load a block of A: [BLOCK_K]\n            a = tl.load(A_ptr + offs_k, mask=mask_k, other=0.0)\n\n            # For B: we want to load a block of rows? Actually, we want to load the block of B at the current k block and the columns in offs_n.\n            # B is [n, k] -> so to get the element at (j, l): B + j * stride_bn + l * stride_bk\n            # We assume stride_bn = k, stride_bk = 1? \n            # Actually, the stride for the first dimension (n) is k, and for the second dimension (k) is 1.\n\n            # We create a pointer to the block in B: \n            B_ptr = B + offs_n[:, None] * stride_bn + offs_k[None, :] * stride_bk   # [BLOCK_N, BLOCK_K]\n\n            b = tl.load(B_ptr, mask=mask_n[:,None] & mask_k[None,:], other=0.0)\n\n            # Compute the product: a[None,:] * b -> [BLOCK_N, BLOCK_K] and then reduce along the k axis -> [BLOCK_N]\n            # We can do:\n            #   product = a[None, :] * b\n            #   c_block = tl.sum(product, axis=1)\n            # But note: we are using tl.sum on a small axis (BLOCK_K=32) and it might be efficient.\n\n            c_block = tl.sum(a[None, :] * b, axis=1)\n            acc += c_block\n\n        # Add bias\n        bias_ptr = bias + offs_n\n        b = tl.load(bias_ptr, mask=mask_n, other=0.0)\n        acc += b\n\n        # Write back to C\n        C_ptr = C + pid * stride_cm + offs_n * stride_cn\n        tl.store(C_ptr, acc, mask=mask_n)\n\n 84. We note that the kernel must be efficient for the given sizes.\n\n 85. We must set the grid in the wrapper function:\n\n        def triton_linear(x, weight, bias):\n            m, k = x.shape\n            n, k2 = weight.shape\n            assert k == k2\n            # Allocate output\n            y = torch.empty(m, n, device=x.device, dtype=x.dtype)\n            # Define grid\n            BLOCK_M = 1   # because we are assigning one row per block in the m dimension? Actually, we are using one block per row and per tile of n.\n            BLOCK_N = 128   # we choose 128 because n=120 and 128>120\n            BLOCK_K = 32\n            grid = (m, triton.cdiv(n, BLOCK_N))\n            # We assume that the n dimension is covered by one block because BLOCK_N=128 and n=120 -> triton.cdiv(120,128)=1.\n            _kernel_linear[grid](x, weight, bias, y, m, n, k, \n                                 x.stride(0), x.stride(1), \n                                 weight.stride(0), weight.stride(1),\n                                 y.stride(0), y.stride(1),\n                                 BLOCK_K=BLOCK_K)\n\n 86. We must also set the kernel to use the correct number of warps. Since BLOCK_N=128, we have 128 elements. We can use 128 threads? But one warp is 32 threads. So 4 warps (128 threads) per block.\n\n        num_warps = 4\n\n 87. We write the kernel with the above plan.\n\n 88. We then replace in the forward of the model:\n\n        # Instead of: x = F.relu(self.fc1(x))\n        x = F.relu(triton_linear(x, self.fc1.weight, self.fc1.bias))\n\n 89. We must be cautious: the original model uses self.fc1 which is an nn.Linear. We are using its weight and bias.\n\n 90. We test with the given test code.\n\n 91. However, note: the problem says the current program is the reference and we are to write a Triton implementation. We are introducing a Triton kernel for the fc1 layer.\n\n 92. We must ensure that the Triton kernel is numerically correct.\n\n 93. We will write the kernel and hope that it compiles and passes correctness.\n\n 94. Since the problem says we are in Phase 1 (compilation) and then Phase 2 (correctness), we focus on that.\n\n 95. We note that the kernel is for float32? The model uses float32? The test input is float32.\n\n 96. We write the kernel in float32.\n\n 97. Let's code accordingly.\n\n 98. We also note that the kernel must be efficient. We are hoping for a speedup.\n\n 99. We write the entire model again with the changes.\n\n 100. We are replacing the EVOLVE-BLOCK.\n\n 101. We note: the test code at the bottom (get_inputs and get_init_inputs) remains unchanged.\n\n 102. We are not changing the model's input and output.\n\n 103. We output the entire block.\n\n 104. We note: the kernel might be overkill for such a small matrix, but it is an exercise.\n\n 105. We hope that with the tensor cores on H100 we might get a speedup? But the matrix is too small for tensor cores. However, we are using FP32 and the tensor cores are for FP16. So we are not using tensor cores.\n\n 106. We might not get a speedup, but we are required to try.\n\n 107. Alternatively, we might consider that the original fc1 is implemented by cuBLAS which is highly optimized. We might be slower.\n\n 108. But the problem says: we are allowed to replace with Triton. We are in Phase 1 and 2.\n\n 109. We proceed.\n\n 110. We write the kernel and the wrapper.\n\n 111. We will also include the Triton kernel in the same block.\n\n 112. We must not change the class name: it must be ModelNew.\n\n 113. We restructure the code:\n\n        import ... (including triton and triton.language as tl)\n\n        @triton.jit\n        def _kernel_linear(...):\n            ... (code above)\n\n        def triton_linear(x, weight, bias):\n            ... (wrapper)\n\n        class ModelNew(nn.Module):\n            ... (same as before, but in the forward, replace the fc1 call)\n\n 114. We note: the test code uses batch_size=1. We must handle any batch size.\n\n 115. We write the kernel accordingly.\n\n 116. We choose the block sizes as constants in the wrapper? Or we can autotune? The problem says we are allowed to do adaptive tuning.\n\n 117. We will start with fixed BLOCK_K=32 and BLOCK_N=128.\n\n 118. We note: the kernel must be generic for any m, n, k.\n\n 119. We write the kernel and the wrapper.\n\n 120. We hope it compiles.\n\n 121. We note: the kernel does not require any synchronization within the block because we are using one row per block and the entire row is computed by one block? Actually, we are using a 2D grid but each block is independent.\n\n 122. We output the code.\n\n 123. Given the complexity and the constraints on the number of lines, we hope we are within the limit.\n\n 124. We do not change the other layers.\n\n 125. We are making a small change in the forward (one line) and adding the kernel and the wrapper.\n\n 126. We output the entire block.\n\n 127. If we run out of time, we might have to simplify.\n\n Let's code accordingly.\n</think>\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef linear_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    m, n, k,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_K: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    x_row_ptr = x_ptr + pid_m * stride_xm\n    weight_col_ptr = weight_ptr + pid_n * stride_wn\n    \n    acc = tl.zeros((1,), dtype=tl.float32)\n    for block_idx in range(0, k, BLOCK_K):\n        k_offs = block_idx + tl.arange(0, BLOCK_K)\n        mask_k = k_offs < k\n        \n        x_val = tl.load(x_row_ptr + k_offs, mask=mask_k, other=0.0)\n        weight_val = tl.load(weight_col_ptr + k_offs, mask=mask_k, other=0.0)\n        \n        acc += tl.sum(x_val * weight_val)\n    \n    bias_val = tl.load(bias_ptr + pid_n)\n    acc += bias_val\n    \n    output_ptr = output_ptr + pid_m * stride_om + pid_n * stride_on\n    tl.store(output_ptr, acc)\n\ndef triton_linear(x, weight, bias):\n    m, k = x.shape\n    n, k2 = weight.shape\n    assert k == k2, \"Incompatible dimensions\"\n    \n    output = torch.empty((m, n), device=x.device, dtype=x.dtype)\n    \n    BLOCK_K = 32\n    BLOCK_N = 128\n    grid = (m, triton.cdiv(n, BLOCK_N))\n    \n    linear_kernel[grid](\n        x, weight, bias, output,\n        m, n, k,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        output.stride(0), output.stride(1),\n        BLOCK_K=BLOCK_K, BLOCK_N=BLOCK_N\n    )\n    \n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n        self.fc2 = nn.Linear(in_features=120, out_features=84)\n        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n    \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        x = x.view(-1, 16*5*5)\n        \n        # Replace fc1 with Triton implementation\n        x = F.relu(triton_linear(x, self.fc1.weight, self.fc1.bias))\n        \n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# Test code for the LeNet-5 model\nbatch_size = 1\nnum_classes = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, 1, 32, 32)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "32aec75c-0a41-43a3-9740-21400f2d192e", "code": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 2048}, num_warps=8),\n    ],\n    key=['n_cols'],\n)\n@triton.jit\ndef _per_row_quantize_int8_kernel(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    stride_x_row,\n    stride_output_row,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    row_start = pid * stride_x_row\n    output_start = pid * stride_output_row\n    \n    row_max = 0.0\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = tl.arange(0, BLOCK_SIZE)\n        mask = col_offset + col_idx < n_cols\n        \n        ptr = x_ptr + row_start + col_offset + col_idx\n        chunk = tl.load(ptr, mask=mask, other=0.0)\n        \n        abs_chunk = tl.abs(chunk)\n        chunk_max = tl.max(tl.where(mask, abs_chunk, 0.0), axis=0)\n        row_max = tl.maximum(row_max, chunk_max)\n    \n    row_max = tl.maximum(row_max, 1e-4)\n    scale = 127.0 / row_max\n    \n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = tl.arange(0, BLOCK_SIZE)\n        mask = col_offset + col_idx < n_cols\n        \n        ptr = x_ptr + row_start + col_offset + col_idx\n        chunk = tl.load(ptr, mask=mask, other=0.0)\n        \n        scaled = chunk * scale\n        rounded = tl.round(scaled)\n        clamped = tl.minimum(tl.maximum(rounded, -127), 127)\n        quantized = clamped.to(tl.int8)\n        \n        out_ptr = output_ptr + output_start + col_offset + col_idx\n        tl.store(out_ptr, quantized, mask=mask)\n    \n    tl.store(output_scale + pid, row_max / 127.0)\n\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    if x.dim() == 1:\n        x = x.view(1, -1)\n    \n    x = x.contiguous()\n    n_rows, n_cols = x.shape\n    output = torch.empty_like(x, dtype=torch.int8)\n    output_scale = torch.empty(n_rows, device=x.device, dtype=torch.float32)\n    \n    grid = (n_rows,)\n    _per_row_quantize_int8_kernel[grid](\n        x, output, output_scale,\n        x.stride(0), output.stride(0),\n        n_cols\n    )\n    \n    return output, output_scale", "language": "python", "parent_id": "082c3956-b274-44ca-865c-bea10fc43597", "generation": 2, "timestamp": 1754597171.1197805, "iteration_found": 26, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.972, "runtime_ms": 10.9, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 10.6, "speed_up": 0.972, "custom_timing": 10.9}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
{"id": "70208f68-63a6-43b4-a3f8-a9a18102a009", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, group normalization, leaky ReLU activation, and element-wise sum.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.hidden_size = hidden_size\n        self.weight = nn.Parameter(torch.empty(hidden_size, input_size))\n        self.bias = nn.Parameter(torch.empty(hidden_size))\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n        \n        # Initialize parameters\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n        # Pre-transpose weight for efficient access\n        self.register_buffer('weight_t', self.weight.t().contiguous())\n\n    def forward(self, x):\n        \"\"\"\n        Performs the forward pass of the model.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        # Convert to FP16 for Tensor Core acceleration\n        x_fp16 = x.to(torch.float16)\n        weight_fp16 = self.weight_t.to(torch.float16)\n        bias_fp16 = self.bias.to(torch.float16)\n        \n        # Allocate output tensor\n        output_fp16 = torch.empty(x.size(0), self.hidden_size, \n                                 device=x.device, dtype=torch.float16)\n        \n        # Launch Triton kernel for optimized matmul\n        grid = lambda META: (triton.cdiv(x.size(0), META['BLOCK_SIZE_M']) * \n                            triton.cdiv(self.hidden_size, META['BLOCK_SIZE_N']),)\n        \n        _matmul_kernel[grid](\n            x_fp16, weight_fp16, bias_fp16, output_fp16,\n            x.size(0), self.hidden_size, x.size(1),\n            x_fp16.stride(0), x_fp16.stride(1),\n            weight_fp16.stride(0), weight_fp16.stride(1),\n            output_fp16.stride(0), output_fp16.stride(1),\n            BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n        )\n        \n        # Convert back to FP32 for remaining operations\n        x = output_fp16.to(torch.float32)\n        x = self.gn(x)\n        x = self.leaky_relu(x)\n        x = x + x\n        return x\n\n# Triton matrix multiplication kernel with fused bias addition\n@triton.jit\ndef _matmul_kernel(\n    a_ptr, b_ptr, bias_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n    \n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=False)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    # Load bias and add to accumulator\n    bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += bias[None, :]\n    \n    # Convert to FP16 and store result\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    tl.store(c_ptrs, acc.to(tl.float16), \n            mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "96e947a1-3108-416f-929c-4cba9fe22dae", "generation": 1, "timestamp": 1754572498.455291, "iteration_found": 3, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "name 'math' is not defined\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/evaluator.py\", line 173, in _verification_worker\n    result: KernelExecResult = eval_kernel_against_ref(\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 484, in eval_kernel_against_ref\n    custom_model = ModelNew(*init_args, **init_kwargs)\n  File \"/tmp/tmpq5clrwep.py\", line 20, in __init__\n    nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\nNameError: name 'math' is not defined\n", "exec_log": "[Child] \u7ed1\u5b9a GPU 2, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 69, in <module>\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 844, in jit\n    return decorator(fn)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 832, in decorator\n    return JITFunction(\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 635, in __init__\n    self.starting_line_number = inspect.getsourcelines(fn)[1]\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/inspect.py\", line 1121, in getsourcelines\n    lines, lnum = findsource(object)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/inspect.py\", line 958, in findsource\n    raise OSError('could not get source code')\nOSError: could not get source code\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0745, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0821, "speed_up": 1.102, "custom_timing": 0.0745}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0745\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0821, 'speed_up': 1.102, 'custom_timing': 0.0745}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0745, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0821, 'speed_up': 1.102, 'custom_timing': 0.0745}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, group normalization, leaky ReLU activation, and element-wise sum.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n\n    def forward(self, x):\n        \"\"\"\n        Performs the forward pass of the model.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = self.fc(x)\n        x = self.gn(x)\n        x = self.leaky_relu(x)\n        x = x + x\n        return x\n\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0745), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0821, 'speed_up': 1.102, 'custom_timing': 0.0745})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, group normalization, leaky ReLU activation, and element-wise sum.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n\n    def forward(self, x):\n        \"\"\"\n        Performs the forward pass of the model.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = self.fc(x)\n        x = self.gn(x)\n        x = self.leaky_relu(x)\n        x = x + x\n        return x\n\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear layer (fc)\n  2. GroupNorm (gn)\n  3. LeakyReLU\n  4. Element-wise addition (x + x)\n\nThe current implementation is entirely in PyTorch and runs on the GPU. We are to rewrite the model using Triton for performance gains.\n\nHowever, note that the entire model is composed of high-level operations (linear, group norm, activation, add). \nWe cannot rewrite the entire model in one Triton kernel because it is a sequence of operations.\n\nBut note: the element-wise addition at the end (x = x + x) is a simple operation and can be fused with the activation in a custom kernel.\n\nHowever, the main computational cost is in the linear layer and the group normalization.\n\nWe are allowed to change only the Triton kernel source. But note that the current code does not have any Triton kernel.\n\nTherefore, we must introduce Triton kernels for the expensive operations.\n\nBut note: the problem says \"Modify ONLY the Triton kernel source within this block\". So we can add Triton kernels and change the forward method to use them.\n\nHowever, the current program does not have any Triton code. So we are allowed to add Triton kernels and call them.\n\nBut note: the constraints say \"\u274c NEVER change @triton.jit function signatures or parameter names\" but we are adding new ones.\n\nWe are also constrained by the input and output of the model: the input is (batch_size, input_size) and output is (batch_size, hidden_size).\n\nWe have two main operations that are expensive: the linear layer and the group norm.\n\nWe can try to fuse the entire forward pass into one Triton kernel? That might be complex.\n\nAlternatively, we can write Triton kernels for each operation? But note that Triton is especially good for element-wise and reduction operations. The linear layer is a matrix multiplication.\n\nGiven the complexity, we might start by replacing the linear layer with a Triton matmul kernel. However, note that the linear layer also has a bias? The current nn.Linear has a bias by default. But we don't see the bias in the __init__ because it's inside nn.Linear.\n\nLooking at the original code: \n    self.fc = nn.Linear(input_size, hidden_size)\n\nWe must account for the bias.\n\nBut note: the current model does not have an explicit bias in the forward pass? It's inside the nn.Linear.\n\nWe have two options:\n\nOption 1: Replace the linear layer with a Triton matmul kernel (including bias) and then write the rest in PyTorch.\n\nOption 2: Also replace the group norm and activation with Triton kernels? But note the element-wise add at the end is trivial.\n\nHowever, the problem says we can only change the Triton kernel source. We are allowed to change the forward method to use our Triton kernels? Yes, because the EVOLVE-BLOCK contains the entire class and functions.\n\nBut note: the constraints say \"\u274c NEVER change @triton.jit function signatures or parameter names\" for existing Triton kernels. Since we are adding new ones, we define the signatures.\n\nWe are going to:\n\n1. Write a Triton kernel for the linear layer (matrix multiplication with bias).\n2. Then, we can write a Triton kernel for the group normalization? But group normalization is a complex operation (mean and variance per group). Alternatively, we can leave it in PyTorch and focus on the matmul.\n\nHowever, note that the group norm is applied on the hidden_size dimension, which is 256, and the groups are 8. So each group has 32 channels. This is a small group and might be memory bound.\n\nGiven that the linear layer is the most expensive (O(batch_size * input_size * hidden_size)), we focus on that.\n\nWe will write a Triton matmul kernel for the linear layer and then use PyTorch for the rest.\n\nBut note: the element-wise operations (LeakyReLU and add) are cheap and can be left in PyTorch.\n\nWe must also note that the linear layer in Triton will require us to store the weight and bias in the model. We can no longer use nn.Linear.\n\nSo we change the __init__:\n\n  Instead of self.fc = nn.Linear(...), we will:\n      - initialize the weight and bias ourselves\n      - and then in the forward, call our Triton matmul kernel.\n\nWe must be cautious: the original nn.Linear uses a weight matrix of shape (hidden_size, input_size) and the input is (batch_size, input_size). The operation is: x @ weight.t() + bias.\n\nWe'll write a Triton kernel that does this.\n\nWe can use the standard matmul kernel from Triton examples, but note we have to add the bias.\n\nWe are going to write a kernel for:\n\n    out = x @ w.t() + bias   [bias is added to each row]\n\nWe can do:\n\n    for each row i in x (batch index) and each column j in w (which corresponds to output feature j):\n        out[i, j] = sum_k (x[i, k] * w[j, k]) + bias[j]\n\nWe can write a Triton kernel that processes blocks of the output matrix.\n\nWe'll use:\n\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)   # M is batch_size\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)   # N is hidden_size\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\nBut note: we are using H100 and want to use Tensor Cores? The problem says we have FP16 Tensor Cores. But our input is float32? \n\nWe have to check the input tensor. The input in get_inputs is torch.randn(batch_size, input_size) which is float32.\n\nWe can either keep it in float32 or convert to float16? But note the group norm is in float32.\n\nWe are going to do everything in float32 for now.\n\nAlternatively, we can use tf32 for the matmul? But note: Triton supports tf32 by setting the environment variable? Or we can use the dot instruction with fp32.\n\nBut to use Tensor Cores, we should use fp16. However, without changing the model's dtype, we might not get the benefit.\n\nWe have two choices:\n\n  a) Convert the input and weight to fp16, do the matmul in fp16, then convert back to fp32 for the group norm? This might be faster.\n\n  b) Do everything in fp32 and use tf32 (if supported).\n\nBut note: the problem says the hardware has FP16 Tensor Cores. So we should use fp16.\n\nHowever, the original model is in fp32. We must be cautious because changing the precision might break the correctness.\n\nBut the problem says: \"Output must match PyTorch reference within float32 tolerance\". If we do the matmul in fp16, the error might be too high.\n\nAlternatively, we can use the Triton kernel in fp32 but without Tensor Cores? That might not be faster.\n\nWe are going to try to write the kernel in fp32 and hope that Triton can use Tensor Cores for fp32 via tf32? But note: H100 supports tf32.\n\nBut note: the problem says \"FP16 Tensor Core\", so they might be optimized for fp16.\n\nGiven the constraints, we will write the kernel in fp16 if the input is converted to fp16. However, the input is fp32.\n\nWe can do:\n\n    x_fp16 = x.to(torch.float16)\n    w_fp16 = self.weight.to(torch.float16)\n    bias_fp16 = self.bias.to(torch.float16)   # but then we add in fp32? or we do everything in fp16?\n\nBut the group norm expects fp32.\n\nWe can do the matmul in fp16 and then convert the result to fp32 for the group norm. This is common practice.\n\nBut we must check the correctness tolerance.\n\nAlternatively, we can do the matmul in fp32 without Tensor Cores? That might not be faster than cuBLAS.\n\nWe are going to assume that the model is allowed to run in mixed precision. We will:\n\n  1. Convert the input to fp16.\n  2. Do the matmul in fp16 (using Tensor Cores) and then convert the result to fp32 and add the bias in fp32? Or do the bias in fp16 and then convert.\n\nBut note: the bias is in fp32? We stored it as fp32. We can convert it to fp16 and then do the addition in fp16, then convert to fp32.\n\nAlternatively, we can do:\n\n    out = (x_fp16 @ w_fp16.t()).to(torch.float32) + self.bias\n\nBut that would be two kernels (one matmul and one add). We want to fuse the bias.\n\nWe can write the Triton kernel to do the matmul and then add the bias in fp32? But then we cannot use the Tensor Cores for the entire operation? Because the matmul is in fp16 and then we have a conversion and then an add.\n\nAlternatively, we can do the entire operation (matmul and bias) in fp16 and then convert to fp32.\n\nWe'll do:\n\n    x_fp16 = x.to(torch.float16)\n    w_fp16 = self.weight.to(torch.float16)\n    bias_fp16 = self.bias.to(torch.float16)\n    out_fp16 = triton_matmul_add(x_fp16, w_fp16, bias_fp16)   # does: x @ w.t() + bias\n    out = out_fp16.to(torch.float32)\n\nBut note: the weight and bias are parameters, so we should store them in fp32 and convert on the fly.\n\nHowever, we are going to store the weight and bias in the model as fp32, and then in the forward we convert to fp16.\n\nWe can cache the fp16 weight and bias? But we have to be careful if the model is training. But the problem does not specify training. We assume inference.\n\nWe are going to write the Triton kernel for the matmul with bias in fp16.\n\nWe will write a kernel that takes:\n    x: (M, K) in fp16\n    w: (N, K) in fp16\n    bias: (N,) in fp16\n    output: (M, N) in fp16\n\nWe'll use a standard matrix multiplication kernel with bias added at the end.\n\nWe must be cautious about the block sizes and use of Tensor Cores.\n\nWe will use the following kernel structure (inspired by the Triton matmul example):\n\n    BLOCK_SIZE_M = 64\n    BLOCK_SIZE_N = 64\n    BLOCK_SIZE_K = 32   # for fp16, 32 is a common choice\n\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # pointers to the first block of A and B\n    x_ptr = x + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptr = w + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk   # note: w is (N, K) so we need to transpose? \n\nWait, we are doing x @ w.t(). So we can think of w as (K, N) in row-major? But we have stored w in (N, K). We can access it as:\n\n    w_ptr = w + (offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk)\n\nBut note: the stride for the second dimension of w is stride_wk, but we want to read contiguous in K? \n\nAlternatively, we can pre-transpose the weight matrix? But that would be done once in __init__.\n\nBut note: the weight is a parameter, so we can store it as (K, N) in row-major? Then we can avoid the transpose in the kernel.\n\nHowever, the original nn.Linear stores weight as (out_features, in_features). We want to avoid the transpose in the kernel.\n\nWe can store the weight in transposed form? Then the kernel becomes:\n\n    x: (M, K)   [contiguous]\n    w: (K, N)   [contiguous]   -> then x @ w is (M, N)\n\nSo we change the weight storage in __init__:\n\n    self.weight = nn.Parameter(torch.empty(hidden_size, input_size).normal_())\n    # but then we transpose it for the kernel? We can do:\n    self.weight_t = self.weight.t().contiguous()\n\nBut then we have to update it on every training step? But we are in inference? Then we can precompute the transpose.\n\nAlternatively, we can avoid storing the transpose and do the transpose in the kernel? That would be inefficient.\n\nWe decide to store the weight in the model as transposed and contiguous? Then we can do:\n\n    self.register_buffer('weight_t', self.weight.t().contiguous())\n\nBut note: we are not allowed to change the model's parameters? The problem says the model must be functionally equivalent.\n\nWe are changing the internal representation, but the input and output must be the same.\n\nAlternatively, we can do the transpose in the kernel without extra storage? We can change the access pattern.\n\nIn the kernel, we can access the weight matrix by swapping the indices? \n\n    w_ptr = w + (offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn)   # if w is stored as (N, K) then stride_wn is the stride for n? and stride_wk is the stride for k.\n\nBut then we want to read a block of (BLOCK_SIZE_K, BLOCK_SIZE_N) but we want to multiply with a block of (BLOCK_SIZE_M, BLOCK_SIZE_K). \n\nAlternatively, we can use the original matmul example that handles both transposed and non-transposed.\n\nWe'll use the kernel from the Triton matmul tutorial that handles A (M, K) and B (K, N) and computes C = A @ B.\n\nBut our B is the weight matrix stored as (N, K). We want to do A @ B^T? That is equivalent to (B @ A^T)^T? \n\nWe can instead do:\n\n    C = A @ B^T = (B @ A^T)^T\n\nBut then we would have to transpose the output? \n\nAlternatively, we can change the kernel to do A (M, K) and B (N, K) and compute:\n\n    for m, n:\n        C[m, n] = sum_k A[m, k] * B[n, k]\n\nThis is a valid matmul if we consider B as (K, N)^T? \n\nSo we can write the kernel without transposing the weight. We just read the weight by rows (each row is a feature vector).\n\nWe can do:\n\n    a_ptr = x_ptr + (offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n    b_ptr = w_ptr + (offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk)   # (BLOCK_SIZE_N, BLOCK_SIZE_K) but note: we want to read the same k for both\n\nThen we do:\n\n    a = tl.load(a_ptr, mask= ...)\n    b = tl.load(b_ptr, mask= ...)\n\n    Then we compute:\n\n        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, K, BLOCK_SIZE_K):\n            a = tl.load(...)\n            b = tl.load(...)\n            acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)   # but note: a is (M, K) and b is (N, K) -> we want (M, N) = a @ b^T\n\n        But the dot product in Triton expects two matrices and does a standard matmul? \n\n        Actually, the dot operation in Triton does:\n\n            acc = tl.dot(a, b)   # where a is (M, K) and b is (K, N) -> then we get (M, N)\n\n        But here we have b as (N, K). So we need to transpose it? \n\n        We can do: b = tl.trans(b)   -> then b is (K, N) and then we can do a @ b -> (M, N)\n\n        But note: we are reading b in a block of (BLOCK_SIZE_N, BLOCK_SIZE_K). We want to transpose that block to (BLOCK_SIZE_K, BLOCK_SIZE_N) for the dot product? \n\n        Alternatively, we can read b in a transposed way? We can change the pointer arithmetic for b to read by columns? But that would be non-contiguous.\n\n        We can instead change the kernel to do:\n\n            b_ptr = w_ptr + (offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn)   # then the block is (BLOCK_SIZE_K, BLOCK_SIZE_N) -> which is contiguous if the stride_wn is 1? \n\n        But the original weight matrix (N, K) is stored row-major: so the stride_wn = K, stride_wk = 1. \n\n        We want to read a block of (BLOCK_SIZE_N, BLOCK_SIZE_K) in row-major? Then we can use:\n\n            b_ptr = w_ptr + (offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk)   # then the block is (BLOCK_SIZE_N, BLOCK_SIZE_K) and stored in row-major.\n\n        Then we can transpose it to (BLOCK_SIZE_K, BLOCK_SIZE_N) for the matmul? \n\n        But note: the dot product in Triton expects the second matrix to be column-contiguous? Actually, it expects the inner dimension to be contiguous.\n\n        Alternatively, we can pre-transpose the weight matrix to (K, N) and then store it as column-major? But that would be non-standard.\n\nGiven the complexity, we decide to pre-transpose the weight matrix to (K, N) and then store it in contiguous memory. Then we can use the standard matmul kernel.\n\nWe do:\n\n    In __init__:\n        self.register_buffer('weight_t', self.weight.t().contiguous())   # now (input_size, hidden_size)\n\n    Then the matmul is: x @ weight_t   [which is (batch_size, input_size) @ (input_size, hidden_size) = (batch_size, hidden_size)]\n\n    Then we add the bias.\n\nSo we change the model:\n\n    Remove self.fc and replace with:\n        self.weight = nn.Parameter(torch.empty(hidden_size, input_size))\n        self.bias = nn.Parameter(torch.empty(hidden_size))\n        # initialize\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n        # and then create the transposed weight buffer (non-parameter) for the kernel\n        self.weight_t = self.weight.t().contiguous()\n\n    Then in forward, we do:\n        x = x.to(torch.float16)\n        weight_t = self.weight_t.to(torch.float16)\n        bias = self.bias.to(torch.float16)\n        output = triton_matmul(x, weight_t, bias)   # does x @ weight_t + bias\n        output = output.to(torch.float32)\n\nBut note: the original model used the linear layer without converting to fp16. We are introducing mixed precision.\n\nWe must hope that the mixed precision matmul is within tolerance.\n\nAlternatively, we can do the matmul in fp32? But then we cannot use Tensor Cores.\n\nWe'll try the fp16 version.\n\nNow, we write the Triton kernel for the matmul with bias.\n\nKernel for triton_matmul:\n\n    @triton.jit\n    def _matmul_kernel(\n        a_ptr, b_ptr, bias_ptr, c_ptr,\n        M, N, K,\n        stride_am, stride_ak,\n        stride_bk, stride_bn,   # because b is (K, N)\n        stride_cm, stride_cn,\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    ):\n        pid = tl.program_id(0)\n        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n        pid_m = pid // num_pid_n\n        pid_n = pid % num_pid_n\n\n        offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n        a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n        b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n            # Load the next block of A and B\n            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n            acc += tl.dot(a, b, allow_tf32=False)\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n\n        # Now add the bias. The bias is of shape (N,). We add to each row of acc.\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += bias[None, :]   # broadcast to each row\n\n        # Convert acc to fp16? The output is fp16.\n        acc = acc.to(tl.float16)\n\n        # Store the result\n        offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n        tl.store(c_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nBut note: the above kernel does the bias addition at the end. However, the bias is in fp16? We are adding it in fp32 and then converting to fp16.\n\nWe can also load the bias in fp16 and convert to fp32? Or do the entire accumulation in fp16? But that might be less accurate.\n\nWe'll do the accumulation in fp32 and then convert to fp16.\n\nWe must also note: the kernel does not handle the entire K dimension in one go? It uses a loop over blocks of K.\n\nNow, we call this kernel from a function:\n\n    def triton_matmul(a, b, bias):\n        # a: (M, K) fp16\n        # b: (K, N) fp16\n        # bias: (N,) fp16\n        M, K = a.shape\n        _, N = b.shape\n        c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        # We use fixed block sizes for now? Or we can autotune.\n        _matmul_kernel[grid](\n            a, b, bias, c,\n            M, N, K,\n            a.stride(0), a.stride(1),\n            b.stride(0), b.stride(1),\n            c.stride(0), c.stride(1),\n            BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n        )\n        return c\n\nBut note: we should autotune the block sizes. However, for simplicity we start with fixed.\n\nAlternatively, we can use the autotuner from the Triton example.\n\nWe'll leave the autotuning for later.\n\nNow, we integrate this into the model.\n\nWe must also note: the rest of the operations (group norm, leaky relu, add) remain in PyTorch.\n\nLet's write the new ModelNew:\n\n    class ModelNew(nn.Module):\n        def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n            super(ModelNew, self).__init__()\n            # Initialize weight and bias for the linear layer\n            self.weight = nn.Parameter(torch.empty(hidden_size, input_size))\n            self.bias = nn.Parameter(torch.empty(hidden_size))\n            # Initialize the parameters\n            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n            # Precompute the transposed weight\n            self.register_buffer('weight_t', self.weight.t().contiguous())\n\n            self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n            self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n\n        def forward(self, x):\n            # Convert input to fp16\n            x_fp16 = x.to(torch.float16)\n            # Convert weight_t and bias to fp16\n            weight_t = self.weight_t.to(torch.float16)\n            bias = self.bias.to(torch.float16)\n            # Do the matmul with bias using Triton\n            x = triton_matmul(x_fp16, weight_t, bias)\n            # Convert back to fp32 for the rest\n            x = x.to(torch.float32)\n            x = self.gn(x)\n            x = self.leaky_relu(x)\n            x = x + x\n            return x\n\nWe must also define the triton_matmul function and the kernel.\n\nBut note: the kernel must be defined at the top level? We can define it inside the EVOLVE-BLOCK? Yes.\n\nWe are going to write the entire code with the kernel and the helper function.\n\nHowever, note that the EVOLVE-BLOCK must contain the entire code for the model and the helper functions? Yes.\n\nBut also note: the original code had batch_size, input_size, ... and the get_inputs and get_init_inputs. We must keep them.\n\nWe'll write:\n\n    import math\n    import torch\n    import torch.nn as nn\n    import triton\n    import triton.language as tl\n\n    # Define the Triton kernel and helper function for matmul\n\n    @triton.jit\n    def _matmul_kernel(\n        a_ptr, b_ptr, bias_ptr, c_ptr,\n        M, N, K,\n        stride_am, stride_ak,\n        stride_bk, stride_bn,\n        stride_cm, stride_cn,\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    ):\n        ... (as above)\n\n    def triton_matmul(a, b, bias):\n        ... (as above)\n\n    class ModelNew(nn.Module):\n        ... (as above)\n\n    batch_size = 128\n    ... (rest unchanged)\n\nBut note: the kernel might be long and we are only allowed to change the EVOLVE-BLOCK. We are replacing the entire block.\n\nHowever, we must be cautious about the performance of the group norm. We are leaving it in PyTorch.\n\nWe hope that the speedup from the matmul in Triton will cover the overhead of the conversions and the rest.\n\nAlternatively, we can also write a Triton kernel for the group norm? But that is more complex.\n\nWe'll start with the matmul.\n\nBut note: the current model has a linear layer that is (512, 256) -> batch_size=128, so the matmul is 128x512x256.\n\nThis is a small matmul? But 128*512*256 = 16.777e6 operations. It might be memory bound? \n\nBut we are using Tensor Cores, so it should be fast.\n\nWe are going to write the entire code.\n\nWe must also note: the kernel must have boundary checks? We have included masks in the loads.\n\nWe are going to try this.\n\nBut note: the original model had an element-wise add (x = x + x) at the end. We are keeping that.\n\nWe must also note: the original model had a negative_slope for LeakyReLU. We are keeping that.\n\nLet's code.\n\nHowever, note that the problem says: \"NEVER remove boundary checks or out-of-bound masks\". We have included masks.\n\nBut note: the kernel for matmul might be complex and we are not autotuning. We might not get the best performance.\n\nBut for now, we hope for a speedup.\n\nWe are going to output the entire code.\n\nOne more thing: the kernel above uses a fixed BLOCK_SIZE. We might want to make it configurable.\n\nWe'll set:\n\n    BLOCK_SIZE_M = 64\n    BLOCK_SIZE_N = 64\n    BLOCK_SIZE_K = 32\n\nBut note: the problem says we can use adaptive tuning. We are not doing it now.\n\nWe might need to adjust these values for the specific problem.\n\nGiven the constraints of the problem, we output the code.\n\nNote: We are changing the model structure and the forward pass.\n\nBut note: the get_inputs and get_init_inputs remain the same.\n\nWe are going to write the code.\n\nIMPORTANT: The kernel must be defined at the top level? Yes, because Triton requires the kernel to be a top-level function.\n\nWe are going to put the kernel and the helper function inside the EVOLVE-BLOCK.\n\nBut note: the EVOLVE-BLOCK is a string that will be executed. We must be careful with the indentation.\n\nWe are going to write the entire code block.\n\nLet's do it.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, group normalization, leaky ReLU activation, and element-wise sum.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.hidden_size = hidden_size\n        self.weight = nn.Parameter(torch.empty(hidden_size, input_size))\n        self.bias = nn.Parameter(torch.empty(hidden_size))\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n        \n        # Initialize parameters\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n        # Pre-transpose weight for efficient access\n        self.register_buffer('weight_t', self.weight.t().contiguous())\n\n    def forward(self, x):\n        \"\"\"\n        Performs the forward pass of the model.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_size).\n\n        Returns:\n            Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        # Convert to FP16 for Tensor Core acceleration\n        x_fp16 = x.to(torch.float16)\n        weight_fp16 = self.weight_t.to(torch.float16)\n        bias_fp16 = self.bias.to(torch.float16)\n        \n        # Allocate output tensor\n        output_fp16 = torch.empty(x.size(0), self.hidden_size, \n                                 device=x.device, dtype=torch.float16)\n        \n        # Launch Triton kernel for optimized matmul\n        grid = lambda META: (triton.cdiv(x.size(0), META['BLOCK_SIZE_M']) * \n                            triton.cdiv(self.hidden_size, META['BLOCK_SIZE_N']),)\n        \n        _matmul_kernel[grid](\n            x_fp16, weight_fp16, bias_fp16, output_fp16,\n            x.size(0), self.hidden_size, x.size(1),\n            x_fp16.stride(0), x_fp16.stride(1),\n            weight_fp16.stride(0), weight_fp16.stride(1),\n            output_fp16.stride(0), output_fp16.stride(1),\n            BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n        )\n        \n        # Convert back to FP32 for remaining operations\n        x = output_fp16.to(torch.float32)\n        x = self.gn(x)\n        x = self.leaky_relu(x)\n        x = x + x\n        return x\n\n# Triton matrix multiplication kernel with fused bias addition\n@triton.jit\ndef _matmul_kernel(\n    a_ptr, b_ptr, bias_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n    \n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=False)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    # Load bias and add to accumulator\n    bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += bias[None, :]\n    \n    # Convert to FP16 and store result\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    tl.store(c_ptrs, acc.to(tl.float16), \n            mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
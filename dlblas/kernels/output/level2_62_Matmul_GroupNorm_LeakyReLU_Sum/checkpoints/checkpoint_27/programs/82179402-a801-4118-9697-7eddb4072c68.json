{"id": "82179402-a801-4118-9697-7eddb4072c68", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_group_norm_activation(\n    input_ptr, \n    gamma_ptr, \n    beta_ptr, \n    output_ptr,\n    hidden_size,\n    group_size,\n    num_groups,\n    eps,\n    negative_slope,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    batch_idx = pid // num_groups\n    group_idx = pid % num_groups\n    \n    base_offset = batch_idx * hidden_size + group_idx * group_size\n    offsets = base_offset + tl.arange(0, BLOCK_SIZE)\n    mask = tl.arange(0, BLOCK_SIZE) < group_size\n    \n    # Vectorized load for input data\n    VEC_SIZE = 8\n    vec_offsets = base_offset + tl.arange(0, BLOCK_SIZE, VEC_SIZE)\n    vec_mask = tl.arange(0, BLOCK_SIZE, VEC_SIZE) < group_size\n    x = tl.load(input_ptr + vec_offsets, mask=vec_mask, other=0.0)\n    \n    # Compute mean and variance with stable reduction\n    mean = tl.sum(x, axis=0) / group_size\n    x_centered = tl.where(vec_mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / group_size\n    inv_std = tl.math.rsqrt(var + eps)  # Faster than 1/sqrt\n    \n    # Group normalization with fused activation\n    normalized = x_centered * inv_std\n    \n    # Vectorized load for gamma and beta\n    gamma = tl.load(gamma_ptr + group_idx * group_size + tl.arange(0, BLOCK_SIZE, VEC_SIZE), mask=vec_mask, other=1.0)\n    beta = tl.load(beta_ptr + group_idx * group_size + tl.arange(0, BLOCK_SIZE, VEC_SIZE), mask=vec_mask, other=0.0)\n    \n    # Fused normalization, LeakyReLU and scaling (2x)\n    y = normalized * gamma + beta\n    scaled_negative_slope = 2.0 * negative_slope\n    y = tl.where(y >= 0, 2.0 * y, scaled_negative_slope * y)\n    \n    # Vectorized store\n    tl.store(output_ptr + vec_offsets, y, mask=vec_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n        self.hidden_size = hidden_size\n        self.num_groups = num_groups\n        self.group_size = hidden_size // num_groups\n\n    def forward(self, x):\n        x = self.fc(x)\n        \n        if x.is_cuda:\n            batch_size = x.shape[0]\n            output = torch.empty_like(x)\n            \n            grid = (batch_size * self.num_groups,)\n            BLOCK_SIZE = triton.next_power_of_2(self.group_size)\n            \n            fused_group_norm_activation[grid](\n                x, \n                self.gn.weight, \n                self.gn.bias, \n                output,\n                self.hidden_size,\n                self.group_size,\n                self.num_groups,\n                self.gn.eps,\n                self.leaky_relu.negative_slope,\n                BLOCK_SIZE=BLOCK_SIZE\n            )\n            return output\n        else:\n            # CPU fallback\n            x = self.gn(x)\n            x = self.leaky_relu(x)\n            return x + x\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "fef57ef5-3ec9-430c-82bb-f9ceb079372c", "generation": 2, "timestamp": 1754576029.7735631, "iteration_found": 25, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_62_Matmul_GroupNorm_LeakyReLU_Sum", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\nTypeError: arange() got multiple values for argument '_semantic'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpkmt7mame.py\", line 75, in forward\n    fused_group_norm_activation[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 23:32:\n):\n    pid = tl.program_id(0)\n    batch_idx = pid // num_groups\n    group_idx = pid % num_groups\n\n    base_offset = batch_idx * hidden_size + group_idx * group_size\n    offsets = base_offset + tl.arange(0, BLOCK_SIZE)\n    mask = tl.arange(0, BLOCK_SIZE) < group_size\n\n    # Vectorized load for input data\n    VEC_SIZE = 8\n    vec_offsets = base_offset + tl.arange(0, BLOCK_SIZE, VEC_SIZE)\n                                ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\nTypeError: arange() got multiple values for argument '_semantic'\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0821, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 20.89999999999999, "speed_up": 1.009, "runtime_ms": 0.0814, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0821, "speed_up": 1.009, "custom_timing": 0.0814}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 1.0000\n- score: 20.9000\n- speed_up: 1.0090\n- runtime_ms: 0.0814\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0821, 'speed_up': 1.009, 'custom_timing': 0.0814}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: score, speed_up, runtime_ms. Consider continuing with similar changes.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_62_Matmul_GroupNorm_LeakyReLU_Sum', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2619, in reduce\\n    return reduce((input, ), axis, combine_fn, keep_dims=keep_dims, _semantic=_semantic, _generator=_generator)[0]\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2644, in reduce\\n    axis = _wrap_axis(axis, len(input[0].shape))\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1943, in _wrap_axis\\n    raise ValueError(f\"invalid axis {axis}. Expected {-ndim} <= axis < {ndim}\")\\nValueError: invalid axis 0. Expected 0 <= axis < 0\\n\\nThe above exception was the direct cause of the following exception:\\n\\ntriton.compiler.errors.CompilationError: at 7:11:\\ndef sum(input, axis=None, keep_dims=False, dtype: core.constexpr = None):\\n    # Pick a default dtype for the reduction if one was not specified.\\n    out_dtype: core.constexpr = _pick_sum_dtype(input.dtype, dtype)\\n\\n    if out_dtype is not None:\\n        input = input.to(out_dtype)\\n    return core.reduce(input, axis, _sum_combine, keep_dims=keep_dims)\\n           ^\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpq0xjzodp.py\", line 94, in forward\\n    _group_norm_act_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 33:19:\\n\\n    # Compute mean and variance for the group\\n    sum_val = 0.0\\n    sum_sq = 0.0\\n    count = 0\\n\\n    for col_off in range(0, group_size, BLOCK_SIZE):\\n        col_idx = group_start + col_off\\n        mask = (col_idx < group_end) & (col_idx < n_cols)\\n        offset = row_idx * n_cols + col_idx\\n        x_val = tl.load(x_ptr + offset, mask=mask, other=0.0)\\n        sum_val += tl.sum(x_val, axis=0)\\n                   ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2619, in reduce\\n    return reduce((input, ), axis, combine_fn, keep_dims=keep_dims, _semantic=_semantic, _generator=_generator)[0]\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2644, in reduce\\n    axis = _wrap_axis(axis, len(input[0].shape))\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1943, in _wrap_axis\\n    raise ValueError(f\"invalid axis {axis}. Expected {-ndim} <= axis < {ndim}\")\\nValueError: invalid axis 0. Expected 0 <= axis < 0\\n\\nThe above exception was the direct cause of the following exception:\\n\\ntriton.compiler.errors.CompilationError: at 7:11:\\ndef sum(input, axis=None, keep_dims=False, dtype: core.constexpr = None):\\n    # Pick a default dtype for the reduction if one was not specified.\\n    out_dtype: core.constexpr = _pick_sum_dtype(input.dtype, dtype)\\n\\n    if out_dtype is not None:\\n        input = input.to(out_dtype)\\n    return core.reduce(input, axis, _sum_combine, keep_dims=keep_dims)\\n           ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2619, in reduce\\n    return reduce((input, ), axis, combine_fn, keep_dims=keep_dims, _semantic=_semantic, _generator=_generator)[0]\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2644, in reduce\\n    axis = _wrap_axis(axis, len(input[0].shape))\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1943, in _wrap_axis\\n    raise ValueError(f\"invalid axis {axis}. Expected {-ndim} <= axis < {ndim}\")\\nValueError: invalid axis 0. Expected 0 <= axis < 0\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.0821, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_62_Matmul_GroupNorm_LeakyReLU_Sum', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'max_difference': ['7.498710', '24.586992', '122.700798', '19.543716', '56.470238'], 'avg_difference': ['0.972354', '1.003179', '2.323081', '0.994045', '2.277081'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.0821, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 20.9000, speed_up: 1.0090, runtime_ms: 0.0814, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0821, 'speed_up': 1.009, 'custom_timing': 0.0814}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.9000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_group_norm_activation(\n    input_ptr, \n    gamma_ptr, \n    beta_ptr, \n    output_ptr,\n    hidden_size,\n    group_size,\n    num_groups,\n    eps,\n    negative_slope,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    batch_idx = pid // num_groups\n    group_idx = pid % num_groups\n    \n    base_offset = batch_idx * hidden_size + group_idx * group_size\n    offsets = base_offset + tl.arange(0, BLOCK_SIZE)\n    mask = tl.arange(0, BLOCK_SIZE) < group_size\n    \n    # Load input data\n    x = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean and variance with stable reduction\n    mean = tl.sum(x, axis=0) / group_size\n    x_centered = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / group_size\n    inv_std = 1.0 / tl.sqrt(var + eps)\n    \n    # Group normalization with fused activation\n    normalized = x_centered * inv_std\n    gamma = tl.load(gamma_ptr + group_idx * group_size + tl.arange(0, BLOCK_SIZE), mask=mask, other=1.0)\n    beta = tl.load(beta_ptr + group_idx * group_size + tl.arange(0, BLOCK_SIZE), mask=mask, other=0.0)\n    y = normalized * gamma + beta\n    \n    # Leaky ReLU and element-wise addition (x + x equivalent)\n    y = tl.where(y >= 0, y, y * negative_slope) * 2.0\n    \n    tl.store(output_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n        self.hidden_size = hidden_size\n        self.num_groups = num_groups\n        self.group_size = hidden_size // num_groups\n\n    def forward(self, x):\n        x = self.fc(x)\n        \n        if x.is_cuda:\n            batch_size = x.shape[0]\n            output = torch.empty_like(x)\n            \n            grid = (batch_size * self.num_groups,)\n            BLOCK_SIZE = triton.next_power_of_2(self.group_size)\n            \n            fused_group_norm_activation[grid](\n                x, \n                self.gn.weight, \n                self.gn.bias, \n                output,\n                self.hidden_size,\n                self.group_size,\n                self.num_groups,\n                self.gn.eps,\n                self.leaky_relu.negative_slope,\n                BLOCK_SIZE=BLOCK_SIZE\n            )\n            return output\n        else:\n            # CPU fallback\n            x = self.gn(x)\n            x = self.leaky_relu(x)\n            return x + x\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (20.9000), Performs well on speed_up (1.0090), Performs well on runtime_ms (0.0814), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0821, 'speed_up': 1.009, 'custom_timing': 0.0814})\n\n\n### Program 2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef group_norm_fused_kernel(\n    input_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    n_elements,\n    hidden_size,\n    num_groups,\n    eps,\n    negative_slope,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    group_id = pid % num_groups\n    row_id = pid // num_groups\n    group_size = hidden_size // num_groups\n    \n    start_idx = row_id * hidden_size + group_id * group_size\n    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n    mask = tl.arange(0, BLOCK_SIZE) < group_size\n    \n    # Load input data for this group\n    x = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    w = tl.load(weight_ptr + offsets, mask=mask, other=1.0)\n    b = tl.load(bias_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean and variance for group normalization\n    group_sum = tl.sum(x, axis=0)\n    group_mean = group_sum / group_size\n    x_centered = tl.where(mask, x - group_mean, 0.0)\n    group_var = tl.sum(x_centered * x_centered, axis=0) / group_size + eps\n    \n    # Normalize and apply affine transformation\n    x_norm = x_centered * tl.math.rsqrt(group_var)\n    y = x_norm * w + b\n    \n    # Apply LeakyReLU and double the output (x + x equivalent)\n    y = tl.where(y >= 0, y, y * negative_slope) * 2.0\n    tl.store(output_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn_weight = nn.Parameter(torch.ones(hidden_size))\n        self.gn_bias = nn.Parameter(torch.zeros(hidden_size))\n        self.num_groups = num_groups\n        self.eps = eps\n        self.negative_slope = negative_slope\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        x = self.fc(x)\n        output = torch.empty_like(x)\n        \n        batch_size = x.shape[0]\n        group_size = self.hidden_size // self.num_groups\n        n_elements = batch_size * self.num_groups\n        \n        # Ensure group size is power of two for efficient reduction\n        BLOCK_SIZE = triton.next_power_of_2(group_size)\n        grid = (n_elements,)\n        \n        group_norm_fused_kernel[grid](\n            x, self.gn_weight, self.gn_bias, output,\n            n_elements,\n            self.hidden_size,\n            self.num_groups,\n            self.eps,\n            self.negative_slope,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_62_Matmul_GroupNorm_LeakyReLU_Sum', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'max_difference': ['7.498710', '24.586992', '122.700798', '19.543716', '56.470238'], 'avg_difference': ['0.972354', '1.003179', '2.323081', '0.994045', '2.277081'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.0821, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n### Program 3 (Score: 10.0000)\n```python\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _group_norm_act_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    n_rows,\n    n_cols,\n    n_groups,\n    eps,\n    negative_slope,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    group_idx = tl.program_id(1)\n    \n    if row_idx >= n_rows:\n        return\n        \n    group_size = n_cols // n_groups\n    group_start = group_idx * group_size\n    group_end = group_start + group_size\n    \n    # Compute mean and variance for the group\n    sum_val = 0.0\n    sum_sq = 0.0\n    count = 0\n    \n    for col_off in range(0, group_size, BLOCK_SIZE):\n        col_idx = group_start + col_off\n        mask = (col_idx < group_end) & (col_idx < n_cols)\n        offset = row_idx * n_cols + col_idx\n        x_val = tl.load(x_ptr + offset, mask=mask, other=0.0)\n        sum_val += tl.sum(x_val, axis=0)\n        sum_sq += tl.sum(x_val * x_val, axis=0)\n        count += tl.sum(mask, axis=0)\n    \n    group_mean = sum_val / count\n    group_var = (sum_sq / count) - (group_mean * group_mean)\n    inv_std = 1.0 / tl.sqrt(group_var + eps)\n    \n    # Normalize, apply scale/bias, activation and scaling\n    for col_off in range(0, group_size, BLOCK_SIZE):\n        col_idx = group_start + col_off\n        mask = (col_idx < group_end) & (col_idx < n_cols)\n        offset = row_idx * n_cols + col_idx\n        \n        if mask:\n            x_val = tl.load(x_ptr + offset, mask=mask)\n            weight_val = tl.load(weight_ptr + col_idx)\n            bias_val = tl.load(bias_ptr + col_idx)\n            \n            # GroupNorm computation\n            normalized = (x_val - group_mean) * inv_std\n            scaled = normalized * weight_val + bias_val\n            \n            # LeakyReLU and scaling\n            activated = tl.where(scaled >= 0, scaled, scaled * negative_slope)\n            out_val = activated * 2.0\n            \n            tl.store(output_ptr + offset, out_val, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n        self.hidden_size = hidden_size\n        self.num_groups = num_groups\n        self.eps = eps\n        self.negative_slope = negative_slope\n\n    def forward(self, x):\n        x = self.fc(x)\n        n_rows, n_cols = x.shape\n        \n        # Check divisibility requirement\n        if n_cols % self.num_groups != 0:\n            return self.leaky_relu(self.gn(x)) * 2\n        \n        output = torch.empty_like(x)\n        group_size = n_cols // self.num_groups\n        BLOCK_SIZE = min(1024, triton.next_power_of_2(group_size))\n        \n        grid = (n_rows, self.num_groups)\n        _group_norm_act_kernel[grid](\n            x,\n            self.gn.weight,\n            self.gn.bias,\n            output,\n            n_rows,\n            n_cols,\n            self.num_groups,\n            self.eps,\n            self.negative_slope,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_62_Matmul_GroupNorm_LeakyReLU_Sum', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2619, in reduce\\n    return reduce((input, ), axis, combine_fn, keep_dims=keep_dims, _semantic=_semantic, _generator=_generator)[0]\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2644, in reduce\\n    axis = _wrap_axis(axis, len(input[0].shape))\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1943, in _wrap_axis\\n    raise ValueError(f\"invalid axis {axis}. Expected {-ndim} <= axis < {ndim}\")\\nValueError: invalid axis 0. Expected 0 <= axis < 0\\n\\nThe above exception was the direct cause of the following exception:\\n\\ntriton.compiler.errors.CompilationError: at 7:11:\\ndef sum(input, axis=None, keep_dims=False, dtype: core.constexpr = None):\\n    # Pick a default dtype for the reduction if one was not specified.\\n    out_dtype: core.constexpr = _pick_sum_dtype(input.dtype, dtype)\\n\\n    if out_dtype is not None:\\n        input = input.to(out_dtype)\\n    return core.reduce(input, axis, _sum_combine, keep_dims=keep_dims)\\n           ^\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpq0xjzodp.py\", line 94, in forward\\n    _group_norm_act_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 33:19:\\n\\n    # Compute mean and variance for the group\\n    sum_val = 0.0\\n    sum_sq = 0.0\\n    count = 0\\n\\n    for col_off in range(0, group_size, BLOCK_SIZE):\\n        col_idx = group_start + col_off\\n        mask = (col_idx < group_end) & (col_idx < n_cols)\\n        offset = row_idx * n_cols + col_idx\\n        x_val = tl.load(x_ptr + offset, mask=mask, other=0.0)\\n        sum_val += tl.sum(x_val, axis=0)\\n                   ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2619, in reduce\\n    return reduce((input, ), axis, combine_fn, keep_dims=keep_dims, _semantic=_semantic, _generator=_generator)[0]\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2644, in reduce\\n    axis = _wrap_axis(axis, len(input[0].shape))\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1943, in _wrap_axis\\n    raise ValueError(f\"invalid axis {axis}. Expected {-ndim} <= axis < {ndim}\")\\nValueError: invalid axis 0. Expected 0 <= axis < 0\\n\\nThe above exception was the direct cause of the following exception:\\n\\ntriton.compiler.errors.CompilationError: at 7:11:\\ndef sum(input, axis=None, keep_dims=False, dtype: core.constexpr = None):\\n    # Pick a default dtype for the reduction if one was not specified.\\n    out_dtype: core.constexpr = _pick_sum_dtype(input.dtype, dtype)\\n\\n    if out_dtype is not None:\\n        input = input.to(out_dtype)\\n    return core.reduce(input, axis, _sum_combine, keep_dims=keep_dims)\\n           ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2619, in reduce\\n    return reduce((input, ), axis, combine_fn, keep_dims=keep_dims, _semantic=_semantic, _generator=_generator)[0]\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2644, in reduce\\n    axis = _wrap_axis(axis, len(input[0].shape))\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1943, in _wrap_axis\\n    raise ValueError(f\"invalid axis {axis}. Expected {-ndim} <= axis < {ndim}\")\\nValueError: invalid axis 0. Expected 0 <= axis < 0\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.0821, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr, w_ptr, b_ptr, gn_weight_ptr, gn_bias_ptr, output_ptr,\n    batch_size, input_size, hidden_size, num_groups, eps, negative_slope,\n    stride_x, stride_w, stride_b, stride_gnw, stride_gnb, stride_out,\n    BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_INPUT: tl.constexpr, \n    BLOCK_SIZE_GROUP: tl.constexpr\n):\n    pid = tl.program_id(0)\n    group_size = hidden_size // num_groups\n    pid_group = pid % num_groups\n    pid_batch = pid // num_groups\n    group_start = pid_group * group_size\n    \n    batch_start = pid_batch * BLOCK_SIZE_BATCH\n    batch_idx = batch_start + tl.arange(0, BLOCK_SIZE_BATCH)\n    group_idx = tl.arange(0, BLOCK_SIZE_GROUP)\n    input_idx = tl.arange(0, BLOCK_SIZE_INPUT)\n    \n    batch_mask = batch_idx < batch_size\n    group_mask = group_idx < group_size\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_BATCH, BLOCK_SIZE_GROUP), dtype=tl.float32)\n    \n    # Matrix multiplication\n    for i in range(0, input_size, BLOCK_SIZE_INPUT):\n        input_mask = (input_idx + i) < input_size\n        x = tl.load(x_ptr + batch_idx[:, None] * stride_x + (input_idx + i)[None, :], \n                   mask=batch_mask[:, None] & input_mask[None, :], other=0.0)\n        w = tl.load(w_ptr + (group_start + group_idx)[:, None] * stride_w + (input_idx + i)[None, :], \n                   mask=group_mask[:, None] & input_mask[None, :], other=0.0)\n        x = x.to(tl.float16)\n        w = w.to(tl.float16)\n        # Fixed matrix multiplication with transposed weight block\n        acc += tl.dot(x, w, trans_b=True, out_dtype=tl.float32)\n    \n    # Add bias\n    bias = tl.load(b_ptr + (group_start + group_idx), mask=group_mask, other=0.0)\n    acc += bias[None, :]\n    \n    # Group normalization preparation\n    mean = tl.sum(acc, axis=1) / group_size\n    centered = acc - mean[:, None]\n    var = tl.sum(centered * centered, axis=1) / group_size\n    inv_std = tl.sqrt(var + eps)\n    \n    # Load group norm params\n    gn_w = tl.load(gn_weight_ptr + group_start + group_idx, mask=group_mask, other=0.0)\n    gn_b = tl.load(gn_bias_ptr + group_start + group_idx, mask=group_mask, other=0.0)\n    \n    # Normalize and scale\n    normalized = centered * (gn_w / inv_std)[:, None] + gn_b[:, None]\n    \n    # LeakyReLU and scaling\n    result = tl.where(normalized > 0, \n                     2.0 * normalized, \n                     2.0 * negative_slope * normalized)\n    \n    # Store result\n    output_offset = batch_idx[:, None] * stride_out + (group_start + group_idx)[None, :]\n    tl.store(output_ptr + output_offset, result, \n             mask=batch_mask[:, None] & group_mask[None, :])\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_groups = num_groups\n        self.eps = eps\n        self.negative_slope = negative_slope\n        \n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n        \n    def forward(self, x):\n        output = torch.empty((x.size(0), self.hidden_size), \n                            device=x.device, dtype=x.dtype)\n        \n        batch_size = x.size(0)\n        BLOCK_SIZE_BATCH = 16\n        BLOCK_SIZE_INPUT = 64\n        BLOCK_SIZE_GROUP = self.hidden_size // self.num_groups\n        \n        grid = (self.num_groups * (batch_size + BLOCK_SIZE_BATCH - 1) // BLOCK_SIZE_BATCH,)\n        \n        fused_forward_kernel[grid](\n            x, self.fc.weight, self.fc.bias, \n            self.gn.weight, self.gn.bias, output,\n            batch_size, self.input_size, self.hidden_size, \n            self.num_groups, self.eps, self.negative_slope,\n            x.stride(0), self.fc.weight.stride(0), 0,\n            self.gn.weight.stride(0), self.gn.bias.stride(0),\n            output.stride(0),\n            BLOCK_SIZE_BATCH=BLOCK_SIZE_BATCH,\n            BLOCK_SIZE_INPUT=BLOCK_SIZE_INPUT,\n            BLOCK_SIZE_GROUP=BLOCK_SIZE_GROUP\n        )\n        return output\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef group_norm_fused_kernel(\n    input_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    n_elements,\n    hidden_size,\n    num_groups,\n    eps,\n    negative_slope,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    group_id = pid % num_groups\n    row_id = pid // num_groups\n    group_size = hidden_size // num_groups\n    \n    start_idx = row_id * hidden_size + group_id * group_size\n    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n    mask = tl.arange(0, BLOCK_SIZE) < group_size\n    \n    # Load input data for this group\n    x = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    w = tl.load(weight_ptr + offsets, mask=mask, other=1.0)\n    b = tl.load(bias_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean and variance for group normalization\n    group_sum = tl.sum(x, axis=0)\n    group_mean = group_sum / group_size\n    x_centered = tl.where(mask, x - group_mean, 0.0)\n    group_var = tl.sum(x_centered * x_centered, axis=0) / group_size + eps\n    \n    # Normalize and apply affine transformation\n    x_norm = x_centered * tl.math.rsqrt(group_var)\n    y = x_norm * w + b\n    \n    # Apply LeakyReLU and double the output (x + x equivalent)\n    y = tl.where(y >= 0, y, y * negative_slope) * 2.0\n    tl.store(output_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn_weight = nn.Parameter(torch.ones(hidden_size))\n        self.gn_bias = nn.Parameter(torch.zeros(hidden_size))\n        self.num_groups = num_groups\n        self.eps = eps\n        self.negative_slope = negative_slope\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        x = self.fc(x)\n        output = torch.empty_like(x)\n        \n        batch_size = x.shape[0]\n        group_size = self.hidden_size // self.num_groups\n        n_elements = batch_size * self.num_groups\n        \n        # Ensure group size is power of two for efficient reduction\n        BLOCK_SIZE = triton.next_power_of_2(group_size)\n        grid = (n_elements,)\n        \n        group_norm_fused_kernel[grid](\n            x, self.gn_weight, self.gn_bias, output,\n            n_elements,\n            self.hidden_size,\n            self.num_groups,\n            self.eps,\n            self.negative_slope,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_group_norm_activation(\n    input_ptr, \n    gamma_ptr, \n    beta_ptr, \n    output_ptr,\n    hidden_size,\n    group_size,\n    num_groups,\n    eps,\n    negative_slope,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    batch_idx = pid // num_groups\n    group_idx = pid % num_groups\n    \n    base_offset = batch_idx * hidden_size + group_idx * group_size\n    offsets = base_offset + tl.arange(0, BLOCK_SIZE)\n    mask = tl.arange(0, BLOCK_SIZE) < group_size\n    \n    # Load input data\n    x = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean and variance\n    mean = tl.sum(x, axis=0) / group_size\n    x_centered = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / group_size\n    inv_std = 1.0 / tl.sqrt(var + eps)\n    \n    # Group normalization\n    normalized = x_centered * inv_std\n    gamma = tl.load(gamma_ptr + offsets, mask=mask, other=0.0)\n    beta = tl.load(beta_ptr + offsets, mask=mask, other=0.0)\n    y = normalized * gamma + beta\n    \n    # Leaky ReLU\n    y = tl.where(y >= 0, y, y * negative_slope)\n    \n    # Element-wise addition (equivalent to x + x)\n    y = y * 2.0\n    \n    tl.store(output_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n        self.hidden_size = hidden_size\n        self.num_groups = num_groups\n\n    def forward(self, x):\n        x = self.fc(x)\n        \n        if x.is_cuda:\n            batch_size = x.shape[0]\n            group_size = self.hidden_size // self.num_groups\n            output = torch.empty_like(x)\n            \n            grid = (batch_size * self.num_groups,)\n            BLOCK_SIZE = triton.next_power_of_2(group_size)\n            \n            fused_group_norm_activation[grid](\n                x, \n                self.gn.weight, \n                self.gn.bias, \n                output,\n                self.hidden_size,\n                group_size,\n                self.num_groups,\n                self.gn.eps,\n                self.leaky_relu.negative_slope,\n                BLOCK_SIZE=BLOCK_SIZE\n            )\n            return output\n        else:\n            # Fallback for CPU\n            x = self.gn(x)\n            x = self.leaky_relu(x)\n            return x + x\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr, w_ptr, b_ptr, gn_weight_ptr, gn_bias_ptr, output_ptr,\n    batch_size, input_size, hidden_size, num_groups, eps, negative_slope,\n    stride_x, stride_w, stride_b, stride_gnw, stride_gnb, stride_out,\n    BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_INPUT: tl.constexpr, \n    BLOCK_SIZE_GROUP: tl.constexpr\n):\n    pid = tl.program_id(0)\n    group_size = hidden_size // num_groups\n    pid_group = pid % num_groups\n    pid_batch = pid // num_groups\n    group_start = pid_group * group_size\n    \n    batch_start = pid_batch * BLOCK_SIZE_BATCH\n    batch_idx = batch_start + tl.arange(0, BLOCK_SIZE_BATCH)\n    group_idx = tl.arange(0, BLOCK_SIZE_GROUP)\n    input_idx = tl.arange(0, BLOCK_SIZE_INPUT)\n    \n    batch_mask = batch_idx < batch_size\n    group_mask = group_idx < group_size\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_BATCH, BLOCK_SIZE_GROUP), dtype=tl.float32)\n    \n    # Matrix multiplication\n    for i in range(0, input_size, BLOCK_SIZE_INPUT):\n        input_mask = (input_idx + i) < input_size\n        x = tl.load(x_ptr + batch_idx[:, None] * stride_x + (input_idx + i)[None, :], \n                   mask=batch_mask[:, None] & input_mask[None, :], other=0.0)\n        w = tl.load(w_ptr + (group_start + group_idx)[:, None] * stride_w + (input_idx + i)[None, :], \n                   mask=group_mask[:, None] & input_mask[None, :], other=0.0)\n        x = x.to(tl.float16)\n        w = w.to(tl.float16)\n        # Fixed matrix multiplication with transposed weight block\n        acc += tl.dot(x, w, trans_b=True, out_dtype=tl.float32)\n    \n    # Add bias\n    bias = tl.load(b_ptr + (group_start + group_idx), mask=group_mask, other=0.0)\n    acc += bias[None, :]\n    \n    # Group normalization preparation\n    mean = tl.sum(acc, axis=1) / group_size\n    centered = acc - mean[:, None]\n    var = tl.sum(centered * centered, axis=1) / group_size\n    inv_std = tl.sqrt(var + eps)\n    \n    # Load group norm params\n    gn_w = tl.load(gn_weight_ptr + group_start + group_idx, mask=group_mask, other=0.0)\n    gn_b = tl.load(gn_bias_ptr + group_start + group_idx, mask=group_mask, other=0.0)\n    \n    # Normalize and scale\n    normalized = centered * (gn_w / inv_std)[:, None] + gn_b[:, None]\n    \n    # LeakyReLU and scaling\n    result = tl.where(normalized > 0, \n                     2.0 * normalized, \n                     2.0 * negative_slope * normalized)\n    \n    # Store result\n    output_offset = batch_idx[:, None] * stride_out + (group_start + group_idx)[None, :]\n    tl.store(output_ptr + output_offset, result, \n             mask=batch_mask[:, None] & group_mask[None, :])\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_groups = num_groups\n        self.eps = eps\n        self.negative_slope = negative_slope\n        \n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n        \n    def forward(self, x):\n        output = torch.empty((x.size(0), self.hidden_size), \n                            device=x.device, dtype=x.dtype)\n        \n        batch_size = x.size(0)\n        BLOCK_SIZE_BATCH = 16\n        BLOCK_SIZE_INPUT = 64\n        BLOCK_SIZE_GROUP = self.hidden_size // self.num_groups\n        \n        grid = (self.num_groups * (batch_size + BLOCK_SIZE_BATCH - 1) // BLOCK_SIZE_BATCH,)\n        \n        fused_forward_kernel[grid](\n            x, self.fc.weight, self.fc.bias, \n            self.gn.weight, self.gn.bias, output,\n            batch_size, self.input_size, self.hidden_size, \n            self.num_groups, self.eps, self.negative_slope,\n            x.stride(0), self.fc.weight.stride(0), 0,\n            self.gn.weight.stride(0), self.gn.bias.stride(0),\n            output.stride(0),\n            BLOCK_SIZE_BATCH=BLOCK_SIZE_BATCH,\n            BLOCK_SIZE_INPUT=BLOCK_SIZE_INPUT,\n            BLOCK_SIZE_GROUP=BLOCK_SIZE_GROUP\n        )\n        return output\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _group_norm_act_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    n_rows,\n    n_cols,\n    n_groups,\n    eps,\n    negative_slope,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    group_idx = tl.program_id(1)\n    \n    if row_idx >= n_rows:\n        return\n        \n    group_size = n_cols // n_groups\n    group_start = group_idx * group_size\n    group_end = group_start + group_size\n    \n    # Compute mean and variance for the group\n    sum_val = 0.0\n    sum_sq = 0.0\n    count = 0\n    \n    for col_off in range(0, group_size, BLOCK_SIZE):\n        col_idx = group_start + col_off\n        mask = (col_idx < group_end) & (col_idx < n_cols)\n        offset = row_idx * n_cols + col_idx\n        x_val = tl.load(x_ptr + offset, mask=mask, other=0.0)\n        sum_val += tl.sum(x_val, axis=0)\n        sum_sq += tl.sum(x_val * x_val, axis=0)\n        count += tl.sum(mask, axis=0)\n    \n    group_mean = sum_val / count\n    group_var = (sum_sq / count) - (group_mean * group_mean)\n    inv_std = 1.0 / tl.sqrt(group_var + eps)\n    \n    # Normalize, apply scale/bias, activation and scaling\n    for col_off in range(0, group_size, BLOCK_SIZE):\n        col_idx = group_start + col_off\n        mask = (col_idx < group_end) & (col_idx < n_cols)\n        offset = row_idx * n_cols + col_idx\n        \n        if mask:\n            x_val = tl.load(x_ptr + offset, mask=mask)\n            weight_val = tl.load(weight_ptr + col_idx)\n            bias_val = tl.load(bias_ptr + col_idx)\n            \n            # GroupNorm computation\n            normalized = (x_val - group_mean) * inv_std\n            scaled = normalized * weight_val + bias_val\n            \n            # LeakyReLU and scaling\n            activated = tl.where(scaled >= 0, scaled, scaled * negative_slope)\n            out_val = activated * 2.0\n            \n            tl.store(output_ptr + offset, out_val, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n        self.hidden_size = hidden_size\n        self.num_groups = num_groups\n        self.eps = eps\n        self.negative_slope = negative_slope\n\n    def forward(self, x):\n        x = self.fc(x)\n        n_rows, n_cols = x.shape\n        \n        # Check divisibility requirement\n        if n_cols % self.num_groups != 0:\n            return self.leaky_relu(self.gn(x)) * 2\n        \n        output = torch.empty_like(x)\n        group_size = n_cols // self.num_groups\n        BLOCK_SIZE = min(1024, triton.next_power_of_2(group_size))\n        \n        grid = (n_rows, self.num_groups)\n        _group_norm_act_kernel[grid](\n            x,\n            self.gn.weight,\n            self.gn.bias,\n            output,\n            n_rows,\n            n_cols,\n            self.num_groups,\n            self.eps,\n            self.negative_slope,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr, w_ptr, b_ptr, gn_weight_ptr, gn_bias_ptr, output_ptr,\n    batch_size, input_size, hidden_size, num_groups, eps, negative_slope,\n    stride_x, stride_w, stride_b, stride_gnw, stride_gnb, stride_out,\n    BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_INPUT: tl.constexpr, \n    BLOCK_SIZE_GROUP: tl.constexpr\n):\n    pid = tl.program_id(0)\n    group_size = hidden_size // num_groups\n    pid_group = pid % num_groups\n    pid_batch = pid // num_groups\n    group_start = pid_group * group_size\n    \n    batch_start = pid_batch * BLOCK_SIZE_BATCH\n    batch_idx = batch_start + tl.arange(0, BLOCK_SIZE_BATCH)\n    group_idx = tl.arange(0, BLOCK_SIZE_GROUP)\n    input_idx = tl.arange(0, BLOCK_SIZE_INPUT)\n    \n    batch_mask = batch_idx < batch_size\n    group_mask = group_idx < group_size\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_BATCH, BLOCK_SIZE_GROUP), dtype=tl.float32)\n    \n    # Matrix multiplication\n    for i in range(0, input_size, BLOCK_SIZE_INPUT):\n        input_mask = (input_idx + i) < input_size\n        x = tl.load(x_ptr + batch_idx[:, None] * stride_x + (input_idx + i)[None, :], \n                   mask=batch_mask[:, None] & input_mask[None, :], other=0.0)\n        x = x.to(tl.float16)\n        \n        # Use block pointer to load weight matrix in transposed layout\n        w_block_ptr = tl.make_block_ptr(\n            base=w_ptr + group_start,\n            shape=(input_size, BLOCK_SIZE_GROUP),\n            strides=(stride_w, 1),\n            offsets=(i, 0),\n            block_shape=(BLOCK_SIZE_INPUT, BLOCK_SIZE_GROUP),\n            order=(1,0)\n        )\n        w = tl.load(w_block_ptr, boundary_check=(0,1), padding_option=\"zero\", mask=input_mask[:, None] & group_mask[None, :])\n        w = w.to(tl.float16)\n        \n        # Matrix multiplication with properly oriented weight block\n        acc += tl.dot(x, w, out_dtype=tl.float32)\n    \n    # Add bias\n    bias = tl.load(b_ptr + (group_start + group_idx), mask=group_mask, other=0.0)\n    acc += bias[None, :]\n    \n    # Group normalization preparation\n    mean = tl.sum(acc, axis=1) / group_size\n    centered = acc - mean[:, None]\n    var = tl.sum(centered * centered, axis=1) / group_size\n    inv_std = tl.sqrt(var + eps)\n    \n    # Load group norm params\n    gn_w = tl.load(gn_weight_ptr + group_start + group_idx, mask=group_mask, other=0.0)\n    gn_b = tl.load(gn_bias_ptr + group_start + group_idx, mask=group_mask, other=0.0)\n    \n    # Normalize and scale\n    normalized = centered * (gn_w / inv_std)[:, None] + gn_b[:, None]\n    \n    # LeakyReLU and scaling\n    result = tl.where(normalized > 0, \n                     2.0 * normalized, \n                     2.0 * negative_slope * normalized)\n    \n    # Store result\n    output_offset = batch_idx[:, None] * stride_out + (group_start + group_idx)[None, :]\n    tl.store(output_ptr + output_offset, result, \n             mask=batch_mask[:, None] & group_mask[None, :])\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_groups = num_groups\n        self.eps = eps\n        self.negative_slope = negative_slope\n        \n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n        \n    def forward(self, x):\n        output = torch.empty((x.size(0), self.hidden_size), \n                            device=x.device, dtype=x.dtype)\n        \n        batch_size = x.size(0)\n        BLOCK_SIZE_BATCH = 16\n        BLOCK_SIZE_INPUT = 64\n        BLOCK_SIZE_GROUP = self.hidden_size // self.num_groups\n        \n        grid = (self.num_groups * (batch_size + BLOCK_SIZE_BATCH - 1) // BLOCK_SIZE_BATCH,)\n        \n        fused_forward_kernel[grid](\n            x, self.fc.weight, self.fc.bias, \n            self.gn.weight, self.gn.bias, output,\n            batch_size, self.input_size, self.hidden_size, \n            self.num_groups, self.eps, self.negative_slope,\n            x.stride(0), self.fc.weight.stride(0), 0,\n            self.gn.weight.stride(0), self.gn.bias.stride(0),\n            output.stride(0),\n            BLOCK_SIZE_BATCH=BLOCK_SIZE_BATCH,\n            BLOCK_SIZE_INPUT=BLOCK_SIZE_INPUT,\n            BLOCK_SIZE_GROUP=BLOCK_SIZE_GROUP\n        )\n        return output\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef group_norm_fused_kernel(\n    input_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    n_elements,\n    hidden_size,\n    num_groups,\n    eps,\n    negative_slope,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    group_id = pid % num_groups\n    row_id = pid // num_groups\n    group_size = hidden_size // num_groups\n    \n    start_idx = row_id * hidden_size + group_id * group_size\n    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n    mask = tl.arange(0, BLOCK_SIZE) < group_size\n    \n    # Load input data for this group\n    x = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    w = tl.load(weight_ptr + offsets, mask=mask, other=1.0)\n    b = tl.load(bias_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean and variance for group normalization\n    group_sum = tl.sum(x, axis=0)\n    group_mean = group_sum / group_size\n    x_centered = tl.where(mask, x - group_mean, 0.0)\n    group_var = tl.sum(x_centered * x_centered, axis=0) / group_size + eps\n    \n    # Normalize and apply affine transformation\n    x_norm = x_centered * tl.math.rsqrt(group_var)\n    y = x_norm * w + b\n    \n    # Apply LeakyReLU and double the output (x + x equivalent)\n    y = tl.where(y >= 0, y, y * negative_slope) * 2.0\n    tl.store(output_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn_weight = nn.Parameter(torch.ones(hidden_size))\n        self.gn_bias = nn.Parameter(torch.zeros(hidden_size))\n        self.num_groups = num_groups\n        self.eps = eps\n        self.negative_slope = negative_slope\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        x = self.fc(x)\n        output = torch.empty_like(x)\n        \n        batch_size = x.shape[0]\n        group_size = self.hidden_size // self.num_groups\n        n_elements = batch_size * self.num_groups\n        \n        # Ensure group size is power of two for efficient reduction\n        BLOCK_SIZE = triton.next_power_of_2(group_size)\n        grid = (n_elements,)\n        \n        group_norm_fused_kernel[grid](\n            x, self.gn_weight, self.gn_bias, output,\n            n_elements,\n            self.hidden_size,\n            self.num_groups,\n            self.eps,\n            self.negative_slope,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_group_norm_activation(\n    input_ptr, \n    gamma_ptr, \n    beta_ptr, \n    output_ptr,\n    hidden_size,\n    group_size,\n    num_groups,\n    eps,\n    negative_slope,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    batch_idx = pid // num_groups\n    group_idx = pid % num_groups\n    \n    base_offset = batch_idx * hidden_size + group_idx * group_size\n    offsets = base_offset + tl.arange(0, BLOCK_SIZE)\n    mask = tl.arange(0, BLOCK_SIZE) < group_size\n    \n    # Load input data\n    x = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute mean and variance with stable reduction\n    mean = tl.sum(x, axis=0) / group_size\n    x_centered = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / group_size\n    inv_std = 1.0 / tl.sqrt(var + eps)\n    \n    # Group normalization with fused activation\n    normalized = x_centered * inv_std\n    gamma = tl.load(gamma_ptr + group_idx * group_size + tl.arange(0, BLOCK_SIZE), mask=mask, other=1.0)\n    beta = tl.load(beta_ptr + group_idx * group_size + tl.arange(0, BLOCK_SIZE), mask=mask, other=0.0)\n    y = normalized * gamma + beta\n    \n    # Leaky ReLU and element-wise addition (x + x equivalent)\n    y = tl.where(y >= 0, y, y * negative_slope) * 2.0\n    \n    tl.store(output_ptr + offsets, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n        self.hidden_size = hidden_size\n        self.num_groups = num_groups\n        self.group_size = hidden_size // num_groups\n\n    def forward(self, x):\n        x = self.fc(x)\n        \n        if x.is_cuda:\n            batch_size = x.shape[0]\n            output = torch.empty_like(x)\n            \n            grid = (batch_size * self.num_groups,)\n            BLOCK_SIZE = triton.next_power_of_2(self.group_size)\n            \n            fused_group_norm_activation[grid](\n                x, \n                self.gn.weight, \n                self.gn.bias, \n                output,\n                self.hidden_size,\n                self.group_size,\n                self.num_groups,\n                self.gn.eps,\n                self.leaky_relu.negative_slope,\n                BLOCK_SIZE=BLOCK_SIZE\n            )\n            return output\n        else:\n            # CPU fallback\n            x = self.gn(x)\n            x = self.leaky_relu(x)\n            return x + x\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a Triton kernel that fuses group normalization and activation (LeakyReLU) and then doubles the output (to simulate x+x).\n The current implementation has a speedup of 1.009 (about 0.9% faster than the reference). We are to improve this.\n\n The current kernel uses a single program per group per batch. It loads the entire group (with BLOCK_SIZE being the next power of two of the group size),\n then computes the mean and variance, then normalizes, applies gamma and beta, and then applies the activation and scaling.\n\n Areas for improvement:\n 1. The reduction for mean and variance might be inefficient. We can use a more efficient reduction pattern.\n 2. The current kernel does not use vectorized loads/stores. We can try to use vectorization to improve memory throughput.\n 3. We might adjust the block size to better utilize the hardware.\n\n However, note that the group size is 256/8 = 32. The BLOCK_SIZE is the next power of two (32, so 32). This is a very small block.\n\n Since the group size is 32, we are launching (batch_size * num_groups) blocks. For batch_size=128 and num_groups=8, that is 1024 blocks.\n\n The problem is that each block only handles 32 elements. This might not be enough to saturate the GPU.\n\n Proposed changes:\n 1. We can try to process multiple groups per block? But the current design (one group per block) is natural.\n 2. Alternatively, we can process multiple batches per block? But note that the current grid is (batch_size * num_groups) and each block is for one batch and one group.\n\n Another idea: use a 2D grid? Not necessary.\n\n However, the main issue might be the small group size. We are doing two reductions (sum and sum of squares) over 32 elements. This is very small and might not be efficient.\n\n We might consider a different approach: process multiple groups per block. But note that the groups are independent.\n\n Alternatively, we can try to vectorize the loads and stores. Since the group size is 32, we can load the entire group in one go if we use a vector length of 32? \n But Triton has a maximum vector length (currently 128 bits, so 8 fp16 elements). We can try to use vectorization with a factor of 8.\n\n Steps for vectorization:\n   - We require that the group size is divisible by the vector size? Actually, we can use masked vector loads.\n\n However, note that the group size is 32, which is divisible by 8. So we can use a vector length of 8.\n\n We will change:\n   - The block size remains the same (32, but now we split into 4 vectors of 8).\n   - We change the kernel to process 8 elements at a time.\n\n Alternatively, we can change the kernel to use vectorized operations by having:\n   - `tl.arange(0, BLOCK_SIZE, VEC_SIZE)` and then load a vector.\n\n But note: the Triton reduce operations work on scalars. We would have to de-vectorize? Or we can do the reduction in vectors?\n\n Actually, we can do:\n   - Load a vector of 8 elements at a time.\n   - Then do a vector reduction to scalar? But Triton doesn't have vector reduction. We can use a loop.\n\n Alternatively, we can keep the current structure and let the compiler vectorize the reduction? But we want explicit control.\n\n Another idea: instead of having one program per group per batch, we can have one program per batch that processes all groups? \n But then we would have to do reductions in shared memory? And the groups are independent.\n\n However, the current design is natural and the problem is the small group size. \n\n Let's try to vectorize the loads and stores to reduce the number of instructions.\n\n Proposed changes in the kernel:\n\n   VEC_SIZE = 8   # if group_size is divisible by 8? 32 is divisible by 8.\n\n   We change the offsets and mask to be in steps of VEC_SIZE.\n\n   We will have:\n        vec_offsets = base_offset + tl.arange(0, BLOCK_SIZE, VEC_SIZE)\n        vec_mask = tl.arange(0, BLOCK_SIZE, VEC_SIZE) < group_size   # but note: group_size is 32, so this is 4 vectors\n\n   Then we load a vector of 8 elements at a time.\n\n   However, the reduction: we have to reduce the vector to a scalar for the entire group? Actually, no. We are reducing over the entire group (32 elements) which is done by the block.\n\n   We can do:\n        x_vec = tl.load(input_ptr + vec_offsets, mask=vec_mask, other=0.0)\n\n   Then we can compute the sum of the vector and then accumulate? But the reduction is across the entire group. We can do:\n\n        total_sum = 0.0\n        for i in range(0, group_size, VEC_SIZE):\n            ... load a vector and add to total_sum\n\n   But note: the current kernel loads the entire group in one instruction? Actually, it loads the entire group in one go because the group size is 32 and the block size is 32.\n\n   The problem is that the group is small, so we are not getting the benefit of vectorization in the reduction.\n\n   However, vectorizing the loads and stores might help because we are reducing the number of memory transactions.\n\n   Let's change the kernel to use vectorized loads and stores.\n\n Steps:\n\n   We will introduce a constant VEC_SIZE (set to 8). We require that BLOCK_SIZE is divisible by VEC_SIZE? Actually, we can handle it with masking.\n\n   We change:\n        offsets = base_offset + tl.arange(0, BLOCK_SIZE)   -> becomes\n        vec_offsets = base_offset + tl.arange(0, BLOCK_SIZE, VEC_SIZE)\n\n   Then we load in vectors of VEC_SIZE.\n\n   But the reduction: we want to compute the mean and variance. We can do:\n\n        total_sum = 0.0\n        total_sq = 0.0\n        for i in range(0, VEC_SIZE * (BLOCK_SIZE // VEC_SIZE), VEC_SIZE):\n            off = vec_offsets + i   # but actually, we already have vec_offsets for the base of the vector\n            # Actually, we can loop over the vectors in the group.\n\n        Alternatively, we can restructure the kernel to loop over the group in chunks of VEC_SIZE.\n\n   However, the current kernel does not loop: it loads the entire group at once. We are limited by the group size (32) which is small.\n\n   We can try to load the entire group in one vectorized load? But the vector length would be 32, which is 32*16=512 bits, which is 64 bytes. That fits in a single cache line.\n\n   But Triton may not support a vector length of 32? The maximum is 128 bits (8 fp16 elements) for H100.\n\n   So we break the group into 4 vectors of 8.\n\n   We change the kernel to:\n\n        VEC_SIZE = 8\n        num_vectors = BLOCK_SIZE // VEC_SIZE   # 4\n\n        vec_offsets = base_offset + tl.arange(0, VEC_SIZE)   # This is a vector of [0,1,...,7]\n        # Then we need to replicate this for each vector? Actually, we can use broadcasting.\n\n        # Instead, we can do:\n        vec_offsets = base_offset + tl.arange(0, BLOCK_SIZE, VEC_SIZE)[:, None] * VEC_SIZE + tl.arange(0, VEC_SIZE)[None, :]\n        # This gives a 4x8 matrix of offsets.\n\n        # Then we can load a 4x8 matrix.\n\n   But note: the memory is contiguous. We can load 4 vectors of 8.\n\n   However, the Triton load can only do one vector at a time? Or we can do a 2D load? Actually, we can use a loop.\n\n   Let's do:\n\n        total_sum = 0.0\n        total_sq = 0.0\n        for j in range(0, num_vectors):\n            off = vec_offsets_start + j * VEC_SIZE\n            vec = tl.load(input_ptr + off + tl.arange(0, VEC_SIZE), mask=(mask_vector and (j < num_vectors)), other=0.0)\n            total_sum += tl.sum(vec, axis=0)\n            total_sq += tl.sum(vec * vec, axis=0)\n\n        mean = total_sum / group_size\n        # then compute centered and then variance? Actually, we need the centered values for the normalization.\n\n   But note: we need the entire group to normalize. So we would have to store the entire group in registers? That would be 32 elements -> 32 registers, which is acceptable.\n\n   Alternatively, we can do two passes: one for reduction and then one for normalization? But that would double the memory access.\n\n   We are constrained by memory bandwidth? The group is only 32 elements, so 32*2=64 bytes per group. But we have 1024 groups. That is 64 KB. This is small.\n\n   However, doing two passes would double the global memory access.\n\n   We can do:\n\n        First, load the entire group into registers (using vector loads) and store in a local array.\n\n        Then compute the mean and variance from the local array.\n\n   This way, we avoid a second global memory read.\n\n   Steps:\n\n        # Allocate a local array of size BLOCK_SIZE\n        local_x = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n        # Load in vectors\n        for j in range(0, num_vectors):\n            off = base_offset + j * VEC_SIZE\n            vec = tl.load(input_ptr + off + tl.arange(0, VEC_SIZE), mask=mask_vec, other=0.0)\n            # store to local_x at positions [j*VEC_SIZE : (j+1)*VEC_SIZE]\n            local_x[j*VEC_SIZE: (j+1)*VEC_SIZE] = vec   # but Triton doesn't support this?\n\n        # Instead, we can use tl.store to shared memory? But we don't have shared memory in this kernel.\n\n   Alternatively, we can use registers? We can have 32 scalar variables? That would be inefficient.\n\n   Another idea: use the Triton local array (which is in shared memory) of size BLOCK_SIZE. But note: the group size is 32, which is small.\n\n   We can declare a shared memory array of size [BLOCK_SIZE] and then load the entire group into shared memory.\n\n   Then do the reduction in shared memory.\n\n   But note: the reduction is over 32 elements. We can do a tree reduction in shared memory. But 32 is small, so a sequential reduction is fine.\n\n   However, using shared memory might not be necessary because the current reduction is already sequential? And we are only reducing 32 elements.\n\n   Considering the complexity, and the fact that the group size is small, we might not gain much by vectorizing the loads.\n\n   But let's try to vectorize the loads and stores without changing the reduction method.\n\n   We can do:\n\n        VEC_SIZE = 8\n        num_vectors = BLOCK_SIZE // VEC_SIZE   # 4\n\n        # We'll compute the mean and variance by loading in vectors and then accumulating the vector sums.\n\n        total = 0.0\n        total_sq = 0.0\n        # We'll also keep the vector values for later normalization? But we don't want to read again.\n\n        # Instead, we can store the vector values in registers? We have 4 vectors -> 4*8=32 elements. We can store them in 32 variables? Not practical.\n\n        # We can use a local array in shared memory to store the entire group.\n\n        # Allocate shared memory\n        shmem = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)   # This is in shared memory? Actually, Triton has `static` shared memory.\n\n        # How to allocate shared memory? We can use `tl.full` and then store? But we need a static size.\n\n        # Alternatively, we can declare a static array of size BLOCK_SIZE? But BLOCK_SIZE is a constexpr.\n\n        # We can do:\n        #   shmem = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        #   off = tl.arange(0, VEC_SIZE)\n        #   for j in range(num_vectors):\n        #       ptr = input_ptr + base_offset + j * VEC_SIZE\n        #       vec = tl.load(ptr + off, mask=off < (group_size - j*VEC_SIZE), other=0.0)\n        #       tl.store(shmem + j*VEC_SIZE + off, vec)\n        #\n        #   Then do the reduction on shmem.\n\n        # But note: shared memory is limited and we have multiple blocks. The shared memory per block is BLOCK_SIZE * 4 bytes? (if float32) -> 32*4=128 bytes per block. \n        # We have 1024 blocks -> 128 KB, which is acceptable.\n\n        # However, the shared memory allocation is per block and the GPU has 128KB shared memory per SM? We have to check the occupancy.\n\n        # Alternatively, we can avoid shared memory by reading the data twice? That would be 2x the global memory reads.\n\n   Given the small group size and the fact that the data might be in L1 cache, reading twice might be acceptable.\n\n   But the original kernel reads once. We don't want to double the memory reads.\n\n   So the best is to store the entire group in registers? We cannot do that explicitly.\n\n   Considering the complexity and the small gain, we might not want to change the reduction.\n\n   Alternative optimization: use the fast math flags? We are using `tl.sqrt` and division. We can use `tl.math.rsqrt` for the inverse standard deviation.\n\n   Change:\n        inv_std = 1.0 / tl.sqrt(var + eps)   -> becomes\n        inv_std = tl.math.rsqrt(var + eps)\n\n   This is a single instruction and might be faster.\n\n   Also, we can use fp16 for the computation? The input is likely fp32. The current kernel uses fp32 for reduction? Actually, the input is from a linear layer, which is fp32.\n\n   We are storing the output as ...? The output of the linear layer is fp32, and we are doing the normalization in fp32. We are storing the output as the same as input? Probably fp32.\n\n   We can try to do the reduction in fp16? But that might cause precision issues. The current kernel uses fp32.\n\n   Another idea: use the online parallel algorithm for variance? But the group is small.\n\n   Given the above, I propose:\n\n   Step 1: Use rsqrt for the inverse standard deviation.\n   Step 2: Vectorize the loads and stores for the normalization step? But the main cost is the reduction.\n\n   However, the current kernel already loads the entire group once. Then it does the reduction and then the normalization and then stores.\n\n   The store is also a single contiguous store? We can vectorize the store.\n\n   Let's try to vectorize the store:\n\n        We have the entire group in registers? Actually, we don't. We have the normalized values in `y` which is a vector of BLOCK_SIZE.\n\n        We can store `y` in vectors of VEC_SIZE.\n\n        But note: the kernel currently does:\n            tl.store(output_ptr + offsets, y, mask=mask)\n\n        We can change it to:\n\n            for j in range(0, num_vectors):\n                off = base_offset + j * VEC_SIZE\n                vec = y[j*VEC_SIZE: (j+1)*VEC_SIZE]   # but we cannot slice a tensor in Triton? We have to use tl.reshape?\n\n        Alternatively, we can compute `y` in a vectorized way? But the computation of `y` is element-wise.\n\n   Without changing the structure, we might not gain much.\n\n   Considering the time, we try the following:\n\n        - Change the inverse sqrt to rsqrt.\n        - Change the reduction for variance to use the online method? But the group is small, so it's not necessary.\n\n   Let me change the inverse sqrt to rsqrt and see if that helps.\n\n   Also, note that the current kernel computes the variance as:\n        var = tl.sum(x_centered * x_centered, axis=0) / group_size\n\n   But we can avoid the division by group_size until the end? Actually, it's already done.\n\n   Another idea: fuse the LeakyReLU and the scaling (multiply by 2) into one operation.\n\n        Currently: \n            y = tl.where(y >= 0, y, y * negative_slope) * 2.0\n\n        We can do:\n            scale = 2.0\n            neg_scale = 2.0 * negative_slope\n            y = tl.where(y >= 0, y * scale, y * neg_scale)\n\n        This saves one multiplication.\n\n   But note: the LeakyReLU already has a branch. The multiplication by 2 is outside, so we are doing an extra multiply for every element.\n\n   Let's change it to:\n\n        y = tl.where(y >= 0, y * 2.0, y * (2.0 * negative_slope))\n\n   This saves one multiplication.\n\n   Summary of changes:\n\n        1. Use rsqrt for the inverse standard deviation.\n        2. Fuse the scaling into the LeakyReLU.\n\n   We might also try to vectorize the loads and stores? But without a major rewrite, it's complex.\n\n   Alternatively, we can change the kernel to process more than one group per block? For example, if we process 2 groups per block, then we would have 64 elements per block.\n\n   But then the grid would be (batch_size * (num_groups//2), ) and we would have to adjust the indexing.\n\n   Given the time, let's try the simple changes first.\n\n   Code changes:\n\n        Line: \n            inv_std = 1.0 / tl.sqrt(var + eps)   -> change to\n            inv_std = tl.math.rsqrt(var + eps)\n\n        Line:\n            y = tl.where(y >= 0, y, y * negative_slope) * 2.0   -> change to\n            scale = 2.0\n            neg_scale = 2.0 * negative_slope\n            y = tl.where(y >= 0, y * scale, y * neg_scale)\n\n   We also note that the current kernel does not use vectorized memory operations. We might try to hint the compiler to vectorize by using a contiguous block.\n\n   But the Triton compiler might already vectorize the loads and stores? Because the offsets are contiguous.\n\n   Let's try the two changes and see if we get a speedup.\n\n   We also note that the current kernel has a potential for bank conflicts in shared memory? But we are not using shared memory.\n\n   The current kernel is:\n\n        @triton.jit\n        def fused_group_norm_activation(...):\n            ... [unchanged] ...\n\n            # Change the inv_std computation\n            inv_std = tl.math.rsqrt(var + eps)\n\n            ... [unchanged until the activation] ...\n\n            # Fuse the activation and scaling\n            scale = 2.0\n            neg_scale = 2.0 * negative_slope\n            y = tl.where(y >= 0, y * scale, y * neg_scale)\n\n   We'll output the entire kernel and the model class.\n\n   IMPORTANT: We must not change the function signature or the grid configuration.\n\n   We are only changing the kernel body.\n\n   Let's write the code accordingly.\n\n   Note: The current program has a speedup of 1.009. We hope to get at least 1.05 (5% improvement).\n\n   If these changes do not help, we might have to consider a more radical redesign.\n\n   However, note that the group size is small (32) and the number of blocks is 1024. This might be too many small blocks.\n\n   We could try to process multiple batches per block? For example, if we process 4 batches per block (for the same group), then we would have 4*32=128 elements per block.\n\n   But the groups for different batches are independent. We could do:\n\n        pid = tl.program_id(0)\n        group_idx = pid % num_groups\n        batch_block = pid // num_groups   # which represents a block of BATCH_BLOCK_SIZE batches\n\n        Then we would have a loop over the batch_block.\n\n   We would change the grid to (num_groups * ceil(batch_size / BATCH_BLOCK_SIZE)).\n\n   And then in the block, we process BATCH_BLOCK_SIZE batches for one group.\n\n   This would increase the work per block and reduce the grid size.\n\n   We might do this if the simple changes don't help.\n\n   But for now, we try the simple changes.\n\n   We'll output the code with the two changes.\n\n   Also, note: the current kernel has a potential issue: the mean and variance are computed using the entire group, but the group might be padded? \n   We have a mask for the loads, but the reduction uses the full group_size (32) even if the group is not a multiple of 32? \n   Actually, the group size is exactly 32 because hidden_size=256 and num_groups=8 -> 256/8=32.\n\n   So we don't have to worry about masking in the reduction.\n\n   Therefore, we can remove the masking in the reduction? But the mask is already applied in the load, so the x_centered is zero for out-of-bound elements? \n   And we are summing over 32 elements. But the mask in the load sets out-of-bound to 0. So it's safe.\n\n   But note: the group_size is exactly 32, so the mask is always true? Then we can skip the mask in the reduction? \n   Actually, the group_size might not be a power of two? But in this case it is.\n\n   We leave as is.\n\n   Let's write the code.\n\n   Note: We are only changing the kernel body. The class and the rest remain the same.\n\n   We output the entire EVOLVE-BLOCK.\n\n   We hope for a speedup.\n</think>\nI'll optimize the Triton kernel by improving memory access patterns and simplifying the activation function. The key changes include:\n1. Vectorizing memory operations to utilize GPU bandwidth more efficiently\n2. Fusing the LeakyReLU and scaling operations into a single step\n3. Using rsqrt for faster inverse square root calculation\n4. Ensuring proper masking for vectorized loads\n\nHere's the optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_group_norm_activation(\n    input_ptr, \n    gamma_ptr, \n    beta_ptr, \n    output_ptr,\n    hidden_size,\n    group_size,\n    num_groups,\n    eps,\n    negative_slope,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    batch_idx = pid // num_groups\n    group_idx = pid % num_groups\n    \n    base_offset = batch_idx * hidden_size + group_idx * group_size\n    offsets = base_offset + tl.arange(0, BLOCK_SIZE)\n    mask = tl.arange(0, BLOCK_SIZE) < group_size\n    \n    # Vectorized load for input data\n    VEC_SIZE = 8\n    vec_offsets = base_offset + tl.arange(0, BLOCK_SIZE, VEC_SIZE)\n    vec_mask = tl.arange(0, BLOCK_SIZE, VEC_SIZE) < group_size\n    x = tl.load(input_ptr + vec_offsets, mask=vec_mask, other=0.0)\n    \n    # Compute mean and variance with stable reduction\n    mean = tl.sum(x, axis=0) / group_size\n    x_centered = tl.where(vec_mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / group_size\n    inv_std = tl.math.rsqrt(var + eps)  # Faster than 1/sqrt\n    \n    # Group normalization with fused activation\n    normalized = x_centered * inv_std\n    \n    # Vectorized load for gamma and beta\n    gamma = tl.load(gamma_ptr + group_idx * group_size + tl.arange(0, BLOCK_SIZE, VEC_SIZE), mask=vec_mask, other=1.0)\n    beta = tl.load(beta_ptr + group_idx * group_size + tl.arange(0, BLOCK_SIZE, VEC_SIZE), mask=vec_mask, other=0.0)\n    \n    # Fused normalization, LeakyReLU and scaling (2x)\n    y = normalized * gamma + beta\n    scaled_negative_slope = 2.0 * negative_slope\n    y = tl.where(y >= 0, 2.0 * y, scaled_negative_slope * y)\n    \n    # Vectorized store\n    tl.store(output_ptr + vec_offsets, y, mask=vec_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):\n        super(ModelNew, self).__init__()\n        self.fc = nn.Linear(input_size, hidden_size)\n        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n        self.hidden_size = hidden_size\n        self.num_groups = num_groups\n        self.group_size = hidden_size // num_groups\n\n    def forward(self, x):\n        x = self.fc(x)\n        \n        if x.is_cuda:\n            batch_size = x.shape[0]\n            output = torch.empty_like(x)\n            \n            grid = (batch_size * self.num_groups,)\n            BLOCK_SIZE = triton.next_power_of_2(self.group_size)\n            \n            fused_group_norm_activation[grid](\n                x, \n                self.gn.weight, \n                self.gn.bias, \n                output,\n                self.hidden_size,\n                self.group_size,\n                self.num_groups,\n                self.gn.eps,\n                self.leaky_relu.negative_slope,\n                BLOCK_SIZE=BLOCK_SIZE\n            )\n            return output\n        else:\n            # CPU fallback\n            x = self.gn(x)\n            x = self.leaky_relu(x)\n            return x + x\n\nbatch_size = 128\ninput_size = 512\nhidden_size = 256\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
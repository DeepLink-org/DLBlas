{"id": "96addb0b-42bd-4549-b774-b175d14f3915", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n    ],\n    key=['reduction_size'],\n)\n@triton.jit\ndef _sum_reduce_kernel(\n    input_ptr,\n    output_ptr,\n    reduction_size,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    start_idx = pid * reduction_size\n    accumulator = 0.0\n    \n    for offset in range(0, reduction_size, BLOCK_SIZE):\n        indices = offset + tl.arange(0, BLOCK_SIZE)\n        mask = indices < reduction_size\n        values = tl.load(input_ptr + start_idx + indices, mask=mask, other=0.0)\n        accumulator += tl.sum(values, axis=0)\n    \n    tl.store(output_ptr + pid, accumulator)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if x.numel() == 0:\n            return torch.empty_like(x).sum(dim=self.dim, keepdim=True)\n            \n        # Move reduction dimension to last position\n        perm = list(range(x.dim()))\n        perm.append(perm.pop(self.dim))\n        x_perm = x.permute(perm).contiguous()\n        \n        # Reshape for efficient reduction\n        reduction_size = x_perm.shape[-1]\n        non_reduced_elements = x_perm.numel() // reduction_size\n        x_2d = x_perm.view(non_reduced_elements, reduction_size)\n        output_2d = torch.empty((non_reduced_elements, 1), device=x.device, dtype=x.dtype)\n        \n        # Configure and launch kernel\n        grid = (non_reduced_elements,)\n        _sum_reduce_kernel[grid](x_2d, output_2d, reduction_size)\n        \n        # Restore original dimensions\n        output_perm = output_2d.view(x_perm.shape[:-1] + (1,))\n        output = output_perm.permute(perm)\n        return output.contiguous()\n\nbatch_size = 16\ndim1 = 256\ndim2 = 256\nreduce_dim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [reduce_dim]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "0ffc524f-410e-4cf6-a422-6bf27d31a31c", "generation": 2, "timestamp": 1754644306.792203, "iteration_found": 19, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.258, "runtime_ms": 0.077, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0199, "speed_up": 0.258, "custom_timing": 0.077}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.33, "runtime_ms": 0.0603, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0199, "speed_up": 0.33, "custom_timing": 0.0603}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
{"id": "a13e9c64-053b-4b21-a0c3-ac53785735ac", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef reduce_sum_kernel(\n    input_ptr,\n    output_ptr,\n    stride_b, \n    stride_m, \n    stride_n,\n    output_stride_b,\n    output_stride_n,\n    B, \n    M, \n    N,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    pid_b = tl.program_id(0)\n    pid_n_block = tl.program_id(1)\n    \n    # Check bounds for batch index\n    if pid_b >= B:\n        return\n    \n    # Calculate column indices for this block\n    n_offsets = pid_n_block * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = n_offsets < N\n    \n    # Initialize accumulator vector for block of columns\n    total = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    # Process entire reduction dimension for this block of columns\n    for m in range(0, M):\n        # Calculate memory locations for this row\n        base = pid_b * stride_b + m * stride_m + n_offsets * stride_n\n        # Load vector of values for this row and column block\n        vals = tl.load(input_ptr + base, mask=mask_n, other=0.0)\n        total += vals\n    \n    # Calculate output location for this block of columns\n    output_base = pid_b * output_stride_b + n_offsets * output_stride_n\n    # Store results for the column block\n    tl.store(output_ptr + output_base, total, mask=mask_n)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() != 3 or self.dim != 1:\n            return torch.sum(x, dim=self.dim, keepdim=True)\n            \n        x_contiguous = x.contiguous()\n        output = torch.empty(x_contiguous.shape[0], 1, x_contiguous.shape[2], \n                            device=x.device, dtype=x.dtype).contiguous()\n        \n        B, M, N = x_contiguous.shape\n        # Use 2D grid: (batch_size, number of column blocks)\n        BLOCK_SIZE_N = 128\n        grid = (B, triton.cdiv(N, BLOCK_SIZE_N))\n        \n        reduce_sum_kernel[grid](\n            x_contiguous, output,\n            x_contiguous.stride(0), x_contiguous.stride(1), x_contiguous.stride(2),\n            output.stride(0), output.stride(2),\n            B, M, N,\n            BLOCK_SIZE_N=BLOCK_SIZE_N\n        )\n        return output\n\nbatch_size = 16\ndim1 = 256\ndim2 = 256\nreduce_dim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [reduce_dim]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "b84e97f8-7b57-4a90-9596-146bff387021", "generation": 3, "timestamp": 1754645817.8501427, "iteration_found": 29, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.4, "runtime_ms": 0.0497, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0199, "speed_up": 0.4, "custom_timing": 0.0497}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.405, "runtime_ms": 0.0491, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0199, "speed_up": 0.405, "custom_timing": 0.0491}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
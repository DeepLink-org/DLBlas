{"id": "b84e97f8-7b57-4a90-9596-146bff387021", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef reduce_sum_kernel(\n    input_ptr,\n    output_ptr,\n    stride_b, \n    stride_m, \n    stride_n,\n    output_stride_b,\n    output_stride_n,\n    B, \n    M, \n    N,\n    BLOCK_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    n = pid % N\n    b = pid // N\n    \n    # Check bounds for batch and column indices\n    if b >= B or n >= N:\n        return\n    \n    input_base = input_ptr + b * stride_b + n * stride_n\n    output_idx = b * output_stride_b + n * output_stride_n\n    \n    # Initialize accumulator\n    total = 0.0\n    # Process elements in chunks of BLOCK_SIZE_M\n    for m_offset in range(0, M, BLOCK_SIZE_M):\n        # Create indices and mask for current chunk\n        offs = tl.arange(0, BLOCK_SIZE_M)\n        m_indices = m_offset + offs\n        mask = m_indices < M\n        \n        # Load chunk of elements\n        ptrs = input_base + m_indices * stride_m\n        vals = tl.load(ptrs, mask=mask, other=0.0)\n        \n        # Reduce chunk and accumulate\n        chunk_sum = tl.sum(vals, axis=0)\n        total += chunk_sum\n    \n    # Store final result\n    tl.store(output_ptr + output_idx, total)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() != 3 or self.dim != 1:\n            return torch.sum(x, dim=self.dim, keepdim=True)\n            \n        x_contiguous = x.contiguous()\n        output = torch.empty(x_contiguous.shape[0], 1, x_contiguous.shape[2], \n                            device=x.device, dtype=x.dtype).contiguous()\n        \n        B, M, N = x_contiguous.shape\n        grid = (B * N,)\n        \n        # Use fixed block size of 1024 for parallel reduction\n        BLOCK = 1024\n        reduce_sum_kernel[grid](\n            x_contiguous, output,\n            x_contiguous.stride(0), x_contiguous.stride(1), x_contiguous.stride(2),\n            output.stride(0), output.stride(2),\n            B, M, N,\n            BLOCK_SIZE_M=BLOCK\n        )\n        return output\n\nbatch_size = 16\ndim1 = 256\ndim2 = 256\nreduce_dim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [reduce_dim]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "a2c954fa-3b51-41d7-aab3-d1b9a93cf823", "generation": 2, "timestamp": 1754641630.6891212, "iteration_found": 8, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.405, "runtime_ms": 0.0491, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0199, "speed_up": 0.405, "custom_timing": 0.0491}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.254, "runtime_ms": 0.0783, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0199, "speed_up": 0.254, "custom_timing": 0.0783}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
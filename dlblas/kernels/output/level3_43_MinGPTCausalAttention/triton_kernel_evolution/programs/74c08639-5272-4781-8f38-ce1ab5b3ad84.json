{"id": "74c08639-5272-4781-8f38-ce1ab5b3ad84", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef attention_kernel(\n    Q, K, V, sm_scale,\n    Out, stride_qz, stride_qh, stride_qm, stride_qk,\n    stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vk, stride_vn,\n    stride_oz, stride_oh, stride_om, stride_on,\n    seq_len,\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // stride_qh\n    off_h = off_hz % stride_qh\n    \n    q_offset = off_z * stride_qz + off_h * stride_qh\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + q_offset,\n        shape=(seq_len, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    k_offset = off_z * stride_kz + off_h * stride_kh\n    K_block_ptr = tl.make_block_ptr(\n        base=K + k_offset,\n        shape=(BLOCK_DMODEL, seq_len),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1)\n    )\n    v_offset = off_z * stride_vz + off_h * stride_vh\n    V_block_ptr = tl.make_block_ptr(\n        base=V + v_offset,\n        shape=(seq_len, BLOCK_DMODEL),\n        strides=(stride_vk, stride_vn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    o_offset = off_z * stride_oz + off_h * stride_oh\n    Out_block_ptr = tl.make_block_ptr(\n        base=Out + o_offset,\n        shape=(seq_len, BLOCK_DMODEL),\n        strides=(stride_om, stride_on),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    \n    q = tl.load(Q_block_ptr, boundary_check=(0, 1))\n    q = (q * sm_scale).to(tl.float16)\n    \n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    \n    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(K_block_ptr, boundary_check=(0, 1))\n        qk = tl.dot(q, k)\n        qk = tl.where(start_m * BLOCK_M + tl.arange(0, BLOCK_M)[:, None] >= start_n + tl.arange(0, BLOCK_N)[None, :], qk, float(\"-inf\"))\n        \n        m_ij = tl.maximum(tl.max(qk, 1), m_i)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        \n        alpha = tl.exp(m_i - m_ij)\n        acc = acc * alpha[:, None]\n        v = tl.load(V_block_ptr, boundary_check=(0, 1))\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        \n        l_i = l_i * alpha + l_ij\n        m_i = m_ij\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    \n    acc = acc / l_i[:, None]\n    tl.store(Out_block_ptr, acc.to(Out.type.element_ty), boundary_check=(0, 1))\n\ndef attention(q, k, v, causal, sm_scale):\n    BLOCK_M, BLOCK_N, num_stages = 128, 64, 3\n    grid = (triton.cdiv(q.shape[1], BLOCK_M), q.shape[0] * q.shape[2])\n    L = q.shape[-1]\n    out = torch.empty_like(q)\n    attention_kernel[grid](\n        q, k, v, sm_scale, out,\n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n        out.stride(0), out.stride(1), out.stride(2), out.stride(3),\n        q.shape[1],\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=L,\n        num_warps=4, num_stages=num_stages\n    )\n    return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        self.attn_dropout = nn.Dropout(attn_pdrop)\n        self.resid_dropout = nn.Dropout(resid_pdrop)\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size()\n        qkv = self.c_attn(x)\n        q, k, v = qkv.split(self.n_embd, dim=2)\n        \n        q = q.view(B, T, self.n_head, C // self.n_head).permute(0, 2, 1, 3)\n        k = k.view(B, T, self.n_head, C // self.n_head).permute(0, 2, 1, 3)\n        v = v.view(B, T, self.n_head, C // self.n_head).permute(0, 2, 1, 3)\n        \n        scale = 1.0 / math.sqrt(k.size(-1))\n        y = attention(q, k, v, causal=True, sm_scale=scale)\n        \n        y = y.permute(0, 2, 1, 3).reshape(B, T, C)\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nbatch_size = 128\nmax_seqlen = 1024\nseq_len = 512\nn_embd = 768\nn_head = 8\nattn_pdrop = 0.0\nresid_pdrop = 0.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, seq_len, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "7b28180a-4479-44da-82f9-841c24420a2b", "generation": 1, "timestamp": 1754595194.2234676, "iteration_found": 0, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_43_MinGPTCausalAttention", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2231, in make_block_ptr\n    return _semantic.make_block_ptr(base, shape, strides, offsets, block_shape, order)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1838, in make_block_ptr\n    return self.tensor(handle, tl.pointer_type(tl.block_type(base.type.element_ty, block_shape)))\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 726, in __init__\n    self.numel = validate_block_shape(self.shape)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/_utils.py\", line 52, in validate_block_shape\n    raise ValueError(f\"Shape element {i} must be a power of 2\")\nValueError: Shape element 1 must be a power of 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpgm7rpu0n.py\", line 131, in forward\n    y = attention(q, k, v, causal=True, sm_scale=scale)\n  File \"/tmp/tmpgm7rpu0n.py\", line 98, in attention\n    attention_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 17:18:\n    stride_oz, stride_oh, stride_om, stride_on,\n    seq_len,\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // stride_qh\n    off_h = off_hz % stride_qh\n\n    q_offset = off_z * stride_qz + off_h * stride_qh\n    Q_block_ptr = tl.make_block_ptr(\n                  ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2231, in make_block_ptr\n    return _semantic.make_block_ptr(base, shape, strides, offsets, block_shape, order)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1838, in make_block_ptr\n    return self.tensor(handle, tl.pointer_type(tl.block_type(base.type.element_ty, block_shape)))\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 726, in __init__\n    self.numel = validate_block_shape(self.shape)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/_utils.py\", line 52, in validate_block_shape\n    raise ValueError(f\"Shape element {i} must be a power of 2\")\nValueError: Shape element 1 must be a power of 2\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.318, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 12.9, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.318, "speed_up": 0.025, "custom_timing": 12.9}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
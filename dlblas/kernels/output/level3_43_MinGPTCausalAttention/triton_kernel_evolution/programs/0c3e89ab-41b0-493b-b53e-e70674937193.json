{"id": "0c3e89ab-41b0-493b-b53e-e70674937193", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _attn_forward_kernel(\n    Q, K, V, Out,\n    stride_qb, stride_qt, stride_qd,\n    stride_kb, stride_kt, stride_kd,\n    stride_vb, stride_vt, stride_vd,\n    stride_ob, stride_ot, stride_od,\n    T: tl.constexpr, hs: tl.constexpr,\n    scale: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_HEAD: tl.constexpr\n):\n    pid = tl.program_id(0)\n    batch_head = pid\n    num_pid_n = tl.cdiv(T, BLOCK_N)\n    pid_m = tl.program_id(1)\n    \n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEAD)\n    \n    Q_ptr = Q + batch_head * stride_qb + offs_m[:, None] * stride_qt + offs_d[None, :] * stride_qd\n    K_ptr = K + batch_head * stride_kb + offs_n[None, :] * stride_kt + offs_d[:, None] * stride_kd\n    V_ptr = V + batch_head * stride_vb + offs_n[:, None] * stride_vt + offs_d[None, :] * stride_vd\n    \n    m_i = tl.full([BLOCK_M], float('-inf'), dtype=tl.float32)\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_HEAD], dtype=tl.float32)\n    \n    for block_n in range(0, num_pid_n):\n        n_start = block_n * BLOCK_N\n        \n        q = tl.load(Q_ptr, mask=(offs_m[:, None] < T) & (offs_d[None, :] < hs), other=0.0)\n        k = tl.load(K_ptr, mask=(offs_n[None, :] < T - n_start) & (offs_d[:, None] < hs), other=0.0)\n        \n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        for d in range(0, hs, BLOCK_HEAD):\n            qd = tl.load(Q_ptr + d, mask=(offs_m[:, None] < T) & (d + offs_d[None, :] < hs), other=0.0)\n            kd = tl.load(K_ptr + d * stride_kd, mask=(offs_n[None, :] < T - n_start) & (d + offs_d[:, None] < hs), other=0.0)\n            qk += tl.dot(qd, kd, allow_tf32=True)\n        \n        qk *= scale\n        mask_n = n_start + offs_n\n        causal_mask = mask_n[None, :] <= offs_m[:, None]\n        qk = tl.where(causal_mask, qk, float('-inf'))\n        \n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        \n        alpha = tl.exp(m_ij - m_i)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        \n        v = tl.load(V_ptr, mask=(mask_n[:, None] < T) & (offs_d[None, :] < hs), other=0.0)\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v, allow_tf32=True)\n        \n        m_i = tl.maximum(m_i, m_ij)\n        K_ptr += BLOCK_N * stride_kt\n        V_ptr += BLOCK_N * stride_vt\n    \n    acc = acc / l_i[:, None]\n    offs_d = tl.arange(0, BLOCK_HEAD)\n    Out_ptr = Out + batch_head * stride_ob + offs_m[:, None] * stride_ot + offs_d[None, :] * stride_od\n    tl.store(Out_ptr, acc, mask=(offs_m[:, None] < T) & (offs_d[None, :] < hs))\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A vanilla multi-head masked self-attention layer with a projection at the end.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # regularization\n        self.attn_dropout = nn.Dropout(attn_pdrop)\n        self.resid_dropout = nn.Dropout(resid_pdrop)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        \n        hs = C // self.n_head\n        scale = 1.0 / math.sqrt(hs)\n        \n        # Combine batch and head dimensions\n        q = q.contiguous()\n        k = k.contiguous()\n        v = v.contiguous()\n        y = torch.empty_like(v)\n        \n        # Configure kernel grid\n        grid = lambda META: (B * self.n_head, triton.cdiv(T, META['BLOCK_M']))\n        \n        # Launch kernel\n        _attn_forward_kernel[grid](\n            q, k, v, y,\n            q.stride(0), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(2), v.stride(3),\n            y.stride(0), y.stride(2), y.stride(3),\n            T, hs, scale,\n            BLOCK_M=64, BLOCK_N=64, BLOCK_HEAD=32\n        )\n        \n        # Reassemble outputs\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nbatch_size = 128\nmax_seqlen = 1024\nseq_len = 512\nn_embd = 768\nn_head = 8\nattn_pdrop = 0.0\nresid_pdrop = 0.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, seq_len, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "7b28180a-4479-44da-82f9-841c24420a2b", "generation": 1, "timestamp": 1754595914.8389785, "iteration_found": 3, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/evaluator.py\", line 173, in _verification_worker\n    result: KernelExecResult = eval_kernel_against_ref(\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 578, in eval_kernel_against_ref\n    graceful_eval_cleanup(context, device, tempfile)\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 230, in graceful_eval_cleanup\n    torch.cuda.empty_cache()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/cuda/memory.py\", line 224, in empty_cache\n    torch._C._cuda_emptyCache()\ntorch.AcceleratorError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n", "exec_log": "[Child] \u7ed1\u5b9a GPU 2, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 10, in <module>\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 844, in jit\n    return decorator(fn)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 832, in decorator\n    return JITFunction(\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 635, in __init__\n    self.starting_line_number = inspect.getsourcelines(fn)[1]\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/inspect.py\", line 1121, in getsourcelines\n    lines, lnum = findsource(object)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/inspect.py\", line 958, in findsource\n    raise OSError('could not get source code')\nOSError: could not get source code\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 12.9, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.318, "speed_up": 0.025, "custom_timing": 12.9}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
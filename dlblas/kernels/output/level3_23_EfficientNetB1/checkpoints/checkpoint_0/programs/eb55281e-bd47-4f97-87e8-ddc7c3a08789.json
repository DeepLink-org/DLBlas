{"id": "eb55281e-bd47-4f97-87e8-ddc7c3a08789", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef relu6_kernel(\n    x_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    zero = tl.full(x.shape, 0, dtype=x.dtype)\n    six = tl.full(x.shape, 6.0, dtype=x.dtype)\n    x = tl.where(x < zero, zero, x)\n    x = tl.where(x > six, six, x)\n    tl.store(output_ptr + offsets, x, mask=mask)\n\nclass ReLU6Triton(nn.Module):\n    def __init__(self):\n        super(ReLU6Triton, self).__init__()\n    \n    def forward(self, x):\n        output = torch.empty_like(x)\n        n_elements = output.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        relu6_kernel[grid](x, output, n_elements, BLOCK_SIZE=1024)\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.mbconv1 = self._make_mbconv_block(32, 16, 1, 1)\n        self.mbconv2 = self._make_mbconv_block(16, 24, 2, 6)\n        self.mbconv3 = self._make_mbconv_block(24, 40, 2, 6)\n        self.mbconv4 = self._make_mbconv_block(40, 80, 2, 6)\n        self.mbconv5 = self._make_mbconv_block(80, 112, 1, 6)\n        self.mbconv6 = self._make_mbconv_block(112, 192, 2, 6)\n        self.mbconv7 = self._make_mbconv_block(192, 320, 1, 6)\n        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(1280)\n        self.fc = nn.Linear(1280, num_classes)\n    \n    def _make_mbconv_block(self, in_channels, out_channels, stride, expand_ratio):\n        hidden_dim = round(in_channels * expand_ratio)\n        return nn.Sequential(\n            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            ReLU6Triton(),\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            ReLU6Triton(),\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.mbconv1(x)\n        x = self.mbconv2(x)\n        x = self.mbconv3(x)\n        x = self.mbconv4(x)\n        x = self.mbconv5(x)\n        x = self.mbconv6(x)\n        x = self.mbconv7(x)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\nbatch_size = 10\ninput_shape = (3, 240, 240)\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "8e5a66de-92ef-495b-a86a-e0b877cc446c", "generation": 1, "timestamp": 1754587370.3150868, "iteration_found": 0, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.798, "runtime_ms": 1.88, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.5, "speed_up": 0.798, "custom_timing": 1.88}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 1.67, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.5, "speed_up": 0.898, "custom_timing": 1.67}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 1.6700\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.5, 'speed_up': 0.898, 'custom_timing': 1.67}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 1.6700, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.5, 'speed_up': 0.898, 'custom_timing': 1.67}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        EfficientNetB1 architecture implementation.\n\n        :param num_classes: The number of output classes (default is 1000 for ImageNet).\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        # Initial convolutional layer\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        # MBConv blocks\n        self.mbconv1 = self._make_mbconv_block(32, 16, 1, 1)\n        self.mbconv2 = self._make_mbconv_block(16, 24, 2, 6)\n        self.mbconv3 = self._make_mbconv_block(24, 40, 2, 6)\n        self.mbconv4 = self._make_mbconv_block(40, 80, 2, 6)\n        self.mbconv5 = self._make_mbconv_block(80, 112, 1, 6)\n        self.mbconv6 = self._make_mbconv_block(112, 192, 2, 6)\n        self.mbconv7 = self._make_mbconv_block(192, 320, 1, 6)\n        \n        # Final convolutional layer\n        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(1280)\n        \n        # Fully connected layer\n        self.fc = nn.Linear(1280, num_classes)\n    \n    def _make_mbconv_block(self, in_channels, out_channels, stride, expand_ratio):\n        \"\"\"\n        Creates a MBConv block.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param stride: Stride of the depthwise convolution.\n        :param expand_ratio: Expansion ratio for the hidden layer.\n        :return: A sequential MBConv block.\n        \"\"\"\n        hidden_dim = round(in_channels * expand_ratio)\n        return nn.Sequential(\n            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True),\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True),\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the EfficientNetB1 model.\n\n        :param x: Input tensor, shape (batch_size, 3, 240, 240)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = F.relu(self.bn1(self.conv1(x)))\n        \n        x = self.mbconv1(x)\n        x = self.mbconv2(x)\n        x = self.mbconv3(x)\n        x = self.mbconv4(x)\n        x = self.mbconv5(x)\n        x = self.mbconv6(x)\n        x = self.mbconv7(x)\n        \n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        \n        return x\n\n# Test code\nbatch_size = 10\ninput_shape = (3, 240, 240)\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (1.6700), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.5, 'speed_up': 0.898, 'custom_timing': 1.67})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        EfficientNetB1 architecture implementation.\n\n        :param num_classes: The number of output classes (default is 1000 for ImageNet).\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        # Initial convolutional layer\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        # MBConv blocks\n        self.mbconv1 = self._make_mbconv_block(32, 16, 1, 1)\n        self.mbconv2 = self._make_mbconv_block(16, 24, 2, 6)\n        self.mbconv3 = self._make_mbconv_block(24, 40, 2, 6)\n        self.mbconv4 = self._make_mbconv_block(40, 80, 2, 6)\n        self.mbconv5 = self._make_mbconv_block(80, 112, 1, 6)\n        self.mbconv6 = self._make_mbconv_block(112, 192, 2, 6)\n        self.mbconv7 = self._make_mbconv_block(192, 320, 1, 6)\n        \n        # Final convolutional layer\n        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(1280)\n        \n        # Fully connected layer\n        self.fc = nn.Linear(1280, num_classes)\n    \n    def _make_mbconv_block(self, in_channels, out_channels, stride, expand_ratio):\n        \"\"\"\n        Creates a MBConv block.\n\n        :param in_channels: Number of input channels.\n        :param out_channels: Number of output channels.\n        :param stride: Stride of the depthwise convolution.\n        :param expand_ratio: Expansion ratio for the hidden layer.\n        :return: A sequential MBConv block.\n        \"\"\"\n        hidden_dim = round(in_channels * expand_ratio)\n        return nn.Sequential(\n            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True),\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True),\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the EfficientNetB1 model.\n\n        :param x: Input tensor, shape (batch_size, 3, 240, 240)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = F.relu(self.bn1(self.conv1(x)))\n        \n        x = self.mbconv1(x)\n        x = self.mbconv2(x)\n        x = self.mbconv3(x)\n        x = self.mbconv4(x)\n        x = self.mbconv5(x)\n        x = self.mbconv6(x)\n        x = self.mbconv7(x)\n        \n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        \n        return x\n\n# Test code\nbatch_size = 10\ninput_shape = (3, 240, 240)\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch implementation of EfficientNetB1. The goal is to rewrite it using Triton for performance improvements.\n However, note that the entire model is built with PyTorch layers. We are not given a specific kernel to optimize.\n\n After reading the context, we see that the dataset consists of kernelbench files (levels 1-5) and each has a PyTorch reference and a Triton custom code.\n The current program is a PyTorch model (EfficientNetB1) and the task is to rewrite it to use Triton for performance.\n\n But note: The example provided was about a quantization kernel. The current program is a full model.\n\n How to proceed?\n The constraints say:\n   - We must not change function signatures, grid configuration, output shapes, etc.\n\n However, the current program is a full model without any Triton code.\n\n Since the problem states: \"Generate functionally equivalent but faster Triton implementations (custom_code)\", \n we are expected to replace the PyTorch operations with Triton kernels where beneficial.\n\n But note: The entire model is a sequence of convolutional layers and batch norms. We cannot rewrite the entire model in one Triton kernel.\n\n The instructions also say: \"Modify ONLY the Triton kernel source within this block\" and we have an EVOLVE-BLOCK.\n\n However, in the current code there is no Triton kernel. So we are in Phase 1: Compilation Pass? But we are at stage \"reference code\".\n\n The evaluation pipeline:\n   1) /verify_plus - Verify compilation & correctness\n\n We are told that the current performance has a speed_up of 0.898 (which is less than 1) meaning the custom code is slower.\n\n The problem: We are to rewrite the program to improve performance.\n\n How can we use Triton?\n We can replace some of the operations (like the convolutions) with Triton kernels. However, note that the model has multiple layers.\n\n Since the entire model is built with nn.Modules, we cannot replace the entire forward pass with one kernel.\n\n Approach:\n\n We are allowed to change the internal implementation. We can try to replace the MBConv blocks with a Triton-optimized version.\n\n However, note the constraints:\n   - We cannot change the function signatures of @triton.jit functions (but there are none currently).\n   - We cannot modify grid configuration, output tensor shapes, or PID logic (but we are introducing new kernels).\n\n We must be cautious.\n\n Let's break down:\n\n The current model has:\n   - Initial conv2d, bn, ReLU\n   - 7 MBConv blocks (each is a sequential of conv, bn, relu, depthwise conv, bn, relu, conv, bn)\n   - Final conv2d, bn, ReLU, pooling, linear.\n\n The MBConv blocks are the main building blocks and might be the bottleneck.\n\n We are going to focus on the MBConv block. We can write a Triton kernel for the depthwise convolution? Or for the entire block?\n\n But note: The depthwise convolution in PyTorch is already efficient. However, we might try to write a fused kernel for the entire MBConv block.\n\n However, the MBConv block has an expansion and then a projection. We have:\n\n   Step 1: 1x1 conv to expand channels (with groups=1)\n   Step 2: 3x3 depthwise conv (with groups=hidden_dim)\n   Step 3: 1x1 conv to project channels back.\n\n We can write a Triton kernel that fuses the entire MBConv block? That would require a lot of work.\n\n Alternatively, we can write a Triton kernel for the depthwise convolution? But note that the depthwise convolution is already implemented in PyTorch with cuDNN.\n\n Given the complexity and the fact that we are not given a specific kernel to optimize, we must reconsider.\n\n The example provided was a single function (per_row_quantize_int8) that was replaced by a Triton kernel.\n\n In our case, we are to optimize the entire model. We cannot write the entire model in one Triton kernel.\n\n We might need to break the model and write Triton kernels for specific operations.\n\n However, note the instruction: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\n We are to replace the current code block with a new one that uses Triton.\n\n Since the current model is slow (speed_up 0.898) we need to improve it.\n\n How about we try to fuse the operations within the MBConv block? We can write a Triton kernel that does:\n\n   - The first 1x1 convolution, then ReLU6, then the 3x3 depthwise convolution, then ReLU6, then the 1x1 convolution.\n\n But note: There are batch normalization layers in between. We cannot easily fuse the batch norm with the convolution in Triton without training?\n\n However, in inference, we can fuse the conv and bn. The current model is in inference mode? The test code does not specify training.\n\n We can precompute the fused weights for the conv and bn for each of the three convolutions. Then we can write a Triton kernel that does:\n\n   input -> (fused conv1x1) -> ReLU6 -> (fused depthwise conv3x3) -> ReLU6 -> (fused conv1x1)\n\n But note: The model is defined as a sequence of layers. We cannot change the structure arbitrarily? We must maintain the same functionality.\n\n However, the problem says: \"functionally equivalent\".\n\n Steps:\n\n 1. Precompute the fused weights and biases for each convolution that is followed by a batch norm (for inference).\n\n    For a convolution followed by batch norm, we can do:\n\n        fused_weight = (conv.weight * (bn.weight / (bn.running_var + bn.eps)).sqrt())[:, :, None, None]\n        fused_bias = bn.bias + (conv.bias - bn.running_mean) * (bn.weight / (bn.running_var + bn.eps)).sqrt()\n\n    But note: the convolutions in the MBConv block are without bias (bias=False). So we have:\n\n        fused_bias = bn.bias - bn.running_mean * (bn.weight / (bn.running_var + bn.eps)).sqrt()\n\n    And then the fused operation is: y = F.conv2d(x, fused_weight, fused_bias, ...)\n\n 2. Then we can write a Triton kernel that does the entire MBConv block without intermediate storage.\n\n However, this is a complex kernel and beyond the scope of a small iterative step.\n\n Given the constraints of the problem (small iterative steps, modify <=20 lines per revision) and the fact that we are at the beginning (Phase 1: Compilation Pass), we might not be able to do this.\n\n Alternatively, we can try to optimize only the depthwise convolution? But note that the depthwise convolution is a 3x3 with groups=hidden_dim (which is the same as the number of input channels). This is already handled by cuDNN.\n\n What if we replace the entire MBConv block with a single Triton kernel? We are going to try a simpler approach:\n\n   We write a Triton kernel that does the entire MBConv block for one block. We call this kernel for each block.\n\n But note: the MBConv block has residual connections? The current model does not have residual connections. The MBConv blocks in EfficientNet usually have residual when stride=1 and input and output channels are the same. But in our code:\n\n   The first block: in=32, out=16 -> no residual\n   The second block: in=16, out=24, stride=2 -> no residual\n\n So we don't have residuals.\n\n However, the current implementation of the MBConv block is:\n\n   def _make_mbconv_block(...):\n        return nn.Sequential( ... )\n\n   and in forward we do: x = self.mbconv1(x), etc.\n\n We can replace the nn.Sequential block with a custom Triton-based module that calls a kernel.\n\n But note: the kernel would have to handle:\n\n   - The expansion (1x1 conv) -> ReLU6\n   - The depthwise (3x3 conv) -> ReLU6\n   - The projection (1x1 conv)\n\n This is a lot of work and might not be within the 20-line constraint.\n\n Considering the complexity and the fact that the current model is already implemented in PyTorch, and we are only allowed to change the EVOLVE-BLOCK, we might not have the time to write such a kernel.\n\n Alternative idea:\n\n   We are told that the current custom_timing is 1.67 ms and the reference_timing is 1.5 ms (so speed_up=0.898). This is for the entire model.\n\n   We might get speedup by just fusing the operations without writing a Triton kernel? We can use PyTorch's built-in fusion or use TensorRT.\n\n   But the problem requires Triton.\n\n Given the instructions, we are to use Triton.\n\n Let's try to write a Triton kernel for the depthwise convolution? But note: the depthwise convolution is already efficient.\n\n Another idea: the ReLU6 activation might be fused with the convolution. We can write a kernel that does convolution (with or without bias) and then ReLU6.\n\n However, we have batch norm in between: the convolution is followed by batch norm and then ReLU6.\n\n We can fuse the conv->bn->ReLU6. This is a known optimization.\n\n We can precompute the fused weights and biases for the conv->bn as above, and then we can write a kernel that does:\n\n   conv_bn_fused -> ReLU6\n\n But note: we are in Triton, and we are not writing the entire convolution in Triton? We can use PyTorch's fused conv2d?\n\n However, the problem requires Triton.\n\n Alternatively, we can write a Triton kernel that does the entire conv->bn->ReLU6 for one layer? But again, we have multiple layers.\n\n Considering the complexity and the line limit, we might not be able to do the entire model.\n\n We are stuck. We need to think of a simpler optimization.\n\n What if we focus on the pointwise convolutions? They are 1x1 and might be memory bound. We can write a Triton kernel for the 1x1 convolution that is optimized for memory access.\n\n The 1x1 convolution can be seen as a matrix multiplication (if we reshape the input to [batch*H*W, in_channels] and the weight to [out_channels, in_channels]).\n\n We can write a Triton kernel for matrix multiplication and use it for the 1x1 convolutions.\n\n But note: we have to do the same for the depthwise? The depthwise is not a standard matrix multiplication.\n\n However, the depthwise convolution is a special case where each channel is convolved with its own filter. We can also write a Triton kernel for depthwise convolution.\n\n Given the time, we might choose to replace only the 1x1 convolutions with a Triton kernel. We can write a Triton kernel for 1x1 convolution that fuses the batch norm and ReLU6.\n\n But note: the MBConv block has two 1x1 convolutions (the first and the last) and one 3x3 depthwise.\n\n We can try to replace the first 1x1 conv (which is an expansion) and the last 1x1 conv (which is a projection) with Triton kernels.\n\n How about we write a Triton kernel for the 1x1 convolution that also does the batch norm and ReLU6?\n\n We call it `conv1x1_bn_relu6_triton` and use it in the MBConv block.\n\n Steps:\n\n   In _make_mbconv_block, instead of having:\n\n        nn.Conv2d(...),\n        nn.BatchNorm2d(...),\n        nn.ReLU6(...)\n\n   we can have a single custom module that does the fused operation using a Triton kernel.\n\n   Similarly for the last 1x1 conv in the block (without ReLU? because the last layer in the block is just conv and bn?).\n\n   But looking at the current block:\n\n        Sequential(\n            Conv2d -> bn -> ReLU6,\n            Conv2d (depthwise) -> bn -> ReLU6,\n            Conv2d -> bn   # no ReLU6 at the end?\n        )\n\n   So the last layer does not have ReLU6.\n\n   Therefore, we need two kernels:\n\n      Kernel1: conv1x1 + bn + ReLU6\n      Kernel2: conv1x1 + bn   (without activation)\n\n   We can write one kernel that optionally does ReLU6.\n\n Given the 20-line constraint, we might not be able to write two kernels.\n\n Alternatively, we can write one kernel that does the 1x1 conv and bn, and then we do the ReLU6 in a separate step? That would not save much.\n\n Due to the complexity and the fact that we are at the beginning, we might start by replacing only one operation.\n\n Let's replace the first 1x1 conv+bn+ReLU6 in the MBConv block with a Triton kernel.\n\n We design a custom Triton-based module for conv1x1_bn_relu6.\n\n How to write the Triton kernel for conv1x1:\n\n   We can treat it as a matrix multiplication.\n\n   Input: [batch, in_channels, H, W]\n   Weight: [out_channels, in_channels, 1, 1]\n   Output: [batch, out_channels, H, W]\n\n   Steps:\n\n     Reshape the input to [batch * H * W, in_channels]\n     Reshape the weight to [out_channels, in_channels]\n     Then output = input @ weight.t()   [batch*H*W, out_channels]\n\n   Then we add the fused bias (for bn) and then do ReLU6.\n\n   But note: we have to fuse the batch norm. We precompute the fused weight and bias for the conv->bn.\n\n   Then the kernel will do:\n\n        output = (input @ fused_weight.t()) + fused_bias   [broadcasted]\n        output = relu6(output)\n\n   Then reshape to [batch, out_channels, H, W]\n\n We can write a Triton kernel for this matrix multiplication.\n\n However, note: the 1x1 convolution in the MBConv block might have a large expansion ratio (e.g., 6) so the in_channels and out_channels might be large.\n\n We can write a Triton kernel for matrix multiplication with the following:\n\n   - We assume the input is 2D: [M, K] and weight is [N, K] -> output [M, N]\n\n   We can use the standard Triton matmul example.\n\n But note: the example in the Triton tutorial is for FP16 and uses tensor cores.\n\n We are on H100 (SM8.9) which supports tensor cores for FP16.\n\n We can write a kernel that uses tensor cores for the matrix multiplication.\n\n Steps for the kernel:\n\n   We will have:\n      input_ptr: pointer to input matrix (M, K) in row-major, FP16? (we are using FP16 for efficiency? but the model is in FP32 by default)\n\n   However, the model is in FP32. We might need to convert to FP16? But the problem does not specify.\n\n   We are allowed to change the internal implementation as long as we maintain functionality.\n\n   We can do the computation in FP16 if we are sure it maintains FP32 tolerance? But the tolerance is for correctness (float32 tolerance).\n\n   Alternatively, we can do the computation in FP32. But tensor cores are faster for FP16.\n\n   We can change the model to use mixed precision? But that might change the behavior.\n\n   Given the constraints, we should stick to FP32 for now.\n\n   However, the tensor cores on H100 also support TF32 (which is a truncated FP32) and we can use it.\n\n   We can set the environment variable to allow TF32? But that is outside the kernel.\n\n   Alternatively, we can write the kernel in FP32 and not use tensor cores? That would not be faster.\n\n   We are in a dilemma.\n\n Given the time, we decide to write a kernel that does the fused operation (matmul in FP32) without tensor cores? That might not be faster than cuBLAS.\n\n But note: our matmul is small? The M = batch * H * W, which for an image of 240x240 and batch=10, M=10*240*240 = 576000, and K=in_channels (say 32) and N=out_channels (16*6=96 for the first expansion). This is a small matrix multiplication.\n\n We might not get a speedup.\n\n Considering the complexity and the line limit, we might abandon this approach.\n\n Alternative idea: we are told to do small iterative steps. We are at the reference code. We are allowed to do any optimization that does not change functionality.\n\n We notice that the current model does not use any of the known efficient implementations of MBConv. For example, we can use the `torch.mobile_optimizer` or fuse the layers using PyTorch's built-in fusion.\n\n But the problem requires Triton.\n\n We might not have a clear path.\n\n Given the above, and the fact that the current model is already implemented in PyTorch and we are not seeing a clear Triton optimization that fits within the constraints, we might consider a different approach.\n\n We are allowed to change the internal implementation. We can try to replace the depthwise convolution with a Triton kernel? The depthwise convolution is a 3x3 convolution with groups=hidden_dim.\n\n We can write a Triton kernel for depthwise convolution that is optimized for the specific filter size (3x3) and then fuse the batch norm and ReLU6.\n\n Steps for depthwise convolution:\n\n   Input: [batch, in_channels, H, W]\n   Weight: [out_channels, 1, 3, 3]   (since groups=in_channels, and out_channels must be divisible by groups? here out_channels = in_channels)\n\n   We can write a kernel that loads a block of the input (a tile) and then applies the 3x3 filter.\n\n   We can then fuse the batch norm and ReLU6.\n\n This is a known optimization and might be faster than cuDNN for some cases.\n\n We will write a Triton kernel for depthwise convolution (3x3) with fused bn and ReLU6.\n\n We design a custom module for the depthwise part:\n\n   Instead of:\n        nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),\n        nn.BatchNorm2d(hidden_dim),\n        nn.ReLU6(inplace=True)\n\n   we will have a single module that does the depthwise conv, bn, and ReLU6 in one Triton kernel.\n\n We will call this kernel `depthwise_conv3x3_bn_relu6_triton`.\n\n We will then replace the depthwise layer in the MBConv block with this custom module.\n\n This is a focused optimization and might give us speedup.\n\n How to write the Triton kernel for depthwise convolution:\n\n   We can use the following approach:\n\n      Each channel is independent. So we can assign one program per channel? But that would not be efficient.\n\n      Alternatively, we can have a 2D grid: [batch * height * width, channels]? Not exactly.\n\n      We can have a 2D grid: [channels, batch * height * width] but then we would have to iterate over the spatial dimension.\n\n   Better: we can have a 3D grid: [channels, batch, height] and then each block handles a row of width?\n\n   Given the complexity, we might follow the standard convolution example in Triton.\n\n   There is an example in the Triton tutorials for convolution. We can adapt it for depthwise.\n\n   However, note: depthwise means that each channel is convolved with its own filter, so we can treat it as a convolution with groups=in_channels.\n\n   The kernel will have:\n\n        pid_batch = program_id(0)\n        pid_channel = program_id(1)\n        pid_y = program_id(2)   // for output row\n\n   Then we iterate over the width? Or use a block for a tile.\n\n   We can do:\n\n        output = output of depthwise conv for one channel for a tile of the image.\n\n   Then we add the fused bn and ReLU6.\n\n   The fused bn: we have a scale and bias per channel:\n\n        output = (output - bn.running_mean[pid_channel]) * (bn.weight[pid_channel] / (bn.running_var[pid_channel] + bn.eps).sqrt()) + bn.bias[pid_channel]\n\n   Then ReLU6: output = min(max(output, 0), 6)\n\n   We will precompute the scale and shift for the bn fusion:\n\n        scale = bn.weight / (bn.running_var + bn.eps).sqrt()\n        shift = bn.bias - bn.running_mean * scale\n\n   Then in the kernel: output = output * scale[channel] + shift[channel]\n\n   Then ReLU6.\n\n   We have to be cautious with the boundaries.\n\n Given the time, we will outline the Triton kernel for depthwise_conv3x3_bn_relu6.\n\n But note: we are limited to 20 lines of change. We cannot write the entire kernel in this block.\n\n We will have to break the problem into steps:\n\n   Step 1: Write the custom module for the fused depthwise conv3x3_bn_relu6.\n   Step 2: Write the Triton kernel for the depthwise convolution (without bn and relu6 first) and then add them.\n\n However, we are in Phase 1: Compilation Pass. We can start with a simple kernel that compiles and then add the fusion.\n\n Given the complexity, we might not be able to do it in one step.\n\n We decide to abandon the depthwise kernel for now and look for a simpler optimization.\n\n Another idea: the final fully connected layer. We can write a Triton kernel for the matrix multiplication of the fully connected layer.\n\n   The input to the fc layer is [batch, 1280]\n   The weight is [num_classes, 1280]\n\n   We can write a Triton kernel for matrix multiplication.\n\n   This is a standard matmul and might be faster if we use tensor cores.\n\n   We can write a kernel in FP16 with tensor cores.\n\n   But the model is in FP32. We would have to convert the input to FP16 and then convert the output back to FP32.\n\n   We can try to do:\n\n        output = fc_triton(x.half(), weight.half(), bias.half()).float()\n\n   But that might break the numerical tolerance.\n\n   Alternatively, we can do the matmul in FP32 without tensor cores.\n\n Given the time, we decide to write a Triton kernel for the fully connected layer.\n\n We will write a kernel that does:\n\n        input: [batch, in_features] -> FP32\n        weight: [out_features, in_features] -> FP32\n        output: [batch, out_features] = input @ weight.t()\n\n   and then add bias.\n\n   We can use the Triton matmul example in FP32.\n\n   However, the example in FP32 might not be faster than cuBLAS.\n\n Given the complexity and the line limit, we might not be able to write a full matmul kernel.\n\n We are running out of options.\n\n Given the above, we might have to reconsider. The current speed_up is 0.898, which is already close to 1. \n\n We are to do small iterative steps. We might start by fusing the batch norm with the following ReLU6 in the depthwise layer? But that is already in PyTorch as `torch.nn.intrinsic.BNReLU` but for Conv+BN+ReLU we have `torch.nn.intrinsic.ConvBnReLU`.\n\n However, the problem requires Triton.\n\n We decide to do nothing for now and submit the same code, because we cannot find a suitable Triton optimization within the constraints.\n\n But that would not improve the speed.\n\n Given the instructions, we must return the code with the EVOLVE-BLOCK.\n\n We will submit the same code as the current program, because we cannot write a Triton kernel in 20 lines that optimizes the entire model.\n\n However, this is not satisfactory.\n\n We notice that the current program has a speed_up of 0.898, which is less than 1. The reference_timing is 1.5 ms and custom_timing is 1.67 ms.\n\n The reference_timing is the time for the reference code (which is the current code) and the custom_timing is for our code? \n But the current code is the reference code? \n\n The meta says: \n   'reference_timing_ms': 1.5, \n   'speed_up': 0.898, \n   'custom_timing': 1.67\n\n So the reference code runs in 1.5 ms and our custom code runs in 1.67 ms. \n\n The speed_up is custom_timing / reference_timing? Or reference_timing / custom_timing?\n\n Usually speed_up = reference_time / custom_time, so if custom_time > reference_time, speed_up < 1.\n\n The problem says: Speedup > 1 required for points.\n\n We are to improve the custom code to be faster than the reference.\n\n How about we try to simply replace the ReLU6 with a Triton kernel? ReLU6 is a simple pointwise operation.\n\n We can write a Triton kernel for ReLU6 and then use it in the model.\n\n This might not give us speedup because ReLU6 is memory bound and already efficient in PyTorch.\n\n But we are to try.\n\n We will write a Triton kernel for ReLU6 and use it in the MBConv block.\n\n We will replace every `nn.ReLU6(inplace=True)` with a call to our Triton kernel.\n\n Steps:\n\n   We write a kernel that does:\n\n        output = min(max(input, 0), 6)\n\n   for every element.\n\n   We will have a grid that covers the entire tensor.\n\n   The kernel will be very simple.\n\n   Example:\n\n        @triton.jit\n        def relu6_kernel(\n            x_ptr,\n            output_ptr,\n            n_elements,\n            BLOCK_SIZE: tl.constexpr,\n        ):\n            pid = tl.program_id(axis=0)\n            block_start = pid * BLOCK_SIZE\n            offsets = block_start + tl.arange(0, BLOCK_SIZE)\n            mask = offsets < n_elements\n            x = tl.load(x_ptr + offsets, mask=mask)\n            x = x + 0  # ensure it's a value\n            x = tl.where(x < 0, 0, x)\n            x = tl.where(x > 6, 6, x)\n            tl.store(output_ptr + offsets, x, mask=mask)\n\n   And then we create a function:\n\n        def relu6_triton(x):\n            output = torch.empty_like(x)\n            n_elements = output.numel()\n            grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n            relu6_kernel[grid](x, output, n_elements, BLOCK_SIZE=1024)\n            return output\n\n   Then in the model, we replace `nn.ReLU6(inplace=True)` with this function.\n\n   But note: the ReLU6 in the model is in the sequential block. We cannot easily replace it because the sequential expects a module.\n\n   We can create a module:\n\n        class ReLU6Triton(nn.Module):\n            def forward(self, x):\n                return relu6_triton(x)\n\n   And then use it.\n\n   However, we must be cautious: the ReLU6 in the original model is in-place. We are not doing in-place.\n\n   We can do in-place by having the kernel store in the same tensor? But we cannot in Triton because we are using a new tensor.\n\n   Alternatively, we can have the kernel modify the input tensor? But the input might be used later.\n\n   We see that in the MBConv block, the ReLU6 is after the conv and bn, and the output is passed to the next layer. We can do out-of-place.\n\n   But the original used `inplace=True` to save memory. We are trading memory for time? We are not sure.\n\n   Given the small change, we might try.\n\n   We will replace in the _make_mbconv_block:\n\n        nn.ReLU6(inplace=True)   ->   ReLU6Triton()\n\n   and similarly for the second ReLU6.\n\n   We then hope that the Triton kernel is faster than the PyTorch ReLU6.\n\n   But note: ReLU6 is very simple and likely memory bound. The Triton kernel might not be faster.\n\n Given that we have no other option, we try.\n\n We will change the following lines in the EVOLVE-BLOCK:\n\n   - We will add the Triton kernel for ReLU6 and the module ReLU6Triton.\n   - In the _make_mbconv_block, we will replace every `nn.ReLU6(inplace=True)` with `ReLU6Triton()`.\n\n   We will also remove the `inplace=True` because our Triton kernel is out-of-place.\n\n   We must also remove the in-place in the forward pass? The original had `inplace=True` in the ReLU6.\n\n   But the model does not have any in-place operation that would be affected by this (like using the same tensor in two places) because it is sequential.\n\n   This change is within 20 lines.\n\n Let's do it.\n\n We will also set the block size to a reasonable number (1024) and use autotune for the kernel? But to keep it simple, we won't autotune.\n\n We hope that this will give us a speedup.\n\n If not, we might need to try something else in the next iteration.\n\n Note: We are also to maintain the same functionality. ReLU6 is the same.\n\n We will now write the code.\n\n However, note: the kernel must be compiled. We are in Phase 1: Compilation Pass.\n\n We will ensure the kernel compiles.\n\n We will also note that the model is on GPU, so the kernel will be on GPU.\n\n Let's write the code.\n</think>\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef relu6_kernel(\n    x_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    zero = tl.full(x.shape, 0, dtype=x.dtype)\n    six = tl.full(x.shape, 6.0, dtype=x.dtype)\n    x = tl.where(x < zero, zero, x)\n    x = tl.where(x > six, six, x)\n    tl.store(output_ptr + offsets, x, mask=mask)\n\nclass ReLU6Triton(nn.Module):\n    def __init__(self):\n        super(ReLU6Triton, self).__init__()\n    \n    def forward(self, x):\n        output = torch.empty_like(x)\n        n_elements = output.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        relu6_kernel[grid](x, output, n_elements, BLOCK_SIZE=1024)\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.mbconv1 = self._make_mbconv_block(32, 16, 1, 1)\n        self.mbconv2 = self._make_mbconv_block(16, 24, 2, 6)\n        self.mbconv3 = self._make_mbconv_block(24, 40, 2, 6)\n        self.mbconv4 = self._make_mbconv_block(40, 80, 2, 6)\n        self.mbconv5 = self._make_mbconv_block(80, 112, 1, 6)\n        self.mbconv6 = self._make_mbconv_block(112, 192, 2, 6)\n        self.mbconv7 = self._make_mbconv_block(192, 320, 1, 6)\n        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(1280)\n        self.fc = nn.Linear(1280, num_classes)\n    \n    def _make_mbconv_block(self, in_channels, out_channels, stride, expand_ratio):\n        hidden_dim = round(in_channels * expand_ratio)\n        return nn.Sequential(\n            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            ReLU6Triton(),\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            ReLU6Triton(),\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.mbconv1(x)\n        x = self.mbconv2(x)\n        x = self.mbconv3(x)\n        x = self.mbconv4(x)\n        x = self.mbconv5(x)\n        x = self.mbconv6(x)\n        x = self.mbconv7(x)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\nbatch_size = 10\ninput_shape = (3, 240, 240)\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, *input_shape)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "06da328d-6b6b-47cc-a027-7c093a7a3aa9", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _triton_bmm_kernel(\n    A_ptr, B_ptr, C_ptr,\n    N, M, K, L,\n    stride_an, stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cn, stride_cm, stride_cl,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid_n = tl.program_id(0)\n    pid_m = tl.program_id(1)\n    pid_l = tl.program_id(2)\n    \n    # Create pointers for the batch\n    offs_n = pid_n\n    a_base = A_ptr + offs_n * stride_an\n    c_base = C_ptr + offs_n * stride_cn\n    \n    # Offsets for M and L dimensions\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_l = pid_l * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    # Pointers to the starting blocks of A and B\n    a_ptrs = a_base + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = B_ptr + offs_k[:, None] * stride_bk + offs_l[None, :] * stride_bn\n    \n    # Loop over K dimension with vectorized loads\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        # Create masks for boundary checks\n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < K - k * BLOCK_K)\n        b_mask = (offs_k[:, None] < K - k * BLOCK_K) & (offs_l[None, :] < L)\n        \n        # Load blocks from A and B\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        # Compute matrix multiplication\n        acc += tl.dot(a, b, allow_tf32=True)\n        \n        # Update pointers for next block\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    # Store result with boundary mask\n    c_ptrs = c_base + offs_m[:, None] * stride_cm + offs_l[None, :] * stride_cl\n    c_mask = (offs_m[:, None] < M) & (offs_l[None, :] < L)\n    tl.store(c_ptrs, acc, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A, B):\n        N, M, K = A.shape\n        L = B.shape[1]\n        C = torch.empty((N, M, L), device=A.device, dtype=A.dtype)\n        \n        # Compute strides\n        stride_an = A.stride(0)\n        stride_am = A.stride(1)\n        stride_ak = A.stride(2)\n        stride_bk = B.stride(0)\n        stride_bn = B.stride(1)\n        stride_cn = C.stride(0)\n        stride_cm = C.stride(1)\n        stride_cl = C.stride(2)\n        \n        # Block dimensions (optimized for H100 architecture)\n        BLOCK_M = 64\n        BLOCK_N = 64\n        BLOCK_K = 32\n        \n        # Grid configuration\n        grid = (N, triton.cdiv(M, BLOCK_M), triton.cdiv(L, BLOCK_N))\n        \n        # Launch kernel\n        _triton_bmm_kernel[grid](\n            A, B, C, N, M, K, L,\n            stride_an, stride_am, stride_ak,\n            stride_bk, stride_bn,\n            stride_cn, stride_cm, stride_cl,\n            BLOCK_M, BLOCK_N, BLOCK_K\n        )\n        return C\n\nN = 16\nM = 1024\nK = 2048\nL = 768\n\ndef get_inputs():\n    A = torch.randn(N, M, K, device='cuda')\n    B = torch.randn(K, L, device='cuda')\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "4cf70bb4-36b8-4938-b26c-b2792b6afb4c", "generation": 1, "timestamp": 1754634781.7432692, "iteration_found": 3, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_10_3D_tensor_matrix_multiplication", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "max_difference": ["0.196243", "0.192368", "0.207581", "0.193771", "0.208481"], "avg_difference": ["0.027962", "0.027944", "0.027942", "0.027942", "0.027952"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0294, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 1.04, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0294, "speed_up": 0.028, "custom_timing": 1.04}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 1.0400\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0294, 'speed_up': 0.028, 'custom_timing': 1.04}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 1.0400, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0294, 'speed_up': 0.028, 'custom_timing': 1.04}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication.\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs 3D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 3D tensor of shape (N, M, K).\n            B (torch.Tensor): Input matrix of shape (K, L).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (N, M, L), resulting from the multiplication of A and B along the last dimension of A.\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 16\nM = 1024\nK = 2048\nL = 768\n\ndef get_inputs():\n    A = torch.randn(N, M, K)\n    B = torch.randn(K, L)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (1.0400), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0294, 'speed_up': 0.028, 'custom_timing': 1.04})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication.\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs 3D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 3D tensor of shape (N, M, K).\n            B (torch.Tensor): Input matrix of shape (K, L).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (N, M, L), resulting from the multiplication of A and B along the last dimension of A.\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 16\nM = 1024\nK = 2048\nL = 768\n\ndef get_inputs():\n    A = torch.randn(N, M, K)\n    B = torch.randn(K, L)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a reference PyTorch implementation that does 3D tensor-matrix multiplication: \n    Input: A (N, M, K), B (K, L)\n    Output: (N, M, L) = A @ B\n\nThe current implementation uses `torch.matmul` which is efficient but we are to write a Triton kernel to potentially speed it up.\n\nWe note that the operation is essentially a batched matrix multiplication. However, note that the batch dimension is N and the matrices are of shape (M, K) multiplied by (K, L). \n\nWe can write a Triton kernel that processes one batch at a time, or we can write a kernel that processes multiple batches. Given the dimensions (N=16, M=1024, K=2048, L=768), we can design a kernel that uses tiling for the matrices.\n\nWe are going to write a kernel that:\n  - Uses a grid of (N, triton.cdiv(M, BLOCK_M), triton.cdiv(L, BLOCK_N))\n  - Each program will handle a block of size [BLOCK_M, BLOCK_N] in the output matrix for one batch.\n\nHow it works:\n  - We'll have three levels of blocking: \n        BLOCK_M: for the M dimension\n        BLOCK_N: for the N dimension (which is the L dimension in the output)\n        BLOCK_K: for the K dimension (reduction)\n\n  - Each program will:\n        pid_batch = tl.program_id(0)  # batch index (0..N-1)\n        pid_m = tl.program_id(1)       # block index for M\n        pid_n = tl.program_id(2)       # block index for N (which is L)\n\n        Then, we compute the offsets for the block in A and B.\n\n  - We'll precompute the pointers for the batch of A and the matrix B.\n\n  - We'll use a block of size [BLOCK_M, BLOCK_K] for A and [BLOCK_K, BLOCK_N] for B.\n\n  - We'll accumulate in a local variable of size [BLOCK_M, BLOCK_N].\n\n  - We'll use vectorized loads and stores to optimize memory access.\n\n  - We must be cautious of boundaries.\n\nSteps in the kernel:\n  1. Compute the pointers for the current batch of A and for B (which is the same for all batches).\n  2. Initialize the accumulator to zero.\n  3. Loop over the K dimension by blocks of BLOCK_K.\n  4. For each block, load a block from A and a block from B (with appropriate masking for boundaries).\n  5. Compute the dot product and accumulate.\n  6. After the loop, store the result.\n\nWe'll tune the block sizes (BLOCK_M, BLOCK_N, BLOCK_K) to fit the hardware.\n\nGiven the H100 architecture, we note:\n  - We have Tensor Cores that can do FP16 32x16x16. We are using FP32? The input is from `torch.randn` which is FP32 by default.\n\nBut note: the reference uses FP32. We can use FP32 computation. However, we can also try to use FP16 if we cast? But the problem does not specify. We must maintain the same dtype.\n\nSo we are going to do FP32.\n\nAlternatively, we can use Tensor Cores for FP32 via tf32. But note: Triton currently does not support tf32 directly. However, we can use the `tl.dot` instruction that can use Tensor Cores for FP32 when the inputs are in fp32 and the accumulator is fp32? Actually, Triton has an option for that.\n\nWe can use:\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    a = tl.load(a_ptr, ...)\n    b = tl.load(b_ptr, ...)\n    a = a.to(tl.float32)   # if we are using FP16, but we are not.\n\nBut note: the inputs are FP32. We can do:\n\n    c = tl.dot(a, b, acc, allow_tf32=True)\n\nBut we must check the Triton documentation. The `allow_tf32` flag is available in `tl.dot`.\n\nAlternatively, we can do manual tiling without tensor cores? But that would be slower.\n\nWe are going to use `tl.dot` with `allow_tf32=True` to potentially use tensor cores.\n\nWe must also note the alignment requirements. The dot product in Triton requires that the inner dimension (K) is a multiple of 16 for FP16 and 8 for FP32? Actually, we can set the `BLOCK_K` to be a multiple of 16 for better memory access and to meet the tensor core requirements.\n\nWe design:\n\n    BLOCK_M = 64\n    BLOCK_N = 64\n    BLOCK_K = 32   # for FP32, we can use 32? But note: the tensor core requirement for FP32 (tf32) is 16? \n\nActually, the H100 Tensor Cores for tf32 are 16x16x8. But we can use any multiple? The tl.dot operation will handle that.\n\nWe are going to use:\n\n    BLOCK_M = 64\n    BLOCK_N = 64\n    BLOCK_K = 32\n\nBut we can also use autotuning.\n\nWe'll write the kernel and then use autotuning.\n\nWe note that the grid will be:\n\n    grid = (N, triton.cdiv(M, BLOCK_M), triton.cdiv(L, BLOCK_N))\n\nWe must be cautious about the memory layout: A is (N, M, K) and we are accessing the batch, then row, then column. Similarly, B is (K, L) -> row-major? Yes.\n\nWe'll use:\n\n    a_ptr = A + pid_batch * M * K + (pid_m * BLOCK_M + arange_m[:, None]) * K + offs_k[None, :]\n    b_ptr = B + (offs_k[:, None]) * L + (pid_n * BLOCK_N + arange_n[None, :])\n\nBut note: the inner dimension for A is K, and for B the inner dimension is L? Actually, we are multiplying A (M, K) with B (K, L). So in A, we have strides: (M*K, K, 1) for batch, row, column? Actually, the tensor A is (N, M, K) so the strides are (M*K, K, 1). Similarly, B is (K, L) -> strides (L, 1).\n\nBut in Triton, we have to compute the pointers correctly.\n\nWe'll define:\n\n    offs_batch = pid_batch * M * K   # base for the batch\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    a_ptrs = a_ptr + offs_batch + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n\nBut note: in the tensor A, the stride for M (the second dimension) is K? Actually, the stride for the second dimension (M) is K, and the third dimension (K) is 1? \n\nWait: the tensor A is (N, M, K). The stride for the first dimension (N) is M*K, the second dimension (M) is K, and the third dimension (K) is 1.\n\nSimilarly, B is (K, L): the first dimension (K) has stride L, the second dimension (L) has stride 1.\n\nWe have to pass the strides to the kernel.\n\nWe'll compute the strides in the kernel as:\n\n    stride_an = M * K\n    stride_am = K\n    stride_ak = 1\n\n    stride_bk = L\n    stride_bn = 1\n\nBut note: the kernel gets pointers to the beginning of the tensors, and we pass the strides as arguments.\n\nWe are going to write the kernel and then the wrapper.\n\nWe must also mask for the boundaries in M and N (and K? but the inner loop already handles K by the loop condition and masking in the load).\n\nWe'll mask the loads for A and B in the inner loop and also mask the store.\n\nSteps:\n\n  1. Define the kernel with the appropriate parameters.\n  2. Precompute the grid.\n  3. Call the kernel.\n\nWe'll use autotuning for the block sizes and number of warps.\n\nLet's code accordingly.\n\nNote: We are not allowed to change the function signature of the forward method. We must return the same tensor.\n\nSo we'll replace the forward method with a call to our Triton kernel.\n\nBut note: the class ModelNew must be an nn.Module.\n\nWe'll write:\n\n    def forward(self, A, B):\n        # Check shapes: A: (N, M, K), B: (K, L)\n        # Output: (N, M, L)\n        return my_triton_matmul(A, B)\n\nWe must write the function `my_triton_matmul`.\n\nWe are going to write:\n\n    @triton.jit\n    def _kernel(...):\n        ...\n\n    def my_triton_matmul(A, B):\n        # Check and preprocess\n        assert A.is_contiguous(), \"A must be contiguous\"\n        assert B.is_contiguous(), \"B must be contiguous\"\n        N, M, K = A.shape\n        L = B.shape[1]\n        output = torch.empty((N, M, L), device=A.device, dtype=A.dtype)\n        # Precompute strides\n        stride_an = M * K\n        stride_am = K\n        stride_ak = 1\n        stride_bk = L\n        stride_bn = 1\n        # Grid: (N, ceil(M/BLOCK_M), ceil(L/BLOCK_N))\n        grid = (N, triton.cdiv(M, BLOCK_M), triton.cdiv(L, BLOCK_N))\n        # Launch kernel\n        _kernel[grid](A, B, output, ...)\n        return output\n\nBut note: we are going to use autotuning so we must define the kernel with autotune.\n\nAlternatively, we can hardcode the block sizes and then tune? But we are going to use the autotune decorator.\n\nWe'll define the kernel with autotune.\n\nGiven the constraints, we are going to write the code accordingly.\n\nWe'll now write the code.\n\nNote: We are only allowed to change the code inside the EVOLVE-BLOCK. So we will replace the entire ModelNew class and the helper functions (get_inputs, etc) with our Triton implementation.\n\nWe must keep the same N, M, K, L and get_inputs.\n\nLet's code accordingly.\n\nImportant: We must use the same tensor names and shapes.\n\nWe are going to replace the forward method and add the Triton kernel.\n\nWe'll write the kernel and the function inside the class? Actually, we can write the kernel outside and then call it in the forward.\n\nBut note: the EVOLVE-BLOCK must contain the entire code. So we'll write:\n\n    import ...\n    import triton\n    import triton.language as tl\n\n    @triton.jit\n    ... \n\n    class ModelNew(nn.Module):\n        ...\n\nWe'll do that.\n\nLet's write the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _triton_bmm_kernel(\n    A_ptr, B_ptr, C_ptr,\n    N, M, K, L,\n    stride_an, stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cn, stride_cm, stride_cl,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid_n = tl.program_id(0)\n    pid_m = tl.program_id(1)\n    pid_l = tl.program_id(2)\n    \n    # Create pointers for the batch\n    offs_n = pid_n\n    a_base = A_ptr + offs_n * stride_an\n    c_base = C_ptr + offs_n * stride_cn\n    \n    # Offsets for M and L dimensions\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_l = pid_l * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    # Pointers to the starting blocks of A and B\n    a_ptrs = a_base + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = B_ptr + offs_k[:, None] * stride_bk + offs_l[None, :] * stride_bn\n    \n    # Loop over K dimension with vectorized loads\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        # Create masks for boundary checks\n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < K - k * BLOCK_K)\n        b_mask = (offs_k[:, None] < K - k * BLOCK_K) & (offs_l[None, :] < L)\n        \n        # Load blocks from A and B\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        # Compute matrix multiplication\n        acc += tl.dot(a, b, allow_tf32=True)\n        \n        # Update pointers for next block\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    # Store result with boundary mask\n    c_ptrs = c_base + offs_m[:, None] * stride_cm + offs_l[None, :] * stride_cl\n    c_mask = (offs_m[:, None] < M) & (offs_l[None, :] < L)\n    tl.store(c_ptrs, acc, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A, B):\n        N, M, K = A.shape\n        L = B.shape[1]\n        C = torch.empty((N, M, L), device=A.device, dtype=A.dtype)\n        \n        # Compute strides\n        stride_an = A.stride(0)\n        stride_am = A.stride(1)\n        stride_ak = A.stride(2)\n        stride_bk = B.stride(0)\n        stride_bn = B.stride(1)\n        stride_cn = C.stride(0)\n        stride_cm = C.stride(1)\n        stride_cl = C.stride(2)\n        \n        # Block dimensions (optimized for H100 architecture)\n        BLOCK_M = 64\n        BLOCK_N = 64\n        BLOCK_K = 32\n        \n        # Grid configuration\n        grid = (N, triton.cdiv(M, BLOCK_M), triton.cdiv(L, BLOCK_N))\n        \n        # Launch kernel\n        _triton_bmm_kernel[grid](\n            A, B, C, N, M, K, L,\n            stride_an, stride_am, stride_ak,\n            stride_bk, stride_bn,\n            stride_cn, stride_cm, stride_cl,\n            BLOCK_M, BLOCK_N, BLOCK_K\n        )\n        return C\n\nN = 16\nM = 1024\nK = 2048\nL = 768\n\ndef get_inputs():\n    A = torch.randn(N, M, K, device='cuda')\n    B = torch.randn(K, L, device='cuda')\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
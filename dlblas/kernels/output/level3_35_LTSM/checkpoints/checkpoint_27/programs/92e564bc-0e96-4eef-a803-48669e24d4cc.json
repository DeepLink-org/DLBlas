{"id": "92e564bc-0e96-4eef-a803-48669e24d4cc", "code": "@triton.jit\ndef linear_kernel(\n    # Pointers to matrices\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    # Matrix dimensions\n    in_features, out_features,\n    # Strides for x\n    stride_xb, stride_xf,\n    # Strides for weight\n    stride_wo, stride_wi,\n    # Strides for output\n    stride_outb, stride_outf,\n    # Block size for the inner loop over input features (must be power of two for better performance)\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_b = tl.program_id(0)  # Batch index\n    pid_o = tl.program_id(1)  # Output feature index\n\n    # Initialize accumulator\n    acc = 0.0\n\n    # Loop over input features in blocks of BLOCK_SIZE_K\n    num_blocks = tl.cdiv(in_features, BLOCK_SIZE_K)\n    for k in range(0, num_blocks):\n        # Create an index range for the current block of input features\n        idx = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        # Create a mask to prevent out-of-bounds accesses for the current block\n        mask = idx < in_features\n\n        # Load a block of x: [BLOCK_SIZE_K]\n        x_offset = pid_b * stride_xb + idx * stride_xf\n        x_val = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n\n        # Load a block of w: [BLOCK_SIZE_K]\n        w_offset = pid_o * stride_wo + idx * stride_wi\n        w_val = tl.load(w_ptr + w_offset, mask=mask, other=0.0)\n\n        # Accumulate the dot product for this block\n        acc += tl.sum(x_val * w_val)\n\n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        acc += bias\n\n    # Write the result to the output\n    output_offset = pid_b * stride_outb + pid_o * stride_outf\n    tl.store(output_ptr + output_offset, acc)", "language": "python", "parent_id": "df276b61-1124-4d90-9cfe-c108ddfcfb64", "generation": 2, "timestamp": 1754594692.494108, "iteration_found": 28, "metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 45.0, "speed_up": 1.25, "runtime_ms": 20.8, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 26.0, "speed_up": 1.25, "custom_timing": 20.8}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 1.0000\n- score: 45.0000\n- speed_up: 1.2500\n- runtime_ms: 20.8000\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 26.0, 'speed_up': 1.25, 'custom_timing': 20.8}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: score, speed_up, runtime_ms. Consider continuing with similar changes.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level3_35_LTSM', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'max_difference': ['1.768798', '1.766837', '1.768900', '1.768048', '1.768057'], 'avg_difference': ['0.686035', '0.686134', '0.685942', '0.686141', '0.686037'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 26.0, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level3_35_LTSM', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'max_difference': ['0.040222', '0.040222', '0.040388', '0.040374', '0.040197'], 'avg_difference': ['0.028012', '0.027988', '0.027967', '0.027967', '0.027919'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 26.0, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 45.0000, speed_up: 1.2500, runtime_ms: 20.8000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 26.0, 'speed_up': 1.25, 'custom_timing': 20.8}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 45.0000)\n```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef linear_kernel(\n    # Pointers to matrices\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    # Matrix dimensions\n    in_features, out_features,\n    # Strides for x\n    stride_xb, stride_xf,\n    # Strides for weight\n    stride_wo, stride_wi,\n    # Strides for output\n    stride_outb, stride_outf,\n    # Block size for features (must be power of two)\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(0)  # Batch index\n    pid_o = tl.program_id(1)  # Output feature index\n\n    # Create a range of indices for the input features (from 0 to BLOCK_SIZE-1)\n    idx = tl.arange(0, BLOCK_SIZE)\n    # Create a mask to prevent out-of-bounds accesses\n    mask = idx < in_features\n\n    # Calculate the pointer offsets for the input row (batch index = pid_b)\n    x_offset = pid_b * stride_xb + idx * stride_xf\n    x = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n\n    # Calculate the pointer offsets for the weight column (output feature index = pid_o)\n    w_offset = pid_o * stride_wo + idx * stride_wi\n    w = tl.load(w_ptr + w_offset, mask=mask, other=0.0)\n\n    # Compute the dot product\n    dot = tl.sum(x * w)\n\n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        dot += bias\n\n    # Write the result to the output\n    output_offset = pid_b * stride_outb + pid_o * stride_outf\n    tl.store(output_ptr + output_offset, dot)\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (45.0000), Performs well on speed_up (1.2500), Performs well on runtime_ms (20.8000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 26.0, 'speed_up': 1.25, 'custom_timing': 20.8})\n\n\n### Program 2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Optimized Triton kernel for linear layer\n@triton.jit\ndef linear_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    in_features, out_features,\n    stride_xb, stride_xf,\n    stride_wo, stride_wi,\n    stride_outb, stride_outf,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(0)  # Batch index\n    pid_o = tl.program_id(1)  # Output feature index\n    \n    # Create feature index range\n    idx = tl.arange(0, BLOCK_SIZE)\n    \n    # Calculate pointers and load data with masking\n    x_offset = pid_b * stride_xb + idx * stride_xf\n    x_mask = idx < in_features\n    x = tl.load(x_ptr + x_offset, mask=x_mask, other=0.0)\n    \n    w_offset = pid_o * stride_wo + idx * stride_wi\n    w_mask = idx < in_features\n    w = tl.load(w_ptr + w_offset, mask=w_mask, other=0.0)\n    \n    # Compute dot product\n    dot = tl.sum(x * w)\n    \n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        dot += bias\n    \n    # Store result\n    output_offset = pid_b * stride_outb + pid_o * stride_outf\n    tl.store(output_ptr + output_offset, dot)\n\n# Optimized linear layer using Triton\nclass TritonLinear(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight)\n        if self.bias is not None:\n            nn.init.zeros_(self.bias)\n    \n    def forward(self, x):\n        if x.dim() != 2:\n            raise ValueError(\"Input must be 2D\")\n        \n        batch_size, in_features = x.shape\n        out_features = self.weight.shape[0]\n        \n        # Ensure contiguous memory\n        x = x.contiguous()\n        weight = self.weight.contiguous()\n        bias = self.bias.contiguous() if self.bias is not None else None\n        \n        # Create output tensor\n        output = torch.empty(batch_size, out_features, \n                             device=x.device, dtype=x.dtype)\n        \n        # Compute block size for kernel\n        block_size = triton.next_power_of_2(in_features)\n        \n        # Launch kernel with 2D grid\n        grid = (batch_size, out_features)\n        linear_kernel[grid](\n            x, weight, bias, output,\n            in_features, out_features,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE=block_size\n        )\n        return output\n\n# Optimized LSTM model\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        super(ModelNew, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Initialize weights directly on device in forward pass\n        self.h0 = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n        self.c0 = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n        \n        self.lstm = nn.LSTM(\n            input_size, \n            hidden_size, \n            num_layers, \n            batch_first=True, \n            dropout=dropout, \n            bidirectional=False\n        )\n        # Initialize LSTM weights\n        for name, param in self.lstm.named_parameters():\n            if 'weight_ih' in name:\n                nn.init.xavier_uniform_(param)\n            elif 'weight_hh' in name:\n                nn.init.orthogonal_(param)\n            elif 'bias' in name:\n                nn.init.zeros_(param)\n                \n        # Use Triton-optimized linear layer\n        self.fc = TritonLinear(hidden_size, output_size)\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        \n        # Expand initial states to match batch size\n        h0 = self.h0.expand(-1, batch_size, -1).contiguous()\n        c0 = self.c0.expand(-1, batch_size, -1).contiguous()\n        \n        # Forward propagate LSTM\n        out, _ = self.lstm(x, (h0, c0))\n        \n        # Apply optimized linear layer to last timestep\n        return self.fc(out[:, -1, :])\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, sequence_length, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level3_35_LTSM', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'max_difference': ['0.040222', '0.040222', '0.040388', '0.040374', '0.040197'], 'avg_difference': ['0.028012', '0.027988', '0.027967', '0.027967', '0.027919'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 26.0, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n### Program 3 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Triton kernel for optimized linear layer\n@triton.jit\ndef linear_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    in_features, out_features,\n    stride_xb, stride_xf,\n    stride_wo, stride_wi,\n    stride_outb, stride_outf,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(0)  # Batch index\n    pid_o = tl.program_id(1)  # Output feature index\n    \n    # Initialize accumulator\n    acc = 0.0\n    \n    # Loop over input features in chunks\n    num_blocks = tl.cdiv(in_features, BLOCK_SIZE)\n    for block_idx in range(0, num_blocks):\n        # Create feature index range for current block\n        idx = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        mask = idx < in_features\n        \n        # Calculate pointers and load data with masking\n        x_offset = pid_b * stride_xb + idx * stride_xf\n        x = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n        \n        w_offset = pid_o * stride_wo + idx * stride_wi\n        w = tl.load(w_ptr + w_offset, mask=mask, other=0.0)\n        \n        # Accumulate dot product\n        acc += tl.sum(x * w)\n    \n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        acc += bias\n    \n    # Store result\n    output_offset = pid_b * stride_outb + pid_o * stride_outf\n    tl.store(output_ptr + output_offset, acc)\n\n# Optimized linear layer using Triton\nclass TritonLinear(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_features))\n        else:\n            self.register_parameter('bias', None)\n    \n    def forward(self, x):\n        if x.dim() != 2:\n            raise ValueError(\"Input must be 2D\")\n        \n        batch_size, in_features = x.shape\n        out_features = self.weight.shape[0]\n        \n        # Ensure contiguous memory\n        x = x.contiguous()\n        weight = self.weight.contiguous()\n        bias = self.bias.contiguous() if self.bias is not None else None\n        \n        # Create output tensor\n        output = torch.empty(batch_size, out_features, \n                             device=x.device, dtype=x.dtype)\n        \n        # Use fixed block size for better correctness\n        BLOCK_SIZE = 128\n        \n        # Launch kernel with 2D grid\n        grid = (batch_size, out_features)\n        linear_kernel[grid](\n            x, weight, bias, output,\n            in_features, out_features,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\n# Optimized LSTM model\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        super(ModelNew, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Initialize weights directly on device in forward pass\n        self.h0 = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n        self.c0 = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n        \n        self.lstm = nn.LSTM(\n            input_size, \n            hidden_size, \n            num_layers, \n            batch_first=True, \n            dropout=dropout, \n            bidirectional=False\n        )\n        # Use Triton-optimized linear layer\n        self.fc = TritonLinear(hidden_size, output_size)\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        \n        # Expand initial states to match batch size\n        h0 = self.h0.expand(-1, batch_size, -1).contiguous()\n        c0 = self.c0.expand(-1, batch_size, -1).contiguous()\n        \n        # Forward propagate LSTM\n        out, _ = self.lstm(x, (h0, c0))\n        \n        # Apply optimized linear layer to last timestep\n        return self.fc(out[:, -1, :])\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, sequence_length, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level3_35_LTSM', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'max_difference': ['1.768798', '1.766837', '1.768900', '1.768048', '1.768057'], 'avg_difference': ['0.686035', '0.686134', '0.685942', '0.686141', '0.686037'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 26.0, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Optimized Triton kernel for linear layer\n@triton.jit\ndef linear_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    in_features, out_features,\n    stride_xb, stride_xf,\n    stride_wo, stride_wi,\n    stride_outb, stride_outf,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(0)  # Batch index\n    pid_o = tl.program_id(1)  # Output feature index\n    \n    # Create feature index range\n    idx = tl.arange(0, BLOCK_SIZE)\n    \n    # Calculate pointers and load data with masking\n    x_offset = pid_b * stride_xb + idx\n    x_mask = idx < in_features\n    x = tl.load(x_ptr + x_offset, mask=x_mask, other=0.0)\n    \n    w_offset = pid_o * stride_wo + idx\n    w_mask = idx < in_features\n    w = tl.load(w_ptr + w_offset, mask=w_mask, other=0.0)\n    \n    # Compute dot product\n    dot = tl.sum(x * w)\n    \n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        dot += bias\n    \n    # Store result\n    output_offset = pid_b * stride_outb + pid_o\n    tl.store(output_ptr + output_offset, dot)\n\n# Triton-optimized linear layer\nclass TritonLinear(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_features))\n        else:\n            self.register_parameter('bias', None)\n    \n    def forward(self, x):\n        if x.dim() != 2:\n            raise ValueError(\"Input must be 2D\")\n        \n        batch_size, in_features = x.shape\n        out_features = self.weight.shape[0]\n        \n        # Ensure contiguous memory\n        x = x.contiguous()\n        weight = self.weight.contiguous()\n        bias = self.bias.contiguous() if self.bias is not None else None\n        \n        # Create output tensor\n        output = torch.empty(batch_size, out_features, \n                             device=x.device, dtype=x.dtype)\n        \n        # Compute optimal block size (power of 2)\n        block_size = triton.next_power_of_2(in_features)\n        \n        # Launch kernel with 2D grid\n        grid = (batch_size, out_features)\n        linear_kernel[grid](\n            x, weight, bias, output,\n            in_features, out_features,\n            x.stride(0), 1,\n            weight.stride(0), 1,\n            output.stride(0), 1,\n            BLOCK_SIZE=block_size\n        )\n        return output\n\n# Optimized LSTM model\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        super(ModelNew, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Initialize hidden states as parameters\n        self.h0 = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n        self.c0 = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n        \n        self.lstm = nn.LSTM(\n            input_size, \n            hidden_size, \n            num_layers, \n            batch_first=True, \n            dropout=dropout, \n            bidirectional=False\n        )\n        # Use Triton-optimized linear layer\n        self.fc = TritonLinear(hidden_size, output_size)\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        \n        # Expand initial states to match batch size\n        h0 = self.h0.expand(-1, batch_size, -1).contiguous()\n        c0 = self.c0.expand(-1, batch_size, -1).contiguous()\n        \n        # Forward propagate LSTM\n        out, _ = self.lstm(x, (h0, c0))\n        \n        # Apply optimized linear layer to last timestep\n        return self.fc(out[:, -1, :])\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, sequence_length, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef linear_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    in_features, out_features,\n    stride_xb, stride_xf,\n    stride_wo, stride_wi,\n    stride_outb, stride_outf,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_b = tl.program_id(0)  # Batch index\n    pid_o = tl.program_id(1)  # Output feature index\n    \n    # Accumulator\n    acc = tl.zeros((1,), dtype=tl.float32)\n    \n    # Loop over K dimension in blocks\n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_offsets = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        # Load x block\n        x_offset = pid_b * stride_xb + k_offsets * stride_xf\n        x = tl.load(\n            x_ptr + x_offset, \n            mask=k_mask, \n            other=0.0, \n            eviction_policy=\"evict_first\"\n        )\n        \n        # Load w block\n        w_offset = pid_o * stride_wo + k_offsets * stride_wi\n        w = tl.load(\n            w_ptr + w_offset, \n            mask=k_mask, \n            other=0.0, \n            eviction_policy=\"evict_first\"\n        )\n        \n        # Update accumulator\n        acc += tl.sum(x * w, axis=0)\n    \n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        acc += bias\n    \n    # Convert and store result\n    output_offset = pid_b * stride_outb + pid_o * stride_outf\n    tl.store(output_ptr + output_offset, acc.to(x.dtype))\n\nclass TritonLinear(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_features))\n        else:\n            self.register_parameter('bias', None)\n    \n    def forward(self, x):\n        if x.dim() != 2:\n            raise ValueError(\"Input must be 2D\")\n        \n        batch_size, in_features = x.shape\n        \n        # Ensure contiguous memory\n        x = x.contiguous()\n        weight = self.weight.contiguous()\n        bias = self.bias.contiguous() if self.bias is not None else None\n        \n        # Create output tensor\n        output = torch.empty(\n            batch_size, self.out_features, \n            device=x.device, dtype=x.dtype\n        )\n        \n        # Configure kernel launch\n        grid = (batch_size, self.out_features)\n        linear_kernel[grid](\n            x, weight, bias, output,\n            in_features, self.out_features,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_K=128  # Optimized for H100 memory channels\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        super(ModelNew, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.h0 = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n        self.c0 = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n        \n        self.lstm = nn.LSTM(\n            input_size, \n            hidden_size, \n            num_layers, \n            batch_first=True, \n            dropout=dropout, \n            bidirectional=False\n        )\n        self.fc = TritonLinear(hidden_size, output_size)\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        h0 = self.h0.expand(-1, batch_size, -1).contiguous()\n        c0 = self.c0.expand(-1, batch_size, -1).contiguous()\n        \n        out, _ = self.lstm(x, (h0, c0))\n        return self.fc(out[:, -1, :])\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, sequence_length, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: -100.0000, Type: Exploratory)\n```python\n@triton.jit\ndef linear_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    in_features, out_features,\n    stride_xb, stride_xf,\n    stride_wo, stride_wi,\n    stride_outb, stride_outf,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(0)  # Batch index\n    pid_o = tl.program_id(1)  # Output feature index\n    \n    # Initialize accumulator\n    acc = 0.0\n    \n    # Loop over all input features\n    for i in range(0, in_features):\n        # Calculate pointers and load data\n        x_offset = pid_b * stride_xb + i * stride_xf\n        x_val = tl.load(x_ptr + x_offset)\n        \n        w_offset = pid_o * stride_wo + i * stride_wi\n        w_val = tl.load(w_ptr + w_offset)\n        \n        # Accumulate product\n        acc += x_val * w_val\n    \n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        acc += bias\n    \n    # Store result\n    output_offset = pid_b * stride_outb + pid_o * stride_outf\n    tl.store(output_ptr + output_offset, acc)\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Optimized Triton kernel for linear layer\n@triton.jit\ndef linear_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    in_features, out_features,\n    stride_xb, stride_xf,\n    stride_wo, stride_wi,\n    stride_outb, stride_outf,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(0)  # Batch index\n    pid_o = tl.program_id(1)  # Output feature index\n    \n    # Create feature index range\n    idx = tl.arange(0, BLOCK_SIZE)\n    \n    # Calculate pointers and load data with masking\n    x_offset = pid_b * stride_xb + idx\n    x_mask = idx < in_features\n    x = tl.load(x_ptr + x_offset, mask=x_mask, other=0.0)\n    \n    w_offset = pid_o * stride_wo + idx\n    w_mask = idx < in_features\n    w = tl.load(w_ptr + w_offset, mask=w_mask, other=0.0)\n    \n    # Compute dot product\n    dot = tl.sum(x * w)\n    \n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        dot += bias\n    \n    # Store result\n    output_offset = pid_b * stride_outb + pid_o\n    tl.store(output_ptr + output_offset, dot)\n\n# Triton-optimized linear layer\nclass TritonLinear(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_features))\n        else:\n            self.register_parameter('bias', None)\n    \n    def forward(self, x):\n        if x.dim() != 2:\n            raise ValueError(\"Input must be 2D\")\n        \n        batch_size, in_features = x.shape\n        out_features = self.weight.shape[0]\n        \n        # Ensure contiguous memory\n        x = x.contiguous()\n        weight = self.weight.contiguous()\n        bias = self.bias.contiguous() if self.bias is not None else None\n        \n        # Create output tensor\n        output = torch.empty(batch_size, out_features, \n                             device=x.device, dtype=x.dtype)\n        \n        # Compute optimal block size (power of 2)\n        block_size = triton.next_power_of_2(in_features)\n        \n        # Launch kernel with 2D grid\n        grid = (batch_size, out_features)\n        linear_kernel[grid](\n            x, weight, bias, output,\n            in_features, out_features,\n            x.stride(0), 1,\n            weight.stride(0), 1,\n            output.stride(0), 1,\n            BLOCK_SIZE=block_size\n        )\n        return output\n\n# Optimized LSTM model\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        super(ModelNew, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Initialize hidden states as parameters\n        self.h0 = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n        self.c0 = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n        \n        self.lstm = nn.LSTM(\n            input_size, \n            hidden_size, \n            num_layers, \n            batch_first=True, \n            dropout=dropout, \n            bidirectional=False\n        )\n        # Use Triton-optimized linear layer\n        self.fc = TritonLinear(hidden_size, output_size)\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        \n        # Expand initial states to match batch size\n        h0 = self.h0.expand(-1, batch_size, -1).contiguous()\n        c0 = self.c0.expand(-1, batch_size, -1).contiguous()\n        \n        # Forward propagate LSTM\n        out, _ = self.lstm(x, (h0, c0))\n        \n        # Apply optimized linear layer to last timestep\n        return self.fc(out[:, -1, :])\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, sequence_length, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Triton kernel for optimized linear layer\n@triton.jit\ndef linear_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    in_features, out_features,\n    stride_xb, stride_xf,\n    stride_wo, stride_wi,\n    stride_outb, stride_outf,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(0)  # Batch index\n    pid_o = tl.program_id(1)  # Output feature index\n    \n    # Initialize accumulator\n    acc = 0.0\n    \n    # Loop over input features in chunks\n    num_blocks = tl.cdiv(in_features, BLOCK_SIZE)\n    for block_idx in range(0, num_blocks):\n        # Create feature index range for current block\n        idx = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        mask = idx < in_features\n        \n        # Calculate pointers and load data with masking\n        x_offset = pid_b * stride_xb + idx * stride_xf\n        x = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n        \n        w_offset = pid_o * stride_wo + idx * stride_wi\n        w = tl.load(w_ptr + w_offset, mask=mask, other=0.0)\n        \n        # Accumulate dot product\n        acc += tl.sum(x * w)\n    \n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        acc += bias\n    \n    # Store result\n    output_offset = pid_b * stride_outb + pid_o * stride_outf\n    tl.store(output_ptr + output_offset, acc)\n\n# Optimized linear layer using Triton\nclass TritonLinear(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_features))\n        else:\n            self.register_parameter('bias', None)\n    \n    def forward(self, x):\n        if x.dim() != 2:\n            raise ValueError(\"Input must be 2D\")\n        \n        batch_size, in_features = x.shape\n        out_features = self.weight.shape[0]\n        \n        # Ensure contiguous memory\n        x = x.contiguous()\n        weight = self.weight.contiguous()\n        bias = self.bias.contiguous() if self.bias is not None else None\n        \n        # Create output tensor\n        output = torch.empty(batch_size, out_features, \n                             device=x.device, dtype=x.dtype)\n        \n        # Use fixed block size for better correctness\n        BLOCK_SIZE = 128\n        \n        # Launch kernel with 2D grid\n        grid = (batch_size, out_features)\n        linear_kernel[grid](\n            x, weight, bias, output,\n            in_features, out_features,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\n# Optimized LSTM model\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        super(ModelNew, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Initialize weights directly on device in forward pass\n        self.h0 = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n        self.c0 = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n        \n        self.lstm = nn.LSTM(\n            input_size, \n            hidden_size, \n            num_layers, \n            batch_first=True, \n            dropout=dropout, \n            bidirectional=False\n        )\n        # Use Triton-optimized linear layer\n        self.fc = TritonLinear(hidden_size, output_size)\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        \n        # Expand initial states to match batch size\n        h0 = self.h0.expand(-1, batch_size, -1).contiguous()\n        c0 = self.c0.expand(-1, batch_size, -1).contiguous()\n        \n        # Forward propagate LSTM\n        out, _ = self.lstm(x, (h0, c0))\n        \n        # Apply optimized linear layer to last timestep\n        return self.fc(out[:, -1, :])\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, sequence_length, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: -100.0000, Type: Exploratory)\n```python\n@triton.jit\ndef linear_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    in_features, out_features,\n    stride_xb, stride_xf,\n    stride_wo, stride_wi,\n    stride_outb, stride_outf,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(0)   # batch index\n    pid_o = tl.program_id(1)   # output feature index\n\n    # We will accumulate in a variable\n    acc = 0.0\n\n    # Loop over the input features in blocks\n    for i in range(0, in_features, BLOCK_SIZE):\n        # Create an index for the current block\n        idx = i + tl.arange(0, BLOCK_SIZE)\n        # Create a mask for the current block (to avoid out-of-bounds)\n        mask = idx < in_features\n\n        # Load a block of x\n        x_offset = pid_b * stride_xb + idx * stride_xf\n        x_val = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n\n        # Load a block of weight (for the current output feature and the current block of input features)\n        w_offset = pid_o * stride_wo + idx * stride_wi\n        w_val = tl.load(w_ptr + w_offset, mask=mask, other=0.0)\n\n        # Accumulate the dot product for this block\n        acc += tl.sum(x_val * w_val)\n\n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        acc += bias\n\n    # Write the result\n    output_offset = pid_b * stride_outb + pid_o * stride_outf\n    tl.store(output_ptr + output_offset, acc)\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef linear_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    in_features, out_features,\n    stride_xb, stride_xf,\n    stride_wo, stride_wi,\n    stride_outb, stride_outf,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_b = tl.program_id(0)  # Batch index\n    pid_o = tl.program_id(1)  # Output feature index\n    \n    # Accumulator\n    acc = tl.zeros((1,), dtype=tl.float32)\n    \n    # Loop over K dimension in blocks\n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_offsets = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        # Load x block\n        x_offset = pid_b * stride_xb + k_offsets * stride_xf\n        x = tl.load(\n            x_ptr + x_offset, \n            mask=k_mask, \n            other=0.0, \n            eviction_policy=\"evict_first\"\n        )\n        \n        # Load w block\n        w_offset = pid_o * stride_wo + k_offsets * stride_wi\n        w = tl.load(\n            w_ptr + w_offset, \n            mask=k_mask, \n            other=0.0, \n            eviction_policy=\"evict_first\"\n        )\n        \n        # Update accumulator\n        acc += tl.sum(x * w, axis=0)\n    \n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        acc += bias\n    \n    # Convert and store result\n    output_offset = pid_b * stride_outb + pid_o * stride_outf\n    tl.store(output_ptr + output_offset, acc.to(x.dtype))\n\nclass TritonLinear(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_features))\n        else:\n            self.register_parameter('bias', None)\n    \n    def forward(self, x):\n        if x.dim() != 2:\n            raise ValueError(\"Input must be 2D\")\n        \n        batch_size, in_features = x.shape\n        \n        # Ensure contiguous memory\n        x = x.contiguous()\n        weight = self.weight.contiguous()\n        bias = self.bias.contiguous() if self.bias is not None else None\n        \n        # Create output tensor\n        output = torch.empty(\n            batch_size, self.out_features, \n            device=x.device, dtype=x.dtype\n        )\n        \n        # Configure kernel launch\n        grid = (batch_size, self.out_features)\n        linear_kernel[grid](\n            x, weight, bias, output,\n            in_features, self.out_features,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_K=128  # Optimized for H100 memory channels\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        super(ModelNew, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.h0 = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n        self.c0 = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n        \n        self.lstm = nn.LSTM(\n            input_size, \n            hidden_size, \n            num_layers, \n            batch_first=True, \n            dropout=dropout, \n            bidirectional=False\n        )\n        self.fc = TritonLinear(hidden_size, output_size)\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        h0 = self.h0.expand(-1, batch_size, -1).contiguous()\n        c0 = self.c0.expand(-1, batch_size, -1).contiguous()\n        \n        out, _ = self.lstm(x, (h0, c0))\n        return self.fc(out[:, -1, :])\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, sequence_length, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef linear_kernel(\n    # Pointers to matrices\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    # Matrix dimensions\n    in_features, out_features,\n    # Strides for x\n    stride_xb, stride_xf,\n    # Strides for weight\n    stride_wo, stride_wi,\n    # Strides for output\n    stride_outb, stride_outf,\n    # Block size for features (must be power of two)\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(0)  # Batch index\n    pid_o = tl.program_id(1)  # Output feature index\n\n    # Create a range of indices for the input features (from 0 to BLOCK_SIZE-1)\n    idx = tl.arange(0, BLOCK_SIZE)\n    # Create a mask to prevent out-of-bounds accesses\n    mask = idx < in_features\n\n    # Calculate the pointer offsets for the input row (batch index = pid_b)\n    x_offset = pid_b * stride_xb + idx * stride_xf\n    x = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n\n    # Calculate the pointer offsets for the weight column (output feature index = pid_o)\n    w_offset = pid_o * stride_wo + idx * stride_wi\n    w = tl.load(w_ptr + w_offset, mask=mask, other=0.0)\n\n    # Compute the dot product\n    dot = tl.sum(x * w)\n\n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        dot += bias\n\n    # Write the result to the output\n    output_offset = pid_b * stride_outb + pid_o * stride_outf\n    tl.store(output_ptr + output_offset, dot)\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current Triton kernel for a linear layer. The current version has a speedup of 1.25 (25% improvement) and a score of 45. We are to improve it further.\n\nThe current kernel:\n- Uses a single block per output feature and batch element to load the entire input row and weight column (if the input features are within the block size).\n- The block size is set to the next power of two of the input features.\n\nHowever, note that the input features might be large and the current approach may not be optimal for memory access patterns, especially if the block size is too large and exceeds the cache.\n\nWe are going to try to optimize by:\n1. Using a loop over multiple blocks for the input features (K dimension) to reduce the per-thread register pressure and allow for better cache utilization.\n2. Accumulating the partial sums in a loop.\n\nThis is similar to the blocked matrix multiplication approach. We break the input feature dimension into chunks (blocks) and compute the dot product in parts.\n\nWe must maintain the same function signature and grid configuration.\n\nLet's rewrite the kernel to use a loop over the K dimension (input features) in blocks. We'll introduce a new constant for the block size in the K dimension (BLOCK_SIZE_K). Note that the grid configuration (batch_size, out_features) remains the same.\n\nWe also note that the current program uses a TritonLinear class that sets the block size as the next power of two of the input features. We are going to change the kernel to use a fixed block size for the inner loop (e.g., 128) and then adjust the kernel accordingly.\n\nHowever, note the constraint: \"NEVER change @triton.jit function signatures or parameter names\". We can only change the body and add new `tl.constexpr` parameters. We'll add a new `BLOCK_SIZE_K` for the inner loop.\n\nBut note: the current kernel uses a single block for the entire input feature dimension. We are going to change it to loop over blocks of size `BLOCK_SIZE_K`.\n\nLet's restructure:\n\n- We keep the program_id(0) for batch and program_id(1) for output feature.\n- We initialize an accumulator to zero.\n- We loop over the input features in steps of `BLOCK_SIZE_K`.\n- For each block, we load a chunk of the input row and the corresponding chunk of the weight column, multiply, and accumulate.\n- Then we add the bias and store.\n\nWe must be cautious with the masking: we have to mask the last block if it is not full.\n\nWe'll change the kernel as follows:\n\n```python\n@triton.jit\ndef linear_kernel(\n    # Pointers to matrices\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    # Matrix dimensions\n    in_features, out_features,\n    # Strides for x\n    stride_xb, stride_xf,\n    # Strides for weight\n    stride_wo, stride_wi,\n    # Strides for output\n    stride_outb, stride_outf,\n    # Block size for the inner loop over input features (must be power of two for better performance)\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_b = tl.program_id(0)  # Batch index\n    pid_o = tl.program_id(1)  # Output feature index\n\n    # Initialize accumulator\n    acc = 0.0\n\n    # Loop over input features in blocks of BLOCK_SIZE_K\n    num_blocks = tl.cdiv(in_features, BLOCK_SIZE_K)\n    for k in range(0, num_blocks):\n        # Create an index range for the current block of input features\n        idx = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        # Create a mask to prevent out-of-bounds accesses for the current block\n        mask = idx < in_features\n\n        # Load a block of x: [BLOCK_SIZE_K]\n        x_offset = pid_b * stride_xb + idx * stride_xf\n        x_val = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n\n        # Load a block of w: [BLOCK_SIZE_K]\n        w_offset = pid_o * stride_wo + idx * stride_wi\n        w_val = tl.load(w_ptr + w_offset, mask=mask, other=0.0)\n\n        # Accumulate the dot product for this block\n        acc += tl.sum(x_val * w_val)\n\n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        acc += bias\n\n    # Write the result to the output\n    output_offset = pid_b * stride_outb + pid_o * stride_outf\n    tl.store(output_ptr + output_offset, acc)\n```\n\nIn the TritonLinear class, we must change the kernel call to pass the new block size. We can set BLOCK_SIZE_K to a fixed value (e.g., 128) that is known to be efficient for the hardware.\n\nHowever, note the constraint: \"NEVER modify grid configuration\". The grid is still (batch_size, out_features). We are only changing the inner working of the kernel.\n\nBut wait: the current program does not include the TritonLinear class in the EVOLVE-BLOCK? Actually, the current program only shows the kernel. The rest of the code (the ModelNew and TritonLinear) is from previous attempts and not in the current program.\n\nLooking at the current program, it is only the kernel. Therefore, we must assume that the kernel is being called from an external context that we cannot change. However, the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\"\n\nBut note: the current program code we are given to rewrite is only the kernel. So we cannot change the class. However, the kernel call in the class must be adjusted to pass the new BLOCK_SIZE_K. But that is outside the kernel.\n\nThis presents a problem: the original kernel had one block size (BLOCK_SIZE) and now we have a new one (BLOCK_SIZE_K). How do we reconcile?\n\nWe have two options:\n1. We change the kernel to use the existing BLOCK_SIZE as the inner block size (BLOCK_SIZE_K). Then the caller does not need to change because they are already passing a block size (as the next power of two of in_features). But note: that block size was intended to cover the entire input feature dimension. Now we are using it as the inner block size and looping. This would work, but the block size passed might be too large (the entire input) and we are using it for the inner loop block, which is inefficient.\n\n2. We change the kernel to take two block sizes: one for the entire input (which we don't need anymore) and one for the inner loop. But we cannot change the signature arbitrarily because the caller is fixed? Actually, the constraint says: \"NEVER change @triton.jit function signatures or parameter names\". So we cannot change the parameter names. We can only change the body and add new constexpr.\n\nBut note: the current kernel has a parameter `BLOCK_SIZE: tl.constexpr`. We are going to repurpose it to be the inner block size and rename it to `BLOCK_SIZE_K` for clarity? However, the constraint says we cannot change the parameter names. So we must keep the name `BLOCK_SIZE` but use it as the inner block size.\n\nAlternatively, we can add a new constexpr parameter? But the constraint says we cannot change the signature. So we cannot add a new parameter.\n\nTherefore, we must repurpose the existing `BLOCK_SIZE` to be the inner block size (BLOCK_SIZE_K). And then the caller (the TritonLinear class) must set this to a fixed value (like 128) instead of the next power of two of the entire input.\n\nBut wait: the current program that calls the kernel is not shown. We are only allowed to change the kernel? And the problem says: \"all other code is LOCKED\". So we cannot change the caller.\n\nThis is a dilemma. However, note that the EVOLVE-BLOCK only contains the kernel. The rest of the code (the ModelNew and TritonLinear) is from previous attempts and not in the current block. The current block is only the kernel.\n\nLooking at the history, the top-performing program (Program 1) uses a kernel without a loop and a block size set to the next power of two of the input features. The TritonLinear class in that program sets the block size as `block_size = triton.next_power_of_2(in_features)`. Then it passes that as BLOCK_SIZE.\n\nIf we change the kernel to use the passed BLOCK_SIZE as the inner block size (and loop over the input features in chunks of that block size), then the kernel will work. But note: the block size passed is the entire input feature dimension? Actually, it is the next power of two of the input features, which is at least as big as the input features. So if we use that as the inner block size, then we are effectively doing one iteration? Because the entire input will be loaded at once.\n\nThat doesn't help. We want to break the input into smaller blocks.\n\nTherefore, we must change the way the block size is passed. But we cannot change the caller because it is locked. So we are stuck?\n\nLet me re-read the constraints: \"NEVER change @triton.jit function signatures or parameter names\". We are allowed to change the body and the meaning of the constexpr? The caller passes a block size that is the next power of two of the input features. We can ignore that and use a fixed inner block size? But we cannot because we don't have a new parameter.\n\nAlternatively, we can hardcode the inner block size in the kernel? But that is not flexible.\n\nWe need to find a solution that does not require changing the caller and does not change the signature.\n\nIdea: We can use the existing BLOCK_SIZE as the inner block size, but then we must change the caller to pass a different value. However, the caller is locked. So we cannot.\n\nAlternatively, we can use two block sizes: we keep the existing BLOCK_SIZE for the entire input? Actually, we don't need it because we are looping. We can ignore the passed BLOCK_SIZE and use a fixed inner block size? But we cannot because the kernel must be compiled with a constexpr.\n\nWe can set the inner block size to a fixed value (like 128) and ignore the passed BLOCK_SIZE? But then the kernel would not be using the passed block size. That is acceptable? The constraint does not say we must use the passed block size. But note: the grid is set by the caller and the kernel must work for any input feature size.\n\nBut the kernel must be generic. We can do:\n\n    BLOCK_SIZE_K = 128   # This is what we want, but we cannot because it must be a constexpr parameter.\n\nWe don't have a way to set it. So we must use the passed BLOCK_SIZE as the inner block size? But then the caller is passing the entire input feature dimension (rounded to the next power of two) as the inner block size. That would be inefficient because it would load the entire row in one go.\n\nSo we are stuck.\n\nAnother idea: We can use the passed BLOCK_SIZE as the maximum block size, but then we break the inner loop into a fixed block size? But we cannot because we don't have a second constexpr.\n\nAlternatively, we can use the existing BLOCK_SIZE as the inner block size and hope that the caller passes a small block size? But the caller passes the next power of two of the input features, which is large.\n\nThis is a problem.\n\nWait: the constraint says \"NEVER modify grid configuration\". We are not modifying the grid. But the kernel is being called with a grid of (batch_size, out_features). The inner block size (BLOCK_SIZE) is passed by the caller. We cannot change that. So we must use that BLOCK_SIZE as the inner block size? Then we are effectively doing the same as before? Then why change?\n\nWe must think differently. The original kernel without a loop might be limited by the number of registers when the input features are large. By breaking it into a loop, we reduce the number of registers per thread and allow more parallelism? But the grid is the same.\n\nBut note: the original kernel uses a single block that covers the entire input row. If the input row is very large, the kernel might not be efficient because of the high register pressure and the long time to load a large block.\n\nTherefore, we change the kernel to use a loop over the input features with a fixed inner block size. But we don't have a new parameter. We can hardcode the inner block size? That would break if the input features are not known at compile time? We can do:\n\n    BLOCK_SIZE_INNER = 128\n\nBut then we are hardcoding and it might not be optimal for all input sizes.\n\nAlternatively, we can compute the inner block size as the minimum of the passed BLOCK_SIZE and 128? But that doesn't help because we are still loading the entire row at once? Actually, no: we are breaking the row into chunks of size BLOCK_SIZE_INNER.\n\nBut note: the passed BLOCK_SIZE is the next power of two of the input features. We want to break the input features (in_features) into chunks of 128. We don't care about the passed BLOCK_SIZE? Actually, we can ignore it? But then we are not using it.\n\nWe can do:\n\n    inner_block_size = 128   # but we cannot because it must be a constexpr? We can use 128 as a constant.\n\nBut the problem is that the kernel must be compiled for a specific block size. We can use a fixed inner block size and then the kernel will be compiled for that block size. Then we can loop over the input features in chunks of that fixed size.\n\nSo we can do:\n\n    BLOCK_SIZE_K = 128   # hardcoded\n\nBut then we are not using the passed BLOCK_SIZE. That is acceptable? The constraint does not say we must use the passed block size. We can ignore it.\n\nBut note: the kernel is called with a BLOCK_SIZE that is the next power of two of the input features. We are not using it. We are using our own inner block size.\n\nSo we can write:\n\n    inner_block_size = 128   # This is a constant, not a parameter.\n\n    num_blocks = tl.cdiv(in_features, inner_block_size)\n\n    for k in range(0, num_blocks):\n        idx = k * inner_block_size + tl.arange(0, inner_block_size)\n        ... \n\nBut note: the inner_block_size must be a compile-time constant. We can hardcode it? But then the kernel is not flexible. However, we can tune it by changing the hardcoded value? But we are not allowed to change the signature.\n\nAlternatively, we can define it as a constexpr? But we don't have a parameter for it.\n\nWe can use the existing BLOCK_SIZE as the inner block size? But that is passed by the caller and is the entire input. That is too big.\n\nAlternatively, we can set the inner block size to a fixed value that we know is good (like 128) and ignore the passed BLOCK_SIZE. The passed BLOCK_SIZE is not used in the kernel. That is acceptable.\n\nBut note: the constraint says we cannot change the function signature. We are not changing it. We are just not using one of the parameters. Is that allowed? The problem does not say we must use every parameter.\n\nSo we can do:\n\n    # We are given BLOCK_SIZE, but we ignore it and use 128 for the inner block size.\n\n    BLOCK_SIZE_K = 128   # We define a constant\n\n    num_blocks = tl.cdiv(in_features, BLOCK_SIZE_K)\n\n    ... \n\nBut note: the BLOCK_SIZE passed by the caller is not used. This is acceptable.\n\nHowever, we must be cautious: the kernel must be compiled with the same block size for every call? Actually, the kernel is compiled for the specific value of the constexpr. But we are hardcoding 128. The caller passes a block size that is the next power of two of the input features, which is variable. We are ignoring it. So the kernel will always use an inner block size of 128.\n\nThis is acceptable because the kernel will work for any input feature size (by looping). And the grid is the same.\n\nTherefore, we will:\n\n- Ignore the passed BLOCK_SIZE (we leave it in the signature but don't use it).\n- Use a fixed inner block size of 128.\n\nBut note: 128 is arbitrary. We can choose 256 or 64? We choose 128 because it is a good starting point for memory access.\n\nAlternatively, we can use the passed BLOCK_SIZE as the inner block size if it is less than or equal to 128? But that is not possible because it is the entire input.\n\nSo we hardcode 128.\n\nLet's rewrite the kernel accordingly.\n\nNote: we must be cautious with the mask in the last block.\n\nWe'll do:\n\n    BLOCK_SIZE_K = 128   # Inner block size\n\n    num_blocks = tl.cdiv(in_features, BLOCK_SIZE_K)\n\n    for k in range(0, num_blocks):\n        # Compute the current block's starting index and mask\n        start_idx = k * BLOCK_SIZE_K\n        offsets = start_idx + tl.arange(0, BLOCK_SIZE_K)\n        mask = offsets < in_features\n\n        # Load x and w for this block\n        x_vals = tl.load(x_ptr + ...)\n        w_vals = tl.load(w_ptr + ...)\n\n        acc += tl.sum(x_vals * w_vals)\n\nBut note: the original kernel used the same block size for the entire input. We are changing to a loop.\n\nWe'll write the kernel accordingly.\n\nHowever, note that the current program has a score of 45 and we are trying to improve. We hope that by reducing the register pressure and improving cache behavior, we get better performance.\n\nBut note: the H100 has shared memory and L2 cache. Breaking the input into blocks might allow data reuse? Actually, each thread is processing one output element and one row of the batch. There is no data reuse across threads. But the inner block size being smaller might allow the data to stay in the cache for the duration of the thread? It might also reduce register usage and allow more warps to be scheduled.\n\nWe'll try.\n\nWe are also going to change the TritonLinear class? But we cannot because it is locked. So we only change the kernel.\n\nBut note: the current program does not show the TritonLinear class. We are only allowed to change the kernel.\n\nTherefore, we output the kernel with the loop and a fixed inner block size of 128.\n\nLet's write the kernel:\n\nNote: we are not using the passed BLOCK_SIZE.\n\nWe'll call the inner block size `BLOCK_SIZE_K` in the code, but note that it is hardcoded to 128.\n\nBut wait: we cannot change the signature. We must keep the parameter `BLOCK_SIZE: tl.constexpr` and not use it? That is acceptable.\n\nAlternatively, we can use the passed BLOCK_SIZE for something else? We don't need to.\n\nSo the kernel becomes:\n\n```python\n@triton.jit\ndef linear_kernel(\n    # Pointers to matrices\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    # Matrix dimensions\n    in_features, out_features,\n    # Strides for x\n    stride_xb, stride_xf,\n    # Strides for weight\n    stride_wo, stride_wi,\n    # Strides for output\n    stride_outb, stride_outf,\n    # Block size for features (must be power of two) - we ignore it and use 128 for the inner loop\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(0)  # Batch index\n    pid_o = tl.program_id(1)  # Output feature index\n\n    # We are going to use a fixed inner block size of 128\n    BLOCK_SIZE_K = 128\n\n    # Initialize accumulator\n    acc = 0.0\n\n    # Loop over input features in blocks of BLOCK_SIZE_K\n    num_blocks = tl.cdiv(in_features, BLOCK_SIZE_K)\n    for k in range(0, num_blocks):\n        # Create an index range for the current block of input features\n        idx = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        # Create a mask to prevent out-of-bounds accesses for the current block\n        mask = idx < in_features\n\n        # Load a block of x: [BLOCK_SIZE_K]\n        x_offset = pid_b * stride_xb + idx * stride_xf\n        x_val = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n\n        # Load a block of w: [BLOCK_SIZE_K]\n        w_offset = pid_o * stride_wo + idx * stride_wi\n        w_val = tl.load(w_ptr + w_offset, mask=mask, other=0.0)\n\n        # Accumulate the dot product for this block\n        acc += tl.sum(x_val * w_val)\n\n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        acc += bias\n\n    # Write the result to the output\n    output_offset = pid_b * stride_outb + pid_o * stride_outf\n    tl.store(output_ptr + output_offset, acc)\n```\n\nBut note: the inner block size must be a power of two? It is (128) and we are using a fixed value. This should be efficient.\n\nHowever, we must ensure that the loop is efficient. The loop count is `num_blocks = ceil(in_features / 128)`. This might be large if in_features is large, but each iteration is independent and the hardware can handle it.\n\nWe hope that by breaking the work into smaller blocks, we reduce the register pressure and allow the compiler to schedule more warps concurrently.\n\nWe are making a change of about 20 lines.\n\nLet's output the code accordingly.\n\nNote: we are not changing the function signature and we are maintaining the same grid and the same output.\n\nWe are also careful with the masking and bounds checks.\n\nWe'll output the entire kernel as the current program is only the kernel.\n\nBut note: the current program block might be only the kernel? The problem says: \"the EVOLVE-BLOCK from the previous version\". And the current program is the kernel.\n\nTherefore, we output the kernel with the loop.\n\nLet me double-check: the kernel must be self-contained? Yes.\n\nWe output only the kernel? Yes, because the problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\" and the current program block is the kernel.\n\nBut note: the example of the EVOLVE-BLOCK in the history includes the entire model. However, the current program we are given is only the kernel. So we output only the kernel.\n\nAlternatively, the problem says: \"Provide the complete new program code.\" and the current program code we are given is the kernel. So we output the kernel.\n\nBut note: the top-performing program (Program 1) only showed the kernel. So we follow.\n\nTherefore, the rewritten program is the kernel above.\n\nWe hope for a performance improvement.\n\nIf we see that the inner block size of 128 is not optimal, we might change it in a future iteration.\n\nWe are also going to note that the kernel now uses a loop. This might introduce more instructions but better memory access patterns.\n\nLet's write the code.\n</think>\n```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef linear_kernel(\n    # Pointers to matrices\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    # Matrix dimensions\n    in_features, out_features,\n    # Strides for x\n    stride_xb, stride_xf,\n    # Strides for weight\n    stride_wo, stride_wi,\n    # Strides for output\n    stride_outb, stride_outf,\n    # Block size for features (must be power of two)\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(0)  # Batch index\n    pid_o = tl.program_id(1)  # Output feature index\n\n    # Initialize accumulator\n    acc = 0.0\n\n    # Loop over input features in chunks of 128 for better cache utilization\n    num_blocks = tl.cdiv(in_features, 128)\n    for block_idx in range(0, num_blocks):\n        # Create index range for current block\n        start_idx = block_idx * 128\n        offsets = start_idx + tl.arange(0, 128)\n        mask = offsets < in_features\n\n        # Load input block\n        x_offset = pid_b * stride_xb + offsets * stride_xf\n        x_vals = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n\n        # Load weight block\n        w_offset = pid_o * stride_wo + offsets * stride_wi\n        w_vals = tl.load(w_ptr + w_offset, mask=mask, other=0.0)\n\n        # Accumulate partial dot product\n        acc += tl.sum(x_vals * w_vals)\n\n    # Add bias if present\n    if b_ptr is not None:\n        bias = tl.load(b_ptr + pid_o)\n        acc += bias\n\n    # Write the result to output\n    output_offset = pid_b * stride_outb + pid_o * stride_outf\n    tl.store(output_ptr + output_offset, acc)\n```"]}}}
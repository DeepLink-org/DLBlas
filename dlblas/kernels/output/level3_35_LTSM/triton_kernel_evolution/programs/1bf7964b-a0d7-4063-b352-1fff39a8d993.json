{"id": "1bf7964b-a0d7-4063-b352-1fff39a8d993", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        \"\"\"\n        Initialize the LSTM model.\n\n        :param input_size: The number of expected features in the input `x`\n        :param hidden_size: The number of features in the hidden state `h`\n        :param num_layers: Number of recurrent layers\n        :param output_size: The number of output features\n        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=False)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass through the LSTM model.\n\n        :param x: The input tensor, shape (batch_size, sequence_length, input_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        batch_size = x.size(0)\n        # Initialize hidden states directly on device with correct batch size\n        h0 = torch.randn((self.num_layers, batch_size, self.hidden_size), device=x.device)\n        c0 = torch.randn((self.num_layers, batch_size, self.hidden_size), device=x.device)\n        \n        # Forward propagate LSTM\n        out, _ = self.lstm(x, (h0, c0))\n        \n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])\n        \n        return out\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, sequence_length, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "9a665631-edce-4f2a-b2ca-7ab9f7aa6dc0", "generation": 1, "timestamp": 1754590370.2910535, "iteration_found": 2, "metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 23.0, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 26.0, "speed_up": 1.13, "custom_timing": 23.0}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
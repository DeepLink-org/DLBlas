{"id": "235bac65-ba27-4ef8-bc45-827b03c5304c", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_K': 256}, num_warps=8),\n    ],\n    key=['in_features'],\n)\n@triton.jit\ndef linear_gelu_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    in_features, out_features,\n    stride_x, stride_w0, stride_w1,\n    stride_out0, stride_out1,\n    BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    \n    # Create ranges for output features with masking\n    n_offsets = tl.arange(0, BLOCK_SIZE_N)\n    n_mask = n_offsets < out_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    # Start of input row for current batch\n    x_row_ptr = x_ptr + pid0 * stride_x\n    \n    # Compute linear layer in blocked fashion\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        # Load input block\n        x_val = tl.load(x_row_ptr + k_offsets, mask=k_mask, other=0.0)\n        \n        # Load weight block (all output features for current K-block)\n        w_block_ptr = w_ptr + n_offsets[:, None] * stride_w0 + k_offsets[None, :] * stride_w1\n        w_val = tl.load(w_block_ptr, mask=n_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        # Accumulate matrix product\n        acc += tl.sum(w_val * x_val[None, :], axis=1)\n    \n    # Add bias\n    b_val = tl.load(b_ptr + n_offsets, mask=n_mask, other=0.0)\n    acc += b_val\n    \n    # Apply exact GELU (matches PyTorch's default implementation)\n    gelu = acc * 0.5 * (1.0 + tl.erf(acc * 0.7071067811865475))\n    \n    # Store results\n    out_row_ptr = output_ptr + pid0 * stride_out0\n    tl.store(out_row_ptr + n_offsets * stride_out1, gelu, mask=n_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.randn(out_features))\n    \n    def forward(self, x):\n        x = x.contiguous()\n        output = torch.empty((x.size(0), self.out_features), device=x.device, dtype=torch.float32)\n        \n        # Configure kernel launch - single dimension grid\n        grid = (x.size(0),)\n        BLOCK_SIZE_N = triton.next_power_of_2(self.out_features)\n        \n        linear_gelu_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features,\n            x.stride(0), self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_N=BLOCK_SIZE_N\n        )\n        \n        # Apply softmax separately for numerical stability\n        return torch.softmax(output, dim=1)\n\nbatch_size = 128\nin_features = 100\nout_features = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "5cd882a2-7012-4d24-bcbf-3a1563458097", "generation": 2, "timestamp": 1754588333.1406336, "iteration_found": 8, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_99_Matmul_GELU_Softmax", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "max_difference": ["0.942832", "0.947345", "0.945562", "0.962750", "0.940612"], "avg_difference": ["0.173899", "0.169176", "0.172117", "0.170154", "0.171274"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0491, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_99_Matmul_GELU_Softmax", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "max_difference": ["0.950805", "0.948142", "0.956120", "0.966316", "0.938648"], "avg_difference": ["0.167335", "0.166851", "0.168548", "0.166868", "0.168610"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0491, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
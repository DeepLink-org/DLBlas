{"id": "e0441a67-99d3-4ead-b250-fbd9447f843b", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=['reduction_size'],\n)\n@triton.jit\ndef _max_reduce_kernel(\n    x_ptr,\n    output_ptr,\n    stride0,\n    stride1,\n    stride2,\n    D0,\n    D1,\n    reduction_size,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    i0 = pid // D1\n    i1 = pid % D1\n    base = i0 * stride0 + i1 * stride1\n    \n    offsets = base + tl.arange(0, BLOCK_SIZE) * stride2\n    mask = tl.arange(0, BLOCK_SIZE) < reduction_size\n    \n    vals = tl.load(x_ptr + offsets, mask=mask, other=-float('inf'))\n    max_val = tl.max(vals, axis=0)\n    \n    tl.store(output_ptr + pid, max_val)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, dim: int):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.dim != 2:\n            perm = [i for i in range(3) if i != self.dim] + [self.dim]\n            x_perm = x.permute(perm).contiguous()\n            reduction_size = x_perm.size(2)\n        else:\n            x_perm = x.contiguous()\n            reduction_size = x.size(2)\n        \n        output_shape = (x_perm.size(0), x_perm.size(1))\n        output = torch.empty(output_shape, device=x_perm.device, dtype=x_perm.dtype)\n        \n        stride0 = x_perm.stride(0) * x_perm.element_size()\n        stride1 = x_perm.stride(1) * x_perm.element_size()\n        stride2 = x_perm.stride(2) * x_perm.element_size()\n        \n        D0, D1 = output_shape\n        grid = (D0 * D1,)\n        \n        _max_reduce_kernel[grid](\n            x_perm, output, \n            stride0, stride1, stride2,\n            D0, D1, reduction_size,\n            BLOCK_SIZE=256\n        )\n        return output\n\nbatch_size = 16\ndim1 = 256\ndim2 = 256\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim1, dim2)\n    return [x]\n\ndef get_init_inputs():\n    return [1]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "e0ea4e52-e9e4-4eb2-a129-784ba8a79b69", "generation": 1, "timestamp": 1754640817.3772836, "iteration_found": 3, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_49_Max_reduction_over_a_dimension", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "max_difference": ["4.803940", "4.756998", "4.665563", "4.894771", "4.384673"], "avg_difference": ["2.240772", "2.222986", "2.220934", "2.232909", "2.220418"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0239, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0253, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0239, "speed_up": 0.945, "custom_timing": 0.0253}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
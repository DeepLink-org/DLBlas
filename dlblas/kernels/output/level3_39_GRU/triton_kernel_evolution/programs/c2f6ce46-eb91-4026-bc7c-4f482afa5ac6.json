{"id": "c2f6ce46-eb91-4026-bc7c-4f482afa5ac6", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gru_cell_kernel(\n    input_ptr,\n    h_prev_ptr,\n    w_ih_ptr,\n    w_hh_ptr,\n    b_ih_ptr,\n    b_hh_ptr,\n    h_next_ptr,\n    batch_size,\n    input_size,\n    hidden_size,\n    stride_input_batch,\n    stride_input_feat,\n    stride_hidden_batch,\n    stride_hidden_feat,\n    stride_w_ih_row,\n    stride_w_ih_col,\n    stride_w_hh_row,\n    stride_w_hh_col,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    batch_idx = pid // hidden_size\n    hidden_idx = pid % hidden_size\n    \n    if batch_idx >= batch_size or hidden_idx >= hidden_size:\n        return\n    \n    # Load current hidden state value\n    h_prev_current = tl.load(h_prev_ptr + batch_idx * stride_hidden_batch + hidden_idx)\n    \n    # Weight pointers for this hidden unit\n    w_ir = w_ih_ptr + hidden_idx * stride_w_ih_row\n    w_iz = w_ih_ptr + (hidden_size + hidden_idx) * stride_w_ih_row\n    w_in = w_ih_ptr + (2 * hidden_size + hidden_idx) * stride_w_ih_row\n    \n    w_hr = w_hh_ptr + hidden_idx * stride_w_hh_row\n    w_hz = w_hh_ptr + (hidden_size + hidden_idx) * stride_w_hh_row\n    w_hn = w_hh_ptr + (2 * hidden_size + hidden_idx) * stride_w_hh_row\n    \n    # Initialize accumulators\n    r_val_input = 0.0\n    z_val_input = 0.0\n    n_val_input = 0.0\n    r_val_hidden = 0.0\n    z_val_hidden = 0.0\n    hn_val_hidden = 0.0\n    \n    # Compute input contributions\n    for k in range(0, input_size, BLOCK_SIZE):\n        k_offs = k + tl.arange(0, BLOCK_SIZE)\n        mask = k_offs < input_size\n        \n        # Load input features\n        x = tl.load(input_ptr + batch_idx * stride_input_batch + k_offs * stride_input_feat, \n                   mask=mask, other=0.0)\n        \n        # Load weights\n        w_ir_val = tl.load(w_ir + k_offs * stride_w_ih_col, mask=mask, other=0.0)\n        w_iz_val = tl.load(w_iz + k_offs * stride_w_ih_col, mask=mask, other=0.0)\n        w_in_val = tl.load(w_in + k_offs * stride_w_ih_col, mask=mask, other=0.0)\n        \n        # Accumulate\n        r_val_input += tl.sum(x * w_ir_val)\n        z_val_input += tl.sum(x * w_iz_val)\n        n_val_input += tl.sum(x * w_in_val)\n    \n    # Compute hidden contributions\n    for k in range(0, hidden_size, BLOCK_SIZE):\n        k_offs = k + tl.arange(0, BLOCK_SIZE)\n        mask = k_offs < hidden_size\n        \n        # Load hidden features\n        h = tl.load(h_prev_ptr + batch_idx * stride_hidden_batch + k_offs * stride_hidden_feat, \n                   mask=mask, other=0.0)\n        \n        # Load weights\n        w_hr_val = tl.load(w_hr + k_offs * stride_w_hh_col, mask=mask, other=0.0)\n        w_hz_val = tl.load(w_hz + k_offs * stride_w_hh_col, mask=mask, other=0.0)\n        w_hn_val = tl.load(w_hn + k_offs * stride_w_hh_col, mask=mask, other=0.0)\n        \n        # Accumulate\n        r_val_hidden += tl.sum(h * w_hr_val)\n        z_val_hidden += tl.sum(h * w_hz_val)\n        hn_val_hidden += tl.sum(h * w_hn_val)\n    \n    # Add biases if present\n    if b_ih_ptr is not None:\n        b_ir = tl.load(b_ih_ptr + hidden_idx)\n        b_iz = tl.load(b_ih_ptr + hidden_size + hidden_idx)\n        b_in = tl.load(b_ih_ptr + 2 * hidden_size + hidden_idx)\n        r_val_input += b_ir\n        z_val_input += b_iz\n        n_val_input += b_in\n        \n    if b_hh_ptr is not None:\n        b_hr = tl.load(b_hh_ptr + hidden_idx)\n        b_hz = tl.load(b_hh_ptr + hidden_size + hidden_idx)\n        b_hn = tl.load(b_hh_ptr + 2 * hidden_size + hidden_idx)\n        r_val_hidden += b_hr\n        z_val_hidden += b_hz\n        hn_val_hidden += b_hn\n    \n    # Compute gate values\n    total_r = r_val_input + r_val_hidden\n    total_z = z_val_input + z_val_hidden\n    r = 1.0 / (1.0 + tl.exp(-total_r))\n    n = n_val_input + r * hn_val_hidden\n    n = tl.tanh(n)\n    z = 1.0 / (1.0 + tl.exp(-total_z))\n    \n    # Compute new hidden state\n    h_next = (1 - z) * n + z * h_prev_current\n    tl.store(h_next_ptr + batch_idx * stride_hidden_batch + hidden_idx, h_next)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        \n        # Initialize weights\n        self.w_ih = nn.ParameterList()\n        self.w_hh = nn.ParameterList()\n        self.b_ih = nn.ParameterList()\n        self.b_hh = nn.ParameterList()\n        \n        for layer in range(num_layers):\n            layer_input_size = input_size if layer == 0 else hidden_size\n            w_ih = nn.Parameter(torch.empty(3 * hidden_size, layer_input_size))\n            w_hh = nn.Parameter(torch.empty(3 * hidden_size, hidden_size))\n            nn.init.xavier_uniform_(w_ih)\n            nn.init.xavier_uniform_(w_hh)\n            self.w_ih.append(w_ih)\n            self.w_hh.append(w_hh)\n            \n            if bias:\n                b_ih = nn.Parameter(torch.empty(3 * hidden_size))\n                b_hh = nn.Parameter(torch.empty(3 * hidden_size))\n                nn.init.zeros_(b_ih)\n                nn.init.zeros_(b_hh)\n                self.b_ih.append(b_ih)\n                self.b_hh.append(b_hh)\n            else:\n                self.b_ih.append(None)\n                self.b_hh.append(None)\n    \n    def forward(self, x):\n        # Handle batch_first option\n        if self.batch_first:\n            x = x.transpose(0, 1)\n        \n        seq_len, batch_size, _ = x.shape\n        device = x.device\n        \n        # Initialize hidden states\n        hx = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n        output = torch.zeros(seq_len, batch_size, self.hidden_size, device=device)\n        \n        # Process each layer\n        for layer in range(self.num_layers):\n            hidden = hx[layer]\n            layer_output = []\n            \n            # Process each time step\n            for t in range(seq_len):\n                input_t = x[t] if layer == 0 else output[t]\n                h_next = torch.empty_like(hidden)\n                \n                # Launch kernel for all (batch, hidden) in parallel\n                grid = (batch_size * self.hidden_size,)\n                w_ih = self.w_ih[layer]\n                w_hh = self.w_hh[layer]\n                b_ih = self.b_ih[layer] if self.bias else None\n                b_hh = self.b_hh[layer] if self.bias else None\n                \n                layer_input_size = self.input_size if layer == 0 else self.hidden_size\n                \n                gru_cell_kernel[grid](\n                    input_t, hidden, w_ih, w_hh, \n                    b_ih, b_hh, h_next,\n                    batch_size, layer_input_size, self.hidden_size,\n                    input_t.stride(0), input_t.stride(1),\n                    hidden.stride(0), hidden.stride(1),\n                    w_ih.stride(0), w_ih.stride(1),\n                    w_hh.stride(0), w_hh.stride(1),\n                    BLOCK_SIZE=128\n                )\n                layer_output.append(h_next)\n                hidden = h_next\n            \n            # Update output for this layer\n            output = torch.stack(layer_output)\n            hx[layer] = hidden\n        \n        # Restore batch dimension if needed\n        if self.batch_first:\n            output = output.transpose(0, 1)\n        \n        return output\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.randn(seq_len, batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "080411e0-f320-46dc-8f87-21f5a6a5289f", "generation": 4, "timestamp": 1754597222.6928668, "iteration_found": 22, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_39_GRU", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpzw_dkgss.py\", line 189, in forward\n    gru_cell_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 34.9, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_39_GRU", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "max_difference": ["2.748638", "2.752624", "3.046130", "2.885633", "2.840887"], "avg_difference": ["0.503491", "0.375955", "0.310584", "0.283842", "0.260828"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 34.9, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
{"id": "a5eb3f2e-461b-49e9-a9f2-1a22494af097", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gru_forward_kernel(\n    layer_index,\n    # Pointers to input tensors\n    x_ptr, h0_ptr, \n    # Weight matrices\n    w_ih_ptr, w_hh_ptr,\n    # Bias vectors\n    b_ih_ptr, b_hh_ptr,\n    # Output tensors\n    output_ptr, hn_ptr,\n    # Tensor dimensions and strides\n    seq_len, batch_size, input_size, hidden_size,\n    stride_x_batch, stride_x_seq, stride_x_feature,\n    stride_h0_layer, stride_h0_batch, stride_h0_feature,\n    stride_w_ih_feature, stride_w_ih_hidden,\n    stride_w_hh_feature, stride_w_hh_hidden,\n    stride_b_ih, stride_b_hh,\n    stride_output_batch, stride_output_seq, stride_output_feature,\n    stride_hn_layer, stride_hn_batch, stride_hn_feature,\n    # Triton constants\n    BLOCK_SIZE: tl.constexpr\n):\n    # Compute program IDs\n    pid_batch = tl.program_id(0)\n    \n    # Layer offset calculation\n    layer_offset = layer_index * stride_h0_layer\n    \n    # Initialize hidden state\n    off_h0 = layer_offset + pid_batch * stride_h0_batch + tl.arange(0, hidden_size)\n    h_prev = tl.load(h0_ptr + off_h0, mask=off_h0 < (layer_index+1)*batch_size*hidden_size, other=0.0)\n    \n    # Process sequence\n    for step in range(seq_len):\n        # Input offset\n        off_x = pid_batch * stride_x_batch + step * stride_x_seq + tl.arange(0, input_size)\n        x = tl.load(x_ptr + off_x, mask=off_x < seq_len*batch_size*input_size, other=0.0)\n        \n        # Compute gates\n        # Reset gate\n        reset_gate = tl.sigmoid(\n            tl.dot(x, tl.load(w_ih_ptr + tl.arange(0, hidden_size) * stride_w_ih_feature)) +\n            tl.dot(h_prev, tl.load(w_hh_ptr + tl.arange(0, hidden_size) * stride_w_hh_feature)) +\n            tl.load(b_ih_ptr) + tl.load(b_hh_ptr)\n        )\n        \n        # Update gate\n        update_gate = tl.sigmoid(\n            tl.dot(x, tl.load(w_ih_ptr + hidden_size * stride_w_ih_feature + tl.arange(0, hidden_size))) +\n            tl.dot(h_prev, tl.load(w_hh_ptr + hidden_size * stride_w_hh_feature + tl.arange(0, hidden_size))) +\n            tl.load(b_ih_ptr + hidden_size) + tl.load(b_hh_ptr + hidden_size)\n        )\n        \n        # New gate\n        new_gate = tl.tanh(\n            tl.dot(x, tl.load(w_ih_ptr + 2 * hidden_size * stride_w_ih_feature + tl.arange(0, hidden_size))) +\n            reset_gate * (tl.dot(h_prev, tl.load(w_hh_ptr + 2 * hidden_size * stride_w_hh_feature + tl.arange(0, hidden_size))) +\n            tl.load(b_ih_ptr + 2 * hidden_size) + tl.load(b_hh_ptr + 2 * hidden_size))\n        )\n        \n        # Compute new hidden state\n        h_new = (1 - update_gate) * new_gate + update_gate * h_prev\n        \n        # Store output for this timestep\n        off_output = pid_batch * stride_output_batch + step * stride_output_seq + tl.arange(0, hidden_size)\n        tl.store(output_ptr + off_output, h_new, mask=off_output < seq_len*batch_size*hidden_size)\n        \n        # Update hidden state for next timestep\n        h_prev = h_new\n    \n    # Store final hidden state\n    off_hn = layer_index * stride_hn_layer + pid_batch * stride_hn_batch + tl.arange(0, hidden_size)\n    tl.store(hn_ptr + off_hn, h_prev, mask=off_hn < batch_size*hidden_size)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        \n        # Initialize weights and biases\n        self.w_ih = nn.ParameterList([\n            nn.Parameter(torch.randn(3 * hidden_size, input_size if i == 0 else hidden_size))\n            for i in range(num_layers)\n        ])\n        self.w_hh = nn.ParameterList([\n            nn.Parameter(torch.randn(3 * hidden_size, hidden_size))\n            for _ in range(num_layers)\n        ])\n        \n        if bias:\n            self.b_ih = nn.ParameterList([\n                nn.Parameter(torch.randn(3 * hidden_size))\n                for _ in range(num_layers)\n            ])\n            self.b_hh = nn.ParameterList([\n                nn.Parameter(torch.randn(3 * hidden_size))\n                for _ in range(num_layers)\n            ])\n        else:\n            self.register_parameter('b_ih', None)\n            self.register_parameter('b_hh', None)\n        \n        # Initialize hidden state\n        self.h0 = torch.randn(num_layers, batch_size, hidden_size)\n    \n    def forward(self, x):\n        if self.batch_first:\n            x = x.transpose(0, 1)\n        \n        seq_len, batch_size, _ = x.size()\n        # Initialize output and hidden state tensors\n        output = torch.empty(seq_len, batch_size, self.hidden_size, device=x.device)\n        hn = torch.empty(self.num_layers, batch_size, self.hidden_size, device=x.device)\n        \n        # Process each layer sequentially\n        current_input = x\n        for layer in range(self.num_layers):\n            layer_output = torch.empty(seq_len, batch_size, self.hidden_size, device=x.device)\n            \n            # Configure grid (batch dimension only)\n            grid = (batch_size,)\n            \n            # Get layer parameters\n            w_ih = self.w_ih[layer]\n            w_hh = self.w_hh[layer]\n            b_ih = self.b_ih[layer] if self.bias else None\n            b_hh = self.b_hh[layer] if self.bias else None\n            \n            # Execute kernel for current layer\n            gru_forward_kernel[grid](\n                layer,  # layer index\n                current_input, self.h0, \n                w_ih, w_hh,\n                b_ih, b_hh,\n                layer_output, hn,\n                seq_len, batch_size, \n                self.input_size if layer == 0 else self.hidden_size, \n                self.hidden_size,\n                current_input.stride(1), current_input.stride(0), current_input.stride(2),\n                self.h0.stride(0), self.h0.stride(1), self.h0.stride(2),\n                w_ih.stride(1), w_ih.stride(0),\n                w_hh.stride(1), w_hh.stride(0),\n                b_ih.stride(0) if self.bias else 0,\n                b_hh.stride(0) if self.bias else 0,\n                layer_output.stride(1), layer_output.stride(0), layer_output.stride(2),\n                hn.stride(0), hn.stride(1), hn.stride(2),\n                BLOCK_SIZE=1024\n            )\n            \n            # Prepare input for next layer\n            current_input = layer_output\n        \n        output = current_input\n        if self.batch_first:\n            output = output.transpose(0, 1)\n        return output\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.randn(seq_len, batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "220e1067-7052-42f0-94c7-8c922eb894d0", "generation": 2, "timestamp": 1754594229.2256641, "iteration_found": 8, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_39_GRU", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpsw8liqzr.py\", line 141, in forward\n    gru_forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 229, in visit_For\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 34.9, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_39_GRU", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp8y900onj.py\", line 120, in forward\n    gru_forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 561, in run\n    bound_args, specialization, options = binder(*args, **kwargs)\n  File \"<string>\", line 4, in dynamic_func\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 369, in specialize_impl\n    raise TypeError(\"Unsupported type: %s\" % type(arg))\nTypeError: Unsupported type: <class 'torch.nn.modules.container.ParameterList'>\n", "runtime_error_name": "builtins.TypeError", "reference_timing_ms": 34.9, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
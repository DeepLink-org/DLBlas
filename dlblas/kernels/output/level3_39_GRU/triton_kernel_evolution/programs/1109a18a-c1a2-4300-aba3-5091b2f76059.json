{"id": "1109a18a-c1a2-4300-aba3-5091b2f76059", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef gru_forward_kernel(\n    # Pointers to tensors\n    x_ptr, hx_ptr, \n    w_ih_ptr, w_hh_ptr,\n    b_ih_ptr, b_hh_ptr,\n    output_ptr, final_hidden_ptr,\n    # Dimensions\n    seq_len, batch_size, input_size, hidden_size,\n    # Strides\n    stride_x_batch, stride_x_seq, stride_x_feat,\n    stride_hx_layer, stride_hx_batch, stride_hx_feat,\n    stride_w_ih0, stride_w_ih1,\n    stride_w_hh0, stride_w_hh1,\n    stride_b_ih, stride_b_hh,\n    stride_out_batch, stride_out_seq, stride_out_feat,\n    stride_final_layer, stride_final_batch, stride_final_feat,\n    # Kernel parameters\n    BLOCK_SIZE_H: tl.constexpr,\n    BLOCK_SIZE_B: tl.constexpr,\n):\n    # Program IDs\n    pid_layer = tl.program_id(0)\n    pid_batch_block = tl.program_id(1)\n    pid_hidden_block = tl.program_id(2)\n    \n    # Offsets for batch and hidden dimensions\n    batch_offs = pid_batch_block * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n    hidden_offs = pid_hidden_block * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n    \n    # Mask for batch and hidden dimensions\n    batch_mask = batch_offs < batch_size\n    hidden_mask = hidden_offs < hidden_size\n    \n    # Pointers to initial hidden state for this layer and batch block\n    hx_ptr_layer = hx_ptr + pid_layer * stride_hx_layer\n    h_prev_ptr = hx_ptr_layer + batch_offs[:, None] * stride_hx_batch + hidden_offs[None, :] * stride_hx_feat\n    \n    # Initialize hidden state\n    h_prev = tl.load(h_prev_ptr, mask=batch_mask[:, None] & hidden_mask[None, :], other=0.0)\n    \n    # Loop over sequence\n    for t in range(seq_len):\n        # Pointers to input at timestep t\n        x_ptr_t = x_ptr + t * stride_x_seq + batch_offs[:, None] * stride_x_batch\n        \n        # Reset gate (r)\n        r = 0.0\n        # Update gate (z)\n        z = 0.0\n        # New gate (n)\n        n = 0.0\n        \n        # Compute input contribution\n        for i in range(0, input_size, BLOCK_SIZE_H):\n            i_offs = i + tl.arange(0, BLOCK_SIZE_H)\n            i_mask = i_offs < input_size\n            \n            # Load input\n            x = tl.load(\n                x_ptr_t + i_offs[None, :] * stride_x_feat,\n                mask=batch_mask[:, None] & i_mask[None, :],\n                other=0.0\n            )\n            \n            # Load weights for reset gate\n            w_ih_r = tl.load(\n                w_ih_ptr + hidden_offs[:, None] * stride_w_ih0 + i_offs[None, :] * stride_w_ih1,\n                mask=hidden_mask[:, None] & i_mask[None, :],\n                other=0.0\n            )\n            r += tl.sum(x[None, :, :] * w_ih_r[:, None, :], axis=2)\n            \n            # Load weights for update gate\n            w_ih_z = tl.load(\n                w_ih_ptr + (hidden_size + hidden_offs[:, None]) * stride_w_ih0 + i_offs[None, :] * stride_w_ih1,\n                mask=hidden_mask[:, None] & i_mask[None, :],\n                other=0.0\n            )\n            z += tl.sum(x[None, :, :] * w_ih_z[:, None, :], axis=2)\n            \n            # Load weights for new gate\n            w_ih_n = tl.load(\n                w_ih_ptr + (2 * hidden_size + hidden_offs[:, None]) * stride_w_ih0 + i_offs[None, :] * stride_w_ih1,\n                mask=hidden_mask[:, None] & i_mask[None, :],\n                other=0.0\n            )\n            n += tl.sum(x[None, :, :] * w_ih_n[:, None, :], axis=2)\n        \n        # Compute hidden contribution\n        for i in range(0, hidden_size, BLOCK_SIZE_H):\n            i_offs = i + tl.arange(0, BLOCK_SIZE_H)\n            i_mask = i_offs < hidden_size\n            \n            # Load hidden state\n            h_val = tl.load(\n                hx_ptr_layer + batch_offs[:, None] * stride_hx_batch + i_offs[None, :] * stride_hx_feat,\n                mask=batch_mask[:, None] & i_mask[None, :],\n                other=0.0\n            )\n            \n            # Load weights for reset gate\n            w_hh_r = tl.load(\n                w_hh_ptr + hidden_offs[:, None] * stride_w_hh0 + i_offs[None, :] * stride_w_hh1,\n                mask=hidden_mask[:, None] & i_mask[None, :],\n                other=0.0\n            )\n            r += tl.sum(h_val[None, :, :] * w_hh_r[:, None, :], axis=2)\n            \n            # Load weights for update gate\n            w_hh_z = tl.load(\n                w_hh_ptr + (hidden_size + hidden_offs[:, None]) * stride_w_hh0 + i_offs[None, :] * stride_w_hh1,\n                mask=hidden_mask[:, None] & i_mask[None, :],\n                other=0.0\n            )\n            z += tl.sum(h_val[None, :, :] * w_hh_z[:, None, :], axis=2)\n            \n            # Load weights for new gate\n            w_hh_n = tl.load(\n                w_hh_ptr + (2 * hidden_size + hidden_offs[:, None]) * stride_w_hh0 + i_offs[None, :] * stride_w_hh1,\n                mask=hidden_mask[:, None] & i_mask[None, :],\n                other=0.0\n            )\n            n += tl.sum(h_val[None, :, :] * w_hh_n[:, None, :], axis=2)\n        \n        # Add biases if available\n        if b_ih_ptr is not None:\n            b_ih_r = tl.load(\n                b_ih_ptr + hidden_offs,\n                mask=hidden_mask,\n                other=0.0\n            )\n            r += b_ih_r[:, None]\n            \n            b_ih_z = tl.load(\n                b_ih_ptr + hidden_size + hidden_offs,\n                mask=hidden_mask,\n                other=0.0\n            )\n            z += b_ih_z[:, None]\n            \n            b_ih_n = tl.load(\n                b_ih_ptr + 2 * hidden_size + hidden_offs,\n                mask=hidden_mask,\n                other=0.0\n            )\n            n += b_ih_n[:, None]\n        \n        if b_hh_ptr is not None:\n            b_hh_r = tl.load(\n                b_hh_ptr + hidden_offs,\n                mask=hidden_mask,\n                other=0.0\n            )\n            r += b_hh_r[:, None]\n            \n            b_hh_z = tl.load(\n                b_hh_ptr + hidden_size + hidden_offs,\n                mask=hidden_mask,\n                other=0.0\n            )\n            z += b_hh_z[:, None]\n            \n            b_hh_n = tl.load(\n                b_hh_ptr + 2 * hidden_size + hidden_offs,\n                mask=hidden_mask,\n                other=0.0\n            )\n            n += b_hh_n[:, None]\n        \n        # Apply activation functions\n        r = tl.sigmoid(r)\n        z = tl.sigmoid(z)\n        n = tl.tanh(n + r * n)\n        \n        # Compute new hidden state\n        h_next = (1 - z) * n + z * h_prev\n        \n        # Store output for this timestep\n        out_ptr_t = output_ptr + t * stride_out_seq + batch_offs[:, None] * stride_out_batch + hidden_offs[None, :] * stride_out_feat\n        tl.store(out_ptr_t, h_next, mask=batch_mask[:, None] & hidden_mask[None, :])\n        \n        # Update hidden state for next timestep\n        h_prev = h_next\n    \n    # Store final hidden state\n    final_ptr = final_hidden_ptr + pid_layer * stride_final_layer + batch_offs[:, None] * stride_final_batch + hidden_offs[None, :] * stride_final_feat\n    tl.store(final_ptr, h_prev, mask=batch_mask[:, None] & hidden_mask[None, :])\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        \n        self.weight_ih = nn.ParameterList()\n        self.weight_hh = nn.ParameterList()\n        self.bias_ih = nn.ParameterList()\n        self.bias_hh = nn.ParameterList()\n        \n        for i in range(num_layers):\n            layer_input_size = input_size if i == 0 else hidden_size\n            self.weight_ih.append(nn.Parameter(torch.empty(3 * hidden_size, layer_input_size)))\n            self.weight_hh.append(nn.Parameter(torch.empty(3 * hidden_size, hidden_size)))\n            if bias:\n                self.bias_ih.append(nn.Parameter(torch.empty(3 * hidden_size)))\n                self.bias_hh.append(nn.Parameter(torch.empty(3 * hidden_size)))\n        \n        self.reset_parameters()\n        self.h0 = torch.randn((num_layers, 1, hidden_size))\n    \n    def reset_parameters(self):\n        for weight in self.weight_ih:\n            nn.init.xavier_uniform_(weight)\n        for weight in self.weight_hh:\n            nn.init.xavier_uniform_(weight)\n        if self.bias:\n            for bias in self.bias_ih:\n                nn.init.zeros_(bias)\n            for bias in self.bias_hh:\n                nn.init.zeros_(bias)\n    \n    def forward(self, x):\n        if self.batch_first:\n            x = x.transpose(0, 1)\n        \n        batch_size = x.size(1)\n        hx = self.h0.expand(self.num_layers, batch_size, self.hidden_size).contiguous().to(x.device)\n        \n        # Initialize output tensor\n        output = torch.empty(x.size(0), batch_size, self.hidden_size, device=x.device, dtype=x.dtype)\n        final_hidden = torch.empty_like(hx)\n        \n        # Triton kernel configuration\n        BLOCK_SIZE_B = 16\n        BLOCK_SIZE_H = 32\n        grid = (\n            self.num_layers,\n            triton.cdiv(batch_size, BLOCK_SIZE_B),\n            triton.cdiv(self.hidden_size, BLOCK_SIZE_H),\n        )\n        \n        # Process each layer\n        for layer in range(self.num_layers):\n            weight_ih = self.weight_ih[layer]\n            weight_hh = self.weight_hh[layer]\n            bias_ih = self.bias_ih[layer] if self.bias else None\n            bias_hh = self.bias_hh[layer] if self.bias else None\n            \n            # Input for this layer\n            layer_input = x if layer == 0 else output\n            \n            gru_forward_kernel[grid](\n                layer_input, hx[layer], \n                weight_ih, weight_hh,\n                bias_ih, bias_hh,\n                output, final_hidden[layer],\n                # Dimensions\n                x.size(0), batch_size, \n                weight_ih.size(1), self.hidden_size,\n                # Strides\n                layer_input.stride(1), layer_input.stride(0), layer_input.stride(2),\n                hx.stride(0), hx.stride(1), hx.stride(2),\n                weight_ih.stride(0), weight_ih.stride(1),\n                weight_hh.stride(0), weight_hh.stride(1),\n                bias_ih.stride(0) if self.bias else 0,\n                bias_hh.stride(0) if self.bias else 0,\n                output.stride(1), output.stride(0), output.stride(2),\n                final_hidden.stride(0), final_hidden.stride(1), final_hidden.stride(2),\n                BLOCK_SIZE_H=BLOCK_SIZE_H,\n                BLOCK_SIZE_B=BLOCK_SIZE_B\n            )\n        \n        if self.batch_first:\n            output = output.transpose(0, 1)\n        \n        return output\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.randn(seq_len, batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "897d0557-d515-4920-b53b-856393ad8005", "generation": 3, "timestamp": 1754596523.64886, "iteration_found": 17, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_39_GRU", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmposfjd0xp.py\", line 263, in forward\n    gru_forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 229, in visit_For\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 34.9, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_39_GRU", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "max_difference": ["3.307436", "3.208063", "3.318467", "3.345726", "3.354910"], "avg_difference": ["0.196288", "0.317436", "0.420251", "0.459821", "0.473675"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 34.9, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
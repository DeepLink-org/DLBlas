{"id": "080411e0-f320-46dc-8f87-21f5a6a5289f", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n@triton.jit\ndef gru_forward_kernel(\n    input_ptr,\n    hidden_ptr,\n    w_ih_ptr,\n    w_hh_ptr,\n    b_ih_ptr,\n    b_hh_ptr,\n    output_ptr,\n    final_hidden_ptr,\n    seq_len,\n    batch_size,\n    input_size,\n    hidden_size,\n    stride_input_batch,\n    stride_input_seq,\n    stride_input_feat,\n    stride_hidden_layer,\n    stride_hidden_batch,\n    stride_hidden_feat,\n    stride_output_batch,\n    stride_output_seq,\n    stride_output_feat,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    batch_idx = pid // hidden_size\n    feat_idx = pid % hidden_size\n\n    if batch_idx >= batch_size or feat_idx >= hidden_size:\n        return\n\n    # Initialize current hidden state\n    h_prev = tl.load(hidden_ptr + batch_idx * stride_hidden_batch + feat_idx)\n    \n    # Pointers to weight matrices for this feature\n    w_ih_ptr += feat_idx * input_size * 3\n    w_hh_ptr += feat_idx * hidden_size * 3\n\n    # Accumulators for gates\n    reset_gate = 0.0\n    update_gate = 0.0\n    new_gate = 0.0\n\n    # Compute input part\n    for k in range(0, input_size, BLOCK_SIZE):\n        k_offs = k + tl.arange(0, BLOCK_SIZE)\n        mask = k_offs < input_size\n        \n        # Load input weights and features\n        w_ir = tl.load(w_ih_ptr + k_offs, mask=mask, other=0.0)\n        w_iz = tl.load(w_ih_ptr + input_size + k_offs, mask=mask, other=0.0)\n        w_in = tl.load(w_ih_ptr + 2*input_size + k_offs, mask=mask, other=0.0)\n        \n        input_val = tl.load(\n            input_ptr + batch_idx * stride_input_batch + k_offs,\n            mask=mask, other=0.0\n        )\n        \n        reset_gate += tl.sum(w_ir * input_val)\n        update_gate += tl.sum(w_iz * input_val)\n        new_gate += tl.sum(w_in * input_val)\n\n    # Compute hidden part\n    for k in range(0, hidden_size, BLOCK_SIZE):\n        k_offs = k + tl.arange(0, BLOCK_SIZE)\n        mask = k_offs < hidden_size\n        \n        # Load hidden weights and features\n        w_hr = tl.load(w_hh_ptr + k_offs, mask=mask, other=0.0)\n        w_hz = tl.load(w_hh_ptr + hidden_size + k_offs, mask=mask, other=0.0)\n        w_hn = tl.load(w_hh_ptr + 2*hidden_size + k_offs, mask=mask, other=0.0)\n        \n        hidden_val = tl.load(\n            hidden_ptr + batch_idx * stride_hidden_batch + k_offs,\n            mask=mask, other=0.0\n        )\n        \n        reset_gate += tl.sum(w_hr * hidden_val)\n        update_gate += tl.sum(w_hz * hidden_val)\n        new_gate += tl.sum(w_hn * hidden_val)\n\n    # Add biases\n    if b_ih_ptr is not None:\n        b_ir = tl.load(b_ih_ptr + feat_idx)\n        b_iz = tl.load(b_ih_ptr + hidden_size + feat_idx)\n        b_in = tl.load(b_ih_ptr + 2*hidden_size + feat_idx)\n        reset_gate += b_ir\n        update_gate += b_iz\n        new_gate += b_in\n        \n    if b_hh_ptr is not None:\n        b_hr = tl.load(b_hh_ptr + feat_idx)\n        b_hz = tl.load(b_hh_ptr + hidden_size + feat_idx)\n        b_hn = tl.load(b_hh_ptr + 2*hidden_size + feat_idx)\n        reset_gate += b_hr\n        update_gate += b_hz\n        new_gate += b_hn\n\n    # Apply activation functions\n    reset_gate = 1.0 / (1.0 + tl.exp(-reset_gate))\n    update_gate = 1.0 / (1.0 + tl.exp(-update_gate))\n    new_gate = libdevice.tanh(new_gate)\n\n    # Update hidden state\n    h_next = (1 - update_gate) * new_gate + update_gate * h_prev\n    tl.store(final_hidden_ptr + batch_idx * stride_hidden_batch + feat_idx, h_next)\n    \n    # Write output for all time steps (placeholder)\n    for t in range(seq_len):\n        off_output = t * stride_output_seq + batch_idx * stride_output_batch + feat_idx * stride_output_feat\n        tl.store(output_ptr + off_output, h_next)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        \n        # Initialize weights\n        self.w_ih = nn.Parameter(torch.Tensor(3 * hidden_size, input_size))\n        self.w_hh = nn.Parameter(torch.Tensor(3 * hidden_size, hidden_size))\n        self.b_ih = nn.Parameter(torch.Tensor(3 * hidden_size)) if bias else None\n        self.b_hh = nn.Parameter(torch.Tensor(3 * hidden_size)) if bias else None\n        \n        # Initialize hidden state\n        self.h0 = torch.randn((num_layers, 10, hidden_size))\n        nn.init.xavier_uniform_(self.w_ih)\n        nn.init.xavier_uniform_(self.w_hh)\n        if bias:\n            nn.init.zeros_(self.b_ih)\n            nn.init.zeros_(self.b_hh)\n    \n    def forward(self, x):\n        self.h0 = self.h0.to(x.device)\n        \n        # Handle batch_first option\n        if self.batch_first:\n            x = x.transpose(0, 1)\n        \n        # Initialize output tensor with correct hidden_size\n        seq_len, batch_size, _ = x.shape\n        output = torch.zeros(seq_len, batch_size, self.hidden_size, \n                             device=x.device, dtype=x.dtype)\n        \n        # Triton kernel configuration\n        grid = lambda meta: (batch_size * self.hidden_size,)\n        \n        # Execute GRU kernel\n        gru_forward_kernel[grid](\n            x, self.h0[0], self.w_ih, self.w_hh, \n            self.b_ih, self.b_hh, output, self.h0[0],\n            seq_len, batch_size, self.input_size, self.hidden_size,\n            x.stride(1), x.stride(0), x.stride(2),\n            self.h0.stride(0), self.h0.stride(1), self.h0.stride(2),\n            output.stride(1), output.stride(0), output.stride(2),\n            BLOCK_SIZE=128\n        )\n        \n        # Restore batch dimension if needed\n        if self.batch_first:\n            output = output.transpose(0, 1)\n        \n        return output\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.randn(seq_len, batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "82f4b519-2532-401c-8492-4033a676a901", "generation": 3, "timestamp": 1754595015.3027878, "iteration_found": 12, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_39_GRU", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "max_difference": ["2.748638", "2.752624", "3.046130", "2.885633", "2.840887"], "avg_difference": ["0.503491", "0.375955", "0.310584", "0.283842", "0.260828"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 34.9, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_39_GRU", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_issue": "Output shape mismatch: Expected torch.Size([512, 10, 256]), got torch.Size([512, 10, 128])", "correctness_issue_name": "correctness_issue", "reference_timing_ms": 34.9, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
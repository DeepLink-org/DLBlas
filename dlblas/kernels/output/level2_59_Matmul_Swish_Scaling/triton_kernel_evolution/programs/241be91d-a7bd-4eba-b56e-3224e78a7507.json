{"id": "241be91d-a7bd-4eba-b56e-3224e78a7507", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_N': 64, 'BLOCK_K': 128}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_N': 128, 'BLOCK_K': 128}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_N': 256, 'BLOCK_K': 128}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_N': 256, 'BLOCK_K': 64}, num_warps=8, num_stages=3),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef fused_linear_swish_scale(\n    input_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    scaling_factor,\n    stride_input,\n    stride_weight,\n    stride_output,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    pid_b = tl.program_id(0)\n    pid_oc = tl.program_id(1)\n    \n    oc_offs = pid_oc * BLOCK_N + tl.arange(0, BLOCK_N)\n    mask_oc = oc_offs < out_features\n    \n    # Precompute base weight pointers for the current output block\n    base_weight_ptrs = weight_ptr + oc_offs * stride_weight\n    \n    # Initialize accumulator in fp32 for numerical stability\n    acc = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    bias = tl.load(bias_ptr + oc_offs, mask=mask_oc, other=0.0)\n    \n    # Loop over K dimension with vectorized loads\n    for k in range(0, tl.cdiv(in_features, BLOCK_K)):\n        k_offs = k * BLOCK_K + tl.arange(0, BLOCK_K)\n        mask_k = k_offs < in_features\n        \n        # Coalesced memory access for input\n        input_vals = tl.load(\n            input_ptr + pid_b * stride_input + k_offs,\n            mask=mask_k,\n            other=0.0,\n            eviction_policy=\"evict_last\"\n        )\n        \n        # Compute weight pointers for current K block\n        weight_ptrs = base_weight_ptrs[:, None] + k_offs[None, :]\n        \n        # Vectorized weight load with appropriate mask\n        weight_vals = tl.load(\n            weight_ptrs,\n            mask=mask_oc[:, None] & mask_k[None, :],\n            other=0.0,\n            eviction_policy=\"evict_first\"\n        )\n        \n        # Use tensor core dot product for better performance\n        input_vals_2D = tl.reshape(input_vals, (1, BLOCK_K))\n        weight_vals_2D = tl.reshape(weight_vals, (BLOCK_N, BLOCK_K))\n        product = tl.dot(input_vals_2D, tl.trans(weight_vals_2D), allow_tf32=True)\n        acc += tl.reshape(product, (BLOCK_N,))\n    \n    # Fused activation and scaling\n    result = (acc + bias) * tl.sigmoid(acc + bias) * scaling_factor\n    \n    # Coalesced output store\n    tl.store(\n        output_ptr + pid_b * stride_output + oc_offs,\n        result,\n        mask=mask_oc\n    )\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scaling_factor = scaling_factor\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        bound = 1 / math.sqrt(in_features) if in_features > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        x = x.contiguous()\n        output = torch.empty((x.size(0), self.out_features), device=x.device, dtype=x.dtype)\n        \n        # Dynamic grid calculation using autotuned BLOCK_N\n        grid = lambda meta: (\n            x.size(0), \n            triton.cdiv(self.out_features, meta['BLOCK_N'])\n        )\n        \n        fused_linear_swish_scale[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features,\n            self.scaling_factor,\n            x.stride(0), self.weight.stride(0), output.stride(0)\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "b250acb2-bd32-4860-b298-a0140167d831", "generation": 3, "timestamp": 1754574881.501203, "iteration_found": 12, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_59_Matmul_Swish_Scaling", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2045, in dot\n    return _semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1504, in dot\n    and rhs.shape[-1].value >= min_dot_size[1], \\\nAssertionError: Input shapes should have M >= 16, N >= 16 and K >= 16\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp6nbo1sps.py\", line 109, in forward\n    fused_linear_swish_scale[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 160, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 146, in kernel_call\n    self.fn.run(\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 55:18:\n        # Vectorized weight load with appropriate mask\n        weight_vals = tl.load(\n            weight_ptrs,\n            mask=mask_oc[:, None] & mask_k[None, :],\n            other=0.0,\n            eviction_policy=\"evict_first\"\n        )\n\n        # Use tensor core dot product for better performance\n        input_vals_2D = tl.reshape(input_vals, (1, BLOCK_K))\n        weight_vals_2D = tl.reshape(weight_vals, (BLOCK_N, BLOCK_K))\n        product = tl.dot(input_vals_2D, tl.trans(weight_vals_2D), allow_tf32=True)\n                  ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2045, in dot\n    return _semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1504, in dot\n    and rhs.shape[-1].value >= min_dot_size[1], \\\nAssertionError: Input shapes should have M >= 16, N >= 16 and K >= 16\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0746, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.896, "runtime_ms": 0.0833, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0746, "speed_up": 0.896, "custom_timing": 0.0833}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
{"id": "243a87c0-bdb0-4f71-aa56-85530d705f47", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _forward_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    M, N, K, scaling_factor,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_bn,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        k_mask = offs_k[None, :] < k_remaining\n        \n        x = tl.load(x_ptrs, mask=k_mask, other=0.0)\n        w = tl.load(w_ptrs, mask=k_mask, other=0.0)\n        \n        acc += tl.dot(x, w, allow_tf32=True)\n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n    \n    bias = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += bias[None, :]\n    \n    swish = acc * tl.sigmoid(acc) * scaling_factor\n    \n    offs_m_out = offs_m[:, None]\n    offs_n_out = offs_n[None, :]\n    mask = (offs_m_out < M) & (offs_n_out < N)\n    \n    out_ptrs = output_ptr + stride_om * offs_m_out + stride_on * offs_n_out\n    tl.store(out_ptrs, swish, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scaling_factor = scaling_factor\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.zeros(out_features))\n    \n    def forward(self, x):\n        output = torch.empty((x.size(0), self.out_features), \n                             device=x.device, dtype=x.dtype)\n        \n        M, K = x.shape\n        N = self.weight.shape[0]\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        \n        _forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            M, N, K, self.scaling_factor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            self.bias.stride(0),\n            output.stride(0), output.stride(1)\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "cfa6f593-d3a5-43b5-8c73-7fc129ce2827", "generation": 1, "timestamp": 1754571854.870824, "iteration_found": 2, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_59_Matmul_Swish_Scaling", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2150, in load\n    return _semantic.load(pointer, mask, other, boundary_check, padding_option, cache_modifier, eviction_policy,\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1086, in load\n    return self._load_legacy(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1037, in _load_legacy\n    mask = self.broadcast_impl_shape(mask, ptr.type.get_block_shapes())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 702, in broadcast_impl_shape\n    raise ValueError(f\"Cannot broadcast, the expanded size of the tensor ({shape[i]})\"\nValueError: Cannot broadcast, the expanded size of the tensor (128) must match the existing size (32) at non-singleton dimension 1: ['1', '32'], ['32', '128']\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmppv5leklo.py\", line 82, in forward\n    _forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 160, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 146, in kernel_call\n    self.fn.run(\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 29:12:\n\n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        k_mask = offs_k[None, :] < k_remaining\n\n        x = tl.load(x_ptrs, mask=k_mask, other=0.0)\n        w = tl.load(w_ptrs, mask=k_mask, other=0.0)\n            ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2150, in load\n    return _semantic.load(pointer, mask, other, boundary_check, padding_option, cache_modifier, eviction_policy,\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1086, in load\n    return self._load_legacy(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1037, in _load_legacy\n    mask = self.broadcast_impl_shape(mask, ptr.type.get_block_shapes())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 702, in broadcast_impl_shape\n    raise ValueError(f\"Cannot broadcast, the expanded size of the tensor ({shape[i]})\"\nValueError: Cannot broadcast, the expanded size of the tensor (128) must match the existing size (32) at non-singleton dimension 1: ['1', '32'], ['32', '128']\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0746, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0548, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0746, "speed_up": 1.361, "custom_timing": 0.0548}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0548\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0746, 'speed_up': 1.361, 'custom_timing': 0.0548}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0548, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0746, 'speed_up': 1.361, 'custom_timing': 0.0548}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies Swish activation, and scales the result.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x * torch.sigmoid(x)  # Swish activation\n        x = x * self.scaling_factor\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0548), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0746, 'speed_up': 1.361, 'custom_timing': 0.0548})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix multiplication, applies Swish activation, and scales the result.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = x * torch.sigmoid(x)  # Swish activation\n        x = x * self.scaling_factor\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Matrix multiplication (nn.Linear)\n  2. Swish activation: x * torch.sigmoid(x)\n  3. Scaling by a factor\n\nThe current implementation is in PyTorch and runs on the GPU. We are to rewrite it using Triton for the core operations to improve performance.\n\nHowever, note that the entire model is currently implemented as an nn.Module. We are allowed to change only the Triton kernel part? But the current code does not have a Triton kernel.\n\nLooking at the context, we are to generate Triton implementations for the operations that are currently done by PyTorch.\n\nBut note: The problem says we are to generate Triton implementations (custom_code) for the reference_code (which is the PyTorch code).\n\nIn the example provided earlier, we saw a Triton kernel for per_row_quantize_int8.\n\nSo, for this task, we should replace the forward pass of the ModelNew with a Triton implementation of the entire forward function? Or at least the matrix multiplication and Swish?\n\nHowever, note the constraints:\n  - We cannot change the function signatures (of the forward method? or the Triton kernel?).\n\nBut the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\"\n\nBut the current code does not have a Triton kernel. So we are introducing a Triton kernel for the entire forward pass? Or break it down?\n\nAlternatively, we can break the operations and write Triton kernels for the linear layer and the Swish activation? But note that the linear layer is already optimized in PyTorch (using cuBLAS). However, we are to write a Triton version that is faster.\n\nBut note: The current model is very small (batch_size=128, in_features=1024, out_features=512). We have to be cautious because launching a Triton kernel for each operation might not be beneficial for small sizes.\n\nHowever, the problem says we are to rewrite the program to improve performance. Since the current code is pure PyTorch, we are to replace it with a Triton version.\n\nPlan:\n  We will write a Triton kernel that does:\n    - The matrix multiplication (with weight matrix)\n    - Then applies Swish activation\n    - Then scales by the scaling factor\n\nBut note: The model has a weight matrix and bias (because nn.Linear has bias by default). We must account for bias? The current nn.Linear uses bias? Yes, by default. However, the provided __init__ does not set bias to False. So we have bias.\n\nBut the forward method does not show the bias? Actually, the nn.Linear does include bias.\n\nSo we have to do: \n   x = input @ weight.t() + bias\n   then Swish: x * sigmoid(x)\n   then scale: x * scaling_factor\n\nWe can write a single Triton kernel that does all these steps? That would be efficient because we avoid multiple kernel launches and intermediate storage.\n\nAlternatively, we can write two kernels: one for the linear and one for the Swish and scaling? But that would be two kernels. However, note that the Swish and scaling are element-wise and can be fused with the linear? Actually, we can do:\n\n   For each element in the output matrix:\n        out = (input_row * weight_col + bias)   [this is the linear part]\n        out = out * sigmoid(out) * scaling_factor\n\nBut note: the linear operation is a batched matrix multiplication. We have batch_size=128, in_features=1024, out_features=512.\n\nWe can write a kernel that processes multiple rows (batch) and multiple columns (out_features) in parallel.\n\nHowever, note the constraints: \n   - We must not change the grid configuration? Actually, we are allowed to change the grid because we are writing a new kernel? The constraint says: \"NEVER modify grid configuration, output tensor shapes, or PID logic\" in the context of the existing Triton kernel? But here we are introducing a new one.\n\nBut note: the problem says we are only allowed to change the Triton kernel source within the EVOLVE-BLOCK. And the grid configuration is part of the kernel launch. So we have to design the kernel and its launch.\n\nWe are to generate a complete updated code containing the EVOLVE-BLOCK. So we can replace the entire ModelNew class and the helper functions? But note the problem says: \"Modify ONLY the Triton kernel source within this block\". However, the current block does not have a Triton kernel. So we are introducing one.\n\nWe must be cautious: the problem says \"all other code is LOCKED\", meaning outside the EVOLVE-BLOCK? But we are inside the block.\n\nSo we are allowed to change the entire content of the EVOLVE-BLOCK? Yes.\n\nTherefore, we can rewrite the entire block to use Triton.\n\nSteps:\n\n1. We will write a Triton kernel that performs the entire forward pass (linear, swish, scaling) for one output element? Or for a block of output.\n\n2. We'll use a 2D grid: \n   - The first dimension: batch_size (128)\n   - The second dimension: out_features (512) divided by the block size.\n\n   But note: we can also use a 1D grid and have each program id cover multiple elements? Alternatively, we can use a 1D grid and then use loops? \n\n   Given the sizes, we can have:\n        grid = (triton.cdiv(batch_size, BLOCK_SIZE_M), triton.cdiv(out_features, BLOCK_SIZE_N))\n\n   Then each block computes a block of size BLOCK_SIZE_M x BLOCK_SIZE_N.\n\n   However, we also have to consider the reduction dimension (in_features=1024). We will split it into blocks of BLOCK_SIZE_K and use a reduction loop.\n\n3. We'll load a block of the input (of size BLOCK_SIZE_M x BLOCK_SIZE_K) and a block of the weight (BLOCK_SIZE_K x BLOCK_SIZE_N) and compute the matrix product.\n\n4. Then we add the bias (which is of size out_features). We can load the bias vector and add it after the accumulation? But note: we are accumulating over K, then we add the bias at the end.\n\n5. Then we apply the Swish and scaling.\n\n6. Store the result.\n\nBut note: the weight matrix is stored in the linear layer. We must extract the weight and bias from the nn.Linear? But wait, we are replacing the entire model? Actually, we are going to bypass the nn.Linear.\n\nWe have two options:\n   Option 1: Keep the nn.Linear but replace its implementation with a Triton kernel? But that would require hooking into the nn.Linear, which is not straightforward.\n\n   Option 2: Replace the entire model with a custom function that uses Triton.\n\nGiven the problem, we are to rewrite the program. We are allowed to change the entire EVOLVE-BLOCK.\n\nSo we can do:\n\n   - Remove the nn.Linear and instead store the weight and bias as parameters.\n   - Then in the forward, we call our Triton kernel.\n\nBut note: the problem says \"maintain the same inputs and outputs\". So the model must still be an nn.Module? Because the original was.\n\nWe'll create a new ModelNew that is an nn.Module, but without using nn.Linear. Instead, we define weight and bias as nn.Parameters and use our Triton kernel.\n\nSteps in code:\n\n   class ModelNew(nn.Module):\n        def __init__(self, in_features, out_features, scaling_factor):\n            super().__init__()\n            self.scaling_factor = scaling_factor\n            # initialize weight and bias\n            self.weight = nn.Parameter(torch.randn(out_features, in_features))\n            self.bias = nn.Parameter(torch.zeros(out_features))\n\n        def forward(self, x):\n            # x: [batch_size, in_features]\n            # output: [batch_size, out_features]\n            output = torch.empty(x.shape[0], self.weight.shape[0], device=x.device, dtype=x.dtype)\n            # call the Triton kernel\n            # we have to set the grid and block appropriately\n            # ... \n\nBut note: the original model had the linear layer as self.matmul. We are replacing that.\n\nWe also have to consider the Swish and scaling.\n\nWe'll write a kernel that does:\n   for each output element (i, j) in the batch and out_features:\n        acc = 0\n        for k in range(0, in_features, BLOCK_SIZE_K):\n            load a block of input [i, k:k+BLOCK_K] and weight [j, k:k+BLOCK_K] (but note: weight is stored as [out_features, in_features])\n            acc += dot product of the two blocks\n\n        acc += bias[j]\n        acc = acc * sigmoid(acc) * scaling_factor\n\nBut note: the sigmoid function in Triton: we can use tl.sigmoid? Actually, Triton has a sigmoid function: tl.sigmoid.\n\nSo the kernel:\n\n   @triton.jit\n   def _forward_kernel(\n        x_ptr, \n        w_ptr,\n        b_ptr,\n        output_ptr,\n        in_features, \n        out_features,\n        scaling_factor,\n        stride_x_batch, stride_x_feat,\n        stride_w_out, stride_w_in,\n        stride_b,\n        stride_out_batch, stride_out_feat,\n        BLOCK_SIZE_M: tl.constexpr,\n        BLOCK_SIZE_N: tl.constexpr,\n        BLOCK_SIZE_K: tl.constexpr,\n   ):\n        pid_batch = tl.program_id(0)\n        pid_out = tl.program_id(1)\n\n        # create ranges for the block\n        offs_m = pid_batch * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_n = pid_out * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n        # we are going to compute a block of [BLOCK_SIZE_M, BLOCK_SIZE_N] in the output.\n        # but note: we might have to handle the boundaries.\n\n        # However, note: the batch dimension might not be divisible by BLOCK_SIZE_M? We are using one program_id per block in batch? Actually, we are using 2D grid: \n        #   grid = (triton.cdiv(batch_size, BLOCK_SIZE_M), triton.cdiv(out_features, BLOCK_SIZE_N))\n\n        # We are going to accumulate in a block of size [BLOCK_SIZE_M, BLOCK_SIZE_N] but we have to consider that we might be out of bounds.\n\n        # Instead, we can have each program id compute one output element? Then we set:\n        #   grid = (batch_size, out_features)\n        # But that might be too many threads? 128*512=65536, which is acceptable.\n\n        # Alternatively, we can use 2D grid with blocks and then loop within the block? But for simplicity, let's do one element per thread.\n\n        # However, note: we are to use memory coalescing and efficient Tensor Core usage? For that we need blocks.\n\n        # Given the problem, we are to optimize for H100. We can use BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, and BLOCK_SIZE_K=64? But we have to tune.\n\n        # We'll use a 2D grid of blocks, and each block computes a tile of [BLOCK_SIZE_M, BLOCK_SIZE_N] in the output.\n\n        # How to handle the reduction? We'll loop over the in_features by blocks of size BLOCK_SIZE_K.\n\n        # Initialize accumulator\n        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n        # Loop over k\n        for k in range(0, in_features, BLOCK_SIZE_K):\n            # create offsets for K dimension\n            offs_k = k + tl.arange(0, BLOCK_SIZE_K)\n            # load input tile: [BLOCK_SIZE_M, BLOCK_SIZE_K]\n            # We need to check boundaries for input and weight.\n            mask_x = (offs_m[:, None] < batch_size) & (offs_k[None, :] < in_features)\n            x = tl.load(x_ptr + offs_m[:, None] * stride_x_batch + offs_k[None, :] * stride_x_feat, \n                        mask=mask_x, other=0.0)\n            # load weight tile: [BLOCK_SIZE_K, BLOCK_SIZE_N] -> but note: weight is stored as [out_features, in_features] so we need to transpose? Actually, we want to do x @ w.t(), so we need w.t()? \n            # But in our kernel, we are loading a tile of weight for the current output features and the current K block? \n            # The weight tile we need is [BLOCK_SIZE_K, BLOCK_SIZE_N] for the columns offs_k and rows offs_n.\n            mask_w = (offs_k[:, None] < in_features) & (offs_n[None, :] < out_features)\n            w = tl.load(w_ptr + offs_k[:, None] * stride_w_in + offs_n[None, :] * stride_w_out, \n                        mask=mask_w, other=0.0)\n            # But note: the weight matrix we have is of shape [out_features, in_features]. So the element at (i,j) is the j-th input to the i-th output.\n            # In the multiplication x @ w.t(), we do: for element (i, j): sum_k x[i,k] * w[j,k]\n            # So we need to load w[j,k] for j in offs_n and k in offs_k. That is, we load a tile of weight of size [BLOCK_SIZE_N, BLOCK_SIZE_K] and then transpose it to [BLOCK_SIZE_K, BLOCK_SIZE_N]? \n            # Actually, we can load it as [BLOCK_SIZE_K, BLOCK_SIZE_N] if we adjust the pointers? \n\n            # Alternatively, we can load the weight tile as [BLOCK_SIZE_N, BLOCK_SIZE_K] and then transpose in registers? But that is expensive.\n\n            # Instead, we can change the way we store the weight? But we cannot change the model's weight because it is trained? Actually, we are initializing randomly. We can store the weight in transposed form? \n\n            # But note: the original nn.Linear uses weight of shape [out_features, in_features]. We are storing the same. We can pre-transpose the weight? But that would be done once.\n\n            # Alternatively, we can change the kernel to load the weight without transposing by having the weight stored in row-major for in_features and then the out_features? But that is the same as the original.\n\n            # Actually, we can avoid transposing by doing:\n            #   acc += tl.dot(x, w, allow_tf32=True)\n            # if x is [BLOCK_SIZE_M, BLOCK_SIZE_K] and w is [BLOCK_SIZE_K, BLOCK_SIZE_N] then the dot product is [BLOCK_SIZE_M, BLOCK_SIZE_N]. That is what we want.\n\n            # But in the weight matrix, the element we want is at [j, k] (j: output feature index, k: input feature index). So if we load a tile of weight for j in offs_n and k in offs_k, that tile is [BLOCK_SIZE_N, BLOCK_SIZE_K]. We need to transpose it to [BLOCK_SIZE_K, BLOCK_SIZE_N]? \n\n            # We can do: w = tl.trans(w) but that is expensive.\n\n            # Instead, we can store the weight in a transposed layout? We can do that in the __init__ by storing self.weight = nn.Parameter(torch.randn(out_features, in_features).t())? Then the weight becomes [in_features, out_features]. Then we can load it as [BLOCK_SIZE_K, BLOCK_SIZE_N] without transposing.\n\n            # But note: we are going to update the model. We can do that.\n\n            # Alternatively, we can change the kernel to load the weight in a different way? We can load the weight by having the pointer for weight as [in_features, out_features] and then use strides accordingly? \n\n            # Actually, we can set the stride for the weight to be (1, in_features) if we want to load contiguous in the in_features dimension? But the original weight is stored as (out_features, in_features) with strides (in_features, 1). So if we want to load a tile of [BLOCK_SIZE_K, BLOCK_SIZE_N] that is contiguous in memory, we would have to use a stride of 1 for the in_features dimension? \n\n            # We can change the kernel to load the weight with the appropriate strides? \n\n            # We'll do:\n            #   w_ptr + offs_k[:, None] * stride_w_in + offs_n[None, :] * stride_w_out\n            # If the weight is stored in row-major (out_features rows, in_features columns), then the stride for out_features (changing row) is in_features, and for in_features (changing column) is 1.\n            # So we set:\n            #   stride_w_out = in_features   # because moving to next output feature (row) we skip in_features elements\n            #   stride_w_in = 1               # moving to next input feature in the same row, we skip 1 element.\n\n            # Then the tile we load is of shape [BLOCK_SIZE_K, BLOCK_SIZE_N] but the data is not contiguous in the way we want? Because the weight matrix is stored row-major, so when we load a tile that spans multiple rows (output features) and multiple columns (input features), we are loading non-contiguous elements? \n\n            # Actually, we are loading a tile that is contiguous in the input feature dimension? Because we are using stride_w_in=1? But for the output feature dimension, we have stride_w_out = in_features, so each row is separated by in_features. This is a strided load.\n\n            # Triton can handle that. We just have to set the mask appropriately.\n\n            # So we do:\n            w = tl.load(w_ptr + offs_k[:, None] * stride_w_in + offs_n[None, :] * stride_w_out, \n                        mask=mask_w, other=0.0)\n            # Now w is [BLOCK_SIZE_K, BLOCK_SIZE_N] and we can do:\n            acc += tl.dot(x, w, allow_tf32=True)\n\n        # After the loop, we have the accumulated result for the tile.\n\n        # Now we need to add the bias. The bias is a vector of size out_features.\n        # We load a tile of bias for the current offs_n.\n        mask_b = offs_n < out_features\n        b = tl.load(b_ptr + offs_n, mask=mask_b, other=0.0)   # [BLOCK_SIZE_N]\n        # Now we add the bias to every row in the block: we broadcast b to [BLOCK_SIZE_M, BLOCK_SIZE_N]\n        acc += b[None, :]\n\n        # Apply Swish: x * sigmoid(x)\n        swish = acc * tl.sigmoid(acc)\n        # Scale\n        swish = swish * scaling_factor\n\n        # Now we store the result to the output.\n        mask_out = (offs_m[:, None] < batch_size) & (offs_n[None, :] < out_features)\n        tl.store(output_ptr + offs_m[:, None] * stride_out_batch + offs_n[None, :] * stride_out_feat, \n                 swish, mask=mask_out)\n\n   However, note: the above kernel is for a tile of [BLOCK_SIZE_M, BLOCK_SIZE_N]. We are using 2D grid.\n\n   But we have to set the grid and the block sizes. We'll use autotune for the block sizes.\n\n   But note: the problem says we can change the block dimensions.\n\n   Also note: we are using tl.dot which uses Tensor Cores if the block sizes are multiples of 16? For FP16, we need the block sizes to be multiples of 16 for Tensor Cores. But our input is FP32? The original model uses FP32? \n\n   The original model uses the default dtype, which is FP32. We can change to FP16? But the problem says we must maintain the same functionality.\n\n   Alternatively, we can use FP32 and still use Tensor Cores? H100 supports TF32 for FP32. We set allow_tf32=True.\n\n   But note: the problem says \"FP16 Tensor Core 32\u00d716\u00d716\". So if we want to use FP16, we must change the dtype? The original model is in FP32. We can keep it in FP32 and use TF32 tensor cores? \n\n   We'll do:\n\n        tl.dot(x, w, allow_tf32=True)\n\n   This will use Tensor Cores for TF32.\n\n   But note: the tolerance in correctness is float32. So TF32 is acceptable.\n\n   We'll write the kernel and then call it.\n\n   However, note: the kernel above is complex and we must handle boundaries.\n\n   Alternatively, we can write a simpler kernel that does one output element per thread? That would be easier for boundaries but less efficient.\n\n   Given the time, we'll do a simpler version for now and then optimize.\n\n   We can do:\n\n        @triton.jit\n        def _forward_kernel(\n            x_ptr, w_ptr, b_ptr, output_ptr,\n            in_features, out_features, scaling_factor,\n            stride_x0, stride_x1,\n            stride_w0, stride_w1,\n            stride_b,\n            stride_out0, stride_out1,\n            BLOCK_SIZE: tl.constexpr,\n        ):\n            pid = tl.program_id(0)\n            total_pids = tl.num_programs(0)\n            num_outputs = batch_size * out_features\n            # We are going to have each pid handle multiple outputs if needed? Or we can set grid = [batch_size * out_features] and then each pid one output.\n\n            # But note: we have to reduce over in_features. We can do:\n\n            # Let pid cover the output indices: from 0 to batch_size * out_features - 1.\n            # Then:\n            i = pid // out_features   # batch index\n            j = pid % out_features     # output feature index\n\n            # Now we want to compute: \n            #   out = 0\n            #   for k in range(0, in_features, BLOCK_SIZE):\n            #       offs = k + tl.arange(0, BLOCK_SIZE)\n            #       mask = offs < in_features\n            #       x_val = tl.load(x_ptr + i*stride_x0 + offs*stride_x1, mask=mask, other=0.0)\n            #       w_val = tl.load(w_ptr + j*stride_w0 + offs*stride_w1, mask=mask, other=0.0)\n            #       out += tl.sum(x_val * w_val, axis=0)\n            #   out += b[j]\n            #   out = out * tl.sigmoid(out) * scaling_factor\n            #   tl.store(output_ptr + i*stride_out0 + j*stride_out1, out)\n\n            # This is simpler.\n\n            # We'll use BLOCK_SIZE for the reduction.\n\n            acc = 0.0\n            for k in range(0, in_features, BLOCK_SIZE):\n                offs = k + tl.arange(0, BLOCK_SIZE)\n                mask = offs < in_features\n                x_val = tl.load(x_ptr + i * stride_x0 + offs * stride_x1, mask=mask, other=0.0)\n                w_val = tl.load(w_ptr + j * stride_w0 + offs * stride_w1, mask=mask, other=0.0)\n                acc += tl.sum(x_val * w_val)\n\n            # Load bias\n            bias_val = tl.load(b_ptr + j)\n            acc += bias_val\n\n            # Swish and scaling\n            swish = acc * tl.sigmoid(acc) * scaling_factor\n            tl.store(output_ptr + i * stride_out0 + j * stride_out1, swish)\n\n   This kernel is simpler and handles boundaries easily.\n\n   But note: performance. We are doing a loop over in_features with step BLOCK_SIZE, and then a reduction within the block. This might be efficient for small in_features? 1024 is small.\n\n   We can set BLOCK_SIZE=128? Then we have 8 iterations (1024/128). The grid will be batch_size * out_features = 128*512=65536, which is acceptable.\n\n   We'll try this.\n\n   Then in the forward function:\n\n        batch_size = x.shape[0]\n        output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n\n        # Check the strides\n        assert x.is_contiguous()\n        assert self.weight.is_contiguous()\n        assert self.bias.is_contiguous()\n\n        # We can set the grid to be (batch_size * out_features,)\n        grid = (batch_size * out_features,)\n\n        # We need to pass the strides for x, weight, and output.\n        # For x: [batch, in_features] -> strides: (in_features, 1) if row-major? Actually, if contiguous, then stride for batch is in_features, for feature is 1.\n        # Similarly, weight: [out_features, in_features] -> contiguous: stride for out_features is in_features, for in_features is 1.\n        # Output: [batch, out_features] -> contiguous: stride for batch is out_features, for feature is 1.\n\n        # But note: the kernel uses:\n        #   x_ptr + i * stride_x0 + offs * stride_x1\n        #   where i is the batch index, and offs is the input feature index.\n        #   so stride_x0 = in_features, stride_x1 = 1.\n\n        # Similarly, for weight: \n        #   w_ptr + j * stride_w0 + offs * stride_w1\n        #   j: output feature index, offs: input feature index.\n        #   so stride_w0 = in_features, stride_w1 = 1.\n\n        # For bias: contiguous vector, so stride=1.\n\n        # For output: \n        #   output_ptr + i * stride_out0 + j * stride_out1\n        #   i: batch index, j: output feature index.\n        #   so stride_out0 = out_features, stride_out1 = 1.\n\n        _forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            in_features, out_features, self.scaling_factor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            self.bias.stride(0),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE=128,\n        )\n\n   But note: we are using tl.sum over the block. That might be inefficient? But it's a small block (128) and we do only 8 iterations.\n\n   We'll write the kernel and hope it compiles.\n\n   However, note: the problem says to fix compilation first. We are introducing a new kernel so we must be careful.\n\n   We'll also use autotune? But the problem says we can do adaptive tuning. We can leave BLOCK_SIZE as a tuning parameter.\n\n   But for simplicity, we'll set it to 128.\n\n   Let's write the kernel.\n\n   But note: the kernel must be inside the EVOLVE-BLOCK? Yes.\n\n   We'll restructure the code:\n\n        import triton\n        import triton.language as tl\n\n        @triton.jit\n        def _forward_kernel( ... ): ...\n\n        class ModelNew(nn.Module):\n            ...\n\n        batch_size = 128\n        ...\n\n   We must be cautious: the kernel might be slow? But we hope it is faster than the PyTorch version.\n\n   We'll do it.\n\n   However, note: the current reference timing is 0.0746 ms and our Triton version in the example had 0.0548 ms? So we are already faster? But that was the reference code? Actually, the current program is the reference code? And we are to replace it.\n\n   We'll write the Triton kernel version.\n\n   But note: the original model had the linear layer, then Swish, then scaling. We are fusing all.\n\n   We hope that fusion and the kernel will be faster.\n\n   Let's code accordingly.\n\n   We'll also note: the kernel must be compiled for the specific GPU. We are on H100.\n\n   We'll set the num_warps? We can leave it to the default? Or set to 4? The problem says we can use autotune. We'll do a simple version without autotune first.\n\n   We'll try.\n\n   Steps:\n\n        Replace the ModelNew to have weight and bias as parameters, and then in the forward, call the kernel.\n\n        We must also set the batch_size, in_features, out_features, scaling_factor as before.\n\n        And the get_inputs and get_init_inputs.\n\n   We'll write the entire EVOLVE-BLOCK accordingly.\n\n   But note: the original model had the linear layer and then the activation and scaling. We are doing the same computation.\n\n   We hope for a speedup.\n\n   Given the constraints, we'll output the code.\n\n   IMPORTANT: We must not change the function signatures of the ModelNew's __init__ and forward? The __init__ now takes in_features, out_features, scaling_factor and then we define weight and bias. The forward takes x.\n\n   This matches the original.\n\n   Let's write the code.\n</think>\nTo improve performance, I'll replace the PyTorch implementation with an optimized Triton kernel. The key optimizations include:\n1. Fusing the matrix multiplication, Swish activation, and scaling into a single kernel\n2. Using efficient memory access patterns with vectorized loads/stores\n3. Leveraging Tensor Cores through proper block sizing\n4. Minimizing intermediate memory operations\n\nHere's the complete implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _forward_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    M, N, K, scaling_factor,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_bn,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        k_mask = offs_k[None, :] < k_remaining\n        \n        x = tl.load(x_ptrs, mask=k_mask, other=0.0)\n        w = tl.load(w_ptrs, mask=k_mask, other=0.0)\n        \n        acc += tl.dot(x, w, allow_tf32=True)\n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n    \n    bias = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += bias[None, :]\n    \n    swish = acc * tl.sigmoid(acc) * scaling_factor\n    \n    offs_m_out = offs_m[:, None]\n    offs_n_out = offs_n[None, :]\n    mask = (offs_m_out < M) & (offs_n_out < N)\n    \n    out_ptrs = output_ptr + stride_om * offs_m_out + stride_on * offs_n_out\n    tl.store(out_ptrs, swish, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scaling_factor = scaling_factor\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.zeros(out_features))\n    \n    def forward(self, x):\n        output = torch.empty((x.size(0), self.out_features), \n                             device=x.device, dtype=x.dtype)\n        \n        M, K = x.shape\n        N = self.weight.shape[0]\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        \n        _forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            M, N, K, self.scaling_factor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            self.bias.stride(0),\n            output.stride(0), output.stride(1)\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "d8927c9c-951b-49ef-8eb3-0bad69117e95", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, \n    v_ptr, \n    output_ptr, \n    scaling_factor,\n    input_size,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)  # row index\n    acc = 0.0\n\n    # Process input vector in blocks\n    for offset in range(0, input_size, BLOCK_SIZE):\n        idxs = offset + tl.arange(0, BLOCK_SIZE)\n        mask = idxs < input_size\n        \n        # Coalesced memory loads\n        x_vals = tl.load(x_ptr + pid * input_size + idxs, mask=mask, other=0.0)\n        v_vals = tl.load(v_ptr + idxs, mask=mask, other=0.0)\n        \n        # Fused multiply-add\n        acc += tl.sum(x_vals * v_vals)\n\n    # Apply scaling and division\n    acc = acc / 2.0 * scaling_factor\n    tl.store(output_ptr + pid, acc)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.scaling_factor = scaling_factor\n        self.input_size = input_size\n\n    def forward(self, x):\n        v = self.weight.sum(dim=0)  # Precomputed vector\n        output = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n        \n        # Launch Triton kernel\n        BLOCK_SIZE = 1024\n        grid = (x.shape[0],)\n        _forward_kernel[grid](\n            x, v, output, \n            self.scaling_factor, \n            self.input_size, \n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output.unsqueeze(1)\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\nscaling_factor = 1.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "fc5c33d6-d68a-4228-a84a-3c7c947bad52", "generation": 2, "timestamp": 1754566362.7327437, "iteration_found": 5, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.903, "runtime_ms": 0.0616, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0556, "speed_up": 0.903, "custom_timing": 0.0616}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 0.0000\n- correctness: 0.0000\n- score: -100.0000\n- stage: no_valid_triton_code\n- feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 0.0000, correctness: 0.0000, score: -100.0000, stage: no_valid_triton_code, feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: -100.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that fuses GEMM, division, summation, and scaling.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, 1).\n        \"\"\"\n        # Precompute weight sum across hidden dimension\n        w_sum = self.weight.sum(dim=0)  # Reduces to [input_size]\n        \n        # Fused: (x @ w_sum) replaces (x @ W.T).sum(dim=1)\n        s = (x @ w_sum) / 2 * self.scaling_factor\n        return s.unsqueeze(1)  # Maintain output shape [batch_size, 1]\n\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\nscaling_factor = 1.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (0.0000), Performs well on correctness (0.0000), Performs well on score (-100.0000), Performs well on stage (no_valid_triton_code), Performs well on feedback (You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.)\n\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that fuses GEMM, division, summation, and scaling.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, 1).\n        \"\"\"\n        # Precompute weight sum across hidden dimension\n        w_sum = self.weight.sum(dim=0)  # Reduces to [input_size]\n        \n        # Fused: (x @ w_sum) replaces (x @ W.T).sum(dim=1)\n        s = (x @ w_sum) / 2 * self.scaling_factor\n        return s.unsqueeze(1)  # Maintain output shape [batch_size, 1]\n\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\nscaling_factor = 1.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a task to rewrite the provided PyTorch code using Triton for performance.\n The current code is a PyTorch model that does:\n   s = (x @ w_sum) / 2 * scaling_factor\n where w_sum = self.weight.sum(dim=0)\n\n However, the feedback says: \"You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\"\n\n Therefore, we must replace the current PyTorch implementation with a Triton kernel.\n\n Steps:\n 1. We note that the current operation is:\n      output = (x @ (weight.sum(dim=0))) / 2 * scaling_factor\n    This can be broken down:\n      Let v = weight.sum(dim=0)  [input_size]\n      Then for each row in x (of shape [batch_size, input_size]), we compute the dot product with v.\n\n 2. We can write a Triton kernel that does:\n      For each row i in the batch:\n          s_i = 0\n          for j in range(input_size):\n              s_i += x[i, j] * v[j]\n          s_i = s_i / 2 * scaling_factor\n\n 3. We will write a kernel that processes one row per program instance (or multiple rows if we use blocking?).\n    Since the vector v is of size `input_size`, and `input_size` might be large, we can use a blocked approach.\n\n 4. We design:\n      - The kernel will have one program per row (so grid = (batch_size,)).\n      - Each program will iterate over the vector v in blocks, accumulating the dot product in a local variable.\n\n 5. However, note that the vector v is the same for all rows. So we can load it once per block? But note: each row of x is independent.\n\n 6. We can do:\n      pid = tl.program_id(0)  # row index\n      We will have a block size (BLOCK_SIZE) for the inner dimension (input_size).\n\n      Initialize accumulator.\n      For offset in range(0, input_size, BLOCK_SIZE):\n          Load a block of x: [pid, offset:offset+BLOCK_SIZE]\n          Load a block of v: [offset:offset+BLOCK_SIZE]\n          Multiply and accumulate.\n\n      Then do: accumulator = accumulator / 2 * scaling_factor\n      Then store the result at output[pid].\n\n 7. But note: the current output is of shape [batch_size, 1]. So we will output a 1D tensor of length batch_size, then unsqueeze.\n\n 8. We must be cautious of the boundaries.\n\n 9. We are going to replace the forward function with a Triton kernel.\n\n 10. We must also note that the current code uses `get_inputs` and `get_init_inputs` for testing. We must preserve the interface.\n\n Implementation:\n\n    We will write a Triton kernel function and call it in the forward.\n\n    Steps in the new ModelNew.forward:\n        - Ensure x is contiguous.\n        - Create an output tensor of shape [batch_size] (then we'll unsqueeze to [batch_size,1]).\n        - Define the grid (batch_size).\n        - Launch the kernel.\n\n    The kernel:\n\n        @triton.jit\n        def _forward_kernel(\n            x_ptr, \n            v_ptr, \n            output_ptr, \n            scaling_factor,\n            input_size,\n            BLOCK_SIZE: tl.constexpr\n        ):\n            pid = tl.program_id(0)  # row index\n\n            # We'll accumulate in a variable\n            acc = 0.0\n\n            # Loop over blocks\n            for offset in range(0, input_size, BLOCK_SIZE):\n                # Create an offset for the current block\n                idxs = offset + tl.arange(0, BLOCK_SIZE)\n                mask = idxs < input_size\n\n                # Load a block of x\n                x_vals = tl.load(x_ptr + pid * input_size + idxs, mask=mask, other=0.0)\n                # Load a block of v\n                v_vals = tl.load(v_ptr + idxs, mask=mask, other=0.0)\n\n                # Accumulate\n                acc += tl.sum(x_vals * v_vals)\n\n            # Now, acc = x[pid] @ v\n            acc = acc / 2.0 * scaling_factor\n\n            # Store the result\n            tl.store(output_ptr + pid, acc)\n\n    Then in forward:\n        v = self.weight.sum(dim=0)   # This is precomputed and is on the same device as x\n        output = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n        assert x.is_contiguous()\n        assert v.is_contiguous()\n\n        # We choose a block size (e.g., 1024). We can also make it configurable.\n        BLOCK_SIZE = 1024\n        grid = (x.shape[0],)\n        _forward_kernel[grid](x, v, output, scaling_factor=self.scaling_factor, input_size=input_size, BLOCK_SIZE=BLOCK_SIZE)\n\n        return output.unsqueeze(1)\n\n 11. However, note: the input_size in the model is fixed at initialization. We can pass it as a constant.\n\n 12. But wait: the model is initialized with `input_size, hidden_size, scaling_factor`. We can store input_size as an attribute.\n\n 13. We change the __init__ to store input_size.\n\n 14. Also note: the current code has a fixed batch_size in the example? Actually, the batch_size is variable (x.shape[0]). So we must handle any batch.\n\n 15. We are going to do:\n\n        class ModelNew(nn.Module):\n            def __init__(self, input_size, hidden_size, scaling_factor):\n                super(ModelNew, self).__init__()\n                self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n                self.scaling_factor = scaling_factor\n                self.input_size = input_size   # store for use in kernel\n\n 16. Then in forward, we can use self.input_size.\n\n 17. Also, we must import triton and set up the kernel.\n\n 18. We must be cautious: the kernel uses tl.load and tl.store. We assume x and v are in the same memory (both float32?).\n\n 19. The current x is given as input, and v is computed from the weight (which is float32). So they are float32.\n\n 20. We must also note that the kernel uses a loop. We can tune BLOCK_SIZE? But for simplicity we use 1024.\n\n 21. We are going to write the kernel and the class.\n\n 22. However, note that the feedback says: \"The key word 'import triton' is not found.\" So we must import triton.\n\n 23. We'll add:\n        import triton\n        import triton.language as tl\n\n 24. We are going to write the entire code block with the EVOLVE-BLOCK.\n\n 25. We note that the current code block also defines batch_size, input_size, hidden_size, scaling_factor, get_inputs, get_init_inputs. We must keep these.\n\n 26. We rewrite the ModelNew class and the kernel.\n\n 27. We remove the existing forward and replace it with the Triton kernel call.\n\n 28. We also note that the current code precomputes w_sum. We can precompute it in the forward (as before) and then pass it to the kernel.\n\n 29. We do not change the parameters.\n\n 30. We write the code accordingly.\n\n 31. Important: We must not change the function signatures of get_inputs and get_init_inputs.\n\n 32. We write:\n\n        import math\n        import torch\n        import torch.nn as nn\n        import triton\n        import triton.language as tl\n\n        @triton.jit\n        def _forward_kernel(\n            x_ptr, \n            v_ptr, \n            output_ptr, \n            scaling_factor,\n            input_size,\n            BLOCK_SIZE: tl.constexpr\n        ):\n            pid = tl.program_id(0)  # row index\n\n            # We'll accumulate in a variable\n            acc = 0.0\n\n            # Loop over blocks\n            for offset in range(0, input_size, BLOCK_SIZE):\n                # Create an offset for the current block\n                idxs = offset + tl.arange(0, BLOCK_SIZE)\n                mask = idxs < input_size\n\n                # Load a block of x\n                x_vals = tl.load(x_ptr + pid * input_size + idxs, mask=mask, other=0.0)\n                # Load a block of v\n                v_vals = tl.load(v_ptr + idxs, mask=mask, other=0.0)\n\n                # Accumulate\n                acc += tl.sum(x_vals * v_vals)\n\n            # Now, acc = x[pid] @ v\n            acc = acc / 2.0 * scaling_factor\n\n            # Store the result\n            tl.store(output_ptr + pid, acc)\n\n        class ModelNew(nn.Module):\n            def __init__(self, input_size, hidden_size, scaling_factor):\n                super(ModelNew, self).__init__()\n                self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n                self.scaling_factor = scaling_factor\n                self.input_size = input_size\n\n            def forward(self, x):\n                # Precompute the vector v = sum(weight, dim=0)\n                v = self.weight.sum(dim=0)   # [input_size]\n\n                # Create an output tensor for the results (per row)\n                output = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n\n                # Check contiguity\n                assert x.is_contiguous()\n                assert v.is_contiguous()\n\n                # Launch kernel\n                BLOCK_SIZE = 1024\n                grid = (x.shape[0],)\n                _forward_kernel[grid](\n                    x, v, output, \n                    scaling_factor=self.scaling_factor, \n                    input_size=self.input_size, \n                    BLOCK_SIZE=BLOCK_SIZE\n                )\n\n                return output.unsqueeze(1)\n\n        # The rest of the code (batch_size, input_size, hidden_size, scaling_factor, get_inputs, get_init_inputs) remains.\n\n 33. However, note: the kernel uses a fixed BLOCK_SIZE. We can make it a constant or try to tune it? But for simplicity, we use 1024.\n\n 34. We note that the kernel must be compiled. We hope that with this, we pass the compilation.\n\n 35. We also note that the kernel does a dot product and then scaling. We hope it is numerically equivalent.\n\n 36. We are ready to output.\n\n 37. IMPORTANT: We output the entire code block as required, with the EVOLVE-BLOCK markers.\n\n 38. We note that the current code block includes the example parameters and functions. We keep them.\n\n 39. We write the code accordingly.\n\n 40. One more note: the kernel must be in the same file and visible. We define it at the top level.\n\n 41. We also note that the kernel uses `tl.sum` which is a reduction within a block. Then we do a loop over blocks. This is correct.\n\n 42. We output the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, \n    v_ptr, \n    output_ptr, \n    scaling_factor,\n    input_size,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)  # row index\n    acc = 0.0\n\n    # Process input vector in blocks\n    for offset in range(0, input_size, BLOCK_SIZE):\n        idxs = offset + tl.arange(0, BLOCK_SIZE)\n        mask = idxs < input_size\n        \n        # Coalesced memory loads\n        x_vals = tl.load(x_ptr + pid * input_size + idxs, mask=mask, other=0.0)\n        v_vals = tl.load(v_ptr + idxs, mask=mask, other=0.0)\n        \n        # Fused multiply-add\n        acc += tl.sum(x_vals * v_vals)\n\n    # Apply scaling and division\n    acc = acc / 2.0 * scaling_factor\n    tl.store(output_ptr + pid, acc)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.scaling_factor = scaling_factor\n        self.input_size = input_size\n\n    def forward(self, x):\n        v = self.weight.sum(dim=0)  # Precomputed vector\n        output = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n        \n        # Launch Triton kernel\n        BLOCK_SIZE = 1024\n        grid = (x.shape[0],)\n        _forward_kernel[grid](\n            x, v, output, \n            self.scaling_factor, \n            self.input_size, \n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output.unsqueeze(1)\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\nscaling_factor = 1.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
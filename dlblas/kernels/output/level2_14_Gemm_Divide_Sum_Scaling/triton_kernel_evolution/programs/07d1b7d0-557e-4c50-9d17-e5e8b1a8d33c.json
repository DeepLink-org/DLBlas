{"id": "07d1b7d0-557e-4c50-9d17-e5e8b1a8d33c", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_kernel(\n    x_ptr,\n    weight_vector_ptr,\n    output_ptr,\n    scaling_factor,\n    input_size,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    total = 0.0\n    \n    for i in range(0, input_size, BLOCK_SIZE):\n        offs = i + tl.arange(0, BLOCK_SIZE)\n        mask = offs < input_size\n        x_val = tl.load(x_ptr + pid * input_size + offs, mask=mask)\n        w_val = tl.load(weight_vector_ptr + offs, mask=mask)\n        total += tl.sum(x_val * w_val, axis=0)\n    \n    output = total * (scaling_factor / 2.0)\n    tl.store(output_ptr + pid, output)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that mathematically reduces operations by leveraging:\n    (x @ weight.T).sum(dim=1) = x @ (weight.sum(dim=0))\n    This avoids full GEMM and explicit reduction steps.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, 1).\n        \"\"\"\n        # Precompute weight vector\n        weight_vector = self.weight.sum(dim=0).contiguous()\n        x = x.contiguous()\n        batch_size = x.size(0)\n        output = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        \n        # Configure and launch kernel\n        grid = (batch_size,)\n        _fused_kernel[grid](\n            x, \n            weight_vector, \n            output, \n            self.scaling_factor,\n            x.size(1),\n            BLOCK_SIZE=1024\n        )\n        return output.unsqueeze(1)\n\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\nscaling_factor = 1.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "a143df20-cb17-4cd9-91d8-966dadfffe67", "generation": 2, "timestamp": 1754567990.999576, "iteration_found": 11, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.906, "runtime_ms": 0.0614, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0556, "speed_up": 0.906, "custom_timing": 0.0614}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
{"id": "1354ba37-0cb3-41f0-b056-5b2ca334e9e2", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_kernel(\n    x_ptr,\n    w_sum_ptr,  # Precomputed weight sum vector\n    output_ptr,\n    scaling_factor,\n    input_size,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    start_idx = pid * input_size\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < input_size\n    \n    # Vectorized load of input row and weight vector\n    x_vals = tl.load(x_ptr + start_idx + offsets, mask=mask, other=0.0)\n    w_vals = tl.load(w_sum_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute dot product with parallel reduction\n    dot = tl.sum(x_vals * w_vals)\n    result = dot * (scaling_factor / 2.0)\n    tl.store(output_ptr + pid, result)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = x.contiguous()\n        # Precompute weight sum vector\n        w_sum = self.weight.sum(dim=0).contiguous()\n        batch_size = x.size(0)\n        output = torch.empty(batch_size, dtype=torch.float32, device=x.device)\n        \n        grid = (batch_size,)\n        block_size = triton.next_power_of_2(x.size(1))\n        _fused_kernel[grid](\n            x, w_sum, output, self.scaling_factor, \n            x.size(1), \n            BLOCK_SIZE=block_size\n        )\n        return output.unsqueeze(1)\n\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\nscaling_factor = 1.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "4044daaf-2e76-44da-8a2b-17278a82249c", "generation": 2, "timestamp": 1754568475.5677595, "iteration_found": 17, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.901, "runtime_ms": 0.0617, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0556, "speed_up": 0.901, "custom_timing": 0.0617}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 26.299999999999994, "speed_up": 1.063, "runtime_ms": 0.0523, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0556, "speed_up": 1.063, "custom_timing": 0.0523}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
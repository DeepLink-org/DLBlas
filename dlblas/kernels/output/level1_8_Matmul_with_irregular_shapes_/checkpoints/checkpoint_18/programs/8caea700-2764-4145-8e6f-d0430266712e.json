{"id": "8caea700-2764-4145-8e6f-d0430266712e", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32}, num_warps=8, num_stages=3),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    offs_m = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid1 * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        mask_a = (offs_m[:, None] < M) & (offs_k[None, :] < k_remaining)\n        mask_b = (offs_k[:, None] < k_remaining) & (offs_n[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n        \n        acc += tl.dot(a, b, allow_tf32=True)\n        \n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    offs_cm = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid1 * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    \n    mask_c = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask_c)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        A = A.contiguous()\n        B = B.contiguous()\n        M, K = A.shape\n        K, N = B.shape\n        C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n        \n        grid = lambda META: (\n            triton.cdiv(M, META['BLOCK_M']),\n            triton.cdiv(N, META['BLOCK_N']),\n        )\n        \n        matmul_kernel[grid](\n            A, B, C,\n            M, N, K,\n            A.stride(0), A.stride(1),\n            B.stride(0), B.stride(1),\n            C.stride(0), C.stride(1)\n        )\n        \n        return C\n\nM = 8205\nK = 2949\nN = 5921\n\ndef get_inputs():\n    A = torch.randn(M, K, device='cuda')\n    B = torch.randn(K, N, device='cuda')\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "1843b84e-b313-4ec4-9660-9859483e4926", "generation": 1, "timestamp": 1754647436.923711, "iteration_found": 1, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_8_Matmul_with_irregular_shapes_", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "max_difference": ["0.256012", "0.247345", "0.240692", "0.241516", "0.237244"], "avg_difference": ["0.033618", "0.033604", "0.033599", "0.033628", "0.033622"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0214, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 6.41, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0214, "speed_up": 0.003, "custom_timing": 6.41}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 6.4100\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0214, 'speed_up': 0.003, 'custom_timing': 6.41}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 6.4100, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0214, 'speed_up': 0.003, 'custom_timing': 6.41}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with irregular shapes\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor with shape (M, K).\n            B: Input tensor with shape (K, N).\n\n        Returns:\n            C: Output tensor with shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 8205\nK = 2949\nN = 5921\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (6.4100), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0214, 'speed_up': 0.003, 'custom_timing': 6.41})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with irregular shapes\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor with shape (M, K).\n            B: Input tensor with shape (K, N).\n\n        Returns:\n            C: Output tensor with shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 8205\nK = 2949\nN = 5921\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does a simple matrix multiplication (C = A * B) with large irregular shapes (M=8205, K=2949, N=5921).\n The current implementation uses `torch.matmul` which is efficient but we are going to replace it with a Triton kernel for potential speedup.\n\n Steps:\n 1. We will write a Triton kernel for matrix multiplication. We note that the problem size is large and irregular, so we need to choose block sizes (BLOCK_M, BLOCK_N, BLOCK_K) appropriately.\n 2. We will use the standard technique of tiling the input matrices and accumulating the result in local memory (using shared memory via `tl.dot` and tensor cores).\n 3. We must handle the irregular sizes by having boundary checks and masking.\n\n Important constraints:\n  - Do not change the function signature of the forward method or the class name (ModelNew).\n  - The inputs are A (M, K) and B (K, N), output is C (M, N).\n\n We are targeting the NVIDIA H100 (SM8.9) which has Tensor Cores. We will use FP16 for computation (if the inputs are FP32, we might consider converting to FP16 for Tensor Cores? But note: the problem does not specify dtype. We assume FP32 for now, but we can use tf32 for Tensor Cores?).\n\n However, note: the problem says the hardware has FP16 Tensor Core 32x16x16. So if we use FP16, we can get the best performance.\n\n But the input tensors are generated by `torch.randn` which by default are FP32. We have two options:\n   Option 1: Convert the inputs to FP16 and then do FP16 matrix multiplication. This might be acceptable if the application allows lower precision.\n   Option 2: Use the Tensor Cores in TF32 mode (which is for FP32). However, Triton currently does not have explicit support for tf32. We can use FP32 and hope that the compiler uses tf32? But note: Triton's `dot` operation for FP32 on Ampere+ uses tf32 by default.\n\n However, the problem states that the hardware has \"FP16 Tensor Core\", so we might get better performance with FP16.\n\n But the problem does not specify the required precision. Since the original code uses FP32, we must ensure that the output is numerically close enough.\n\n Let's design the kernel for FP32. We can use the following:\n\n   We'll write a kernel that uses FP32 accumulation. However, we can break the K dimension into chunks and use FP32 for accumulation.\n\n Alternatively, we can write two versions: one for FP16 and one for FP32? But the problem does not specify. We are allowed to change the internal implementation as long as the inputs and outputs are the same.\n\n However, note: the output must be the same as the reference. The reference is in FP32. If we do the computation in FP16, we might lose precision.\n\n So we have two choices:\n\n  1. Use FP32 computation without Tensor Cores -> slow but accurate.\n  2. Use FP16 with Tensor Cores and hope that the precision loss is acceptable? But the problem says \"within float32 tolerance\". That might be too strict for FP16.\n\n Alternatively, we can use the Tensor Cores in TF32 mode for FP32. Triton does support this by setting the environment variable? Or we can use the `allow_tf32` flag in the kernel? Actually, in Triton we can set the `fp32_enabled` and `fp32_fast_accum` options? But I don't see that in the current Triton language.\n\n Actually, in Triton the `dot` operation for FP32 inputs will automatically use TF32 on Ampere and later if the environment variable `TRITON_ENABLE_TF32` is set. But by default, it is set to 1? We cannot rely on that in the kernel code.\n\n However, the problem says we are running on H100, which supports TF32. We can write the kernel in FP32 and let the compiler use TF32 for the dot product.\n\n So we write the kernel in FP32 and hope that the Triton compiler uses TF32 for the dot operations.\n\n Steps for the kernel:\n\n  We will use a 2D grid: the grid will be (ceil(M / BLOCK_M), ceil(N / BLOCK_N)).\n  Each program will compute a block of size BLOCK_M x BLOCK_N of the output matrix.\n\n  We will split the K dimension into blocks of size BLOCK_K. Then we will load a block of A (BLOCK_M x BLOCK_K) and a block of B (BLOCK_K x BLOCK_N) and compute the dot product.\n\n  We need to handle boundaries: if the block goes beyond the matrix dimensions, we mask the loads.\n\n  We'll use:\n    pid_0 = program_id(0)  # for rows\n    pid_1 = program_id(1)  # for columns\n\n  We'll precompute pointers for A and B and the strides.\n\n  We'll accumulate in a local variable (a 2D array of zeros) of size (BLOCK_M, BLOCK_N).\n\n  Then for k in range(0, K, BLOCK_K):\n        load a tile from A: [BLOCK_M, BLOCK_K] at position [pid_0*BLOCK_M, k]\n        load a tile from B: [BLOCK_K, BLOCK_N] at position [k, pid_1*BLOCK_N]\n        Do a matrix multiplication of the two tiles (using tl.dot) and accumulate.\n\n  Then store the result.\n\n  We must choose BLOCK_M, BLOCK_N, BLOCK_K appropriately. We note that the Tensor Core requires specific shapes. For TF32, the recommended tile is 128x128x16? But we are constrained by the shared memory and registers.\n\n  Alternatively, we can use the `triton.ops.matmul` which is already optimized? But the problem requires a Triton kernel.\n\n  We are writing a custom kernel.\n\n  Let's set:\n      BLOCK_M = 128\n      BLOCK_N = 128\n      BLOCK_K = 32   (for TF32, the tensor core operation is 16x16x16, but we can use 32 for K dimension and split internally? The Triton dot operation will handle it.)\n\n  We note that the Triton `dot` operation can be used on two blocks of the appropriate shape. We will use:\n        a = tl.load(ptr_A, mask=mask_A)   # (BLOCK_M, BLOCK_K)\n        b = tl.load(ptr_B, mask=mask_B)   # (BLOCK_K, BLOCK_N) but note: we need to transpose B? Actually, we have stored B in row-major and we are loading a block. But for matrix multiplication, we need the second matrix to be transposed? Actually, no: the operation is A (M x K) * B (K x N) -> C (M x N). We can load B without transposing? Then the dot product is between A and B? But note: the dot product in Triton is:\n\n        tl.dot(a, b)   # expects a to be of shape (M, K) and b of shape (K, N) and produces (M, N)\n\n        So we don't need to transpose.\n\n  However, the memory layout: A is (M, K) and B is (K, N). We are loading a tile from A: [pid_0*BLOCK_M: pid_0*BLOCK_M+BLOCK_M, k: k+BLOCK_K] -> (BLOCK_M, BLOCK_K)\n        and from B: [k: k+BLOCK_K, pid_1*BLOCK_N: pid_1*BLOCK_N+BLOCK_N] -> (BLOCK_K, BLOCK_N). Then we can do tl.dot(a, b).\n\n  But note: the dot product requires that the inner dimension (K) is the same and contiguous? We are loading contiguous blocks? We are using row-major.\n\n  We need to set the strides appropriately.\n\n  Steps in the kernel:\n\n    pid_0 = tl.program_id(0)\n    pid_1 = tl.program_id(1)\n\n    # Create pointers for the first blocks of A and B.\n    offs_am = pid_0 * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_1 * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    # Loop over K\n    for k in range(0, K, BLOCK_K):\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n        # Load a and b with masking for boundaries in K and also in M and N? \n        # For the K dimension, we only need to check if k+BLOCK_K > K? We can mask the last block.\n        mask_a = (offs_am[:, None] < M) & (offs_k[None, :] < K - k)   # but note: we are iterating by BLOCK_K, so we can do:\n        mask_b = (offs_k[:, None] < K - k) & (offs_bn[None, :] < N)   # actually, for B we need to check the row index (k dimension) and column index (N)\n\n        # But note: the row index for A is offs_am, which we already bounded by M? Actually, we set offs_am to start at pid_0*BLOCK_M and we have BLOCK_M elements. We need to mask if offs_am is beyond M? Similarly for offs_bn beyond N? Also, for the current chunk in K, we need to mask if offs_k is beyond the remaining K (i.e., k_index < min(BLOCK_K, K - k)).\n\n        # Alternatively, we can precompute the masks for the entire tile? But note: the K dimension changes per iteration.\n\n        # Let's do:\n        #   mask_a = (offs_am[:, None] < M) & ((offs_k[None, :] + k) < K)\n        #   mask_b = ((offs_k[:, None] + k) < K) & (offs_bn[None, :] < N)\n\n        # But note: we are iterating k from 0 to K in steps of BLOCK_K. We can also compute:\n        #   k_remaining = K - k\n        #   and then mask the k dimension by: offs_k < k_remaining\n\n        # Actually, we can compute:\n        #   k_remaining = K - k\n        #   mask_k = offs_k < k_remaining   # (BLOCK_K,)\n        #   mask_a = (offs_am[:, None] < M) & mask_k[None, :]\n        #   mask_b = mask_k[:, None] & (offs_bn[None, :] < N)\n\n        a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n\n        acc += tl.dot(a, b)\n\n    # Now, write back the result\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n    mask_c = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask_c)\n\n  We also need to set the strides. We assume the tensors are contiguous.\n\n  We can compute the strides in the host code and pass them to the kernel.\n\n  We'll set:\n      stride_am = A.stride(0)   # typically K\n      stride_ak = A.stride(1)   # typically 1 if row-major? Actually, if A is (M, K) and contiguous, then stride(0)=K, stride(1)=1.\n      Similarly, B: (K, N) -> stride(0)=N, stride(1)=1? But wait: in row-major, the next row is N elements away? Actually, no: the next row is N? Actually, the stride for the first dimension (K) is the number of elements in a row? So for B, which is (K, N), the stride(0) is N, stride(1)=1.\n\n  However, we should get the strides from the tensors.\n\n  We'll pass:\n      stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn.\n\n  Note: for B, the first dimension is k (index 0) and the second is n (index 1). So:\n      stride_bk = B.stride(0)\n      stride_bn = B.stride(1)\n\n  Similarly for C.\n\n  We assume the tensors are contiguous. If not, we will have to use the strides.\n\n  But note: the problem says the inputs are from `get_inputs` which uses `torch.randn` and by default are contiguous? We can make sure by calling `.contiguous()` in the host function.\n\n  Now, in the host code:\n\n    We create the output tensor: C = torch.empty(M, N, device=A.device, dtype=A.dtype)\n\n    We compute the grid: \n        grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n\n    We set the kernel parameters: \n        [grid] kernel(A, B, C, M, N, K, \n                      stride_am, stride_ak, \n                      stride_bk, stride_bn,\n                      stride_cm, stride_cn,\n                      BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K)\n\n  We must choose BLOCK_M, BLOCK_N, BLOCK_K. We note that the shared memory required per block is:\n        BLOCK_M * BLOCK_K * size_of(float) + BLOCK_K * BLOCK_N * size_of(float) \n        = (BLOCK_M * BLOCK_K + BLOCK_K * BLOCK_N) * 4 bytes.\n\n  For BLOCK_M=128, BLOCK_N=128, BLOCK_K=32: \n        (128*32 + 32*128) * 4 = (4096+4096)*4 = 32768 bytes = 32KB per block.\n\n  The H100 has 228KB of shared memory per SM? But we are limited to 48KB per block? Actually, we can set the shared memory per block. But note: we are not using shared memory explicitly. The Triton compiler will use shared memory for the tiles? Actually, the `tl.dot` operation uses shared memory for the tiles? We don't have to manage it.\n\n  We will use the autotuner to try different block sizes and number of warps.\n\n  We'll write the kernel with autotuning.\n\n  However, note: the problem says to modify only the EVOLVE-BLOCK. We are going to replace the entire `forward` method with a call to our Triton kernel.\n\n  We must also note: the original model returns a tensor of shape (M, N). Our kernel does the same.\n\n  Let's write the kernel and the host function.\n\n  We'll create a new class `ModelNew` that uses the Triton kernel.\n\n  We must also handle the case when the inputs are not contiguous. We can make them contiguous in the host code.\n\n  Steps in the forward method:\n\n        A = A.contiguous()\n        B = B.contiguous()\n        M, K = A.shape\n        K, N = B.shape\n        C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n\n        # Check the device and data type? We assume CUDA and float32.\n        # Call the kernel\n\n  We'll define the kernel with autotuning.\n\n  We note that the matrix sizes are fixed (M, K, N) as global constants? But the model should work for any input? The problem says the inputs are A (M, K) and B (K, N). But in the `get_inputs`, the sizes are fixed. However, the model should be general? Actually, the problem says: \"the original program\" has fixed M, K, N. But the model class does not store these. The forward method takes two tensors of the given shapes.\n\n  So we will compute M, K from A and K, N from B? Actually, we have:\n        M, K1 = A.shape\n        K2, N = B.shape\n        assert K1 == K2\n\n  We'll do that.\n\n  Now, the complete code:\n\n      import triton\n      import triton.language as tl\n\n      @triton.autotune(...)\n      @triton.jit\n      def _matmul_kernel(...):\n          ... as above ...\n\n      class ModelNew(nn.Module):\n          def forward(self, A, B):\n              # Make sure contiguous and on CUDA\n              A = A.contiguous()\n              B = B.contiguous()\n              M, K = A.shape\n              K, N = B.shape\n              C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n\n              # Strides\n              stride_am = A.stride(0)\n              stride_ak = A.stride(1)\n              stride_bk = B.stride(0)\n              stride_bn = B.stride(1)\n              stride_cm = C.stride(0)\n              stride_cn = C.stride(1)\n\n              # Launch kernel\n              grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n              _matmul_kernel[grid](A, B, C, M, N, K, \n                                   stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n                                   BLOCK_M, BLOCK_N, BLOCK_K, ...)   # but we are autotuning, so we pass the block sizes as config?\n\n          Actually, we are going to set the block sizes via the autotuner. We'll define the kernel with the block sizes as `tl.constexpr`.\n\n  We must choose a set of configs for autotuning.\n\n  We'll try:\n        BLOCK_M = [128, 64, 32]\n        BLOCK_N = [128, 64, 32]\n        BLOCK_K = [32, 16]\n        and different number of warps and stages.\n\n  But note: the problem says we cannot change the grid configuration? Actually, the grid is computed from M, N and the block sizes. We are allowed to change the block sizes because that's an adaptive tuning.\n\n  We'll define the autotuner with configs that vary BLOCK_M, BLOCK_N, BLOCK_K and also the number of warps and stages.\n\n  We note that the tensor core requires at least 8 warps? Actually, we can set the number of warps to 8 for tensor cores.\n\n  We'll try:\n\n        configs = []\n        for BLOCK_M in [64, 128, 256]:\n          for BLOCK_N in [64, 128, 256]:\n            for BLOCK_K in [16, 32, 64]:\n              for num_warps in [4, 8]:\n                configs.append(triton.Config({'BLOCK_M': BLOCK_M, 'BLOCK_N': BLOCK_N, 'BLOCK_K': BLOCK_K}, num_warps=num_warps))\n\n        But note: the block sizes must be powers of two? Not necessarily, but for efficient memory access they often are.\n\n  However, the problem has irregular sizes: M=8205, K=2949, N=5921. So we should choose block sizes that divide these? Not necessarily, because we have masking.\n\n  We'll write the kernel and then the host function.\n\n  We are going to replace the entire EVOLVE-BLOCK.\n\n  IMPORTANT: We must keep the same class name (ModelNew) and the same get_inputs and get_init_inputs.\n\n  We'll also keep the global constants M, K, N and the functions get_inputs and get_init_inputs.\n\n  Let's write the code.\n\n  Note: We are not allowed to change the grid configuration? Actually, the grid configuration is the number of blocks? We are computing it as (ceil(M/BLOCK_M), ceil(N/BLOCK_N)), which is fixed for given M, N and block sizes. We are not changing the logic of the grid, so it's allowed.\n\n  We are also not changing the output tensor shape.\n\n  We are introducing a Triton kernel and removing the torch.matmul.\n\n  We hope that this will be faster.\n\n  But note: the current reference timing is 0.0214 ms? That seems too fast for such a large matrix? Maybe it's a typo? Actually, the meta says:\n        'reference_timing_ms': 0.0214, 'speed_up': 0.003, 'custom_timing': 6.41\n\n  The reference timing is 0.0214 ms and our custom timing was 6.41 ms? That means the current custom code (which is the torch.matmul) is 0.0214 ms? Then why did we get 6.41 ms for the custom code? That doesn't make sense.\n\n  Actually, the current program is using torch.matmul and the reference timing is 0.0214 ms? Then the custom_timing is 6.41 ms? That doesn't make sense because the current program is the same as the reference? Maybe the meta is from a previous run?\n\n  We are going to write a Triton kernel that is efficient. We hope to get close to the torch.matmul performance? Or even better? But note: torch.matmul uses cuBLAS which is highly optimized.\n\n  However, the problem says we are to get at least 5% speedup. So we hope that our Triton kernel can be faster? But it's challenging.\n\n  But note: the problem says the hardware is H100 and we are using the tensor cores. We are writing a kernel that uses tensor cores (via tl.dot) so we might get good performance.\n\n  We'll try.\n\n  Now, the complete code for the EVOLVE-BLOCK.\n\n  We must include the necessary imports and the kernel.\n\n  We'll write the kernel inside the class? No, we write it at the top level.\n\n  We are going to replace the entire block.\n\n  We note: the original block defined the class ModelNew and the functions get_inputs and get_init_inputs.\n\n  We will keep the same structure but change the forward method and add the kernel.\n\n  Let's write the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32}, num_warps=8, num_stages=3),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    offs_m = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid1 * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        mask_a = (offs_m[:, None] < M) & (offs_k[None, :] < k_remaining)\n        mask_b = (offs_k[:, None] < k_remaining) & (offs_n[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n        \n        acc += tl.dot(a, b, allow_tf32=True)\n        \n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    offs_cm = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid1 * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    \n    mask_c = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask_c)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        A = A.contiguous()\n        B = B.contiguous()\n        M, K = A.shape\n        K, N = B.shape\n        C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n        \n        grid = lambda META: (\n            triton.cdiv(M, META['BLOCK_M']),\n            triton.cdiv(N, META['BLOCK_N']),\n        )\n        \n        matmul_kernel[grid](\n            A, B, C,\n            M, N, K,\n            A.stride(0), A.stride(1),\n            B.stride(0), B.stride(1),\n            C.stride(0), C.stride(1)\n        )\n        \n        return C\n\nM = 8205\nK = 2949\nN = 5921\n\ndef get_inputs():\n    A = torch.randn(M, K, device='cuda')\n    B = torch.randn(K, N, device='cuda')\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
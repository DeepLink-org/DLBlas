{"id": "4f0687ce-0563-4929-b22f-3d25e3eea56e", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef pointwise_conv_kernel(\n    input_ptr, weight_ptr, output_ptr, bias_ptr,\n    in_channels, out_channels, M,\n    stride_im, stride_ic,\n    stride_wk, stride_wn,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(out_channels, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    input_ptrs = input_ptr + offs_m[:, None] * stride_im + offs_k[None, :] * stride_ic\n    weight_ptrs = weight_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(in_channels, BLOCK_SIZE_K)):\n        k_remaining = in_channels - k * BLOCK_SIZE_K\n        mask_k = offs_k < k_remaining\n        \n        a = tl.load(input_ptrs, \n                   mask=mask_k[None, :] & (offs_m[:, None] < M), \n                   other=0.0)\n        b = tl.load(weight_ptrs, \n                   mask=mask_k[:, None] & (offs_n[None, :] < out_channels), \n                   other=0.0)\n        \n        accumulator += tl.dot(a, b, allow_tf32=True)\n        input_ptrs += BLOCK_SIZE_K * stride_ic\n        weight_ptrs += BLOCK_SIZE_K * stride_wk\n\n    if bias_ptr != 0:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < out_channels, other=0.0)\n        accumulator += bias[None, :]\n\n    offs_m_mask = offs_m[:, None] < M\n    offs_n_mask = offs_n[None, :] < out_channels\n    output_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    tl.store(output_ptrs, accumulator, mask=offs_m_mask & offs_n_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.depthwise(x)\n        batch_size, in_channels, height, width = x.shape\n        \n        x_reshaped = x.permute(0, 2, 3, 1).reshape(-1, in_channels).contiguous()\n        M, K = x_reshaped.shape\n        N = self.pointwise.weight.shape[0]\n        \n        weight_t = self.pointwise.weight.view(N, K).t().contiguous()\n        y = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_M = 64\n        BLOCK_SIZE_N = 64\n        BLOCK_SIZE_K = 32\n        grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n        \n        bias_ptr = self.pointwise.bias.data_ptr() if self.pointwise.bias is not None else 0\n        \n        pointwise_conv_kernel[grid](\n            x_reshaped, weight_t, y, bias_ptr,\n            K, N, M,\n            x_reshaped.stride(0), x_reshaped.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            y.stride(0), y.stride(1),\n            BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K\n        )\n        \n        output = y.view(batch_size, height, width, N).permute(0, 3, 1, 2)\n        return output\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "c86bfb97-0805-4049-8b3e-49e3ec71d8e5", "generation": 3, "timestamp": 1754994834.5383468, "iteration_found": 24, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_86_conv_depthwise_separable_2D", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2150, in load\n    return _semantic.load(pointer, mask, other, boundary_check, padding_option, cache_modifier, eviction_policy,\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1086, in load\n    return self._load_legacy(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1017, in _load_legacy\n    raise ValueError(f\"Unsupported ptr type {ptr.type.__repr__()} in `tl.load`\")\nValueError: Unsupported ptr type <['64'], int32> in `tl.load`\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp3e2vo6a0.py\", line 78, in forward\n    pointwise_conv_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 39:15:\n                   mask=mask_k[None, :] & (offs_m[:, None] < M), \n                   other=0.0)\n        b = tl.load(weight_ptrs, \n                   mask=mask_k[:, None] & (offs_n[None, :] < out_channels), \n                   other=0.0)\n\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        input_ptrs += BLOCK_SIZE_K * stride_ic\n        weight_ptrs += BLOCK_SIZE_K * stride_wk\n\n    if bias_ptr != 0:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < out_channels, other=0.0)\n               ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2150, in load\n    return _semantic.load(pointer, mask, other, boundary_check, padding_option, cache_modifier, eviction_policy,\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1086, in load\n    return self._load_legacy(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1017, in _load_legacy\n    raise ValueError(f\"Unsupported ptr type {ptr.type.__repr__()} in `tl.load`\")\nValueError: Unsupported ptr type <['64'], int32> in `tl.load`\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0541, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_86_conv_depthwise_separable_2D", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2150, in load\n    return _semantic.load(pointer, mask, other, boundary_check, padding_option, cache_modifier, eviction_policy,\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1086, in load\n    return self._load_legacy(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1017, in _load_legacy\n    raise ValueError(f\"Unsupported ptr type {ptr.type.__repr__()} in `tl.load`\")\nValueError: Unsupported ptr type <['64'], int64> in `tl.load`\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpnui7iine.py\", line 79, in forward\n    pointwise_conv_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 40:15:\n                   other=0.0)\n        b = tl.load(weight_ptrs, \n                   mask=mask_k[:, None] & (offs_n[None, :] < out_channels), \n                   other=0.0)\n\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        input_ptrs += BLOCK_SIZE_K * stride_ic\n        weight_ptrs += BLOCK_SIZE_K * stride_wk\n\n    if bias_ptr != 0:\n        offs_n_int64 = offs_n.to(tl.int64)  # Cast to int64 for pointer arithmetic\n        bias = tl.load(bias_ptr + offs_n_int64, mask=offs_n < out_channels, other=0.0)\n               ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2150, in load\n    return _semantic.load(pointer, mask, other, boundary_check, padding_option, cache_modifier, eviction_policy,\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1086, in load\n    return self._load_legacy(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1017, in _load_legacy\n    raise ValueError(f\"Unsupported ptr type {ptr.type.__repr__()} in `tl.load`\")\nValueError: Unsupported ptr type <['64'], int64> in `tl.load`\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0541, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 0.0000\n- score: 10.0000\n- stage: wrong_answer\n- verify_payload: {'id': 'level1_86_conv_depthwise_separable_2D', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2150, in load\\n    return _semantic.load(pointer, mask, other, boundary_check, padding_option, cache_modifier, eviction_policy,\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1086, in load\\n    return self._load_legacy(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1017, in _load_legacy\\n    raise ValueError(f\"Unsupported ptr type {ptr.type.__repr__()} in `tl.load`\")\\nValueError: Unsupported ptr type <[\\'64\\'], int64> in `tl.load`\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpnui7iine.py\", line 79, in forward\\n    pointwise_conv_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 40:15:\\n                   other=0.0)\\n        b = tl.load(weight_ptrs, \\n                   mask=mask_k[:, None] & (offs_n[None, :] < out_channels), \\n                   other=0.0)\\n\\n        accumulator += tl.dot(a, b, allow_tf32=True)\\n        input_ptrs += BLOCK_SIZE_K * stride_ic\\n        weight_ptrs += BLOCK_SIZE_K * stride_wk\\n\\n    if bias_ptr != 0:\\n        offs_n_int64 = offs_n.to(tl.int64)  # Cast to int64 for pointer arithmetic\\n        bias = tl.load(bias_ptr + offs_n_int64, mask=offs_n < out_channels, other=0.0)\\n               ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2150, in load\\n    return _semantic.load(pointer, mask, other, boundary_check, padding_option, cache_modifier, eviction_policy,\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1086, in load\\n    return self._load_legacy(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1017, in _load_legacy\\n    raise ValueError(f\"Unsupported ptr type {ptr.type.__repr__()} in `tl.load`\")\\nValueError: Unsupported ptr type <[\\'64\\'], int64> in `tl.load`\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.0541, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.0930, runtime_ms: 0.5790, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0541, 'speed_up': 0.093, 'custom_timing': 0.579}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.0010, runtime_ms: 54.1000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0541, 'speed_up': 0.001, 'custom_timing': 54.1}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.2730, runtime_ms: 0.1980, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0541, 'speed_up': 0.273, 'custom_timing': 0.198}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    @staticmethod\n    @triton.jit\n    def _depthwise_conv_kernel(\n        x_ptr,\n        weight_ptr,\n        output_ptr,\n        in_channels, height, width,\n        stride_h, stride_w,\n        padding_h, padding_w,\n        dilation_h, dilation_w,\n        kernel_size,\n        output_height, output_width,\n        x_batch_stride, x_channel_stride, x_height_stride, x_width_stride,\n        weight_channel_stride, weight_height_stride, weight_width_stride,\n        output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n        BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n    ):\n        pid_b = tl.program_id(0)\n        pid_c = tl.program_id(1)\n        pid_hw = tl.program_id(2)\n        \n        output_width_blocks = tl.cdiv(output_width, BLOCK_W)\n        pid_h = pid_hw // output_width_blocks\n        pid_w = pid_hw % output_width_blocks\n        \n        h_start = pid_h * BLOCK_H\n        w_start = pid_w * BLOCK_W\n        \n        h_offsets = h_start + tl.arange(0, BLOCK_H)\n        w_offsets = w_start + tl.arange(0, BLOCK_W)\n        \n        h_mask = h_offsets < output_height\n        w_mask = w_offsets < output_width\n        block_mask = h_mask[:, None] & w_mask[None, :]\n        \n        accumulator = tl.zeros((BLOCK_H, BLOCK_W), dtype=tl.float32)\n        \n        for ki in range(kernel_size):\n            for kj in range(kernel_size):\n                input_i = h_offsets * stride_h + ki * dilation_h - padding_h\n                input_j = w_offsets * stride_w + kj * dilation_w - padding_w\n                \n                input_i_mask = (input_i >= 0) & (input_i < height)\n                input_j_mask = (input_j >= 0) & (input_j < width)\n                input_mask = input_i_mask[:, None] & input_j_mask[None, :] & block_mask\n                \n                input_ptr = x_ptr + pid_b * x_batch_stride + pid_c * x_channel_stride\n                input_val = tl.load(\n                    input_ptr + input_i[:, None] * x_height_stride + input_j[None, :] * x_width_stride,\n                    mask=input_mask,\n                    other=0.0\n                )\n                \n                weight_val = tl.load(\n                    weight_ptr + pid_c * weight_channel_stride + ki * weight_height_stride + kj * weight_width_stride\n                )\n                \n                accumulator += input_val * weight_val\n        \n        output_ptr_base = output_ptr + pid_b * output_batch_stride + pid_c * output_channel_stride\n        output_offsets = h_offsets[:, None] * output_height_stride + w_offsets[None, :] * output_width_stride\n        tl.store(output_ptr_base + output_offsets, accumulator, mask=block_mask)\n                    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        stride = self.depthwise.stride\n        padding = self.depthwise.padding\n        dilation = self.depthwise.dilation\n        weight = self.depthwise.weight\n        bias = self.depthwise.bias\n        \n        batch, in_channels, height, width = x.shape\n        kernel_size = weight.shape[2]\n        \n        output_height = (height + 2 * padding[0] - dilation[0] * (kernel_size - 1) - 1) // stride[0] + 1\n        output_width = (width + 2 * padding[1] - dilation[1] * (kernel_size - 1) - 1) // stride[1] + 1\n        \n        x = x.contiguous()\n        weight = weight.contiguous()\n        output = torch.empty((batch, in_channels, output_height, output_width), device=x.device, dtype=x.dtype)\n        \n        grid = (batch, in_channels, triton.cdiv(output_height, 16) * triton.cdiv(output_width, 16))\n        \n        self._depthwise_conv_kernel[grid](\n            x, weight, output,\n            in_channels, height, width,\n            stride[0], stride[1],\n            padding[0], padding[1],\n            dilation[0], dilation[1],\n            kernel_size,\n            output_height, output_width,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            weight.stride(0), weight.stride(2), weight.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            BLOCK_H=16, BLOCK_W=16\n        )\n        \n        if bias is not None:\n            output += bias.reshape(1, -1, 1, 1)\n            \n        output = self.pointwise(output)\n        return output\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.2730), Performs well on runtime_ms (0.1980), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0541, 'speed_up': 0.273, 'custom_timing': 0.198})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef depthwise_conv2d_kernel(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    stride, padding, dilation, kernel_size,\n    in_channels, height, width, height_out, width_out,\n    x_bs, x_cs, x_hs, x_ws,\n    w_cs, w_ks, w_hs, w_ws,\n    out_bs, out_cs, out_hs, out_ws,\n    BIAS_EXISTS: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    n_elements = in_channels * height_out * width_out\n    batch_idx = pid // n_elements\n    pid = pid % n_elements\n    \n    ch_idx = pid // (height_out * width_out)\n    hw_idx = pid % (height_out * width_out)\n    h_idx = hw_idx // width_out\n    w_idx = hw_idx % width_out\n    \n    acc = 0.0\n    for kh in range(kernel_size):\n        for kw in range(kernel_size):\n            h_in = h_idx * stride + kh * dilation - padding\n            w_in = w_idx * stride + kw * dilation - padding\n            if h_in >= 0 and h_in < height and w_in >= 0 and w_in < width:\n                x_offset = batch_idx * x_bs + ch_idx * x_cs + h_in * x_hs + w_in * x_ws\n                w_offset = ch_idx * w_cs + kh * w_hs + kw * w_ws\n                x_val = tl.load(x_ptr + x_offset)\n                w_val = tl.load(w_ptr + w_offset)\n                acc += x_val * w_val\n    \n    if BIAS_EXISTS:\n        bias_val = tl.load(bias_ptr + ch_idx)\n        acc += bias_val\n        \n    out_offset = batch_idx * out_bs + ch_idx * out_cs + h_idx * out_hs + w_idx * out_ws\n    tl.store(output_ptr + out_offset, acc)\n\n@triton.jit\ndef pointwise_conv2d_kernel(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    in_channels, height, width, out_channels,\n    x_bs, x_cs, x_hs, x_ws,\n    w_ocs, w_ics, w_hs, w_ws,\n    out_bs, out_cs, out_hs, out_ws,\n    BIAS_EXISTS: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    n_elements = out_channels * height * width\n    batch_idx = pid // n_elements\n    pid = pid % n_elements\n    \n    ch_out_idx = pid // (height * width)\n    hw_idx = pid % (height * width)\n    h_idx = hw_idx // width\n    w_idx = hw_idx % width\n    \n    acc = 0.0\n    for ch_in in range(in_channels):\n        x_offset = batch_idx * x_bs + ch_in * x_cs + h_idx * x_hs + w_idx * x_ws\n        w_offset = ch_out_idx * w_ocs + ch_in * w_ics\n        x_val = tl.load(x_ptr + x_offset)\n        w_val = tl.load(w_ptr + w_offset)\n        acc += x_val * w_val\n    \n    if BIAS_EXISTS:\n        bias_val = tl.load(bias_ptr + ch_out_idx)\n        acc += bias_val\n        \n    out_offset = batch_idx * out_bs + ch_out_idx * out_cs + h_idx * out_hs + w_idx * out_ws\n    tl.store(output_ptr + out_offset, acc)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        stride = self.depthwise.stride[0]\n        padding = self.depthwise.padding[0]\n        dilation = self.depthwise.dilation[0]\n        kernel_size = self.depthwise.kernel_size[0]\n        \n        B, C, H, W = x.shape\n        H_out = (H + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1\n        W_out = (W + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1\n        \n        x = x.contiguous()\n        depthwise_out = torch.empty((B, C, H_out, W_out), device=x.device, dtype=x.dtype)\n        \n        grid_depth = (B * C * H_out * W_out,)\n        depthwise_conv2d_kernel[grid_depth](\n            x, \n            self.depthwise.weight.contiguous(), \n            self.depthwise.bias.contiguous() if self.depthwise.bias is not None else None,\n            depthwise_out,\n            stride, padding, dilation, kernel_size,\n            C, H, W, H_out, W_out,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            self.depthwise.weight.stride(0), self.depthwise.weight.stride(1), \n            self.depthwise.weight.stride(2), self.depthwise.weight.stride(3),\n            depthwise_out.stride(0), depthwise_out.stride(1), \n            depthwise_out.stride(2), depthwise_out.stride(3),\n            BIAS_EXISTS=self.depthwise.bias is not None,\n            BLOCK_SIZE=min(1024, triton.next_power_of_2(H_out * W_out))\n        )\n        \n        pointwise_out = torch.empty((B, self.pointwise.out_channels, H_out, W_out), \n                                   device=x.device, dtype=x.dtype)\n        \n        grid_point = (B * self.pointwise.out_channels * H_out * W_out,)\n        pointwise_conv2d_kernel[grid_point](\n            depthwise_out,\n            self.pointwise.weight.contiguous(),\n            self.pointwise.bias.contiguous() if self.pointwise.bias is not None else None,\n            pointwise_out,\n            C, H_out, W_out, self.pointwise.out_channels,\n            depthwise_out.stride(0), depthwise_out.stride(1), \n            depthwise_out.stride(2), depthwise_out.stride(3),\n            self.pointwise.weight.stride(0), self.pointwise.weight.stride(1),\n            self.pointwise.weight.stride(2), self.pointwise.weight.stride(3),\n            pointwise_out.stride(0), pointwise_out.stride(1), \n            pointwise_out.stride(2), pointwise_out.stride(3),\n            BIAS_EXISTS=self.pointwise.bias is not None,\n            BLOCK_SIZE=min(1024, triton.next_power_of_2(H_out * W_out))\n        )\n        \n        return pointwise_out\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.0010), Performs well on runtime_ms (54.1000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0541, 'speed_up': 0.001, 'custom_timing': 54.1})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    @staticmethod\n    @triton.jit\n    def _depthwise_conv_kernel(\n        x_ptr,\n        weight_ptr,\n        output_ptr,\n        in_channels, height, width,\n        stride_h, stride_w,\n        padding_h, padding_w,\n        dilation_h, dilation_w,\n        kernel_size,\n        output_height, output_width,\n        x_batch_stride, x_channel_stride, x_height_stride, x_width_stride,\n        weight_channel_stride, weight_height_stride, weight_width_stride,\n        output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n        BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n    ):\n        pid_b = tl.program_id(0)\n        pid_c = tl.program_id(1)\n        pid_hw = tl.program_id(2)\n        \n        output_width_blocks = tl.cdiv(output_width, BLOCK_W)\n        pid_h = pid_hw // output_width_blocks\n        pid_w = pid_hw % output_width_blocks\n        \n        h_start = pid_h * BLOCK_H\n        w_start = pid_w * BLOCK_W\n        \n        h_offsets = h_start + tl.arange(0, BLOCK_H)\n        w_offsets = w_start + tl.arange(0, BLOCK_W)\n        \n        h_mask = h_offsets < output_height\n        w_mask = w_offsets < output_width\n        block_mask = h_mask[:, None] & w_mask[None, :]\n        \n        accumulator = tl.zeros((BLOCK_H, BLOCK_W), dtype=tl.float32)\n        base_ptr = x_ptr + pid_b * x_batch_stride + pid_c * x_channel_stride\n        \n        for ki in range(kernel_size):\n            for kj in range(kernel_size):\n                input_i = h_offsets * stride_h + ki * dilation_h - padding_h\n                input_j = w_offsets * stride_w + kj * dilation_w - padding_w\n                \n                input_i_mask = (input_i >= 0) & (input_i < height)\n                input_j_mask = (input_j >= 0) & (input_j < width)\n                input_mask = input_i_mask[:, None] & input_j_mask[None, :] & block_mask\n                \n                input_val = tl.load(\n                    base_ptr + input_i[:, None] * x_height_stride + input_j[None, :] * x_width_stride,\n                    mask=input_mask,\n                    other=0.0\n                )\n                \n                weight_val = tl.load(\n                    weight_ptr + pid_c * weight_channel_stride + ki * weight_height_stride + kj * weight_width_stride\n                )\n                \n                accumulator += input_val * weight_val\n        \n        output_ptr_base = output_ptr + pid_b * output_batch_stride + pid_c * output_channel_stride\n        output_offsets = h_offsets[:, None] * output_height_stride + w_offsets[None, :] * output_width_stride\n        tl.store(output_ptr_base + output_offsets, accumulator, mask=block_mask)\n    \n    @staticmethod\n    @triton.jit\n    def _pointwise_conv_kernel(\n        input_ptr, weight_ptr, output_ptr,\n        M, N, K,\n        stride_im, stride_ik,\n        stride_wk, stride_wn,\n        stride_om, stride_on,\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    ):\n        pid = tl.program_id(0)\n        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n        pid_m = pid // num_pid_n\n        pid_n = pid % num_pid_n\n\n        offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n        input_ptrs = input_ptr + offs_m[:, None] * stride_im + offs_k[None, :] * stride_ik\n        weight_ptrs = weight_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n            k_remaining = K - k * BLOCK_SIZE_K\n            mask_k = offs_k < k_remaining\n            \n            a = tl.load(input_ptrs, \n                       mask=mask_k[None, :] & (offs_m[:, None] < M), \n                       other=0.0)\n            b = tl.load(weight_ptrs, \n                       mask=mask_k[:, None] & (offs_n[None, :] < N), \n                       other=0.0)\n            \n            accumulator += tl.dot(a, b, allow_tf32=True)\n            input_ptrs += BLOCK_SIZE_K * stride_ik\n            weight_ptrs += BLOCK_SIZE_K * stride_wk\n\n        offs_m_mask = offs_m[:, None] < M\n        offs_n_mask = offs_n[None, :] < N\n        output_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n        tl.store(output_ptrs, accumulator, mask=offs_m_mask & offs_n_mask)\n                    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        stride = self.depthwise.stride\n        padding = self.depthwise.padding\n        dilation = self.depthwise.dilation\n        weight = self.depthwise.weight\n        bias = self.depthwise.bias\n        \n        batch, in_channels, height, width = x.shape\n        kernel_size = weight.shape[2]\n        \n        output_height = (height + 2 * padding[0] - dilation[0] * (kernel_size - 1) - 1) // stride[0] + 1\n        output_width = (width + 2 * padding[1] - dilation[1] * (kernel_size - 1) - 1) // stride[1] + 1\n        \n        x = x.contiguous()\n        weight = weight.contiguous()\n        output_depthwise = torch.empty((batch, in_channels, output_height, output_width), device=x.device, dtype=x.dtype)\n        \n        grid = (batch, in_channels, triton.cdiv(output_height, 32) * triton.cdiv(output_width, 32))\n        \n        self._depthwise_conv_kernel[grid](\n            x, weight, output_depthwise,\n            in_channels, height, width,\n            stride[0], stride[1],\n            padding[0], padding[1],\n            dilation[0], dilation[1],\n            kernel_size,\n            output_height, output_width,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            weight.stride(0), weight.stride(2), weight.stride(3),\n            output_depthwise.stride(0), output_depthwise.stride(1), output_depthwise.stride(2), output_depthwise.stride(3),\n            BLOCK_H=32, BLOCK_W=32\n        )\n        \n        if bias is not None:\n            output_depthwise += bias.reshape(1, -1, 1, 1)\n            \n        # Pointwise convolution using Triton kernel\n        batch, in_channels, height, width = output_depthwise.shape\n        x_reshaped = output_depthwise.permute(0, 2, 3, 1).reshape(-1, in_channels).contiguous()\n        M, K = x_reshaped.shape\n        N = self.pointwise.weight.shape[0]\n        \n        weight_t = self.pointwise.weight.view(N, K).t().contiguous()\n        y = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_M = 128\n        BLOCK_SIZE_N = 128\n        BLOCK_SIZE_K = 32\n        grid_pointwise = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n        \n        self._pointwise_conv_kernel[grid_pointwise](\n            x_reshaped, weight_t, y,\n            M, N, K,\n            x_reshaped.stride(0), x_reshaped.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            y.stride(0), y.stride(1),\n            BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K\n        )\n        \n        if self.pointwise.bias is not None:\n            y += self.pointwise.bias\n        \n        output = y.view(batch, height, width, N).permute(0, 3, 1, 2)\n        return output\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.0930), Performs well on runtime_ms (0.5790), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0541, 'speed_up': 0.093, 'custom_timing': 0.579})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Performs a depthwise-separable 2D convolution operation with optimized Triton kernel.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    @staticmethod\n    @triton.jit\n    def _depthwise_conv_kernel(\n        x_ptr,\n        weight_ptr,\n        output_ptr,\n        in_channels, height, width,\n        stride_h, stride_w,\n        padding_h, padding_w,\n        dilation_h, dilation_w,\n        kernel_size,\n        output_height, output_width,\n        x_batch_stride, x_channel_stride, x_height_stride, x_width_stride,\n        weight_channel_stride, weight_height_stride, weight_width_stride,\n        output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n        BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n    ):\n        pid_b = tl.program_id(0)\n        pid_c = tl.program_id(1)\n        pid_h = tl.program_id(2)\n        pid_w = tl.program_id(3)\n        \n        h_start = pid_h * BLOCK_H\n        w_start = pid_w * BLOCK_W\n        \n        h_offsets = h_start + tl.arange(0, BLOCK_H)\n        w_offsets = w_start + tl.arange(0, BLOCK_W)\n        \n        h_mask = (h_offsets < output_height)\n        w_mask = (w_offsets < output_width)\n        block_mask = h_mask[:, None] & w_mask[None, :]\n        \n        output_block = tl.zeros((BLOCK_H, BLOCK_W), dtype=tl.float32)\n        \n        for ki in tl.static_range(kernel_size):\n            for kj in tl.static_range(kernel_size):\n                input_i = h_offsets * stride_h + ki * dilation_h - padding_h\n                input_j = w_offsets * stride_w + kj * dilation_w - padding_w\n                \n                input_i_mask = (input_i >= 0) & (input_i < height)\n                input_j_mask = (input_j >= 0) & (input_j < width)\n                input_mask = input_i_mask[:, None] & input_j_mask[None, :] & block_mask\n                \n                input_ptr = x_ptr + pid_b * x_batch_stride + pid_c * x_channel_stride\n                input_val = tl.load(\n                    input_ptr + input_i * x_height_stride + input_j * x_width_stride,\n                    mask=input_mask,\n                    other=0.0\n                )\n                \n                weight_val = tl.load(\n                    weight_ptr + pid_c * weight_channel_stride + ki * weight_height_stride + kj * weight_width_stride\n                )\n                \n                output_block += input_val * weight_val\n                \n        output_ptr_base = output_ptr + pid_b * output_batch_stride + pid_c * output_channel_stride\n        for i in range(BLOCK_H):\n            for j in range(BLOCK_W):\n                if block_mask[i, j]:\n                    tl.store(\n                        output_ptr_base + (h_start + i) * output_height_stride + (w_start + j) * output_width_stride,\n                        output_block[i, j]\n                    )\n                    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        stride = self.depthwise.stride\n        padding = self.depthwise.padding\n        dilation = self.depthwise.dilation\n        weight = self.depthwise.weight\n        bias = self.depthwise.bias\n        \n        batch, in_channels, height, width = x.shape\n        kernel_size = weight.shape[2]\n        \n        output_height = (height + 2 * padding[0] - dilation[0] * (kernel_size - 1) - 1) // stride[0] + 1\n        output_width = (width + 2 * padding[1] - dilation[1] * (kernel_size - 1) - 1) // stride[1] + 1\n        \n        x = x.contiguous()\n        weight = weight.contiguous()\n        output = torch.empty((batch, in_channels, output_height, output_width), device=x.device, dtype=x.dtype)\n        \n        grid = (batch, in_channels, triton.cdiv(output_height, 16), triton.cdiv(output_width, 16))\n        \n        self._depthwise_conv_kernel[grid](\n            x, weight, output,\n            in_channels, height, width,\n            stride[0], stride[1],\n            padding[0], padding[1],\n            dilation[0], dilation[1],\n            kernel_size,\n            output_height, output_width,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            weight.stride(0), weight.stride(2), weight.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            BLOCK_H=16, BLOCK_W=16\n        )\n        \n        if bias is not None:\n            output += bias.reshape(1, -1, 1, 1)\n            \n        output = self.pointwise(output)\n        return output\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef separable_conv_forward_kernel(\n    # Input tensor pointers\n    x_ptr,\n    depthwise_weight_ptr,\n    pointwise_weight_ptr,\n    bias_ptr,\n    output_ptr,\n    # Tensor dimensions\n    batch_size, in_channels, height, width,\n    kernel_size, stride, padding, dilation,\n    out_channels, height_out, width_out,\n    # Stride information\n    x_batch_stride, x_channel_stride, x_h_stride, x_w_stride,\n    dw_channel_stride, dw_kh_stride, dw_kw_stride,\n    pw_out_stride, pw_in_stride,\n    bias_stride,\n    output_batch_stride, output_channel_stride, output_h_stride, output_w_stride,\n    # Kernel configuration\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_elems = batch_size * out_channels * height_out * width_out\n    \n    # Exit if out of bounds\n    if pid * BLOCK_SIZE >= num_elems:\n        return\n    \n    # Generate offsets for this block\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < num_elems\n    \n    # Decompose offsets into output tensor indices using integer arithmetic\n    w_out = offsets % width_out\n    offsets = offsets // width_out\n    h_out = offsets % height_out\n    offsets = offsets // height_out\n    oc = offsets % out_channels\n    b = offsets // out_channels\n    \n    # Compute input window start positions\n    h_start = h_out * stride - padding\n    w_start = w_out * stride - padding\n    \n    # Initialize accumulators\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Optimized depthwise convolution\n    for kh in range(kernel_size):\n        h_in = h_start + kh * dilation\n        h_in_bound = (h_in >= 0) & (h_in < height)\n        \n        for kw in range(kernel_size):\n            w_in = w_start + kw * dilation\n            w_in_bound = (w_in >= 0) & (w_in < width)\n            in_bounds = h_in_bound & w_in_bound\n            \n            # Calculate input offsets once per kernel position\n            x_offset = (b * x_batch_stride + \n                       h_in * x_h_stride + \n                       w_in * x_w_stride)\n            \n            # Calculate depthwise weight offset\n            dw_offset = kh * dw_kh_stride + kw * dw_kw_stride\n            \n            # Process all channels in vectorized manner\n            for c in range(0, in_channels, BLOCK_SIZE):\n                c_offsets = c + tl.arange(0, BLOCK_SIZE)\n                c_mask = c_offsets < in_channels\n                \n                # Load input values for all channels\n                x_vals = tl.load(\n                    x_ptr + x_offset + c_offsets * x_channel_stride,\n                    mask=in_bounds & mask & c_mask,\n                    other=0.0\n                )\n                \n                # Load depthwise weights for all channels\n                dw_vals = tl.load(\n                    depthwise_weight_ptr + dw_offset + c_offsets * dw_channel_stride,\n                    mask=c_mask,\n                    other=0.0\n                )\n                \n                # Multiply-accumulate\n                acc += x_vals * dw_vals\n    \n    # Vectorized pointwise convolution\n    pointwise_acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for c in range(0, in_channels, BLOCK_SIZE):\n        c_offsets = c + tl.arange(0, BLOCK_SIZE)\n        c_mask = c_offsets < in_channels\n        \n        # Load pointwise weights\n        pw_vals = tl.load(\n            pointwise_weight_ptr + oc * pw_out_stride + c_offsets * pw_in_stride,\n            mask=c_mask,\n            other=0.0\n        )\n        pointwise_acc += acc * pw_vals\n    \n    # Add bias if present\n    if bias_ptr != 0:\n        bias_val = tl.load(bias_ptr + oc * bias_stride)\n        pointwise_acc += bias_val\n    \n    # Calculate output pointer offset\n    output_offset = (b * output_batch_stride + \n                    oc * output_channel_stride + \n                    h_out * output_h_stride + \n                    w_out * output_w_stride)\n    \n    # Store result\n    tl.store(output_ptr + output_offset, pointwise_acc, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, \n                 stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        \n        # Depthwise convolution parameters\n        self.depthwise_weight = nn.Parameter(\n            torch.empty(in_channels, 1, kernel_size, kernel_size)\n        )\n        # Pointwise convolution parameters\n        self.pointwise_weight = nn.Parameter(\n            torch.empty(out_channels, in_channels, 1, 1)\n        )\n        # Bias parameter\n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_channels))\n        else:\n            self.register_parameter('bias', None)\n            \n        # Initialize parameters\n        nn.init.kaiming_uniform_(self.depthwise_weight, a=math.sqrt(5))\n        nn.init.kaiming_uniform_(self.pointwise_weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.pointwise_weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, _, height, width = x.shape\n        \n        # Calculate output dimensions\n        height_out = (height + 2 * self.padding - \n                     self.dilation * (self.kernel_size - 1) - 1) // self.stride + 1\n        width_out = (width + 2 * self.padding - \n                    self.dilation * (self.kernel_size - 1) - 1) // self.stride + 1\n        \n        # Create output tensor\n        output = torch.empty(\n            (batch_size, self.out_channels, height_out, width_out),\n            device=x.device, dtype=x.dtype\n        )\n        \n        # Compute total number of output elements\n        total_elems = batch_size * self.out_channels * height_out * width_out\n        \n        # Setup kernel grid\n        grid = lambda meta: (triton.cdiv(total_elems, meta['BLOCK_SIZE']),)\n        \n        # Get tensor strides\n        x_strides = (x.stride(0), x.stride(1), x.stride(2), x.stride(3))\n        dw_strides = (\n            self.depthwise_weight.stride(0),\n            self.depthwise_weight.stride(2),\n            self.depthwise_weight.stride(3)\n        )\n        pw_strides = (\n            self.pointwise_weight.stride(0),\n            self.pointwise_weight.stride(1)\n        )\n        bias_stride = self.bias.stride(0) if self.bias is not None else 0\n        output_strides = (\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3)\n        )\n        \n        # Kernel parameters\n        kernel_params = [\n            x, self.depthwise_weight, self.pointwise_weight,\n            self.bias.data_ptr() if self.bias is not None else 0, output,\n            # Dimensions\n            batch_size, self.in_channels, height, width,\n            self.kernel_size, self.stride, self.padding, self.dilation,\n            self.out_channels, height_out, width_out,\n            # Strides\n            *x_strides,\n            *dw_strides,\n            *pw_strides,\n            bias_stride,\n            *output_strides\n        ]\n        \n        # Launch kernel with fixed BLOCK_SIZE as keyword argument\n        separable_conv_forward_kernel[grid](*kernel_params, BLOCK_SIZE=1024)\n        \n        return output\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    @staticmethod\n    @triton.jit\n    def _depthwise_conv_kernel(\n        x_ptr,\n        weight_ptr,\n        output_ptr,\n        in_channels, height, width,\n        stride_h, stride_w,\n        padding_h, padding_w,\n        dilation_h, dilation_w,\n        kernel_size,\n        output_height, output_width,\n        x_batch_stride, x_channel_stride, x_height_stride, x_width_stride,\n        weight_channel_stride, weight_height_stride, weight_width_stride,\n        output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n        BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n    ):\n        pid_b = tl.program_id(0)\n        pid_c = tl.program_id(1)\n        pid_hw = tl.program_id(2)\n        \n        output_width_blocks = tl.cdiv(output_width, BLOCK_W)\n        pid_h = pid_hw // output_width_blocks\n        pid_w = pid_hw % output_width_blocks\n        \n        h_start = pid_h * BLOCK_H\n        w_start = pid_w * BLOCK_W\n        \n        h_offsets = h_start + tl.arange(0, BLOCK_H)\n        w_offsets = w_start + tl.arange(0, BLOCK_W)\n        \n        h_mask = h_offsets < output_height\n        w_mask = w_offsets < output_width\n        block_mask = h_mask[:, None] & w_mask[None, :]\n        \n        accumulator = tl.zeros((BLOCK_H, BLOCK_W), dtype=tl.float32)\n        base_ptr = x_ptr + pid_b * x_batch_stride + pid_c * x_channel_stride\n        \n        for ki in range(kernel_size):\n            for kj in range(kernel_size):\n                input_i = h_offsets * stride_h + ki * dilation_h - padding_h\n                input_j = w_offsets * stride_w + kj * dilation_w - padding_w\n                \n                input_i_mask = (input_i >= 0) & (input_i < height)\n                input_j_mask = (input_j >= 0) & (input_j < width)\n                input_mask = input_i_mask[:, None] & input_j_mask[None, :] & block_mask\n                \n                input_val = tl.load(\n                    base_ptr + input_i[:, None] * x_height_stride + input_j[None, :] * x_width_stride,\n                    mask=input_mask,\n                    other=0.0\n                )\n                \n                weight_val = tl.load(\n                    weight_ptr + pid_c * weight_channel_stride + ki * weight_height_stride + kj * weight_width_stride\n                )\n                \n                accumulator += input_val * weight_val\n        \n        output_ptr_base = output_ptr + pid_b * output_batch_stride + pid_c * output_channel_stride\n        output_offsets = h_offsets[:, None] * output_height_stride + w_offsets[None, :] * output_width_stride\n        tl.store(output_ptr_base + output_offsets, accumulator, mask=block_mask)\n    \n    @staticmethod\n    @triton.jit\n    def _pointwise_conv_kernel(\n        input_ptr, weight_ptr, output_ptr,\n        M, N, K,\n        stride_im, stride_ik,\n        stride_wk, stride_wn,\n        stride_om, stride_on,\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    ):\n        pid = tl.program_id(0)\n        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n        pid_m = pid // num_pid_n\n        pid_n = pid % num_pid_n\n\n        offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n        input_ptrs = input_ptr + offs_m[:, None] * stride_im + offs_k[None, :] * stride_ik\n        weight_ptrs = weight_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n            k_remaining = K - k * BLOCK_SIZE_K\n            mask_k = offs_k < k_remaining\n            \n            a = tl.load(input_ptrs, \n                       mask=mask_k[None, :] & (offs_m[:, None] < M), \n                       other=0.0)\n            b = tl.load(weight_ptrs, \n                       mask=mask_k[:, None] & (offs_n[None, :] < N), \n                       other=0.0)\n            \n            accumulator += tl.dot(a, b, allow_tf32=True)\n            input_ptrs += BLOCK_SIZE_K * stride_ik\n            weight_ptrs += BLOCK_SIZE_K * stride_wk\n\n        offs_m_mask = offs_m[:, None] < M\n        offs_n_mask = offs_n[None, :] < N\n        output_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n        tl.store(output_ptrs, accumulator, mask=offs_m_mask & offs_n_mask)\n                    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        stride = self.depthwise.stride\n        padding = self.depthwise.padding\n        dilation = self.depthwise.dilation\n        weight = self.depthwise.weight\n        bias = self.depthwise.bias\n        \n        batch, in_channels, height, width = x.shape\n        kernel_size = weight.shape[2]\n        \n        output_height = (height + 2 * padding[0] - dilation[0] * (kernel_size - 1) - 1) // stride[0] + 1\n        output_width = (width + 2 * padding[1] - dilation[1] * (kernel_size - 1) - 1) // stride[1] + 1\n        \n        x = x.contiguous()\n        weight = weight.contiguous()\n        output_depthwise = torch.empty((batch, in_channels, output_height, output_width), device=x.device, dtype=x.dtype)\n        \n        grid = (batch, in_channels, triton.cdiv(output_height, 32) * triton.cdiv(output_width, 32))\n        \n        self._depthwise_conv_kernel[grid](\n            x, weight, output_depthwise,\n            in_channels, height, width,\n            stride[0], stride[1],\n            padding[0], padding[1],\n            dilation[0], dilation[1],\n            kernel_size,\n            output_height, output_width,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            weight.stride(0), weight.stride(2), weight.stride(3),\n            output_depthwise.stride(0), output_depthwise.stride(1), output_depthwise.stride(2), output_depthwise.stride(3),\n            BLOCK_H=32, BLOCK_W=32\n        )\n        \n        if bias is not None:\n            output_depthwise += bias.reshape(1, -1, 1, 1)\n            \n        # Pointwise convolution using Triton kernel\n        batch, in_channels, height, width = output_depthwise.shape\n        x_reshaped = output_depthwise.permute(0, 2, 3, 1).reshape(-1, in_channels).contiguous()\n        M, K = x_reshaped.shape\n        N = self.pointwise.weight.shape[0]\n        \n        weight_t = self.pointwise.weight.view(N, K).t().contiguous()\n        y = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_M = 128\n        BLOCK_SIZE_N = 128\n        BLOCK_SIZE_K = 32\n        grid_pointwise = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n        \n        self._pointwise_conv_kernel[grid_pointwise](\n            x_reshaped, weight_t, y,\n            M, N, K,\n            x_reshaped.stride(0), x_reshaped.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            y.stride(0), y.stride(1),\n            BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K\n        )\n        \n        if self.pointwise.bias is not None:\n            y += self.pointwise.bias\n        \n        output = y.view(batch, height, width, N).permute(0, 3, 1, 2)\n        return output\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    @staticmethod\n    @triton.jit\n    def _depthwise_conv_kernel(\n        x_ptr,\n        weight_ptr,\n        output_ptr,\n        in_channels, height, width,\n        stride_h, stride_w,\n        padding_h, padding_w,\n        dilation_h, dilation_w,\n        kernel_size,\n        output_height, output_width,\n        x_batch_stride, x_channel_stride, x_height_stride, x_width_stride,\n        weight_channel_stride, weight_height_stride, weight_width_stride,\n        output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n        BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n    ):\n        pid_b = tl.program_id(0)\n        pid_c = tl.program_id(1)\n        pid_hw = tl.program_id(2)\n        \n        output_width_blocks = tl.cdiv(output_width, BLOCK_W)\n        pid_h = pid_hw // output_width_blocks\n        pid_w = pid_hw % output_width_blocks\n        \n        h_start = pid_h * BLOCK_H\n        w_start = pid_w * BLOCK_W\n        \n        h_offsets = h_start + tl.arange(0, BLOCK_H)\n        w_offsets = w_start + tl.arange(0, BLOCK_W)\n        \n        h_mask = h_offsets < output_height\n        w_mask = w_offsets < output_width\n        block_mask = h_mask[:, None] & w_mask[None, :]\n        \n        accumulator = tl.zeros((BLOCK_H, BLOCK_W), dtype=tl.float32)\n        \n        for ki in range(kernel_size):\n            for kj in range(kernel_size):\n                input_i = h_offsets * stride_h + ki * dilation_h - padding_h\n                input_j = w_offsets * stride_w + kj * dilation_w - padding_w\n                \n                input_i_mask = (input_i >= 0) & (input_i < height)\n                input_j_mask = (input_j >= 0) & (input_j < width)\n                input_mask = input_i_mask[:, None] & input_j_mask[None, :] & block_mask\n                \n                input_ptr = x_ptr + pid_b * x_batch_stride + pid_c * x_channel_stride\n                input_val = tl.load(\n                    input_ptr + input_i[:, None] * x_height_stride + input_j[None, :] * x_width_stride,\n                    mask=input_mask,\n                    other=0.0\n                )\n                \n                weight_val = tl.load(\n                    weight_ptr + pid_c * weight_channel_stride + ki * weight_height_stride + kj * weight_width_stride\n                )\n                \n                accumulator += input_val * weight_val\n        \n        output_ptr_base = output_ptr + pid_b * output_batch_stride + pid_c * output_channel_stride\n        output_offsets = h_offsets[:, None] * output_height_stride + w_offsets[None, :] * output_width_stride\n        tl.store(output_ptr_base + output_offsets, accumulator, mask=block_mask)\n                    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        stride = self.depthwise.stride\n        padding = self.depthwise.padding\n        dilation = self.depthwise.dilation\n        weight = self.depthwise.weight\n        bias = self.depthwise.bias\n        \n        batch, in_channels, height, width = x.shape\n        kernel_size = weight.shape[2]\n        \n        output_height = (height + 2 * padding[0] - dilation[0] * (kernel_size - 1) - 1) // stride[0] + 1\n        output_width = (width + 2 * padding[1] - dilation[1] * (kernel_size - 1) - 1) // stride[1] + 1\n        \n        x = x.contiguous()\n        weight = weight.contiguous()\n        output = torch.empty((batch, in_channels, output_height, output_width), device=x.device, dtype=x.dtype)\n        \n        grid = (batch, in_channels, triton.cdiv(output_height, 16) * triton.cdiv(output_width, 16))\n        \n        self._depthwise_conv_kernel[grid](\n            x, weight, output,\n            in_channels, height, width,\n            stride[0], stride[1],\n            padding[0], padding[1],\n            dilation[0], dilation[1],\n            kernel_size,\n            output_height, output_width,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            weight.stride(0), weight.stride(2), weight.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            BLOCK_H=16, BLOCK_W=16\n        )\n        \n        if bias is not None:\n            output += bias.reshape(1, -1, 1, 1)\n            \n        output = self.pointwise(output)\n        return output\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    @staticmethod\n    @triton.jit\n    def _depthwise_conv_kernel(\n        x_ptr,\n        weight_ptr,\n        output_ptr,\n        in_channels, height, width,\n        stride_h, stride_w,\n        padding_h, padding_w,\n        dilation_h, dilation_w,\n        kernel_size: tl.constexpr,\n        output_height, output_width,\n        x_batch_stride, x_channel_stride, x_height_stride, x_width_stride,\n        weight_channel_stride, weight_height_stride, weight_width_stride,\n        output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n        BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n    ):\n        pid_b = tl.program_id(0)\n        pid_c = tl.program_id(1)\n        pid_hw = tl.program_id(2)\n        \n        output_width_blocks = tl.cdiv(output_width, BLOCK_W)\n        pid_h = pid_hw // output_width_blocks\n        pid_w = pid_hw % output_width_blocks\n        \n        h_start = pid_h * BLOCK_H\n        w_start = pid_w * BLOCK_W\n        \n        h_offsets = h_start + tl.arange(0, BLOCK_H)\n        w_offsets = w_start + tl.arange(0, BLOCK_W)\n        \n        h_mask = h_offsets < output_height\n        w_mask = w_offsets < output_width\n        block_mask = h_mask[:, None] & w_mask[None, :]\n        \n        # Preload weight kernel for current channel\n        weight_vals = tl.zeros((kernel_size, kernel_size), dtype=tl.float32)\n        for ki in tl.static_range(0, kernel_size):\n            for kj in tl.static_range(0, kernel_size):\n                w_offset = pid_c * weight_channel_stride + ki * weight_height_stride + kj * weight_width_stride\n                weight_vals = tl.store(weight_vals, [ki, kj], tl.load(weight_ptr + w_offset))\n        \n        accumulator = tl.zeros((BLOCK_H, BLOCK_W), dtype=tl.float32)\n        \n        # Unrolled kernel loops\n        for ki in tl.static_range(0, kernel_size):\n            for kj in tl.static_range(0, kernel_size):\n                input_i = h_offsets * stride_h + ki * dilation_h - padding_h\n                input_j = w_offsets * stride_w + kj * dilation_w - padding_w\n                \n                input_i_mask = (input_i >= 0) & (input_i < height)\n                input_j_mask = (input_j >= 0) & (input_j < width)\n                input_mask = input_i_mask[:, None] & input_j_mask[None, :] & block_mask\n                \n                input_ptr = x_ptr + pid_b * x_batch_stride + pid_c * x_channel_stride\n                input_val = tl.load(\n                    input_ptr + input_i[:, None] * x_height_stride + input_j[None, :] * x_width_stride,\n                    mask=input_mask,\n                    other=0.0\n                )\n                \n                # Multiply with preloaded weight\n                w_val = weight_vals[ki, kj]\n                accumulator += input_val * w_val\n        \n        output_ptr_base = output_ptr + pid_b * output_batch_stride + pid_c * output_channel_stride\n        output_offsets = h_offsets[:, None] * output_height_stride + w_offsets[None, :] * output_width_stride\n        tl.store(output_ptr_base + output_offsets, accumulator, mask=block_mask)\n                    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        stride = self.depthwise.stride\n        padding = self.depthwise.padding\n        dilation = self.depthwise.dilation\n        weight = self.depthwise.weight\n        bias = self.depthwise.bias\n        \n        batch, in_channels, height, width = x.shape\n        kernel_size = weight.shape[2]\n        \n        output_height = (height + 2 * padding[0] - dilation[0] * (kernel_size - 1) - 1) // stride[0] + 1\n        output_width = (width + 2 * padding[1] - dilation[1] * (kernel_size - 1) - 1) // stride[1] + 1\n        \n        x = x.contiguous()\n        weight = weight.contiguous()\n        output = torch.empty((batch, in_channels, output_height, output_width), device=x.device, dtype=x.dtype)\n        \n        grid = (batch, in_channels, triton.cdiv(output_height, 16) * triton.cdiv(output_width, 32))\n        \n        self._depthwise_conv_kernel[grid](\n            x, weight, output,\n            in_channels, height, width,\n            stride[0], stride[1],\n            padding[0], padding[1],\n            dilation[0], dilation[1],\n            kernel_size,\n            output_height, output_width,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            weight.stride(0), weight.stride(2), weight.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            BLOCK_H=16, BLOCK_W=32\n        )\n        \n        if bias is not None:\n            output += bias.reshape(1, -1, 1, 1)\n            \n        output = self.pointwise(output)\n        return output\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    @staticmethod\n    @triton.jit\n    def _depthwise_conv_kernel(\n        x_ptr,\n        weight_ptr,\n        output_ptr,\n        in_channels, height, width,\n        stride_h, stride_w,\n        padding_h, padding_w,\n        dilation_h, dilation_w,\n        kernel_size: tl.constexpr,\n        output_height, output_width,\n        x_batch_stride, x_channel_stride, x_height_stride, x_width_stride,\n        weight_channel_stride, weight_height_stride, weight_width_stride,\n        output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n        BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n    ):\n        pid_b = tl.program_id(0)\n        pid_c = tl.program_id(1)\n        pid_hw = tl.program_id(2)\n        \n        num_blocks_w = tl.cdiv(output_width, BLOCK_W)\n        pid_h = pid_hw // num_blocks_w\n        pid_w = pid_hw % num_blocks_w\n        \n        h_start = pid_h * BLOCK_H\n        w_start = pid_w * BLOCK_W\n        \n        h_offsets = h_start + tl.arange(0, BLOCK_H)\n        w_offsets = w_start + tl.arange(0, BLOCK_W)\n        \n        h_mask = (h_offsets < output_height)\n        w_mask = (w_offsets < output_width)\n        block_mask = h_mask[:, None] & w_mask[None, :]\n        \n        output_block = tl.zeros((BLOCK_H, BLOCK_W), dtype=tl.float32)\n        \n        # Preload weights for the entire kernel\n        weight_vector = tl.zeros((kernel_size, kernel_size), dtype=tl.float32)\n        for ki in tl.static_range(kernel_size):\n            for kj in tl.static_range(kernel_size):\n                weight_vector = weight_vector.to(weight_ptr.dtype.element_ty)  # Ensure correct type\n                weight_val = tl.load(\n                    weight_ptr + pid_c * weight_channel_stride + \n                    ki * weight_height_stride + kj * weight_width_stride\n                )\n                weight_vector = tl.where(\n                    (tl.full((1,), ki, dtype=tl.int32), tl.full((1,), kj, dtype=tl.int32)),\n                    weight_val,\n                    weight_vector\n                )\n        \n        # Unroll kernel loops for better optimization\n        for ki in tl.static_range(kernel_size):\n            for kj in tl.static_range(kernel_size):\n                input_i = h_offsets * stride_h + ki * dilation_h - padding_h\n                input_j = w_offsets * stride_w + kj * dilation_w - padding_w\n                \n                input_i_mask = (input_i >= 0) & (input_i < height)\n                input_j_mask = (input_j >= 0) & (input_j < width)\n                input_mask = input_i_mask[:, None] & input_j_mask[None, :] & block_mask\n                \n                input_ptr_base = x_ptr + pid_b * x_batch_stride + pid_c * x_channel_stride\n                input_vals = tl.load(\n                    input_ptr_base + input_i[:, None] * x_height_stride + input_j[None, :] * x_width_stride,\n                    mask=input_mask,\n                    other=0.0\n                )\n                \n                # Get preloaded weight value\n                weight_val = weight_vector[ki, kj]\n                output_block += input_vals * weight_val\n                \n        output_ptrs = output_ptr + pid_b * output_batch_stride + pid_c * output_channel_stride + \\\n                      h_offsets[:, None] * output_height_stride + w_offsets[None, :] * output_width_stride\n        tl.store(output_ptrs, output_block, mask=block_mask)\n                    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        stride = self.depthwise.stride\n        padding = self.depthwise.padding\n        dilation = self.depthwise.dilation\n        weight = self.depthwise.weight\n        bias = self.depthwise.bias\n        \n        batch, in_channels, height, width = x.shape\n        kernel_size = weight.shape[2]\n        \n        output_height = (height + 2 * padding[0] - dilation[0] * (kernel_size - 1) - 1) // stride[0] + 1\n        output_width = (width + 2 * padding[1] - dilation[1] * (kernel_size - 1) - 1) // stride[1] + 1\n        \n        x = x.contiguous()\n        weight = weight.contiguous()\n        output = torch.empty((batch, in_channels, output_height, output_width), device=x.device, dtype=x.dtype)\n        \n        num_blocks_h = triton.cdiv(output_height, 16)\n        num_blocks_w = triton.cdiv(output_width, 16)\n        grid = (batch, in_channels, num_blocks_h * num_blocks_w)\n        \n        self._depthwise_conv_kernel[grid](\n            x, weight, output,\n            in_channels, height, width,\n            stride[0], stride[1],\n            padding[0], padding[1],\n            dilation[0], dilation[1],\n            kernel_size,\n            output_height, output_width,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            weight.stride(0), weight.stride(2), weight.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            BLOCK_H=16, BLOCK_W=16\n        )\n        \n        if bias is not None:\n            output += bias.reshape(1, -1, 1, 1)\n            \n        output = self.pointwise(output)\n        return output\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef separable_conv_forward_kernel(\n    # Input tensor pointers\n    x_ptr,\n    depthwise_weight_ptr,\n    pointwise_weight_ptr,\n    bias_ptr,\n    output_ptr,\n    # Tensor dimensions\n    batch_size, in_channels, height, width,\n    kernel_size, stride, padding, dilation,\n    out_channels, height_out, width_out,\n    # Stride information\n    x_batch_stride, x_channel_stride, x_h_stride, x_w_stride,\n    dw_channel_stride, dw_kh_stride, dw_kw_stride,\n    pw_out_stride, pw_in_stride,\n    bias_stride,\n    output_batch_stride, output_channel_stride, output_h_stride, output_w_stride,\n    # Kernel configuration\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_elems = batch_size * out_channels * height_out * width_out\n    \n    # Exit if out of bounds\n    if pid * BLOCK_SIZE >= num_elems:\n        return\n    \n    # Generate offsets for this block\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < num_elems\n    \n    # Decompose offsets into output tensor indices using integer arithmetic\n    w_out = offsets % width_out\n    offsets = offsets // width_out\n    h_out = offsets % height_out\n    offsets = offsets // height_out\n    oc = offsets % out_channels\n    b = offsets // out_channels\n    \n    # Compute input window start positions\n    h_start = h_out * stride - padding\n    w_start = w_out * stride - padding\n    \n    # Initialize accumulators\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Optimized depthwise convolution\n    for kh in range(kernel_size):\n        h_in = h_start + kh * dilation\n        h_in_bound = (h_in >= 0) & (h_in < height)\n        \n        for kw in range(kernel_size):\n            w_in = w_start + kw * dilation\n            w_in_bound = (w_in >= 0) & (w_in < width)\n            in_bounds = h_in_bound & w_in_bound\n            \n            # Calculate input offsets once per kernel position\n            x_offset = (b * x_batch_stride + \n                       h_in * x_h_stride + \n                       w_in * x_w_stride)\n            \n            # Calculate depthwise weight offset\n            dw_offset = kh * dw_kh_stride + kw * dw_kw_stride\n            \n            # Process all channels in vectorized manner\n            for c in range(0, in_channels, BLOCK_SIZE):\n                c_offsets = c + tl.arange(0, BLOCK_SIZE)\n                c_mask = c_offsets < in_channels\n                \n                # Load input values for all channels\n                x_vals = tl.load(\n                    x_ptr + x_offset + c_offsets * x_channel_stride,\n                    mask=in_bounds & mask & c_mask,\n                    other=0.0\n                )\n                \n                # Load depthwise weights for all channels\n                dw_vals = tl.load(\n                    depthwise_weight_ptr + dw_offset + c_offsets * dw_channel_stride,\n                    mask=c_mask,\n                    other=0.0\n                )\n                \n                # Multiply-accumulate\n                acc += x_vals * dw_vals\n    \n    # Vectorized pointwise convolution\n    pointwise_acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for c in range(0, in_channels, BLOCK_SIZE):\n        c_offsets = c + tl.arange(0, BLOCK_SIZE)\n        c_mask = c_offsets < in_channels\n        \n        # Load pointwise weights\n        pw_vals = tl.load(\n            pointwise_weight_ptr + oc * pw_out_stride + c_offsets * pw_in_stride,\n            mask=c_mask,\n            other=0.0\n        )\n        pointwise_acc += acc * pw_vals\n    \n    # Add bias if present\n    if bias_ptr != 0:\n        bias_val = tl.load(bias_ptr + oc * bias_stride)\n        pointwise_acc += bias_val\n    \n    # Calculate output pointer offset\n    output_offset = (b * output_batch_stride + \n                    oc * output_channel_stride + \n                    h_out * output_h_stride + \n                    w_out * output_w_stride)\n    \n    # Store result\n    tl.store(output_ptr + output_offset, pointwise_acc, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, \n                 stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        \n        # Depthwise convolution parameters\n        self.depthwise_weight = nn.Parameter(\n            torch.empty(in_channels, 1, kernel_size, kernel_size)\n        )\n        # Pointwise convolution parameters\n        self.pointwise_weight = nn.Parameter(\n            torch.empty(out_channels, in_channels, 1, 1)\n        )\n        # Bias parameter\n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_channels))\n        else:\n            self.register_parameter('bias', None)\n            \n        # Initialize parameters\n        nn.init.kaiming_uniform_(self.depthwise_weight, a=math.sqrt(5))\n        nn.init.kaiming_uniform_(self.pointwise_weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.pointwise_weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, _, height, width = x.shape\n        \n        # Calculate output dimensions\n        height_out = (height + 2 * self.padding - \n                     self.dilation * (self.kernel_size - 1) - 1) // self.stride + 1\n        width_out = (width + 2 * self.padding - \n                    self.dilation * (self.kernel_size - 1) - 1) // self.stride + 1\n        \n        # Create output tensor\n        output = torch.empty(\n            (batch_size, self.out_channels, height_out, width_out),\n            device=x.device, dtype=x.dtype\n        )\n        \n        # Compute total number of output elements\n        total_elems = batch_size * self.out_channels * height_out * width_out\n        \n        # Setup kernel grid\n        grid = lambda meta: (triton.cdiv(total_elems, meta['BLOCK_SIZE']),)\n        \n        # Get tensor strides\n        x_strides = (x.stride(0), x.stride(1), x.stride(2), x.stride(3))\n        dw_strides = (\n            self.depthwise_weight.stride(0),\n            self.depthwise_weight.stride(2),\n            self.depthwise_weight.stride(3)\n        )\n        pw_strides = (\n            self.pointwise_weight.stride(0),\n            self.pointwise_weight.stride(1)\n        )\n        bias_stride = self.bias.stride(0) if self.bias is not None else 0\n        output_strides = (\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3)\n        )\n        \n        # Kernel parameters\n        kernel_params = [\n            x, self.depthwise_weight, self.pointwise_weight,\n            self.bias.data_ptr() if self.bias is not None else 0, output,\n            # Dimensions\n            batch_size, self.in_channels, height, width,\n            self.kernel_size, self.stride, self.padding, self.dilation,\n            self.out_channels, height_out, width_out,\n            # Strides\n            *x_strides,\n            *dw_strides,\n            *pw_strides,\n            bias_stride,\n            *output_strides\n        ]\n        \n        # Launch kernel with fixed BLOCK_SIZE as keyword argument\n        separable_conv_forward_kernel[grid](*kernel_params, BLOCK_SIZE=1024)\n        \n        return output\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef pointwise_conv_kernel(\n    input_ptr, weight_ptr, output_ptr, bias_ptr,\n    in_channels, out_channels, M,\n    stride_im, stride_ic,\n    stride_wk, stride_wn,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(out_channels, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    input_ptrs = input_ptr + offs_m[:, None] * stride_im + offs_k[None, :] * stride_ic\n    weight_ptrs = weight_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(in_channels, BLOCK_SIZE_K)):\n        k_remaining = in_channels - k * BLOCK_SIZE_K\n        mask_k = offs_k < k_remaining\n        \n        a = tl.load(input_ptrs, \n                   mask=mask_k[None, :] & (offs_m[:, None] < M), \n                   other=0.0)\n        b = tl.load(weight_ptrs, \n                   mask=mask_k[:, None] & (offs_n[None, :] < out_channels), \n                   other=0.0)\n        \n        accumulator += tl.dot(a, b, allow_tf32=True)\n        input_ptrs += BLOCK_SIZE_K * stride_ic\n        weight_ptrs += BLOCK_SIZE_K * stride_wk\n\n    if bias_ptr != 0:\n        offs_n_int64 = offs_n.to(tl.int64)  # Cast to int64 for pointer arithmetic\n        bias = tl.load(bias_ptr + offs_n_int64, mask=offs_n < out_channels, other=0.0)\n        accumulator += bias[None, :]\n\n    offs_m_mask = offs_m[:, None] < M\n    offs_n_mask = offs_n[None, :] < out_channels\n    output_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    tl.store(output_ptrs, accumulator, mask=offs_m_mask & offs_n_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.depthwise(x)\n        batch_size, in_channels, height, width = x.shape\n        \n        x_reshaped = x.permute(0, 2, 3, 1).reshape(-1, in_channels).contiguous()\n        M, K = x_reshaped.shape\n        N = self.pointwise.weight.shape[0]\n        \n        weight_t = self.pointwise.weight.view(N, K).t().contiguous()\n        y = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_M = 128\n        BLOCK_SIZE_N = 64\n        BLOCK_SIZE_K = 16\n        grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n        \n        bias_ptr = self.pointwise.bias.data_ptr() if self.pointwise.bias is not None else 0\n        \n        pointwise_conv_kernel[grid](\n            x_reshaped, weight_t, y, bias_ptr,\n            K, N, M,\n            x_reshaped.stride(0), x_reshaped.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            y.stride(0), y.stride(1),\n            BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K\n        )\n        \n        output = y.view(batch_size, height, width, N).permute(0, 3, 1, 2)\n        return output\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a current program that fails with a compilation error in the Triton kernel for the pointwise convolution.\n The error is: \"ValueError: Unsupported ptr type <['64'], int64> in `tl.load`\"\n This occurs in the line: `bias = tl.load(bias_ptr + offs_n_int64, mask=offs_n < out_channels, other=0.0)`\n\n The issue is that we are casting `offs_n` to `tl.int64` for pointer arithmetic, but Triton expects pointer offsets to be of type `int32`.\n We should instead cast the `offs_n` to `tl.int32` if necessary, but note that Triton's `tl.arange` by default returns `int32`.\n However, the error says we are using `int64` for the pointer offset.\n\n Looking at the code: \n     offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)   -> This is int32 because BLOCK_SIZE_N is a constant and pid_n is int32.\n Then we do: offs_n_int64 = offs_n.to(tl.int64)   -> This casts to int64.\n\n But the `tl.load` expects the pointer offset to be int32. So we should avoid the cast to int64.\n\n However, note that the bias tensor is of size `out_channels` (a 1D tensor). We can safely use `offs_n` as int32 if the index does not exceed 2^31.\n Since the number of output channels is not huge, we can use int32.\n\n Therefore, we can remove the cast to int64 and use `offs_n` directly.\n\n Let's change the line:\n     bias = tl.load(bias_ptr + offs_n_int64, mask=offs_n < out_channels, other=0.0)\n to:\n     bias = tl.load(bias_ptr + offs_n, mask=offs_n < out_channels, other=0.0)\n\n Also, note that the bias pointer is passed as an integer (data_ptr) and then we add an offset of `offs_n` (which is in elements). \n This is acceptable because the bias tensor is contiguous.\n\n Additionally, we must check the mask: `mask=offs_n < out_channels` is correct.\n\n We are also going to make a few other improvements for performance:\n\n 1. The current pointwise convolution kernel uses a dot product over the channel dimension (K). \n    We have set BLOCK_SIZE_M=128, BLOCK_SIZE_N=64, BLOCK_SIZE_K=16. \n    We might consider adjusting these block sizes to better utilize the H100 tensor cores.\n\n 2. The H100 has tensor cores that support FP16 and BF16 with a 16x16x16 tile size. However, we are using allow_tf32=True which uses 16x16x16 for FP32 accumulation? \n    Actually, allow_tf32 is for using TF32 on the tensor cores. But our data is in FP32? \n\n    Let me clarify: the input tensor `x` is the output of depthwise convolution which is in FP32? \n    The original problem does not specify, but the reference PyTorch code is in FP32. \n\n    However, note that the Triton kernel is in FP32. We are using `allow_tf32=True` which is acceptable for FP32 inputs.\n\n 3. We might consider increasing BLOCK_SIZE_K to 32 or 64 to increase the arithmetic intensity.\n\n 4. We also note that the grid is set as (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),). \n    This is a 1D grid. We are using a 2D kernel launch with a 1D grid, which is fine.\n\n 5. We are using a fixed set of block sizes. We can make the kernel tunable by using `triton.autotune` in the future, but for now we fix.\n\n Considering the above, we will:\n   a) Fix the bias loading by removing the cast to int64.\n   b) Adjust the block sizes for better performance. We choose:\n        BLOCK_SIZE_M = 64   (to fit more blocks in the SM and because 128 might be too large for register usage)\n        BLOCK_SIZE_N = 64\n        BLOCK_SIZE_K = 32\n\n   c) Also, we note that the kernel does not handle the bias correctly when the bias pointer is zero (meaning no bias). \n      We have an if condition: `if bias_ptr != 0: ...` which is correct.\n\n   d) We must ensure that the mask for loading the bias is correct: `mask=offs_n < out_channels`\n\n 6. Additionally, we must check the mask for the input and weight loads. They are:\n        a: mask = mask_k[None, :] & (offs_m[:, None] < M)\n        b: mask = mask_k[:, None] & (offs_n[None, :] < out_channels)\n\n    But note: in the weight load, the condition is `offs_n[None, :] < out_channels` but `offs_n` is of size BLOCK_SIZE_N and we are at pid_n, so we are only loading a block of the weight matrix. \n    The condition should be `(offs_n[None, :] < out_channels)` but note that `out_channels` is the total number of output channels, and we are only loading a block of columns. \n    This condition is correct because we are masking based on the global index.\n\n However, we are passing `out_channels` as the second dimension of the weight matrix? Actually, the weight matrix in the pointwise convolution is of shape [K, N] (because we transposed the pointwise weight to be [K, N]). \n So the condition for the weight should be `(offs_n[None, :] < N)`, but we passed `out_channels` as the argument for the kernel and set `N = out_channels`. So it's consistent.\n\n Let's change the kernel:\n\n   - Remove the cast to int64 for the bias offset.\n   - Adjust the block sizes.\n\n But note: the current program only does the pointwise convolution in Triton. The depthwise convolution is done by PyTorch (self.depthwise(x)). \n This might be the reason for the poor performance? We are not accelerating the depthwise part.\n\n However, the task is to rewrite the current Triton kernel for the pointwise convolution to fix the compilation error and then improve performance.\n\n We are going to change the kernel for the pointwise convolution.\n\n But note: the problem says that the previous version had a compilation error. So we must fix that first.\n\n We'll write the fixed and optimized pointwise convolution kernel.\n\n However, note that the depthwise convolution is done by PyTorch and then we do the pointwise in Triton. \n We might consider fusing the two, but the current structure is:\n\n   x = self.depthwise(x)   # This is done by PyTorch, which might be slow.\n   ... then pointwise in Triton.\n\n To get speedup, we must accelerate the depthwise convolution as well? But the problem is that the current program only provides a Triton kernel for the pointwise.\n\n Since the problem says that the depthwise is done by PyTorch, and we are only to rewrite the Triton part, we focus on the pointwise.\n\n Alternatively, we can try to accelerate the depthwise convolution too? But the problem does not specify. \n However, looking at the top performing programs, they have Triton kernels for depthwise too.\n\n But note: the current program (the one we are rewriting) only has a Triton kernel for the pointwise convolution.\n\n We are allowed to change the entire program? The instructions say: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\"\n\n But the current program has a depthwise convolution done by PyTorch. We are not allowed to change the signature of the ModelNew class? \n However, we are allowed to change the entire block. We can look at the top performing programs and see that they implement both depthwise and pointwise in Triton.\n\n Given that the current program is too slow (because depthwise is in PyTorch) and we are allowed to change the entire block, we can take inspiration from the top programs.\n\n We choose the top performing program that has both depthwise and pointwise in Triton and that has the highest score (Program 1 in the top performers).\n\n But note: the current program we are given is only for the pointwise. We are allowed to rewrite the entire block.\n\n We are going to replace the entire ModelNew class with one that does both depthwise and pointwise in Triton.\n\n However, note that the problem says: \"Provide the complete new program code.\" and we are allowed to change the entire EVOLVE-BLOCK.\n\n We choose the following from the top performers (Program 1) because it has a speed_up of 0.273 (which is the best among the top performers):\n\n   Program 1 (Score: 20.0000) with speed_up=0.273\n\n But note: the reference time is 0.0541 ms and the custom time is 0.198 ms -> that's a slowdown. \n However, the other top programs are even slower (0.001 and 0.093). So this is the best.\n\n Alternatively, we can take the program that has the pointwise done with a matrix multiplication and the depthwise in Triton (Program 3 in the top performers) which has a speed_up of 0.093 (which is worse than 0.273).\n\n So we choose the Program 1:\n\n   It has a depthwise Triton kernel and then uses self.pointwise (PyTorch) for the pointwise.\n\n   However, note that in Program 1 the pointwise convolution is done by PyTorch, so we are not accelerating the pointwise? \n   But the entire operation is 0.198 ms vs the reference 0.0541 ms -> still a slowdown.\n\n We must try to accelerate both.\n\n Alternatively, we can take the program that uses two Triton kernels (one for depthwise and one for pointwise) and that has a speed_up of 0.093 (Program 3). \n But that is slower than Program 1.\n\n Given the complexity and the fact that we are in an iterative process, we decide to fix the current pointwise kernel and then try to accelerate the depthwise as well by writing a Triton kernel for it.\n\n However, note that the current program (the one we are given) does not have a Triton kernel for depthwise. \n We are going to rewrite the ModelNew.forward to use Triton for depthwise and then use the fixed pointwise kernel.\n\n Steps:\n\n  1. Write a Triton kernel for depthwise convolution (we can take from the top performers).\n  2. Fix the pointwise kernel by removing the int64 cast and adjusting block sizes.\n\n We are going to combine the two.\n\n We choose the depthwise kernel from Program 1 (the top performer) and then use our fixed pointwise kernel for the pointwise convolution.\n\n But note: the pointwise kernel we have in the current program is for a 2D matrix multiplication (after reshaping). \n We can use that.\n\n Alternatively, we can use the pointwise convolution as a 1x1 convolution with a Triton kernel that is designed for it? \n But the current pointwise kernel is a general matrix multiplication.\n\n We decide to do:\n\n   depthwise: Triton kernel (from Program 1)\n   pointwise: our fixed pointwise kernel (with the int64 cast removed and block sizes adjusted)\n\n But note: the pointwise kernel expects the input to be reshaped to [batch*height*width, in_channels]. \n We can do that after the depthwise.\n\n Let's write the new ModelNew:\n\n   We will keep the same __init__.\n\n   In forward:\n        We will run the depthwise convolution with a Triton kernel (instead of self.depthwise(x)).\n        Then we reshape the result and run the pointwise kernel.\n\n We take the depthwise kernel from Program 1 (the first top program) and adapt it.\n\n However, note that the Program 1 depthwise kernel does not include the bias. We have to handle bias separately.\n\n Steps for the depthwise kernel:\n\n   - We need to compute the output shape.\n   - We need to set the grid and launch parameters.\n\n We are going to write:\n\n   depthwise_out = ...  # computed by Triton kernel\n\n   then add bias if present.\n\n   then reshape and do pointwise.\n\n We are going to write two Triton kernels: one for depthwise and one for pointwise.\n\n We are going to put both kernels inside the ModelNew class as static methods.\n\n We are going to fix the pointwise kernel by removing the int64 cast and adjusting the block sizes.\n\n Let's write the code accordingly.\n\n However, note: the current program we are given has a pointwise kernel that we are going to fix and reuse.\n\n But the depthwise kernel we are going to write is from Program 1.\n\n We note that the depthwise kernel in Program 1 has the following signature:\n\n    @triton.jit\n    def _depthwise_conv_kernel(\n        x_ptr,\n        weight_ptr,\n        output_ptr,\n        in_channels, height, width,\n        stride_h, stride_w,\n        padding_h, padding_w,\n        dilation_h, dilation_w,\n        kernel_size,\n        output_height, output_width,\n        x_batch_stride, x_channel_stride, x_height_stride, x_width_stride,\n        weight_channel_stride, weight_height_stride, weight_width_stride,\n        output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n        BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n    ):\n\n We are going to use that.\n\n We also note that in Program 1, the grid is set as:\n\n        grid = (batch, in_channels, triton.cdiv(output_height, 16) * triton.cdiv(output_width, 16))\n\n and the block sizes are 16x16.\n\n We will use that.\n\n Then, after depthwise, we reshape and run the fixed pointwise kernel.\n\n We are going to adjust the pointwise kernel block sizes to:\n\n        BLOCK_SIZE_M = 64\n        BLOCK_SIZE_N = 64\n        BLOCK_SIZE_K = 32\n\n and remove the int64 cast for the bias.\n\n We are going to write the entire ModelNew class.\n\n Let's do it.\n\n IMPORTANT: We must not change the interface of ModelNew (the __init__ and forward signature).\n\n We are going to write:\n\n   class ModelNew(nn.Module):\n        def __init__(...):  # same as before\n\n        @staticmethod\n        @triton.jit\n        def _depthwise_conv_kernel(...):   # taken from Program 1\n\n        @staticmethod\n        @triton.jit\n        def _pointwise_conv_kernel(...):   # fixed version of the current pointwise kernel (without int64 cast)\n\n        def forward(...): \n            # compute depthwise with Triton kernel\n            # then pointwise with Triton kernel\n\n We hope this will fix the compilation error and improve performance.\n\n However, note that the current depthwise kernel in Program 1 does not handle bias. \n We will handle bias by adding it after the depthwise kernel if present.\n\n Let's code accordingly.\n\n We are going to write the entire block.\n\n Note: The test code at the bottom must be preserved.\n\n Let's write the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef pointwise_conv_kernel(\n    input_ptr, weight_ptr, output_ptr, bias_ptr,\n    in_channels, out_channels, M,\n    stride_im, stride_ic,\n    stride_wk, stride_wn,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(out_channels, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    input_ptrs = input_ptr + offs_m[:, None] * stride_im + offs_k[None, :] * stride_ic\n    weight_ptrs = weight_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(in_channels, BLOCK_SIZE_K)):\n        k_remaining = in_channels - k * BLOCK_SIZE_K\n        mask_k = offs_k < k_remaining\n        \n        a = tl.load(input_ptrs, \n                   mask=mask_k[None, :] & (offs_m[:, None] < M), \n                   other=0.0)\n        b = tl.load(weight_ptrs, \n                   mask=mask_k[:, None] & (offs_n[None, :] < out_channels), \n                   other=0.0)\n        \n        accumulator += tl.dot(a, b, allow_tf32=True)\n        input_ptrs += BLOCK_SIZE_K * stride_ic\n        weight_ptrs += BLOCK_SIZE_K * stride_wk\n\n    if bias_ptr != 0:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < out_channels, other=0.0)\n        accumulator += bias[None, :]\n\n    offs_m_mask = offs_m[:, None] < M\n    offs_n_mask = offs_n[None, :] < out_channels\n    output_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    tl.store(output_ptrs, accumulator, mask=offs_m_mask & offs_n_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.depthwise(x)\n        batch_size, in_channels, height, width = x.shape\n        \n        x_reshaped = x.permute(0, 2, 3, 1).reshape(-1, in_channels).contiguous()\n        M, K = x_reshaped.shape\n        N = self.pointwise.weight.shape[0]\n        \n        weight_t = self.pointwise.weight.view(N, K).t().contiguous()\n        y = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_M = 64\n        BLOCK_SIZE_N = 64\n        BLOCK_SIZE_K = 32\n        grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n        \n        bias_ptr = self.pointwise.bias.data_ptr() if self.pointwise.bias is not None else 0\n        \n        pointwise_conv_kernel[grid](\n            x_reshaped, weight_t, y, bias_ptr,\n            K, N, M,\n            x_reshaped.stride(0), x_reshaped.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            y.stride(0), y.stride(1),\n            BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K\n        )\n        \n        output = y.view(batch_size, height, width, N).permute(0, 3, 1, 2)\n        return output\n\n# Test code\nbatch_size = 16\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nwidth = 256\nheight = 256\nstride = 1\npadding = 0\ndilation = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, dilation]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "39528397-b0df-4b12-a916-beaf54c9a8e4", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    batch_size,\n    input_size,\n    hidden_size,\n    stride_x0, stride_x1,\n    stride_w0, stride_w1,\n    stride_b,\n    stride_out0, stride_out1,\n    BLOCK_K: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    \n    # Load input row\n    x_offsets = pid0 * stride_x0 + tl.arange(0, BLOCK_K)\n    x_mask = tl.arange(0, BLOCK_K) < input_size\n    x_row = tl.load(x_ptr + x_offsets, mask=x_mask, other=0.0)\n    \n    row_sum = 0.0\n    # Process hidden units in chunks of BLOCK_H\n    for j in range(0, hidden_size, BLOCK_H):\n        # Create offsets for current hidden block\n        h_offsets = j + tl.arange(0, BLOCK_H)\n        mask_h = h_offsets < hidden_size\n        \n        # Load weight block for current chunk\n        w_ptrs = w_ptr + h_offsets[:, None] * stride_w0 + tl.arange(0, BLOCK_K)[None, :] * stride_w1\n        w_mask = mask_h[:, None] & x_mask[None, :]\n        w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        # Load bias block for current chunk\n        b_ptrs = b_ptr + h_offsets * stride_b\n        b_block = tl.load(b_ptrs, mask=mask_h, other=0.0)\n        \n        # Compute dot product for current chunk\n        dot_block = tl.sum(x_row[None, :] * w_block, axis=1) + b_block\n        \n        # Apply sigmoid and accumulate\n        sig_block = tl.sigmoid(dot_block)\n        sig_block_masked = tl.where(mask_h, sig_block, 0.0)\n        row_sum += tl.sum(sig_block_masked)\n    \n    # Store result\n    tl.store(output_ptr + pid0 * stride_out0, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        x = x.contiguous()\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        output = torch.empty(x.size(0), 1, device=x.device, dtype=x.dtype)\n        \n        # Compute block sizes\n        BLOCK_K = triton.next_power_of_2(self.input_size)\n        BLOCK_H = 8  # Fixed chunk size for hidden dimension\n        \n        # Launch fused kernel\n        grid = (x.size(0),)\n        fused_kernel[grid](\n            x,\n            weight,\n            bias,\n            output,\n            x.size(0),\n            self.input_size,\n            self.hidden_size,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0), output.stride(1),\n            BLOCK_K=BLOCK_K,\n            BLOCK_H=BLOCK_H\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "fc341538-506b-49a2-8be2-844c3aa8daeb", "generation": 4, "timestamp": 1754576600.9665294, "iteration_found": 26, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.758, "runtime_ms": 0.0637, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0483, "speed_up": 0.758, "custom_timing": 0.0637}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.827, "runtime_ms": 0.0584, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0483, "speed_up": 0.827, "custom_timing": 0.0584}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.8270\n- runtime_ms: 0.0584\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0483, 'speed_up': 0.827, 'custom_timing': 0.0584}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: runtime_ms. Consider continuing with similar changes.\n- Metrics showing regression: speed_up. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.8330, runtime_ms: 0.0580, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0483, 'speed_up': 0.833, 'custom_timing': 0.058}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.8810, runtime_ms: 0.0548, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0483, 'speed_up': 0.881, 'custom_timing': 0.0548}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.8270, runtime_ms: 0.0584, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0483, 'speed_up': 0.827, 'custom_timing': 0.0584}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    batch_size,\n    input_size,\n    hidden_size,\n    stride_x0, stride_x1,\n    stride_w0, stride_w1,\n    stride_b,\n    stride_out0, stride_out1,\n    BLOCK_K: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    \n    # Load input row\n    x_offsets = pid0 * stride_x0 + tl.arange(0, BLOCK_K)\n    x_mask = tl.arange(0, BLOCK_K) < input_size\n    x_row = tl.load(x_ptr + x_offsets, mask=x_mask, other=0.0)\n    \n    # Load weight block\n    h_offsets = tl.arange(0, BLOCK_H)\n    w_ptrs = w_ptr + h_offsets[:, None] * stride_w0 + tl.arange(0, BLOCK_K)[None, :] * stride_w1\n    w_mask = (h_offsets[:, None] < hidden_size) & (tl.arange(0, BLOCK_K)[None, :] < input_size)\n    w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n    \n    # Load bias block\n    b_ptrs = b_ptr + h_offsets * stride_b\n    b_mask = h_offsets < hidden_size\n    b_vec = tl.load(b_ptrs, mask=b_mask, other=0.0)\n    \n    # Compute dot product\n    dot_vec = tl.sum(x_row[None, :] * w_block, axis=1)\n    dot_vec += b_vec\n    \n    # Apply sigmoid and accumulate\n    sig_vec = tl.sigmoid(dot_vec)\n    sig_vec_masked = tl.where(b_mask, sig_vec, 0.0)\n    row_sum = tl.sum(sig_vec_masked, axis=0)\n    \n    # Store result\n    tl.store(output_ptr + pid0 * stride_out0, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        x = x.contiguous()\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        output = torch.empty(x.size(0), 1, device=x.device, dtype=x.dtype)\n        \n        # Compute block sizes\n        BLOCK_K = triton.next_power_of_2(self.input_size)\n        BLOCK_H = triton.next_power_of_2(self.hidden_size)\n        \n        # Launch fused kernel\n        grid = (x.size(0),)\n        fused_kernel[grid](\n            x,\n            weight,\n            bias,\n            output,\n            x.size(0),\n            self.input_size,\n            self.hidden_size,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0), output.stride(1),\n            BLOCK_K=BLOCK_K,\n            BLOCK_H=BLOCK_H,\n            num_warps=4\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.8270), Performs well on runtime_ms (0.0584), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0483, 'speed_up': 0.827, 'custom_timing': 0.0584})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, W_ptr, b_ptr, output_ptr,\n    input_size, hidden_size,\n    stride_xb, stride_w0, stride_b, stride_outb,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # batch index\n    \n    # Create offset array using compile-time constant BLOCK_SIZE\n    offs = tl.arange(0, BLOCK_SIZE)\n    # Compute base pointer for current batch\n    x_batch_ptr = x_ptr + pid0 * stride_xb\n    # Create mask to prevent out-of-bound accesses\n    mask = offs < input_size\n    # Load input row with boundary check\n    x_row = tl.load(x_batch_ptr + offs, mask=mask, other=0.0)\n    \n    total = 0.0\n    for j in range(0, hidden_size):\n        # Load bias for neuron j\n        bias_j = tl.load(b_ptr + j)\n        # Load weights for neuron j with same boundary mask\n        w_row = tl.load(W_ptr + j * stride_w0 + offs, mask=mask, other=0.0)\n        # Compute dot product\n        dot = tl.sum(x_row * w_row)\n        # Compute linear output + bias\n        linear_out = bias_j + dot\n        # Apply sigmoid\n        sigmoid_val = 1.0 / (1.0 + tl.exp(-linear_out))\n        total += sigmoid_val\n    \n    # Store final result\n    tl.store(output_ptr + pid0 * stride_outb, total)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        # Compute next power-of-two block size\n        self.BLOCK_SIZE = 1\n        while self.BLOCK_SIZE < input_size:\n            self.BLOCK_SIZE *= 2\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=torch.float32)\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        W = self.linear.weight.contiguous()\n        b = self.linear.bias.contiguous()\n        \n        # Kernel configuration\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, W, b, output,\n            self.input_size, self.hidden_size,\n            x.stride(0), W.stride(0), \n            0, output.stride(0),\n            BLOCK_SIZE=self.BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.8810), Performs well on runtime_ms (0.0548), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0483, 'speed_up': 0.881, 'custom_timing': 0.0548})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr, w_ptr, b_ptr, out_ptr,\n    batch_size, input_size, hidden_size,\n    stride_x0, stride_x1,\n    stride_w0, stride_w1,\n    stride_b,\n    stride_out0,\n    BLOCK_IN: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n\n    # Load input row\n    x_offset = pid * stride_x0\n    x_offsets = x_offset + tl.arange(0, BLOCK_IN) * stride_x1\n    x_mask = tl.arange(0, BLOCK_IN) < input_size\n    x_vals = tl.load(x_ptr + x_offsets, mask=x_mask, other=0.0)\n    \n    acc = 0.0\n    # Compute linear + sigmoid + reduction\n    for j in range(hidden_size):\n        # Load weights and bias\n        w_start = j * stride_w0\n        w_offsets = w_start + tl.arange(0, BLOCK_IN) * stride_w1\n        w_vals = tl.load(w_ptr + w_offsets, mask=x_mask, other=0.0)\n        b_val = tl.load(b_ptr + j * stride_b)\n        \n        # Compute dot product\n        dot = tl.sum(x_vals * w_vals)\n        # Linear + Sigmoid\n        z = dot + b_val\n        s = 1.0 / (1.0 + tl.exp(-z))\n        acc += s\n    \n    # Store reduced result\n    tl.store(out_ptr + pid * stride_out0, acc)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        weight = self.linear.weight\n        bias = self.linear.bias\n        \n        # Prepare contiguous inputs\n        x = x.contiguous()\n        weight = weight.contiguous()\n        bias = bias.contiguous()\n        \n        # Allocate output\n        output = torch.empty(batch_size, 1, device=x.device, dtype=x.dtype)\n        \n        # Launch kernel\n        grid = (batch_size,)\n        fused_forward_kernel[grid](\n            x, weight, bias, output,\n            batch_size, self.input_size, self.hidden_size,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0),\n            BLOCK_IN=triton.next_power_of_2(self.input_size),\n        )\n        \n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.8330), Performs well on runtime_ms (0.0580), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0483, 'speed_up': 0.833, 'custom_timing': 0.058})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, W_ptr, b_ptr, output_ptr,\n    input_size, hidden_size,\n    stride_xb, stride_w0, stride_b, stride_outb,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # batch index\n    \n    # Load input row\n    offs_x = tl.arange(0, input_size)\n    x_row = tl.load(x_ptr + pid0 * stride_xb + offs_x)\n    \n    total = 0.0\n    for j in range(0, hidden_size):\n        # Load bias for neuron j\n        bias_j = tl.load(b_ptr + j)\n        # Load weights for neuron j\n        offs_w = tl.arange(0, input_size)\n        w_row = tl.load(W_ptr + j * stride_w0 + offs_w)\n        # Compute dot product\n        dot = tl.sum(x_row * w_row)\n        # Compute linear output + bias\n        linear_out = bias_j + dot\n        # Apply sigmoid\n        sigmoid_val = 1.0 / (1.0 + tl.exp(-linear_out))\n        total += sigmoid_val\n    \n    # Store final result\n    tl.store(output_ptr + pid0 * stride_outb, total)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=torch.float32)\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        W = self.linear.weight.contiguous()\n        b = self.linear.bias.contiguous()\n        \n        # Kernel configuration\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, W, b, output,\n            self.input_size, self.hidden_size,\n            x.stride(0), W.stride(0), \n            0, output.stride(0),\n            BLOCK_SIZE=32\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x_batch,\n    stride_x_feature,\n    stride_weight_hidden,\n    stride_weight_feature,\n    stride_bias,\n    stride_out_batch,\n    BLOCK_INPUT: tl.constexpr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Load input row with vectorized access\n    input_start = pid * stride_x_batch\n    input_offsets = input_start + tl.arange(0, BLOCK_INPUT) * stride_x_feature\n    mask = tl.arange(0, BLOCK_INPUT) < input_size\n    x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)\n    \n    row_sum = 0.0\n    \n    # Process 4 hidden units per iteration\n    for j in tl.static_range(0, hidden_size//4):\n        j0 = j * 4\n        j1 = j0 + 1\n        j2 = j0 + 2\n        j3 = j0 + 3\n\n        # Load 4 weight rows\n        w_start0 = j0 * stride_weight_hidden\n        w_offsets0 = w_start0 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row0 = tl.load(weight_ptr + w_offsets0, mask=mask, other=0.0)\n        \n        w_start1 = j1 * stride_weight_hidden\n        w_offsets1 = w_start1 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row1 = tl.load(weight_ptr + w_offsets1, mask=mask, other=0.0)\n        \n        w_start2 = j2 * stride_weight_hidden\n        w_offsets2 = w_start2 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row2 = tl.load(weight_ptr + w_offsets2, mask=mask, other=0.0)\n        \n        w_start3 = j3 * stride_weight_hidden\n        w_offsets3 = w_start3 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row3 = tl.load(weight_ptr + w_offsets3, mask=mask, other=0.0)\n        \n        # Load 4 bias elements\n        b0 = tl.load(bias_ptr + j0 * stride_bias)\n        b1 = tl.load(bias_ptr + j1 * stride_bias)\n        b2 = tl.load(bias_ptr + j2 * stride_bias)\n        b3 = tl.load(bias_ptr + j3 * stride_bias)\n        \n        # Compute 4 dot products + biases\n        dot0 = tl.sum(x_row * w_row0) + b0\n        dot1 = tl.sum(x_row * w_row1) + b1\n        dot2 = tl.sum(x_row * w_row2) + b2\n        dot3 = tl.sum(x_row * w_row3) + b3\n        \n        # Apply sigmoid to all 4 results\n        s0 = 1.0 / (1.0 + tl.exp(-dot0))\n        s1 = 1.0 / (1.0 + tl.exp(-dot1))\n        s2 = 1.0 / (1.0 + tl.exp(-dot2))\n        s3 = 1.0 / (1.0 + tl.exp(-dot3))\n        \n        # Accumulate all 4 sigmoid outputs\n        row_sum += s0 + s1 + s2 + s3\n    \n    # Store reduced result\n    output_offset = pid * stride_out_batch\n    tl.store(output_ptr + output_offset, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=x.dtype)\n        \n        # Ensure tensor memory layout is contiguous\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        # Calculate next power-of-two block size\n        BLOCK_INPUT = triton.next_power_of_2(self.input_size)\n        \n        # Launch kernel\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, weight, bias, output,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0),\n            BLOCK_INPUT,\n            self.input_size,\n            self.hidden_size,\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, W_ptr, b_ptr, output_ptr,\n    input_size, hidden_size,\n    stride_xb, stride_w0, stride_b, stride_outb,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # batch index\n    \n    # Create offset array using compile-time constant BLOCK_SIZE\n    offs = tl.arange(0, BLOCK_SIZE)\n    # Compute base pointer for current batch\n    x_batch_ptr = x_ptr + pid0 * stride_xb\n    # Create mask to prevent out-of-bound accesses\n    mask = offs < input_size\n    # Load input row with boundary check\n    x_row = tl.load(x_batch_ptr + offs, mask=mask, other=0.0)\n    \n    total = 0.0\n    for j in range(0, hidden_size):\n        # Load bias for neuron j\n        bias_j = tl.load(b_ptr + j)\n        # Load weights for neuron j with same boundary mask\n        w_row = tl.load(W_ptr + j * stride_w0 + offs, mask=mask, other=0.0)\n        # Compute dot product\n        dot = tl.sum(x_row * w_row)\n        # Compute linear output + bias\n        linear_out = bias_j + dot\n        # Apply sigmoid\n        sigmoid_val = 1.0 / (1.0 + tl.exp(-linear_out))\n        total += sigmoid_val\n    \n    # Store final result\n    tl.store(output_ptr + pid0 * stride_outb, total)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        # Compute next power-of-two block size\n        self.BLOCK_SIZE = 1\n        while self.BLOCK_SIZE < input_size:\n            self.BLOCK_SIZE *= 2\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=torch.float32)\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        W = self.linear.weight.contiguous()\n        b = self.linear.bias.contiguous()\n        \n        # Kernel configuration\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, W, b, output,\n            self.input_size, self.hidden_size,\n            x.stride(0), W.stride(0), \n            0, output.stride(0),\n            BLOCK_SIZE=self.BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x_batch,\n    stride_x_feature,\n    stride_weight_hidden,\n    stride_weight_feature,\n    stride_bias,\n    stride_out_batch,\n    BLOCK_INPUT: tl.constexpr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Load input row with vectorized access\n    input_start = pid * stride_x_batch\n    input_offsets = input_start + tl.arange(0, BLOCK_INPUT) * stride_x_feature\n    mask = tl.arange(0, BLOCK_INPUT) < input_size\n    x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)\n    \n    row_sum = 0.0\n    \n    # Process hidden units in groups of 4\n    n_iters = hidden_size // 4\n    for i in tl.static_range(n_iters):\n        j = i * 4\n        \n        # Load 4 weight rows with vectorized access\n        w_start0 = j * stride_weight_hidden\n        w_offsets0 = w_start0 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row0 = tl.load(weight_ptr + w_offsets0, mask=mask, other=0.0)\n        \n        w_start1 = (j+1) * stride_weight_hidden\n        w_offsets1 = w_start1 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row1 = tl.load(weight_ptr + w_offsets1, mask=mask, other=0.0)\n        \n        w_start2 = (j+2) * stride_weight_hidden\n        w_offsets2 = w_start2 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row2 = tl.load(weight_ptr + w_offsets2, mask=mask, other=0.0)\n        \n        w_start3 = (j+3) * stride_weight_hidden\n        w_offsets3 = w_start3 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row3 = tl.load(weight_ptr + w_offsets3, mask=mask, other=0.0)\n        \n        # Load 4 bias elements\n        b0 = tl.load(bias_ptr + (j+0) * stride_bias)\n        b1 = tl.load(bias_ptr + (j+1) * stride_bias)\n        b2 = tl.load(bias_ptr + (j+2) * stride_bias)\n        b3 = tl.load(bias_ptr + (j+3) * stride_bias)\n        \n        # Compute dot products + bias\n        dot0 = tl.sum(x_row * w_row0) + b0\n        dot1 = tl.sum(x_row * w_row1) + b1\n        dot2 = tl.sum(x_row * w_row2) + b2\n        dot3 = tl.sum(x_row * w_row3) + b3\n        \n        # Sigmoid activation and accumulation\n        s0 = 1.0 / (1.0 + tl.exp(-dot0))\n        s1 = 1.0 / (1.0 + tl.exp(-dot1))\n        s2 = 1.0 / (1.0 + tl.exp(-dot2))\n        s3 = 1.0 / (1.0 + tl.exp(-dot3))\n        row_sum += s0 + s1 + s2 + s3\n    \n    # Store reduced result\n    output_offset = pid * stride_out_batch\n    tl.store(output_ptr + output_offset, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=x.dtype)\n        \n        # Ensure tensor memory layout is contiguous\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        # Calculate next power-of-two block size\n        BLOCK_INPUT = triton.next_power_of_2(self.input_size)\n        \n        # Launch kernel\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, weight, bias, output,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0),\n            BLOCK_INPUT,\n            self.input_size,\n            self.hidden_size,\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr, w_ptr, b_ptr, out_ptr,\n    batch_size, input_size, hidden_size,\n    stride_x0, stride_x1,\n    stride_w0, stride_w1,\n    stride_b,\n    stride_out0,\n    BLOCK_IN: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n\n    # Load input row\n    x_offset = pid * stride_x0\n    x_offsets = x_offset + tl.arange(0, BLOCK_IN) * stride_x1\n    x_mask = tl.arange(0, BLOCK_IN) < input_size\n    x_vals = tl.load(x_ptr + x_offsets, mask=x_mask, other=0.0)\n    \n    acc = 0.0\n    # Compute linear + sigmoid + reduction\n    for j in range(hidden_size):\n        # Load weights and bias\n        w_start = j * stride_w0\n        w_offsets = w_start + tl.arange(0, BLOCK_IN) * stride_w1\n        w_vals = tl.load(w_ptr + w_offsets, mask=x_mask, other=0.0)\n        b_val = tl.load(b_ptr + j * stride_b)\n        \n        # Compute dot product\n        dot = tl.sum(x_vals * w_vals)\n        # Linear + Sigmoid\n        z = dot + b_val\n        s = 1.0 / (1.0 + tl.exp(-z))\n        acc += s\n    \n    # Store reduced result\n    tl.store(out_ptr + pid * stride_out0, acc)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        weight = self.linear.weight\n        bias = self.linear.bias\n        \n        # Prepare contiguous inputs\n        x = x.contiguous()\n        weight = weight.contiguous()\n        bias = bias.contiguous()\n        \n        # Allocate output\n        output = torch.empty(batch_size, 1, device=x.device, dtype=x.dtype)\n        \n        # Launch kernel\n        grid = (batch_size,)\n        fused_forward_kernel[grid](\n            x, weight, bias, output,\n            batch_size, self.input_size, self.hidden_size,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0),\n            BLOCK_IN=triton.next_power_of_2(self.input_size),\n        )\n        \n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, W_ptr, b_ptr, output_ptr,\n    input_size, hidden_size,\n    stride_xb, stride_w0, stride_b, stride_outb,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # batch index\n    \n    # Load input row\n    offs_x = tl.arange(0, input_size)\n    x_row = tl.load(x_ptr + pid0 * stride_xb + offs_x)\n    \n    total = 0.0\n    for j in range(0, hidden_size):\n        # Load bias for neuron j\n        bias_j = tl.load(b_ptr + j)\n        # Load weights for neuron j\n        offs_w = tl.arange(0, input_size)\n        w_row = tl.load(W_ptr + j * stride_w0 + offs_w)\n        # Compute dot product\n        dot = tl.sum(x_row * w_row)\n        # Compute linear output + bias\n        linear_out = bias_j + dot\n        # Apply sigmoid\n        sigmoid_val = 1.0 / (1.0 + tl.exp(-linear_out))\n        total += sigmoid_val\n    \n    # Store final result\n    tl.store(output_ptr + pid0 * stride_outb, total)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=torch.float32)\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        W = self.linear.weight.contiguous()\n        b = self.linear.bias.contiguous()\n        \n        # Kernel configuration\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, W, b, output,\n            self.input_size, self.hidden_size,\n            x.stride(0), W.stride(0), \n            0, output.stride(0),\n            BLOCK_SIZE=32\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x_batch,\n    stride_x_feature,\n    stride_weight_hidden,\n    stride_weight_feature,\n    stride_bias,\n    stride_out_batch,\n    BLOCK_INPUT: tl.constexpr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Load input row with vectorized access\n    input_start = pid * stride_x_batch\n    input_offsets = input_start + tl.arange(0, BLOCK_INPUT) * stride_x_feature\n    mask = tl.arange(0, BLOCK_INPUT) < input_size\n    x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)\n    \n    row_sum = 0.0\n    \n    # Process 4 hidden units per iteration\n    for j in tl.static_range(0, hidden_size//4):\n        j0 = j * 4\n        j1 = j0 + 1\n        j2 = j0 + 2\n        j3 = j0 + 3\n\n        # Load 4 weight rows\n        w_start0 = j0 * stride_weight_hidden\n        w_offsets0 = w_start0 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row0 = tl.load(weight_ptr + w_offsets0, mask=mask, other=0.0)\n        \n        w_start1 = j1 * stride_weight_hidden\n        w_offsets1 = w_start1 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row1 = tl.load(weight_ptr + w_offsets1, mask=mask, other=0.0)\n        \n        w_start2 = j2 * stride_weight_hidden\n        w_offsets2 = w_start2 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row2 = tl.load(weight_ptr + w_offsets2, mask=mask, other=0.0)\n        \n        w_start3 = j3 * stride_weight_hidden\n        w_offsets3 = w_start3 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row3 = tl.load(weight_ptr + w_offsets3, mask=mask, other=0.0)\n        \n        # Load 4 bias elements\n        b0 = tl.load(bias_ptr + j0 * stride_bias)\n        b1 = tl.load(bias_ptr + j1 * stride_bias)\n        b2 = tl.load(bias_ptr + j2 * stride_bias)\n        b3 = tl.load(bias_ptr + j3 * stride_bias)\n        \n        # Compute 4 dot products + biases\n        dot0 = tl.sum(x_row * w_row0) + b0\n        dot1 = tl.sum(x_row * w_row1) + b1\n        dot2 = tl.sum(x_row * w_row2) + b2\n        dot3 = tl.sum(x_row * w_row3) + b3\n        \n        # Apply sigmoid to all 4 results\n        s0 = 1.0 / (1.0 + tl.exp(-dot0))\n        s1 = 1.0 / (1.0 + tl.exp(-dot1))\n        s2 = 1.0 / (1.0 + tl.exp(-dot2))\n        s3 = 1.0 / (1.0 + tl.exp(-dot3))\n        \n        # Accumulate all 4 sigmoid outputs\n        row_sum += s0 + s1 + s2 + s3\n    \n    # Store reduced result\n    output_offset = pid * stride_out_batch\n    tl.store(output_ptr + output_offset, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=x.dtype)\n        \n        # Ensure tensor memory layout is contiguous\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        # Calculate next power-of-two block size\n        BLOCK_INPUT = triton.next_power_of_2(self.input_size)\n        \n        # Launch kernel\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, weight, bias, output,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0),\n            BLOCK_INPUT,\n            self.input_size,\n            self.hidden_size,\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    batch_size,\n    input_size,\n    hidden_size,\n    stride_x0, stride_x1,\n    stride_w0, stride_w1,\n    stride_b,\n    stride_out0, stride_out1,\n    BLOCK_K: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    \n    # Load input row\n    x_offsets = pid0 * stride_x0 + tl.arange(0, BLOCK_K)\n    x_mask = tl.arange(0, BLOCK_K) < input_size\n    x_row = tl.load(x_ptr + x_offsets, mask=x_mask, other=0.0)\n    \n    # Load weight block\n    h_offsets = tl.arange(0, BLOCK_H)\n    w_ptrs = w_ptr + h_offsets[:, None] * stride_w0 + tl.arange(0, BLOCK_K)[None, :] * stride_w1\n    w_mask = (h_offsets[:, None] < hidden_size) & (tl.arange(0, BLOCK_K)[None, :] < input_size)\n    w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n    \n    # Load bias block\n    b_ptrs = b_ptr + h_offsets * stride_b\n    b_mask = h_offsets < hidden_size\n    b_vec = tl.load(b_ptrs, mask=b_mask, other=0.0)\n    \n    # Compute dot product\n    dot_vec = tl.sum(x_row[None, :] * w_block, axis=1)\n    dot_vec += b_vec\n    \n    # Apply sigmoid and accumulate\n    sig_vec = tl.sigmoid(dot_vec)\n    sig_vec_masked = tl.where(b_mask, sig_vec, 0.0)\n    row_sum = tl.sum(sig_vec_masked, axis=0)\n    \n    # Store result\n    tl.store(output_ptr + pid0 * stride_out0, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        x = x.contiguous()\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        output = torch.empty(x.size(0), 1, device=x.device, dtype=x.dtype)\n        \n        # Compute block sizes\n        BLOCK_K = triton.next_power_of_2(self.input_size)\n        BLOCK_H = triton.next_power_of_2(self.hidden_size)\n        \n        # Launch fused kernel\n        grid = (x.size(0),)\n        fused_kernel[grid](\n            x,\n            weight,\n            bias,\n            output,\n            x.size(0),\n            self.input_size,\n            self.hidden_size,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0), output.stride(1),\n            BLOCK_K=BLOCK_K,\n            BLOCK_H=BLOCK_H,\n            num_warps=4\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program, which is a Triton kernel for a neural network layer that does:\n  1. For each batch element (pid0), load an input row of size `input_size`.\n  2. Load a block of weights of size `hidden_size x input_size` (but using masks for boundaries).\n  3. Load a bias vector of size `hidden_size`.\n  4. Compute the dot product of the input row with each weight row (i.e., matrix-vector product) and add bias.\n  5. Apply sigmoid to each element of the resulting vector.\n  6. Sum the sigmoid outputs for the entire hidden layer to get a scalar per batch element.\n  7. Store that scalar in the output.\n\nThe current implementation has a speed_up of 0.827 (which is less than 1, meaning it's slower than the reference PyTorch).\n\nWe note that the kernel is loading the entire weight matrix for each batch element. This is inefficient because:\n  - The weight matrix is the same for all batch elements, so we are reading it multiple times (once per batch element).\n  - The kernel is using two-dimensional blocks: BLOCK_K (for input_size) and BLOCK_H (for hidden_size). However, the current implementation loads the entire weight block for each batch element.\n\nWe can try to change the kernel to use a different approach:\n\nInspiration from the top-performing programs and diverse programs:\n  - Program 2 (which has a speed_up of 0.881) uses a loop over the hidden_size and loads one weight row at a time. This might be more cache friendly because the weight row is reused for the entire batch? But note: in our case, we are processing one batch element per program_id(0). So we cannot share weights across batch elements in the same kernel launch.\n\nAlternatively, we can try to use a different blocking strategy that processes multiple batch elements together to share the weight matrix? However, note the output is a scalar per batch element.\n\nBut note: the problem size is small (input_size=10, hidden_size=20, batch_size=128). The current kernel might be suffering from inefficient memory access patterns and/or not enough parallelism.\n\nLooking at the current kernel:\n  - It uses one program_id per batch element (grid = (batch_size,)).\n  - Each program loads a row of input (size input_size) and the entire weight matrix (size hidden_size x input_size) and the entire bias vector.\n\nThis leads to:\n  - The weight matrix is loaded `batch_size` times (128 times) from global memory, which is very expensive.\n\nWe can restructure the kernel to load the weight matrix once and then process all batch elements? But note that the kernel is launched with grid (batch_size,). We cannot share data across different program_ids (they run on different SMs?).\n\nAlternatively, we can change the kernel to have two-dimensional grid:\n  - program_id(0): batch index, program_id(1): hidden index? But then we have to reduce across hidden and then across batch? Actually, we want one output per batch.\n\nAlternatively, we can use a kernel that does:\n  - Each program_id(0) is for a hidden unit. Then we load the weight row for that hidden unit and the bias for that hidden unit. Then we iterate over all batch elements to compute the dot product for that hidden unit and then apply sigmoid and then do a reduction? But note: we want to sum the sigmoids for each batch element.\n\nBut note: we are summing over the hidden dimension. So we cannot avoid reading each weight row at least once per batch element? Actually, no: each weight row is used for every batch element. So we can load each weight row once and then use it for all batch elements.\n\nProposed restructured kernel:\n\nWe can change the kernel to:\n  - Have a grid of (hidden_size,). Each program_id corresponds to a hidden unit.\n  - We load the weight row for that hidden unit and the bias.\n  - We then compute the dot product of that weight row with every batch element (so we have to load the entire input matrix? or at least the entire column for that hidden unit? Actually, we need the entire input matrix).\n\nBut note: the input matrix is of size (batch_size, input_size). We would have to load the entire input matrix? That is 128x10=1280 elements. We can load it in blocks? But then we have to reduce over the hidden dimension? Actually, we want to compute for each batch element and each hidden unit: sigmoid(x_i * w_j + b_j) and then sum over j for each i.\n\nWe can do:\n  - We allocate a temporary buffer of size (batch_size, hidden_size) to store the linear outputs? But that would be 128*20=2560 elements, which is small. Then we can have a second kernel to do the sigmoid and reduction? But the problem says we want one kernel.\n\nAlternatively, we can do a two-pass approach? But the problem is small, so we can do:\n\n  Kernel1: Compute the entire linear layer (matrix multiplication) and then apply sigmoid? Then we have a matrix of (batch_size, hidden_size). Then we do a reduction over the hidden_size dimension? That would be two kernels.\n\nBut the current design is one kernel. We are constrained by the fact that the evolution system expects one kernel.\n\nAlternatively, we can use atomics? But we are doing a reduction over hidden_size for each batch element. We could have each hidden unit add its sigmoid value to the output of the corresponding batch element? But we cannot do that without atomics and that would be slow.\n\nGiven the constraints, we might stick to the current structure but try to optimize the memory access.\n\nAlternative idea from the diverse programs: Program D2 (which has a speed_up of 0.881) uses a loop that processes 4 hidden units at a time. This might help with memory bandwidth because we are loading 4 weight rows at once and then doing 4 dot products.\n\nWe can try to change the current kernel to process multiple hidden units per batch element? But note: we are already loading the entire weight block. However, the current kernel uses a block of BLOCK_H (which is the next power of two of hidden_size, so 32) and then masks out the extra. So we are loading 32*input_size (32*10=320) elements for the weight block per batch element. That is 320*128 = 40960 elements, but the actual weight matrix is only 20*10=200 elements. So we are reading 20 times more than necessary.\n\nTherefore, we should change the kernel to avoid loading the entire weight block for each batch element.\n\nWe can instead loop over the hidden_size in chunks? For example, we can set BLOCK_H to 4 (or 8) and then loop over the hidden_size in steps of BLOCK_H.\n\nRevised plan for the kernel:\n\n  Let BLOCK_H = 4 (or 8, a fixed block size for the hidden dimension).\n  Then, we will have:\n      pid0 = batch index\n      and then we loop over j in range(0, hidden_size, BLOCK_H)\n\n  In each iteration:\n      - Load a block of weights of size [BLOCK_H, input_size] (but only the active part, using mask)\n      - Load a block of biases of size BLOCK_H (masked)\n      - Compute the dot products for these BLOCK_H hidden units with the input row (which is already loaded for this batch element)\n      - Apply sigmoid and accumulate the sum for these BLOCK_H units.\n\n  Then, after the loop, we store the accumulated sum.\n\nThis way, we only load a small block of weights at a time, and we reuse the input row for multiple weight rows.\n\nWe also note that the current kernel uses a BLOCK_K that is the next power of two of input_size (which is 16). But input_size is 10. So we are loading 16 elements for the input row, but only 10 are used. That is acceptable.\n\nWe will change the kernel to:\n\n  pid0 = tl.program_id(0)   # batch index\n\n  # Load input row (same as before)\n  x_offsets = ... [masked for input_size]\n\n  row_sum = 0.0\n  for j in range(0, hidden_size, BLOCK_H):\n      # j: start index of the current block in hidden dimension\n\n      # Create offsets for the current block in hidden dimension\n      h_offsets = j + tl.arange(0, BLOCK_H)\n      mask_h = h_offsets < hidden_size   # for the hidden dimension block\n\n      # Load weights: [BLOCK_H, BLOCK_K]\n      w_ptrs = w_ptr + h_offsets[:, None] * stride_w0 + tl.arange(0, BLOCK_K)[None, :] * stride_w1\n      w_mask = mask_h[:, None] & x_mask[None, :]   # both hidden and input dimensions\n      w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n\n      # Load biases: [BLOCK_H]\n      b_ptrs = b_ptr + h_offsets * stride_b\n      b_block = tl.load(b_ptrs, mask=mask_h, other=0.0)\n\n      # Compute dot product: [BLOCK_H]\n      dot_block = tl.sum(x_row[None, :] * w_block, axis=1) + b_block\n\n      # Apply sigmoid and accumulate\n      sig_block = tl.sigmoid(dot_block)\n      # Mask out the extra hidden units in the block\n      sig_block_masked = tl.where(mask_h, sig_block, 0.0)\n      row_sum += tl.sum(sig_block_masked)\n\n  # Store the row_sum\n\nWe choose BLOCK_H to be a constant. Since hidden_size is 20, we can set BLOCK_H=8 or 16? We want to minimize the number of iterations. With BLOCK_H=8, we would do 3 iterations (0:8, 8:16, 16:20). We also note that BLOCK_H must be a power of two? Not necessarily, but we can set it to 8.\n\nWe also note that the current kernel uses `tl.sum` over the BLOCK_H dimension. This is a reduction in the Triton kernel and might be efficient for small BLOCK_H.\n\nAdditionally, we can change the kernel to use fewer warps? Currently, it uses num_warps=4. We might try without specifying or with fewer warps? But the autotuner is not used.\n\nBut note: the kernel is currently launched with `num_warps=4`. We can remove that and let Triton choose? Or we can set it to 1? We are only using one block per batch element, and the block is small.\n\nAlternatively, we can change the grid to be two-dimensional? Actually, no: we are only iterating over the batch.\n\nWe will set BLOCK_H=8.\n\nAlso, we note that the current kernel uses `BLOCK_K = triton.next_power_of_2(input_size)`. For input_size=10, that is 16. We keep that.\n\nWe also note that the current kernel has a `dot_vec = tl.sum(...)` and then `dot_vec += b_vec`. We are doing the same in the loop.\n\nLet's rewrite the kernel accordingly.\n\nWe also note: the current kernel uses `sig_vec_masked = tl.where(b_mask, sig_vec, 0.0)` and then `tl.sum(sig_vec_masked)`. In the loop version, we are doing the same: `sig_block_masked = tl.where(mask_h, sig_block, 0.0)` and then `row_sum += tl.sum(sig_block_masked)`.\n\nWe must be cautious: the entire hidden dimension is being processed in blocks, and we are accumulating the row_sum.\n\nWe will also remove the `num_warps=4` from the kernel launch because we are changing the kernel and the workload per thread might be less? Actually, we are looping over the hidden dimension in blocks, so the work per program is about the same.\n\nBut note: the kernel launch configuration is the same: grid=(batch_size,).\n\nWe'll try with BLOCK_H=8.\n\nLet's write the new kernel.\n\nImportant: We must not change the function signature (so we keep the same parameters) but we can change the internal logic.\n\nWe are going to change the kernel body and also the kernel launch (remove the num_warps? Actually, the current kernel launch has `num_warps=4`. We can leave it for now and see if performance improves. If not, we can remove it in a future iteration).\n\nWe are going to replace the entire kernel body.\n\nWe'll keep the same BLOCK_K and BLOCK_H as before? Actually, we are now using BLOCK_H in a different way: it is the block size for the hidden dimension in the loop. We are going to set it to 8. But note: the original BLOCK_H was the next power of two of hidden_size (which was 20 -> 32). We are now going to use a fixed BLOCK_H=8. We can pass it as a `tl.constexpr` and set it to 8.\n\nBut note: the kernel parameters already have BLOCK_K and BLOCK_H. We are going to change the meaning of BLOCK_H? Actually, we can. In the ModelNew.forward, we set:\n   BLOCK_K = triton.next_power_of_2(self.input_size)\n   BLOCK_H = 8   # fixed block size for the hidden dimension\n\nSo we change the kernel to use BLOCK_H as the block size for the hidden dimension in the loop.\n\nAlternatively, we can rename the constant? But the signature must remain the same. So we keep the name.\n\nWe change the kernel to:\n\n  pid0 = tl.program_id(0)\n  # Load input row (same as before)\n  x_offsets = ... [masked for input_size]\n\n  row_sum = 0.0\n  # Loop over hidden_size in steps of BLOCK_H\n  for j in range(0, hidden_size, BLOCK_H):\n      h_offsets = j + tl.arange(0, BLOCK_H)\n      mask_h = h_offsets < hidden_size\n\n      # Load weight block: [BLOCK_H, BLOCK_K]\n      w_ptrs = w_ptr + h_offsets[:, None] * stride_w0 + tl.arange(0, BLOCK_K)[None, :] * stride_w1\n      w_mask = mask_h[:, None] & x_mask[None, :]\n      w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n\n      # Load bias block: [BLOCK_H]\n      b_ptrs = b_ptr + h_offsets * stride_b\n      b_block = tl.load(b_ptrs, mask=mask_h, other=0.0)\n\n      # Compute dot product: [BLOCK_H]\n      dot_block = tl.sum(x_row[None, :] * w_block, axis=1) + b_block\n\n      # Apply sigmoid and accumulate\n      sig_block = tl.sigmoid(dot_block)\n      sig_block_masked = tl.where(mask_h, sig_block, 0.0)\n      row_sum += tl.sum(sig_block_masked)\n\n  tl.store(...)\n\nWe hope that by reducing the amount of weight data loaded per batch element (from 32*16 to 8*16 per iteration, and we do 3 iterations) we reduce the memory bandwidth pressure.\n\nWe also note that the input row is loaded once and reused for all iterations.\n\nWe change the ModelNew.forward to set BLOCK_H=8 (a constant) instead of the next power of two of hidden_size.\n\nBut note: the kernel signature requires BLOCK_H to be a tl.constexpr. We are passing it from the Python side.\n\nSo in the forward function:\n   BLOCK_K = triton.next_power_of_2(self.input_size)\n   BLOCK_H = 8   # we fix it to 8\n\nWe remove the `num_warps=4` from the kernel launch? We can leave it for now and see.\n\nLet's write the new code.\n\nNote: We are only changing the kernel body and the setting of BLOCK_H in the forward function.\n\nWe are not changing the class structure or the function signatures.\n\nWe are also not changing the input/output tensors.\n\nWe hope this improves the speed.\n\nLet's output the code accordingly.\n</think>\nTo improve the performance of the Triton kernel, we restructured the implementation to process hidden units in smaller blocks rather than loading the entire weight matrix at once. This reduces memory bandwidth pressure by:\n1. Loading weights in smaller chunks (BLOCK_H=8)\n2. Reusing the input row across multiple weight blocks\n3. Reducing unnecessary memory reads for masked-out elements\n\nThe changes maintain the same functionality while optimizing memory access patterns for better performance.\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    batch_size,\n    input_size,\n    hidden_size,\n    stride_x0, stride_x1,\n    stride_w0, stride_w1,\n    stride_b,\n    stride_out0, stride_out1,\n    BLOCK_K: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    \n    # Load input row\n    x_offsets = pid0 * stride_x0 + tl.arange(0, BLOCK_K)\n    x_mask = tl.arange(0, BLOCK_K) < input_size\n    x_row = tl.load(x_ptr + x_offsets, mask=x_mask, other=0.0)\n    \n    row_sum = 0.0\n    # Process hidden units in chunks of BLOCK_H\n    for j in range(0, hidden_size, BLOCK_H):\n        # Create offsets for current hidden block\n        h_offsets = j + tl.arange(0, BLOCK_H)\n        mask_h = h_offsets < hidden_size\n        \n        # Load weight block for current chunk\n        w_ptrs = w_ptr + h_offsets[:, None] * stride_w0 + tl.arange(0, BLOCK_K)[None, :] * stride_w1\n        w_mask = mask_h[:, None] & x_mask[None, :]\n        w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        # Load bias block for current chunk\n        b_ptrs = b_ptr + h_offsets * stride_b\n        b_block = tl.load(b_ptrs, mask=mask_h, other=0.0)\n        \n        # Compute dot product for current chunk\n        dot_block = tl.sum(x_row[None, :] * w_block, axis=1) + b_block\n        \n        # Apply sigmoid and accumulate\n        sig_block = tl.sigmoid(dot_block)\n        sig_block_masked = tl.where(mask_h, sig_block, 0.0)\n        row_sum += tl.sum(sig_block_masked)\n    \n    # Store result\n    tl.store(output_ptr + pid0 * stride_out0, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        x = x.contiguous()\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        output = torch.empty(x.size(0), 1, device=x.device, dtype=x.dtype)\n        \n        # Compute block sizes\n        BLOCK_K = triton.next_power_of_2(self.input_size)\n        BLOCK_H = 8  # Fixed chunk size for hidden dimension\n        \n        # Launch fused kernel\n        grid = (x.size(0),)\n        fused_kernel[grid](\n            x,\n            weight,\n            bias,\n            output,\n            x.size(0),\n            self.input_size,\n            self.hidden_size,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0), output.stride(1),\n            BLOCK_K=BLOCK_K,\n            BLOCK_H=BLOCK_H\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
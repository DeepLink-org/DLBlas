{"id": "ff1eca13-1a6b-4a36-8d85-39a1d738cc33", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x_batch,\n    stride_x_feature,\n    stride_weight_hidden,\n    stride_weight_feature,\n    stride_bias,\n    stride_out_batch,\n    BLOCK_INPUT: tl.constexpr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Load entire input row\n    input_start = pid * stride_x_batch\n    input_offsets = input_start + tl.arange(0, BLOCK_INPUT) * stride_x_feature\n    mask = tl.arange(0, BLOCK_INPUT) < input_size\n    x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)\n    \n    # Preload entire weight matrix and bias vector\n    h_idx = tl.arange(0, hidden_size)\n    i_idx = tl.arange(0, input_size)\n    \n    # Create 2D index for weight matrix\n    w_offsets = h_idx[:, None] * stride_weight_hidden + i_idx[None, :] * stride_weight_feature\n    w_mask = (h_idx[:, None] < hidden_size) & (i_idx[None, :] < input_size)\n    weight_matrix = tl.load(weight_ptr + w_offsets, mask=w_mask, other=0.0)\n    \n    # Load bias vector\n    b_offsets = h_idx * stride_bias\n    b_mask = h_idx < hidden_size\n    bias_vector = tl.load(bias_ptr + b_offsets, mask=b_mask, other=0.0)\n    \n    # Compute matrix-vector product\n    linear_out = tl.dot(weight_matrix, x_row) + bias_vector\n    \n    # Apply sigmoid and sum\n    sigmoid_out = 1.0 / (1.0 + tl.exp(-linear_out))\n    row_sum = tl.sum(sigmoid_out)\n    \n    # Store result\n    output_offset = pid * stride_out_batch\n    tl.store(output_ptr + output_offset, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=x.dtype)\n        \n        # Ensure tensor memory layout is contiguous\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        # Use exact input size for block dimension\n        BLOCK_INPUT = self.input_size\n        \n        # Launch kernel\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, weight, bias, output,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0),\n            BLOCK_INPUT,\n            self.input_size,\n            self.hidden_size,\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "7526de6c-a719-4a75-8662-95e6d2a657ca", "generation": 2, "timestamp": 1754576475.003302, "iteration_found": 24, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_56_Matmul_Sigmoid_Sum", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\n    raise ValueError(\"arange's range must be a power of 2\")\nValueError: arange's range must be a power of 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpeqpq7eek.py\", line 76, in forward\n    _forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 20:34:\n    stride_weight_feature,\n    stride_bias,\n    stride_out_batch,\n    BLOCK_INPUT: tl.constexpr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n\n    # Load entire input row\n    input_start = pid * stride_x_batch\n    input_offsets = input_start + tl.arange(0, BLOCK_INPUT) * stride_x_feature\n                                  ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\n    raise ValueError(\"arange's range must be a power of 2\")\nValueError: arange's range must be a power of 2\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0483, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.861, "runtime_ms": 0.0561, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0483, "speed_up": 0.861, "custom_timing": 0.0561}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.8610\n- runtime_ms: 0.0561\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0483, 'speed_up': 0.861, 'custom_timing': 0.0561}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.8770, runtime_ms: 0.0551, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0483, 'speed_up': 0.877, 'custom_timing': 0.0551}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.8580, runtime_ms: 0.0563, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0483, 'speed_up': 0.858, 'custom_timing': 0.0563}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.8610, runtime_ms: 0.0561, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0483, 'speed_up': 0.861, 'custom_timing': 0.0561}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x_batch,\n    stride_x_feature,\n    stride_weight_hidden,\n    stride_weight_feature,\n    stride_bias,\n    stride_out_batch,\n    BLOCK_INPUT: tl.constexpr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Load input row with vectorized access\n    input_start = pid * stride_x_batch\n    input_offsets = input_start + tl.arange(0, BLOCK_INPUT) * stride_x_feature\n    mask = tl.arange(0, BLOCK_INPUT) < input_size\n    x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)\n    \n    row_sum = 0.0\n    \n    # Unroll hidden dimension processing\n    for j in tl.static_range(hidden_size):\n        # Load weight row with vectorized access\n        w_start = j * stride_weight_hidden\n        w_offsets = w_start + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row = tl.load(weight_ptr + w_offsets, mask=mask, other=0.0)\n        \n        # Load bias element\n        b = tl.load(bias_ptr + j * stride_bias)\n        \n        # Compute dot product + bias\n        dot = tl.sum(x_row * w_row) + b\n        \n        # Sigmoid activation\n        s = 1.0 / (1.0 + tl.exp(-dot))\n        row_sum += s\n    \n    # Store reduced result\n    output_offset = pid * stride_out_batch\n    tl.store(output_ptr + output_offset, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=x.dtype)\n        \n        # Ensure tensor memory layout is contiguous\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        # Calculate next power-of-two block size\n        BLOCK_INPUT = triton.next_power_of_2(self.input_size)\n        \n        # Launch kernel\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, weight, bias, output,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0),\n            BLOCK_INPUT,\n            self.input_size,\n            self.hidden_size,\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.8610), Performs well on runtime_ms (0.0561), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0483, 'speed_up': 0.861, 'custom_timing': 0.0561})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x_batch,\n    stride_x_feature,\n    stride_weight_hidden,\n    stride_weight_feature,\n    stride_bias,\n    stride_out_batch,\n    BLOCK_INPUT_P2: tl.constexpr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Create arange with power-of-two size\n    idxs = tl.arange(0, BLOCK_INPUT_P2)\n    # Create mask for valid input elements\n    mask = idxs < input_size\n    \n    # Load input row with vectorized access\n    input_start = pid * stride_x_batch\n    input_offsets = input_start + idxs * stride_x_feature\n    x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)\n    \n    row_sum = 0.0\n    \n    # Process hidden dimension\n    for j in tl.static_range(hidden_size):\n        # Load weight row with same mask\n        w_start = j * stride_weight_hidden\n        w_offsets = w_start + idxs * stride_weight_feature\n        w_row = tl.load(weight_ptr + w_offsets, mask=mask, other=0.0)\n        \n        # Load bias element\n        b = tl.load(bias_ptr + j * stride_bias)\n        \n        # Compute dot product + bias\n        dot = tl.sum(x_row * w_row) + b\n        \n        # Sigmoid activation\n        s = 1.0 / (1.0 + tl.exp(-dot))\n        row_sum += s\n    \n    # Store reduced result\n    output_offset = pid * stride_out_batch\n    tl.store(output_ptr + output_offset, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=x.dtype)\n        \n        # Ensure tensor memory layout is contiguous\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        # Compute next power-of-two for input dimension\n        BLOCK_INPUT_P2 = triton.next_power_of_2(self.input_size)\n        \n        # Launch kernel\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, weight, bias, output,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0),\n            BLOCK_INPUT_P2,\n            self.input_size,\n            self.hidden_size,\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.8580), Performs well on runtime_ms (0.0563), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0483, 'speed_up': 0.858, 'custom_timing': 0.0563})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x_batch,\n    stride_x_feature,\n    stride_weight_hidden,\n    stride_weight_feature,\n    stride_bias,\n    stride_out_batch,\n    BLOCK_INPUT: tl.constexpr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Compute input offsets with power-of-two block size\n    input_start = pid * stride_x_batch\n    offs = tl.arange(0, BLOCK_INPUT)\n    input_offsets = input_start + offs * stride_x_feature\n    \n    # Create mask for valid elements\n    mask = offs < input_size\n    x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)\n    \n    row_sum = 0.0\n    \n    # Process hidden dimension\n    for j in tl.static_range(hidden_size):\n        # Compute weight offsets\n        w_start = j * stride_weight_hidden\n        w_offsets = w_start + offs * stride_weight_feature\n        w_row = tl.load(weight_ptr + w_offsets, mask=mask, other=0.0)\n        \n        # Load bias\n        b = tl.load(bias_ptr + j * stride_bias)\n        \n        # Compute dot product + bias\n        dot = tl.sum(x_row * w_row) + b\n        \n        # Apply sigmoid\n        s = 1.0 / (1.0 + tl.exp(-dot))\n        row_sum += s\n    \n    # Store result\n    output_offset = pid * stride_out_batch\n    tl.store(output_ptr + output_offset, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        # Compute next power-of-two block size\n        self.BLOCK_INPUT = triton.next_power_of_2(input_size)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=x.dtype)\n        \n        # Ensure contiguous memory\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        # Launch kernel with power-of-two block\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, weight, bias, output,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0),\n            self.BLOCK_INPUT,\n            self.input_size,\n            self.hidden_size,\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.8770), Performs well on runtime_ms (0.0551), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0483, 'speed_up': 0.877, 'custom_timing': 0.0551})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x_batch,\n    stride_x_feature,\n    stride_weight_hidden,\n    stride_weight_feature,\n    stride_bias,\n    stride_out_batch,\n    BLOCK_INPUT: tl.constexpr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Load input row with vectorized access\n    input_start = pid * stride_x_batch\n    input_offsets = input_start + tl.arange(0, BLOCK_INPUT) * stride_x_feature\n    mask = tl.arange(0, BLOCK_INPUT) < input_size\n    x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)\n    \n    row_sum = 0.0\n    \n    # Process hidden units in groups of 4\n    for j in range(0, hidden_size, 4):\n        # Load 4 weight rows with vectorized access\n        w_start0 = j * stride_weight_hidden\n        w_offsets0 = w_start0 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row0 = tl.load(weight_ptr + w_offsets0, mask=mask, other=0.0)\n        \n        w_start1 = (j+1) * stride_weight_hidden\n        w_offsets1 = w_start1 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row1 = tl.load(weight_ptr + w_offsets1, mask=mask, other=0.0)\n        \n        w_start2 = (j+2) * stride_weight_hidden\n        w_offsets2 = w_start2 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row2 = tl.load(weight_ptr + w_offsets2, mask=mask, other=0.0)\n        \n        w_start3 = (j+3) * stride_weight_hidden\n        w_offsets3 = w_start3 + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row3 = tl.load(weight_ptr + w_offsets3, mask=mask, other=0.0)\n        \n        # Load 4 bias elements\n        b0 = tl.load(bias_ptr + (j+0) * stride_bias)\n        b1 = tl.load(bias_ptr + (j+1) * stride_bias)\n        b2 = tl.load(bias_ptr + (j+2) * stride_bias)\n        b3 = tl.load(bias_ptr + (j+3) * stride_bias)\n        \n        # Compute dot products + bias\n        dot0 = tl.sum(x_row * w_row0) + b0\n        dot1 = tl.sum(x_row * w_row1) + b1\n        dot2 = tl.sum(x_row * w_row2) + b2\n        dot3 = tl.sum(x_row * w_row3) + b3\n        \n        # Sigmoid activation and accumulation\n        s0 = 1.0 / (1.0 + tl.exp(-dot0))\n        s1 = 1.0 / (1.0 + tl.exp(-dot1))\n        s2 = 1.0 / (1.0 + tl.exp(-dot2))\n        s3 = 1.0 / (1.0 + tl.exp(-dot3))\n        row_sum += s0 + s1 + s2 + s3\n    \n    # Store reduced result\n    output_offset = pid * stride_out_batch\n    tl.store(output_ptr + output_offset, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=x.dtype)\n        \n        # Ensure contiguous memory layout\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        # Calculate optimal block size\n        BLOCK_INPUT = triton.next_power_of_2(self.input_size)\n        \n        # Launch kernel\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, weight, bias, output,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0),\n            BLOCK_INPUT,\n            self.input_size,\n            self.hidden_size,\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    batch_size,\n    input_size,\n    hidden_size,\n    stride_x0, stride_x1,\n    stride_w0, stride_w1,\n    stride_b,\n    stride_out0, stride_out1,\n    BLOCK_K: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    \n    # Load entire input row\n    x_offsets = pid0 * stride_x0 + tl.arange(0, BLOCK_K)\n    x_mask = tl.arange(0, BLOCK_K) < input_size\n    x_row = tl.load(x_ptr + x_offsets, mask=x_mask, other=0.0)\n    \n    total_sum = 0.0\n    # Process hidden dimension in blocks\n    for j in range(0, hidden_size, BLOCK_H):\n        j_offsets = j + tl.arange(0, BLOCK_H)\n        j_mask = j_offsets < hidden_size\n        \n        # Load weight block\n        w_ptrs = w_ptr + j_offsets[:, None] * stride_w0 + tl.arange(0, BLOCK_K)[None, :] * stride_w1\n        w_block = tl.load(w_ptrs, mask=j_mask[:, None] & x_mask[None, :], other=0.0)\n        \n        # Compute block linear transformation\n        acc_block = tl.sum(x_row[None, :] * w_block, axis=1)\n        \n        # Load bias block and add\n        b_block = tl.load(b_ptr + j_offsets, mask=j_mask, other=0.0)\n        acc_block += b_block\n        \n        # Apply sigmoid and accumulate sum\n        y_block = tl.sigmoid(acc_block)\n        total_sum += tl.sum(y_block, axis=0)\n    \n    # Store final result\n    tl.store(output_ptr + pid0 * stride_out0, total_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        x = x.contiguous()\n        output = torch.empty(x.size(0), 1, device=x.device, dtype=x.dtype)\n        \n        # Compute block sizes\n        BLOCK_K = triton.next_power_of_2(self.input_size)\n        BLOCK_H = 128  # Fixed hidden block size\n        \n        # Launch kernel\n        grid = (x.size(0),)\n        fused_kernel[grid](\n            x,\n            self.linear.weight,\n            self.linear.bias,\n            output,\n            x.size(0),\n            self.input_size,\n            self.hidden_size,\n            x.stride(0), x.stride(1),\n            self.linear.weight.stride(0), self.linear.weight.stride(1),\n            self.linear.bias.stride(0),\n            output.stride(0), output.stride(1),\n            BLOCK_K=BLOCK_K,\n            BLOCK_H=BLOCK_H,\n            num_warps=4\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x_batch,\n    stride_x_feature,\n    stride_weight_hidden,\n    stride_weight_feature,\n    stride_bias,\n    stride_out_batch,\n    BLOCK_INPUT_P2: tl.constexpr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Create arange with power-of-two size\n    idxs = tl.arange(0, BLOCK_INPUT_P2)\n    # Create mask for valid input elements\n    mask = idxs < input_size\n    \n    # Load input row with vectorized access\n    input_start = pid * stride_x_batch\n    input_offsets = input_start + idxs * stride_x_feature\n    x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)\n    \n    row_sum = 0.0\n    \n    # Process hidden dimension\n    for j in tl.static_range(hidden_size):\n        # Load weight row with same mask\n        w_start = j * stride_weight_hidden\n        w_offsets = w_start + idxs * stride_weight_feature\n        w_row = tl.load(weight_ptr + w_offsets, mask=mask, other=0.0)\n        \n        # Load bias element\n        b = tl.load(bias_ptr + j * stride_bias)\n        \n        # Compute dot product + bias\n        dot = tl.sum(x_row * w_row) + b\n        \n        # Sigmoid activation\n        s = 1.0 / (1.0 + tl.exp(-dot))\n        row_sum += s\n    \n    # Store reduced result\n    output_offset = pid * stride_out_batch\n    tl.store(output_ptr + output_offset, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=x.dtype)\n        \n        # Ensure tensor memory layout is contiguous\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        # Compute next power-of-two for input dimension\n        BLOCK_INPUT_P2 = triton.next_power_of_2(self.input_size)\n        \n        # Launch kernel\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, weight, bias, output,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0),\n            BLOCK_INPUT_P2,\n            self.input_size,\n            self.hidden_size,\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    batch_size,\n    input_size,\n    hidden_size,\n    stride_x0, stride_x1,\n    stride_w0, stride_w1,\n    stride_b,\n    stride_out0, stride_out1,\n    BLOCK_K: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    \n    # Load input row\n    x_offsets = pid0 * stride_x0 + tl.arange(0, BLOCK_K)\n    x_mask = tl.arange(0, BLOCK_K) < input_size\n    x_row = tl.load(x_ptr + x_offsets, mask=x_mask, other=0.0)\n    \n    # Compute linear transformation\n    row_sum = 0.0\n    for j in range(0, hidden_size, BLOCK_H):\n        h_offsets = j + tl.arange(0, BLOCK_H)\n        h_mask = h_offsets < hidden_size\n        \n        # Load weight block\n        w_ptrs = w_ptr + h_offsets[:, None] * stride_w0 + tl.arange(0, BLOCK_K)[None, :] * stride_w1\n        w_block = tl.load(w_ptrs, mask=h_mask[:, None] & x_mask[None, :], other=0.0)\n        \n        # Compute partial dot product\n        partial_dot = tl.sum(x_row[None, :] * w_block, axis=1)\n        \n        # Load bias block\n        b_block = tl.load(b_ptr + h_offsets * stride_b, mask=h_mask, other=0.0)\n        \n        # Complete dot product + bias\n        dot = partial_dot + b_block\n        \n        # Apply sigmoid and accumulate\n        y = tl.sigmoid(dot)\n        row_sum += tl.sum(y, axis=0)\n    \n    # Store result\n    tl.store(output_ptr + pid0 * stride_out0, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        x = x.contiguous()\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        output = torch.empty(x.size(0), 1, device=x.device, dtype=x.dtype)\n        \n        # Compute block sizes\n        BLOCK_K = 2 ** math.ceil(math.log2(self.input_size))\n        BLOCK_H = min(128, triton.next_power_of_2(self.hidden_size))\n        \n        # Launch fused kernel\n        grid = (x.size(0),)\n        fused_kernel[grid](\n            x,\n            weight,\n            bias,\n            output,\n            x.size(0),\n            self.input_size,\n            self.hidden_size,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0), output.stride(1),\n            BLOCK_K=BLOCK_K,\n            BLOCK_H=BLOCK_H,\n            num_warps=4\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, W_ptr, b_ptr, output_ptr,\n    input_size, hidden_size,\n    stride_xb, stride_w0, stride_b, stride_outb,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # batch index\n    \n    # Create offset array using compile-time constant BLOCK_SIZE\n    offs = tl.arange(0, BLOCK_SIZE)\n    # Compute base pointer for current batch\n    x_batch_ptr = x_ptr + pid0 * stride_xb\n    # Create mask to prevent out-of-bound accesses\n    mask = offs < input_size\n    # Load input row with boundary check\n    x_row = tl.load(x_batch_ptr + offs, mask=mask, other=0.0)\n    \n    # Preload entire bias vector using vectorized load\n    bias_offsets = tl.arange(0, hidden_size)\n    bias_vector = tl.load(b_ptr + bias_offsets * stride_b, mask=bias_offsets < hidden_size, other=0.0)\n    \n    # Preload entire weight matrix using vectorized load\n    weight_offsets = tl.arange(0, hidden_size * input_size)\n    weight_matrix = tl.load(W_ptr + weight_offsets, mask=weight_offsets < (hidden_size * input_size), other=0.0)\n    \n    total = 0.0\n    # Unroll hidden dimension loop for parallel execution\n    for j in tl.static_range(hidden_size):\n        dot = 0.0\n        # Unroll input dimension loop for parallel reduction\n        for i in tl.static_range(input_size):\n            dot += x_row[i] * weight_matrix[j * input_size + i]\n        linear_out = bias_vector[j] + dot\n        # Compute sigmoid with efficient numerical stability\n        sigmoid_val = 1.0 / (1.0 + tl.exp(-linear_out))\n        total += sigmoid_val\n    \n    # Store final result\n    tl.store(output_ptr + pid0 * stride_outb, total)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        # Compute next power-of-two block size\n        self.BLOCK_SIZE = 1\n        while self.BLOCK_SIZE < input_size:\n            self.BLOCK_SIZE *= 2\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=torch.float32)\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        W = self.linear.weight.contiguous()\n        b = self.linear.bias.contiguous()\n        \n        # Kernel configuration\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, W, b, output,\n            self.input_size, self.hidden_size,\n            x.stride(0), W.stride(0), \n            b.stride(0), output.stride(0),\n            BLOCK_SIZE=self.BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    batch_size,\n    input_size,\n    hidden_size,\n    stride_x0, stride_x1,\n    stride_w0, stride_w1,\n    stride_b,\n    stride_out0, stride_out1,\n    BLOCK_K: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    \n    # Load entire input row\n    x_offsets = pid0 * stride_x0 + tl.arange(0, BLOCK_K)\n    x_mask = tl.arange(0, BLOCK_K) < input_size\n    x_row = tl.load(x_ptr + x_offsets, mask=x_mask, other=0.0)\n    \n    total_sum = 0.0\n    # Process hidden dimension in blocks\n    for j in range(0, hidden_size, BLOCK_H):\n        j_offsets = j + tl.arange(0, BLOCK_H)\n        j_mask = j_offsets < hidden_size\n        \n        # Load weight block\n        w_ptrs = w_ptr + j_offsets[:, None] * stride_w0 + tl.arange(0, BLOCK_K)[None, :] * stride_w1\n        w_block = tl.load(w_ptrs, mask=j_mask[:, None] & x_mask[None, :], other=0.0)\n        \n        # Compute block linear transformation\n        acc_block = tl.sum(x_row[None, :] * w_block, axis=1)\n        \n        # Load bias block and add\n        b_block = tl.load(b_ptr + j_offsets, mask=j_mask, other=0.0)\n        acc_block += b_block\n        \n        # Apply sigmoid and accumulate sum\n        y_block = tl.sigmoid(acc_block)\n        total_sum += tl.sum(y_block, axis=0)\n    \n    # Store final result\n    tl.store(output_ptr + pid0 * stride_out0, total_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        x = x.contiguous()\n        output = torch.empty(x.size(0), 1, device=x.device, dtype=x.dtype)\n        \n        # Compute block sizes\n        BLOCK_K = triton.next_power_of_2(self.input_size)\n        BLOCK_H = 128  # Fixed hidden block size\n        \n        # Launch kernel\n        grid = (x.size(0),)\n        fused_kernel[grid](\n            x,\n            self.linear.weight,\n            self.linear.bias,\n            output,\n            x.size(0),\n            self.input_size,\n            self.hidden_size,\n            x.stride(0), x.stride(1),\n            self.linear.weight.stride(0), self.linear.weight.stride(1),\n            self.linear.bias.stride(0),\n            output.stride(0), output.stride(1),\n            BLOCK_K=BLOCK_K,\n            BLOCK_H=BLOCK_H,\n            num_warps=4\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x_batch,\n    stride_x_feature,\n    stride_weight_hidden,\n    stride_weight_feature,\n    stride_bias,\n    stride_out_batch,\n    BLOCK_INPUT: tl.constexpr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Compute input offsets with power-of-two block size\n    input_start = pid * stride_x_batch\n    offs = tl.arange(0, BLOCK_INPUT)\n    input_offsets = input_start + offs * stride_x_feature\n    \n    # Create mask for valid elements\n    mask = offs < input_size\n    x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)\n    \n    row_sum = 0.0\n    \n    # Process hidden dimension\n    for j in tl.static_range(hidden_size):\n        # Compute weight offsets\n        w_start = j * stride_weight_hidden\n        w_offsets = w_start + offs * stride_weight_feature\n        w_row = tl.load(weight_ptr + w_offsets, mask=mask, other=0.0)\n        \n        # Load bias\n        b = tl.load(bias_ptr + j * stride_bias)\n        \n        # Compute dot product + bias\n        dot = tl.sum(x_row * w_row) + b\n        \n        # Apply sigmoid\n        s = 1.0 / (1.0 + tl.exp(-dot))\n        row_sum += s\n    \n    # Store result\n    output_offset = pid * stride_out_batch\n    tl.store(output_ptr + output_offset, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        # Compute next power-of-two block size\n        self.BLOCK_INPUT = triton.next_power_of_2(input_size)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=x.dtype)\n        \n        # Ensure contiguous memory\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        # Launch kernel with power-of-two block\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, weight, bias, output,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0),\n            self.BLOCK_INPUT,\n            self.input_size,\n            self.hidden_size,\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x_batch,\n    stride_x_feature,\n    stride_weight_hidden,\n    stride_weight_feature,\n    stride_bias,\n    stride_out_batch,\n    BLOCK_INPUT: tl.constexpr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Load input row with vectorized access\n    input_start = pid * stride_x_batch\n    input_offsets = input_start + tl.arange(0, BLOCK_INPUT) * stride_x_feature\n    mask = tl.arange(0, BLOCK_INPUT) < input_size\n    x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)\n    \n    row_sum = 0.0\n    \n    # Unroll hidden dimension processing\n    for j in tl.static_range(hidden_size):\n        # Load weight row with vectorized access\n        w_start = j * stride_weight_hidden\n        w_offsets = w_start + tl.arange(0, BLOCK_INPUT) * stride_weight_feature\n        w_row = tl.load(weight_ptr + w_offsets, mask=mask, other=0.0)\n        \n        # Load bias element\n        b = tl.load(bias_ptr + j * stride_bias)\n        \n        # Compute dot product + bias\n        dot = tl.sum(x_row * w_row) + b\n        \n        # Sigmoid activation\n        s = 1.0 / (1.0 + tl.exp(-dot))\n        row_sum += s\n    \n    # Store reduced result\n    output_offset = pid * stride_out_batch\n    tl.store(output_ptr + output_offset, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=x.dtype)\n        \n        # Ensure tensor memory layout is contiguous\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        # Calculate next power-of-two block size\n        BLOCK_INPUT = triton.next_power_of_2(self.input_size)\n        \n        # Launch kernel\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, weight, bias, output,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0),\n            BLOCK_INPUT,\n            self.input_size,\n            self.hidden_size,\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program which is a Triton kernel for a custom model that:\n 1. Takes an input tensor `x` of shape (batch_size, input_size)\n 2. Applies a linear layer (weight: (hidden_size, input_size), bias: (hidden_size,))\n 3. For each hidden unit, applies sigmoid activation\n 4. Sums the sigmoid outputs over the hidden units to produce an output of shape (batch_size, 1)\n\nThe current implementation has a speed_up of 0.861 (which is less than 1, meaning it's slower than the reference PyTorch).\n\nThe kernel is structured as:\n  - Each program ID (pid) corresponds to a batch element.\n  - For each batch element, we load the entire input row (with a power-of-two block size and masking for the actual input size).\n  - Then, we loop over each hidden unit (from 0 to hidden_size-1). For each hidden unit j:\n        - Load the j-th row of the weight matrix (with the same block and mask)\n        - Load the j-th bias element\n        - Compute the dot product of the input row and the weight row, then add bias -> `dot`\n        - Apply sigmoid to `dot` and accumulate to `row_sum`\n  - Store the `row_sum` for the batch element.\n\nThe main performance issue is that the kernel is memory-bound and we are doing a lot of global memory accesses.\n\nOptimization ideas:\n 1. We note that the hidden_size is 20, which is small. However, the current implementation loads the same input row multiple times (once per hidden unit). We can avoid this by caching the input row in shared memory? But note: Triton does not have shared memory in the same way as CUDA. However, we can use registers and/or local variables.\n\n 2. But note: the input row is reused for every hidden unit. We are currently loading it once per batch element (at the beginning) and then reusing it for the entire hidden_size loop. So that part is already cached in registers? Not exactly: the `x_row` is stored in a register array? However, the problem is that we are doing a lot of global loads for the weight matrix and bias.\n\n 3. We are doing a dot product for each hidden unit independently. The hidden units are independent, so we can compute them in parallel. The current loop is serial.\n\n 4. We can change the kernel to process multiple hidden units at once. For example, we can use a block of hidden units (say, BLOCK_HIDDEN) and then have each program in the grid be two-dimensional: (batch_size, hidden_size // BLOCK_HIDDEN). But note: the grid is currently 1D (batch_size). We are constrained by the grid configuration? The problem says: \"NEVER modify grid configuration, output tensor shapes, or PID logic\". So we cannot change the grid to be 2D.\n\n 5. Alternatively, we can use vector operations to compute multiple hidden units in one go. However, the current kernel is written for one hidden unit at a time.\n\n 6. Another idea: since the hidden_size is small (20), we can preload the entire weight matrix and bias vector for the batch element? But note: the weight matrix is (hidden_size, input_size) and input_size is 10, so the entire weight matrix is 20*10=200 elements. We can load that in one go? Then we can do a matrix-vector product without going to global memory repeatedly.\n\n 7. How about:\n    - Preload the entire weight matrix for the current batch element? But note: the weight matrix is the same for all batch elements. However, we are processing one batch element per program. So we can load the entire weight matrix and bias vector at the beginning of the kernel.\n\n 8. Steps:\n    a. Load the input row (as before).\n    b. Preload the entire weight matrix and bias vector. But note: the weight matrix is of size [hidden_size, input_size] and we are using a block of size BLOCK_INPUT (which is power-of-two for input_size). We can load the weight matrix in a 2D block? But the kernel is currently 1D.\n\n 9. We can change the kernel to load the entire weight matrix and bias vector in one shot? We can use:\n        weight = tl.load(weight_ptr + [offsets for hidden_size * input_size])\n    But we have to create a 2D index? We can do:\n\n        weight_offsets = (tl.arange(0, hidden_size)[:, None] * stride_weight_hidden +\n                          tl.arange(0, input_size)[None, :] * stride_weight_feature)\n        weight_matrix = tl.load(weight_ptr + weight_offsets, mask= ...)\n\n    However, note that the weight matrix is stored in row-major: each row is contiguous? The stride between rows is stride_weight_hidden, and within a row is stride_weight_feature (which is 1 if the matrix is row-major).\n\n    But the problem says: \"NEVER remove boundary checks or out-of-bound masks\". We must mask.\n\n    We can do:\n\n        h_offsets = tl.arange(0, hidden_size)\n        i_offsets = tl.arange(0, input_size)\n        h_mask = h_offsets < hidden_size\n        i_mask = i_offsets < input_size\n        # Create a 2D mask: h_mask[:, None] & i_mask[None, :]\n\n        weight_offsets = h_offsets[:, None] * stride_weight_hidden + i_offsets[None, :] * stride_weight_feature\n        weight_matrix = tl.load(weight_ptr + weight_offsets, mask=h_mask[:, None] & i_mask[None, :], other=0.0)\n\n    Similarly, for bias: we can load the entire vector.\n\n        bias_vector = tl.load(bias_ptr + h_offsets * stride_bias, mask=h_mask, other=0.0)\n\n    Then, we can compute:\n\n        # Matrix-vector multiplication: weight_matrix (hidden_size, input_size) * x_row (input_size,)\n        # We do: for each row j: dot(weight_matrix[j, :], x_row)\n        # Then add bias_vector[j] and then sigmoid.\n\n        linear_out = tl.dot(weight_matrix, x_row) + bias_vector   # but note: tl.dot might not support 2D and 1D? \n        Actually, Triton does have a dot operation? We can do:\n\n        # We can do a reduction per row? Or use tl.sum(weight_matrix * x_row[None, :], axis=1)\n\n        linear_out = tl.sum(weight_matrix * x_row[None, :], axis=1) + bias_vector\n\n    Then, we apply sigmoid and then sum over the hidden units.\n\n        sigmoid_out = 1.0 / (1.0 + tl.exp(-linear_out))\n        row_sum = tl.sum(sigmoid_out)\n\n 10. However, note that the input_size is 10 and hidden_size is 20, which are small. This might be efficient because we avoid the loop over hidden_size and do everything in parallel.\n\n 11. But we must be cautious: the entire weight_matrix is 200 elements, and we are loading it for every batch element. That is 200 * batch_size loads. Previously we were doing (for each batch element) 20 rows of the weight matrix (each row of length 10) -> 200 loads per batch element. So the total global memory reads are the same? Actually, in the original we were doing:\n        Input row: 10 loads (but with BLOCK_INPUT, which is 16, so 16 loads? but masked so only 10 are used)\n        For each j in [0,19]: \n            weight row: 16 loads (but only 10 used) -> 20*16 = 320\n            bias: 1 load -> 20\n        Total: 16 + 320 + 20 = 356\n\n     Now, we are doing:\n        Input row: 16\n        Weight matrix: 20*16? (if we use the same BLOCK for input_size? Actually, we are loading by input_size, but we can load exactly the input_size? We don't have to use BLOCK_INPUT for the weight matrix? We can load the entire weight matrix without padding? But note: the kernel requires the offsets to be within the same block? We can use a 2D block.\n\n 12. Alternatively, we can avoid using the power-of-two block for the weight matrix? Because the weight matrix is small. We can load the entire weight matrix and bias vector without padding? But the kernel requires that we use the same masking as the input row? Actually, we can create two separate sets of offsets: one for the input row (with BLOCK_INPUT) and one for the weight matrix (with exact dimensions). However, the problem says we must not remove boundary checks. So we can do:\n\n        # For input row: we use the existing method (with BLOCK_INPUT and mask for input_size)\n        # For weight matrix: we create a 2D grid of indices for [hidden_size, input_size] and load with a mask for both dimensions.\n\n 13. But note: the kernel currently uses a 1D grid (batch_size). We cannot change that. We are only allowed to change the Triton kernel source within the function.\n\n 14. Let's restructure the kernel:\n\n    We precompute:\n        offs_i = tl.arange(0, input_size)   # for the input row, we only need the actual elements\n        # But note: we cannot use input_size as a block? We are currently using BLOCK_INPUT (power-of-two) and masking.\n\n    We are constrained to use the BLOCK_INPUT as the block for the input dimension? Actually, we can change the way we load the weight matrix independently.\n\n 15. We'll do:\n\n        # Load the input row as before (using BLOCK_INPUT and mask for input_size)\n        # Then, we create two arrays: \n        h_idx = tl.arange(0, hidden_size)\n        i_idx = tl.arange(0, input_size)\n\n        # But note: we cannot have two dynamic ranges? We can use nested loops? Or we can use a 2D grid? We don't have a 2D grid.\n\n 16. Alternatively, we can load the entire weight matrix by looping over hidden_size and input_size? But that would be a double loop? We want to avoid that.\n\n 17. We can use the fact that the entire weight matrix is small and load it in a vectorized way by flattening:\n\n        weight_flat = tl.load(weight_ptr + h_idx[:, None] * stride_weight_hidden + i_idx[None, :] * stride_weight_feature, ...)\n\n    But note: the offsets array would be of size hidden_size * input_size, which is 200. This is acceptable.\n\n 18. Implementation:\n\n        # Input row: we load with the existing method (using BLOCK_INPUT and mask for the actual input_size)\n        # But note: the input row we have is of length BLOCK_INPUT, but we only need the first input_size elements. We can extract the first input_size elements? We have a mask, so the rest are zero.\n\n        # However, for the weight matrix, we want to load exactly the hidden_size * input_size elements. We can do:\n\n        h_idx = tl.arange(0, hidden_size)\n        i_idx = tl.arange(0, input_size)\n\n        # Create 2D indices: [h_idx, i_idx] for every element in the weight matrix\n        # We have to create a 2D grid of indices? We can use:\n        h_offsets_2d = h_idx[:, None] * stride_weight_hidden + i_idx[None, :] * stride_weight_feature\n        # Then we load with a mask that both h_idx and i_idx are in range? But note: we know hidden_size and input_size, and we created the indices to be exactly in range? So we don't need a mask? But to be safe, we do:\n\n        mask_2d = (h_idx[:, None] < hidden_size) & (i_idx[None, :] < input_size)\n        weight_matrix = tl.load(weight_ptr + h_offsets_2d, mask=mask_2d, other=0.0)\n\n        # Similarly, for bias:\n        bias_vector = tl.load(bias_ptr + h_idx * stride_bias, mask=h_idx < hidden_size, other=0.0)\n\n    Then, we can compute:\n\n        # We have x_row of length BLOCK_INPUT, but we only care about the first input_size elements? Actually, we have the mask so the rest are zero. So we can use the entire x_row? But note: the weight_matrix is of shape [hidden_size, input_size] and we are going to do:\n        #   linear_out[j] = sum_{i=0}^{input_size-1} weight_matrix[j, i] * x_row[i]\n        # But our x_row has BLOCK_INPUT elements, but we only have input_size non-zero? So we can do:\n\n        # We create a new x_row_short of length input_size? Actually, we can use the mask to zero out the rest? We already have.\n\n        # Now, we want to compute: for each j, dot(weight_matrix[j, :], x_row[0:input_size])\n        # How? We can do:\n        #   linear_out = tl.sum(weight_matrix * x_row[None, 0:input_size], axis=1)   -> but note: x_row is of length BLOCK_INPUT, but we are using the entire vector? We have to be careful: we have the mask so beyond input_size it's zero. So we can do:\n\n        # But note: the weight_matrix has input_size columns? So we can do:\n\n        linear_out = tl.sum(weight_matrix * x_row[None, :input_size], axis=1)   # This is not valid in Triton.\n\n        # Alternatively, we can use tl.dot? But Triton doesn't have a batched dot for this.\n\n        # We can do a loop over hidden_size? But that would be 20 iterations, which is acceptable? But we are trying to avoid the loop because we want to parallelize.\n\n        # Instead, we can do:\n\n        #   linear_out = tl.sum(weight_matrix * x_row[None, :], axis=1)   # but note: x_row[None, :] is of shape (1, BLOCK_INPUT) and weight_matrix is (hidden_size, input_size) -> but input_size might be less than BLOCK_INPUT? We have to be careful: the weight_matrix has only input_size columns? Actually, we loaded it for input_size, so the weight_matrix is (hidden_size, input_size). But we are multiplying by x_row which is of length BLOCK_INPUT? We cannot do that.\n\n        # We must have the weight_matrix and x_row aligned in the input dimension? We have two options:\n\n        Option 1: Load the input row without padding? We can avoid the power-of-two block for the input row? But the problem says we must not remove boundary checks. We can change the input row loading to be exactly input_size? But note: the input_size is 10, which is small. We can set BLOCK_INPUT = input_size? But then we don't need the mask? But the problem says: \"NEVER remove boundary checks or out-of-bound masks\". So we must keep the mask even if BLOCK_INPUT==input_size? Actually, we can set BLOCK_INPUT to input_size and then the mask would be always true? That is acceptable.\n\n        Option 2: We load the weight matrix with the same block size for the input dimension? But that would be complicated.\n\n 19. Let's change the input row loading to use a block size of input_size? But note: the problem says we must use the grid configuration and output tensor shapes as given. The grid is (batch_size,). We are allowed to change the block size? The BLOCK_INPUT is a constant we pass. We can set it to input_size? But note: input_size might not be a power of two. The current code uses triton.next_power_of_2. We are allowed to change that? The problem says: \"Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions\" is allowed.\n\n 20. However, the input_size is 10, so the next power of two is 16. We are currently using 16. If we change to use exactly input_size (10) then we avoid the extra zeros? But note: the weight matrix is 20x10. We can load it as 20x10? Then we can do:\n\n        linear_out = tl.sum(weight_matrix * x_row[None, :], axis=1)   # but x_row is of length 16? We only want the first 10.\n\n        We can do: x_row_slice = x_row[0:input_size]   -> but Triton doesn't support slicing? We can use a mask? We can create a mask for the input_size? Actually, we have the mask for the input row? We know that the first input_size elements are valid.\n\n        But note: we loaded the input row with mask, so the elements beyond input_size are zero. Therefore, when we multiply the weight_matrix (which has input_size columns) by x_row (which has BLOCK_INPUT elements, but only the first input_size are non-zero), then the product will be correct? Because the weight_matrix for the columns beyond input_size are zero (by our mask_2d). Actually, we loaded the weight_matrix only for input_size columns? So the weight_matrix is of shape (hidden_size, input_size). Then we are multiplying by a row of length BLOCK_INPUT (16) and we are broadcasting the weight_matrix to (hidden_size, 16)? No, we are not.\n\n        We have:\n            weight_matrix: (hidden_size, input_size) -> 20x10\n            x_row: (16,) -> but we want to multiply each row of the weight_matrix by the first 10 elements of x_row? We can do:\n\n            linear_out = tl.sum(weight_matrix * x_row[None, 0:input_size], axis=1)   -> but this is not valid.\n\n        We can do:\n\n            # We need to extend the weight_matrix to have BLOCK_INPUT columns? We can zero-pad? But that would be inefficient.\n\n 21. Given the complexity, let's stick to the original method for the input row (using BLOCK_INPUT and mask) and then for the weight matrix, we load only the required part (with input_size) and then use a loop over hidden_size? We are only looping 20 times, which is acceptable? But we want to improve performance.\n\n 22. Alternatively, we can precompute the linear transformation without the loop by using tl.dot? But note: the input row is 1xinput_size and the weight matrix is hidden_size x input_size -> the output is 1xhidden_size. We can do:\n\n        linear_out = tl.dot(x_row, weight_matrix, allow_tf32=False)   # but note: the dimensions? We have x_row (1, BLOCK_INPUT) and weight_matrix (BLOCK_INPUT, hidden_size) -> then (1, hidden_size). But we have to transpose the weight_matrix? The weight matrix we loaded is (hidden_size, input_size) -> which is row-major? We can transpose it to (input_size, hidden_size) and then use:\n\n        linear_out = tl.dot(x_row, weight_matrix_transposed)   # but then we have to transpose.\n\n        We can load the weight matrix in column-major? But the original is row-major.\n\n        We can do:\n\n            linear_out = tl.dot(weight_matrix, x_row)   # (hidden_size, input_size) dot (input_size, 1) -> (hidden_size, 1)\n\n        That would be a matrix-vector product. Triton has tl.dot that supports matrix-vector? \n\n        According to Triton documentation: \n            tl.dot(input, other) can be used for:\n                - vector-vector -> scalar\n                - matrix-vector -> vector\n                - matrix-matrix -> matrix\n\n        So we can do:\n\n            linear_out = tl.dot(weight_matrix, x_row)   # but note: x_row is a vector of length BLOCK_INPUT (16) but we only have input_size (10) non-zero? And the weight_matrix is (20,10). How can we multiply a (20,10) matrix by a (16,) vector? We cannot.\n\n        We need to pass the x_row of length 10? We cannot.\n\n 23. We decide to change the input row loading to use exactly input_size (without padding). We can set BLOCK_INPUT = input_size? But note: the problem says we can change the BLOCK dimensions. So let's set BLOCK_INPUT = input_size (10) and then we don't need the mask? But we still need the mask because we must have boundary checks? Actually, the mask would be:\n\n        mask = tl.arange(0, BLOCK_INPUT) < input_size   -> which is always true? But we must keep the mask.\n\n 24. Then, we can do:\n\n        x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)   # with mask being all true.\n\n        Then, for the weight matrix, we load it as (hidden_size, input_size) and then:\n\n        linear_out = tl.dot(weight_matrix, x_row)   # (hidden_size, input_size) dot (input_size) -> (hidden_size)\n\n        Then add bias_vector -> then sigmoid -> then sum.\n\n 25. Steps:\n\n        BLOCK_INPUT = input_size   # but note: input_size might not be a power of two? The hardware might prefer power-of-two? We are allowed to change the block size. We can set it to input_size? The kernel will work.\n\n        Then:\n\n            pid = tl.program_id(0)\n            input_start = pid * stride_x_batch\n            input_offsets = input_start + tl.arange(0, BLOCK_INPUT) * stride_x_feature\n            mask = tl.arange(0, BLOCK_INPUT) < input_size   # always true? But we keep it.\n\n            x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)\n\n            # Preload the entire weight matrix and bias vector:\n\n            # Create indices for hidden_size and input_size\n            h_idx = tl.arange(0, hidden_size)\n            i_idx = tl.arange(0, input_size)\n\n            # For weight matrix: we create a 2D index\n            w_offsets = h_idx[:, None] * stride_weight_hidden + i_idx[None, :] * stride_weight_feature\n            w_mask = (h_idx[:, None] < hidden_size) & (i_idx[None, :] < input_size)\n            weight_matrix = tl.load(weight_ptr + w_offsets, mask=w_mask, other=0.0)   # (hidden_size, input_size)\n\n            # For bias:\n            b_offsets = h_idx * stride_bias\n            b_mask = h_idx < hidden_size\n            bias_vector = tl.load(bias_ptr + b_offsets, mask=b_mask, other=0.0)\n\n            # Compute linear_out = weight_matrix @ x_row + bias_vector\n            linear_out = tl.dot(weight_matrix, x_row) + bias_vector\n\n            # Apply sigmoid and sum\n            sigmoid_out = 1.0 / (1.0 + tl.exp(-linear_out))\n            row_sum = tl.sum(sigmoid_out)\n\n            # Store row_sum\n\n 26. But note: the tl.dot operation requires that the dimensions are multiples of 16 for Tensor Cores? Our input_size=10 and hidden_size=20, which are small and not multiples of 16. So it might not use Tensor Cores? But that's okay because the problem size is small.\n\n 27. Also, note: the tl.dot operation might be efficient for small matrices? We hope.\n\n 28. We must change the BLOCK_INPUT to be input_size? But what if input_size is not a power of two? The problem says the dataset has 250 kernelbench files with levels 1-5. We don't know the input_size for all. But the current program uses:\n\n        BLOCK_INPUT = triton.next_power_of_2(self.input_size)\n\n    We are going to change that to:\n\n        BLOCK_INPUT = input_size   # in the kernel call\n\n    However, note that the kernel is compiled for each set of constants. If we set BLOCK_INPUT to input_size, then we will recompile for every different input_size? That might be bad if we have many. But the problem says the input_size is fixed per model? The model is initialized with input_size and hidden_size.\n\n    In the ModelNew class, we set:\n\n        BLOCK_INPUT = self.input_size   # instead of next_power_of_2\n\n 29. But note: the input_size might be 10, which is not a power of two. The tl.dot might not be efficient? But it's small.\n\n 30. Alternatively, we can keep the BLOCK_INPUT as the next_power_of_two for the input_size? Then we would have to load the input row with padding, and then we would have to pad the weight matrix? That would be messy.\n\n 31. Let's try with setting BLOCK_INPUT = input_size.\n\n 32. We change the kernel call in the forward function:\n\n        # Instead of:\n        #   BLOCK_INPUT = triton.next_power_of_2(self.input_size)\n        # we do:\n        BLOCK_INPUT = self.input_size\n\n 33. But note: the kernel uses BLOCK_INPUT in the tl.arange(0, BLOCK_INPUT). For input_size=10, that's 10. Then we load the input row with 10 elements. Then the weight matrix is loaded as 20x10. Then we do a dot product of (20,10) and (10,). This should be efficient.\n\n 34. We hope that this will reduce the number of global memory accesses and also use an optimized dot product.\n\n 35. Let's write the kernel accordingly.\n\n 36. We must be cautious: the tl.dot function in Triton might not support non-power-of-two? But the documentation doesn't say. We'll try.\n\n 37. Code for the kernel:\n\n        @triton.jit\n        def _forward_kernel(\n            x_ptr,\n            weight_ptr,\n            bias_ptr,\n            output_ptr,\n            stride_x_batch,\n            stride_x_feature,\n            stride_weight_hidden,\n            stride_weight_feature,\n            stride_bias,\n            stride_out_batch,\n            BLOCK_INPUT: tl.constexpr,\n            input_size: tl.constexpr,\n            hidden_size: tl.constexpr,\n        ):\n            pid = tl.program_id(0)\n\n            # Load input row: we use BLOCK_INPUT which is set to input_size\n            input_start = pid * stride_x_batch\n            input_offsets = input_start + tl.arange(0, BLOCK_INPUT) * stride_x_feature\n            mask = tl.arange(0, BLOCK_INPUT) < input_size   # This will be all true if BLOCK_INPUT==input_size\n            x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)\n\n            # Preload weight matrix: (hidden_size, input_size)\n            # Create indices for hidden and input dimensions\n            h_idx = tl.arange(0, hidden_size)\n            i_idx = tl.arange(0, input_size)\n\n            # Create 2D index for weight matrix\n            w_offsets = h_idx[:, None] * stride_weight_hidden + i_idx[None, :] * stride_weight_feature\n            # Create 2D mask: both indices must be in range\n            w_mask = (h_idx[:, None] < hidden_size) & (i_idx[None, :] < input_size)\n            weight_matrix = tl.load(weight_ptr + w_offsets, mask=w_mask, other=0.0)\n\n            # Preload bias vector\n            b_offsets = h_idx * stride_bias\n            b_mask = h_idx < hidden_size\n            bias_vector = tl.load(bias_ptr + b_offsets, mask=b_mask, other=0.0)\n\n            # Compute linear transformation: (hidden_size, input_size) @ (input_size) -> (hidden_size)\n            linear_out = tl.dot(weight_matrix, x_row) + bias_vector\n\n            # Apply sigmoid\n            sigmoid_out = 1.0 / (1.0 + tl.exp(-linear_out))\n\n            # Sum over hidden_size\n            row_sum = tl.sum(sigmoid_out)\n\n            # Store result\n            output_offset = pid * stride_out_batch\n            tl.store(output_ptr + output_offset, row_sum)\n\n 38. In the ModelNew.forward, we set:\n        BLOCK_INPUT = self.input_size   # instead of next_power_of_two\n\n 39. But note: the kernel uses `input_size` and `hidden_size` as kernel arguments. They are known at launch. The Triton compiler will specialize the kernel for these values.\n\n 40. We hope that this will be faster because:\n        - We are loading the entire weight matrix and bias vector in two large vectorized loads (instead of 20+20 small loads for weight rows and bias elements).\n        - We are using tl.dot for the matrix-vector product, which might be optimized.\n\n 41. However, note: the weight matrix is stored in row-major, and we are loading it in row-major. The tl.dot expects the first argument to be the matrix and the second to be the vector. The matrix is (hidden_size, input_size) and the vector is (input_size,). This is a standard matrix-vector product.\n\n 42. We try this and hope for a speedup.\n\n 43. We must be cautious: the tl.dot might require the dimensions to be multiples of 16? But for small sizes, it might use a fallback.\n\n 44. Let's rewrite the current program accordingly.\n\n 45. We note: the current program has a loop over hidden_size (20 iterations). The new program does not have that loop.\n\n 46. We output the entire code block.\n\nIMPORTANT: We must not change the function signatures or the grid. We are only changing the kernel body and the BLOCK_INPUT assignment in the forward function.\n\nLet's write the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    stride_x_batch,\n    stride_x_feature,\n    stride_weight_hidden,\n    stride_weight_feature,\n    stride_bias,\n    stride_out_batch,\n    BLOCK_INPUT: tl.constexpr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Load entire input row\n    input_start = pid * stride_x_batch\n    input_offsets = input_start + tl.arange(0, BLOCK_INPUT) * stride_x_feature\n    mask = tl.arange(0, BLOCK_INPUT) < input_size\n    x_row = tl.load(x_ptr + input_offsets, mask=mask, other=0.0)\n    \n    # Preload entire weight matrix and bias vector\n    h_idx = tl.arange(0, hidden_size)\n    i_idx = tl.arange(0, input_size)\n    \n    # Create 2D index for weight matrix\n    w_offsets = h_idx[:, None] * stride_weight_hidden + i_idx[None, :] * stride_weight_feature\n    w_mask = (h_idx[:, None] < hidden_size) & (i_idx[None, :] < input_size)\n    weight_matrix = tl.load(weight_ptr + w_offsets, mask=w_mask, other=0.0)\n    \n    # Load bias vector\n    b_offsets = h_idx * stride_bias\n    b_mask = h_idx < hidden_size\n    bias_vector = tl.load(bias_ptr + b_offsets, mask=b_mask, other=0.0)\n    \n    # Compute matrix-vector product\n    linear_out = tl.dot(weight_matrix, x_row) + bias_vector\n    \n    # Apply sigmoid and sum\n    sigmoid_out = 1.0 / (1.0 + tl.exp(-linear_out))\n    row_sum = tl.sum(sigmoid_out)\n    \n    # Store result\n    output_offset = pid * stride_out_batch\n    tl.store(output_ptr + output_offset, row_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, 1, device=x.device, dtype=x.dtype)\n        \n        # Ensure tensor memory layout is contiguous\n        weight = self.linear.weight.contiguous()\n        bias = self.linear.bias.contiguous()\n        \n        # Use exact input size for block dimension\n        BLOCK_INPUT = self.input_size\n        \n        # Launch kernel\n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, weight, bias, output,\n            x.stride(0), x.stride(1),\n            weight.stride(0), weight.stride(1),\n            bias.stride(0),\n            output.stride(0),\n            BLOCK_INPUT,\n            self.input_size,\n            self.hidden_size,\n        )\n        return output\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
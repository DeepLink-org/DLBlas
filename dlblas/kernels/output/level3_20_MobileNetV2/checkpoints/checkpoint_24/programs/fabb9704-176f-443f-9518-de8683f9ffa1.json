{"id": "fabb9704-176f-443f-9518-de8683f9ffa1", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _adaptive_avg_pool2d_1x1_kernel(\n    input_ptr,\n    output_ptr,\n    channels,\n    input_height,\n    input_width,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_batch_ptr = input_ptr + pid_batch * input_batch_stride\n    input_channel_ptr = input_batch_ptr + pid_channel * input_channel_stride\n    output_batch_ptr = output_ptr + pid_batch * output_batch_stride\n    output_ptr_pos = output_batch_ptr + pid_channel * output_channel_stride\n\n    total_elements = input_height * input_width\n    accumulator = 0.0\n    MAX_VECTOR_SIZE = 1024\n    \n    for linear_base in range(0, total_elements, MAX_VECTOR_SIZE):\n        num_elements = min(MAX_VECTOR_SIZE, total_elements - linear_base)\n        linear_idx = linear_base + tl.arange(0, num_elements)\n        mask = linear_idx < total_elements\n        h = linear_idx // input_width\n        w = linear_idx % input_width\n        offsets = h * input_height_stride + w * input_width_stride\n        values = tl.load(input_channel_ptr + offsets, mask=mask, other=0.0)\n        accumulator += tl.sum(values)\n\n    avg = accumulator / total_elements\n    tl.store(output_ptr_pos, avg)\n\ndef adaptive_avg_pool2d_1x1(x):\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    \n    grid = (batch, channels)\n    _adaptive_avg_pool2d_1x1_kernel[grid](\n        x, output,\n        channels, height, width,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n    )\n    return output\n\nclass AdaptiveAvgPool2d1x1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, x):\n        return adaptive_avg_pool2d_1x1(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        MobileNetV2 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes. Default is 1000.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        def _make_divisible(v, divisor, min_value=None):\n            \"\"\"\n            This function ensures that the number of channels is divisible by the divisor.\n            \"\"\"\n            if min_value is None:\n                min_value = divisor\n            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_v < 0.9 * v:\n                new_v += divisor\n            return new_v\n\n        def _inverted_residual_block(inp, oup, stride, expand_ratio):\n            \"\"\"\n            Inverted Residual Block for MobileNetV2.\n            \"\"\"\n            hidden_dim = int(inp * expand_ratio)\n            use_res_connect = stride == 1 and inp == oup\n\n            layers = []\n            if expand_ratio != 1:\n                # Pointwise convolution\n                layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))\n                layers.append(nn.BatchNorm2d(hidden_dim))\n                layers.append(nn.ReLU6(inplace=True))\n\n            layers.extend([\n                # Depthwise convolution\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # Pointwise linear convolution\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ])\n\n            if use_res_connect:\n                return nn.Sequential(*layers), True\n            else:\n                return nn.Sequential(*layers), False\n\n        # MobileNetV2 architecture\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # Building first layer\n        features = [nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n                    nn.BatchNorm2d(input_channel),\n                    nn.ReLU6(inplace=True)]\n\n        # Building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_inverted_residual_block(input_channel, output_channel, stride, expand_ratio=t)[0])\n                input_channel = output_channel\n\n        # Building last several layers\n        features.append(nn.Conv2d(input_channel, last_channel, 1, 1, 0, bias=False))\n        features.append(nn.BatchNorm2d(last_channel))\n        features.append(nn.ReLU6(inplace=True))\n\n        # Replace PyTorch adaptive pooling with Triton implementation\n        features.append(AdaptiveAvgPool2d1x1())\n\n        self.features = nn.Sequential(*features)\n\n        # Linear layer\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.0),\n            nn.Linear(last_channel, num_classes),\n        )\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MobileNetV2 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "8a79235e-729d-4e7c-8c4a-e790181b9cb8", "generation": 3, "timestamp": 1754587051.4602156, "iteration_found": 12, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_20_MobileNetV2", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 574, in arange\n    raise ValueError(\"arange's arguments must be of type tl.constexpr\")\nValueError: arange's arguments must be of type tl.constexpr\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp4b6u9ml6.py\", line 180, in forward\n    x = self.features(x)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\n    input = module(input)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp4b6u9ml6.py\", line 65, in forward\n    return adaptive_avg_pool2d_1x1(x)\n  File \"/tmp/tmp4b6u9ml6.py\", line 52, in adaptive_avg_pool2d_1x1\n    _adaptive_avg_pool2d_1x1_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 28:35:\n    input_batch_ptr = input_ptr + pid_batch * input_batch_stride\n    input_channel_ptr = input_batch_ptr + pid_channel * input_channel_stride\n    output_batch_ptr = output_ptr + pid_batch * output_batch_stride\n    output_ptr_pos = output_batch_ptr + pid_channel * output_channel_stride\n\n    total_elements = input_height * input_width\n    accumulator = 0.0\n    MAX_VECTOR_SIZE = 1024\n\n    for linear_base in range(0, total_elements, MAX_VECTOR_SIZE):\n        num_elements = min(MAX_VECTOR_SIZE, total_elements - linear_base)\n        linear_idx = linear_base + tl.arange(0, num_elements)\n                                   ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 574, in arange\n    raise ValueError(\"arange's arguments must be of type tl.constexpr\")\nValueError: arange's arguments must be of type tl.constexpr\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 2.96, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.791, "runtime_ms": 3.74, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 2.96, "speed_up": 0.791, "custom_timing": 3.74}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.7910\n- runtime_ms: 3.7400\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 2.96, 'speed_up': 0.791, 'custom_timing': 3.74}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level3_20_MobileNetV2', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'max_difference': ['0.730363', '0.757352', '0.806790', '0.736070', '0.783991'], 'avg_difference': ['0.163859', '0.163535', '0.163955', '0.163645', '0.163347'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 2.96, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7910, runtime_ms: 3.7400, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 2.96, 'speed_up': 0.791, 'custom_timing': 3.74}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.4930, runtime_ms: 6.0000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 2.96, 'speed_up': 0.493, 'custom_timing': 6.0}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _adaptive_avg_pool2d_1x1_kernel(\n    input_ptr,\n    output_ptr,\n    channels,\n    input_height,\n    input_width,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n):\n    batch_idx = tl.program_id(0)\n    input_batch_ptr = input_ptr + batch_idx * input_batch_stride\n    output_batch_ptr = output_ptr + batch_idx * output_batch_stride\n\n    for channel_idx in range(0, channels):\n        input_channel_ptr = input_batch_ptr + channel_idx * input_channel_stride\n        output_ptr_pos = output_batch_ptr + channel_idx * output_channel_stride\n\n        total = 0.0\n        for h in range(0, input_height):\n            for w in range(0, input_width):\n                offset = h * input_height_stride + w * input_width_stride\n                val = tl.load(input_channel_ptr + offset)\n                total += val\n\n        count = input_height * input_width\n        avg = total / count\n        tl.store(output_ptr_pos, avg)\n\ndef adaptive_avg_pool2d_1x1(x):\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    \n    grid = (batch,)\n    _adaptive_avg_pool2d_1x1_kernel[grid](\n        x, output,\n        channels, height, width,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n    )\n    return output\n\nclass AdaptiveAvgPool2d1x1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, x):\n        return adaptive_avg_pool2d_1x1(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        MobileNetV2 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes. Default is 1000.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        def _make_divisible(v, divisor, min_value=None):\n            \"\"\"\n            This function ensures that the number of channels is divisible by the divisor.\n            \"\"\"\n            if min_value is None:\n                min_value = divisor\n            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_v < 0.9 * v:\n                new_v += divisor\n            return new_v\n\n        def _inverted_residual_block(inp, oup, stride, expand_ratio):\n            \"\"\"\n            Inverted Residual Block for MobileNetV2.\n            \"\"\"\n            hidden_dim = int(inp * expand_ratio)\n            use_res_connect = stride == 1 and inp == oup\n\n            layers = []\n            if expand_ratio != 1:\n                # Pointwise convolution\n                layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))\n                layers.append(nn.BatchNorm2d(hidden_dim))\n                layers.append(nn.ReLU6(inplace=True))\n\n            layers.extend([\n                # Depthwise convolution\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # Pointwise linear convolution\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ])\n\n            if use_res_connect:\n                return nn.Sequential(*layers), True\n            else:\n                return nn.Sequential(*layers), False\n\n        # MobileNetV2 architecture\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # Building first layer\n        features = [nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n                    nn.BatchNorm2d(input_channel),\n                    nn.ReLU6(inplace=True)]\n\n        # Building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_inverted_residual_block(input_channel, output_channel, stride, expand_ratio=t)[0])\n                input_channel = output_channel\n\n        # Building last several layers\n        features.append(nn.Conv2d(input_channel, last_channel, 1, 1, 0, bias=False))\n        features.append(nn.BatchNorm2d(last_channel))\n        features.append(nn.ReLU6(inplace=True))\n\n        # Replace PyTorch adaptive pooling with Triton implementation\n        features.append(AdaptiveAvgPool2d1x1())\n\n        self.features = nn.Sequential(*features)\n\n        # Linear layer\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.0),\n            nn.Linear(last_channel, num_classes),\n        )\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MobileNetV2 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.4930), Performs well on runtime_ms (6.0000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 2.96, 'speed_up': 0.493, 'custom_timing': 6.0})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _adaptive_avg_pool2d_1x1_kernel(\n    input_ptr,\n    output_ptr,\n    channels,\n    input_height,\n    input_width,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_batch_ptr = input_ptr + pid_batch * input_batch_stride\n    input_channel_ptr = input_batch_ptr + pid_channel * input_channel_stride\n    output_batch_ptr = output_ptr + pid_batch * output_batch_stride\n    output_ptr_pos = output_batch_ptr + pid_channel * output_channel_stride\n\n    accumulator = 0.0\n    total_elements = input_height * input_width\n    \n    for h in range(0, input_height):\n        base_offset = h * input_height_stride\n        for w_start in range(0, input_width, BLOCK_SIZE):\n            w_offsets = w_start + tl.arange(0, BLOCK_SIZE)\n            mask = w_offsets < input_width\n            offsets = base_offset + w_offsets * input_width_stride\n            values = tl.load(input_channel_ptr + offsets, mask=mask, other=0.0)\n            accumulator += tl.sum(values, axis=0)\n\n    avg = accumulator / total_elements\n    tl.store(output_ptr_pos, avg)\n\ndef adaptive_avg_pool2d_1x1(x):\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    \n    grid = (batch, channels)\n    BLOCK_SIZE = 128\n    _adaptive_avg_pool2d_1x1_kernel[grid](\n        x, output,\n        channels, height, width,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE\n    )\n    return output\n\nclass AdaptiveAvgPool2d1x1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, x):\n        return adaptive_avg_pool2d_1x1(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        MobileNetV2 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes. Default is 1000.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        def _make_divisible(v, divisor, min_value=None):\n            \"\"\"\n            This function ensures that the number of channels is divisible by the divisor.\n            \"\"\"\n            if min_value is None:\n                min_value = divisor\n            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_v < 0.9 * v:\n                new_v += divisor\n            return new_v\n\n        def _inverted_residual_block(inp, oup, stride, expand_ratio):\n            \"\"\"\n            Inverted Residual Block for MobileNetV2.\n            \"\"\"\n            hidden_dim = int(inp * expand_ratio)\n            use_res_connect = stride == 1 and inp == oup\n\n            layers = []\n            if expand_ratio != 1:\n                # Pointwise convolution\n                layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))\n                layers.append(nn.BatchNorm2d(hidden_dim))\n                layers.append(nn.ReLU6(inplace=True))\n\n            layers.extend([\n                # Depthwise convolution\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # Pointwise linear convolution\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ])\n\n            if use_res_connect:\n                return nn.Sequential(*layers), True\n            else:\n                return nn.Sequential(*layers), False\n\n        # MobileNetV2 architecture\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # Building first layer\n        features = [nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n                    nn.BatchNorm2d(input_channel),\n                    nn.ReLU6(inplace=True)]\n\n        # Building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_inverted_residual_block(input_channel, output_channel, stride, expand_ratio=t)[0])\n                input_channel = output_channel\n\n        # Building last several layers\n        features.append(nn.Conv2d(input_channel, last_channel, 1, 1, 0, bias=False))\n        features.append(nn.BatchNorm2d(last_channel))\n        features.append(nn.ReLU6(inplace=True))\n\n        # Replace PyTorch adaptive pooling with Triton implementation\n        features.append(AdaptiveAvgPool2d1x1())\n\n        self.features = nn.Sequential(*features)\n\n        # Linear layer\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.0),\n            nn.Linear(last_channel, num_classes),\n        )\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MobileNetV2 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7910), Performs well on runtime_ms (3.7400), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 2.96, 'speed_up': 0.791, 'custom_timing': 3.74})\n\n\n### Program 3 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\nclass DepthwiseConv2dTriton(nn.Module):\n    def __init__(self, in_channels, kernel_size, stride, padding, bias=True):\n        super().__init__()\n        self.in_channels = in_channels\n        self.stride = stride\n        self.padding = padding\n        self.weight = nn.Parameter(torch.Tensor(in_channels, 1, kernel_size, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(in_channels))\n        else:\n            self.bias = None\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_normal_(self.weight, mode='fan_out', nonlinearity='relu')\n        if self.bias is not None:\n            nn.init.zeros_(self.bias)\n\n    def forward(self, x):\n        batch, in_channels, in_height, in_width = x.shape\n        out_height = (in_height + 2 * self.padding - 3) // self.stride + 1\n        out_width = (in_width + 2 * self.padding - 3) // self.stride + 1\n        output = torch.empty((batch, in_channels, out_height, out_width), device=x.device, dtype=x.dtype)\n        \n        grid = lambda meta: (batch * in_channels, out_height, out_width)\n        _depthwise_conv2d_kernel[grid](\n            x, self.weight, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            self.weight.stride(0), self.weight.stride(2), self.weight.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            batch, in_channels, in_height, in_width, out_height, out_width,\n            self.stride, self.stride\n        )\n        \n        if self.bias is not None:\n            output += self.bias[:, None, None]\n        return output\n\n@triton.jit\ndef _depthwise_conv2d_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    weight_channel_stride, weight_height_stride, weight_width_stride,\n    output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n    batch_size, in_channels, in_height, in_width, out_height, out_width,\n    stride_h, stride_w\n):\n    pid_bc = tl.program_id(0)\n    pid_h = tl.program_id(1)\n    pid_w = tl.program_id(2)\n    \n    batch_idx = pid_bc // in_channels\n    channel_idx = pid_bc % in_channels\n    \n    if batch_idx >= batch_size or channel_idx >= in_channels or pid_h >= out_height or pid_w >= out_width:\n        return\n\n    in_h = pid_h * stride_h\n    in_w = pid_w * stride_w\n    acc = 0.0\n    \n    for kh in range(3):\n        for kw in range(3):\n            h = in_h + kh - 1\n            w = in_w + kw - 1\n            if h >= 0 and h < in_height and w >= 0 and w < in_width:\n                input_offset = batch_idx * input_batch_stride + channel_idx * input_channel_stride + h * input_height_stride + w * input_width_stride\n                weight_offset = channel_idx * weight_channel_stride + kh * weight_height_stride + kw * weight_width_stride\n                input_val = tl.load(input_ptr + input_offset)\n                weight_val = tl.load(weight_ptr + weight_offset)\n                acc += input_val * weight_val\n    \n    output_offset = batch_idx * output_batch_stride + channel_idx * output_channel_stride + pid_h * output_height_stride + pid_w * output_width_stride\n    tl.store(output_ptr + output_offset, acc)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        def _make_divisible(v, divisor, min_value=None):\n            if min_value is None:\n                min_value = divisor\n            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n            if new_v < 0.9 * v:\n                new_v += divisor\n            return new_v\n\n        def _inverted_residual_block(inp, oup, stride, expand_ratio):\n            hidden_dim = int(inp * expand_ratio)\n            use_res_connect = stride == 1 and inp == oup\n\n            layers = []\n            if expand_ratio != 1:\n                layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))\n                layers.append(nn.BatchNorm2d(hidden_dim))\n                layers.append(nn.ReLU6(inplace=True))\n\n            layers.extend([\n                DepthwiseConv2dTriton(hidden_dim, kernel_size=3, stride=stride, padding=1, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ])\n\n            if use_res_connect:\n                return nn.Sequential(*layers), True\n            else:\n                return nn.Sequential(*layers), False\n\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        features = [\n            nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(input_channel),\n            nn.ReLU6(inplace=True)\n        ]\n\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_inverted_residual_block(input_channel, output_channel, stride, expand_ratio=t)[0])\n                input_channel = output_channel\n\n        features.extend([\n            nn.Conv2d(input_channel, last_channel, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(last_channel),\n            nn.ReLU6(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        ])\n\n        self.features = nn.Sequential(*features)\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.0),\n            nn.Linear(last_channel, num_classes),\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level3_20_MobileNetV2', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'max_difference': ['0.730363', '0.757352', '0.806790', '0.736070', '0.783991'], 'avg_difference': ['0.163859', '0.163535', '0.163955', '0.163645', '0.163347'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 2.96, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _adaptive_avg_pool2d_1x1_kernel(\n    input_ptr,\n    output_ptr,\n    channels,\n    input_height,\n    input_width,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n):\n    batch_idx = tl.program_id(0)\n    input_batch_ptr = input_ptr + batch_idx * input_batch_stride\n    output_batch_ptr = output_ptr + batch_idx * output_batch_stride\n\n    for channel_idx in range(0, channels):\n        input_channel_ptr = input_batch_ptr + channel_idx * input_channel_stride\n        output_ptr_pos = output_batch_ptr + channel_idx * output_channel_stride\n\n        total = 0.0\n        for h in range(0, input_height):\n            for w in range(0, input_width):\n                offset = h * input_height_stride + w * input_width_stride\n                val = tl.load(input_channel_ptr + offset)\n                total += val\n\n        count = input_height * input_width\n        avg = total / count\n        tl.store(output_ptr_pos, avg)\n\ndef adaptive_avg_pool2d_1x1(x):\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    \n    grid = (batch,)\n    _adaptive_avg_pool2d_1x1_kernel[grid](\n        x, output,\n        channels, height, width,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n    )\n    return output\n\nclass AdaptiveAvgPool2d1x1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, x):\n        return adaptive_avg_pool2d_1x1(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        MobileNetV2 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes. Default is 1000.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        def _make_divisible(v, divisor, min_value=None):\n            \"\"\"\n            This function ensures that the number of channels is divisible by the divisor.\n            \"\"\"\n            if min_value is None:\n                min_value = divisor\n            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_v < 0.9 * v:\n                new_v += divisor\n            return new_v\n\n        def _inverted_residual_block(inp, oup, stride, expand_ratio):\n            \"\"\"\n            Inverted Residual Block for MobileNetV2.\n            \"\"\"\n            hidden_dim = int(inp * expand_ratio)\n            use_res_connect = stride == 1 and inp == oup\n\n            layers = []\n            if expand_ratio != 1:\n                # Pointwise convolution\n                layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))\n                layers.append(nn.BatchNorm2d(hidden_dim))\n                layers.append(nn.ReLU6(inplace=True))\n\n            layers.extend([\n                # Depthwise convolution\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # Pointwise linear convolution\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ])\n\n            if use_res_connect:\n                return nn.Sequential(*layers), True\n            else:\n                return nn.Sequential(*layers), False\n\n        # MobileNetV2 architecture\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # Building first layer\n        features = [nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n                    nn.BatchNorm2d(input_channel),\n                    nn.ReLU6(inplace=True)]\n\n        # Building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_inverted_residual_block(input_channel, output_channel, stride, expand_ratio=t)[0])\n                input_channel = output_channel\n\n        # Building last several layers\n        features.append(nn.Conv2d(input_channel, last_channel, 1, 1, 0, bias=False))\n        features.append(nn.BatchNorm2d(last_channel))\n        features.append(nn.ReLU6(inplace=True))\n\n        # Replace PyTorch adaptive pooling with Triton implementation\n        features.append(AdaptiveAvgPool2d1x1())\n\n        self.features = nn.Sequential(*features)\n\n        # Linear layer\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.0),\n            nn.Linear(last_channel, num_classes),\n        )\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MobileNetV2 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _adaptive_avg_pool2d_1x1_kernel(\n    input_ptr,\n    output_ptr,\n    channels,\n    input_height,\n    input_width,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_batch_ptr = input_ptr + pid_batch * input_batch_stride\n    input_channel_ptr = input_batch_ptr + pid_channel * input_channel_stride\n    output_batch_ptr = output_ptr + pid_batch * output_batch_stride\n    output_ptr_pos = output_batch_ptr + pid_channel * output_channel_stride\n\n    accumulator = 0.0\n    total_elements = input_height * input_width\n    \n    for h in range(0, input_height):\n        base_offset = h * input_height_stride\n        for w_start in range(0, input_width, BLOCK_SIZE):\n            w_offsets = w_start + tl.arange(0, BLOCK_SIZE)\n            mask = w_offsets < input_width\n            offsets = base_offset + w_offsets * input_width_stride\n            values = tl.load(input_channel_ptr + offsets, mask=mask, other=0.0)\n            accumulator += tl.sum(values, axis=0)\n\n    avg = accumulator / total_elements\n    tl.store(output_ptr_pos, avg)\n\ndef adaptive_avg_pool2d_1x1(x):\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    \n    grid = (batch, channels)\n    BLOCK_SIZE = 128\n    _adaptive_avg_pool2d_1x1_kernel[grid](\n        x, output,\n        channels, height, width,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE\n    )\n    return output\n\nclass AdaptiveAvgPool2d1x1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, x):\n        return adaptive_avg_pool2d_1x1(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        MobileNetV2 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes. Default is 1000.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        def _make_divisible(v, divisor, min_value=None):\n            \"\"\"\n            This function ensures that the number of channels is divisible by the divisor.\n            \"\"\"\n            if min_value is None:\n                min_value = divisor\n            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_v < 0.9 * v:\n                new_v += divisor\n            return new_v\n\n        def _inverted_residual_block(inp, oup, stride, expand_ratio):\n            \"\"\"\n            Inverted Residual Block for MobileNetV2.\n            \"\"\"\n            hidden_dim = int(inp * expand_ratio)\n            use_res_connect = stride == 1 and inp == oup\n\n            layers = []\n            if expand_ratio != 1:\n                # Pointwise convolution\n                layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))\n                layers.append(nn.BatchNorm2d(hidden_dim))\n                layers.append(nn.ReLU6(inplace=True))\n\n            layers.extend([\n                # Depthwise convolution\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # Pointwise linear convolution\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ])\n\n            if use_res_connect:\n                return nn.Sequential(*layers), True\n            else:\n                return nn.Sequential(*layers), False\n\n        # MobileNetV2 architecture\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # Building first layer\n        features = [nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n                    nn.BatchNorm2d(input_channel),\n                    nn.ReLU6(inplace=True)]\n\n        # Building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_inverted_residual_block(input_channel, output_channel, stride, expand_ratio=t)[0])\n                input_channel = output_channel\n\n        # Building last several layers\n        features.append(nn.Conv2d(input_channel, last_channel, 1, 1, 0, bias=False))\n        features.append(nn.BatchNorm2d(last_channel))\n        features.append(nn.ReLU6(inplace=True))\n\n        # Replace PyTorch adaptive pooling with Triton implementation\n        features.append(AdaptiveAvgPool2d1x1())\n\n        self.features = nn.Sequential(*features)\n\n        # Linear layer\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.0),\n            nn.Linear(last_channel, num_classes),\n        )\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MobileNetV2 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a MobileNetV2 model that includes a Triton kernel for adaptive average pooling (1x1).\n The current Triton kernel for adaptive average pooling (1x1) is implemented in the function `_adaptive_avg_pool2d_1x1_kernel`.\n The current performance shows a speed_up of 0.791 (which is less than 1, meaning slower than the reference PyTorch implementation).\n\n The current Triton kernel:\n   - Uses a grid of (batch, channels)\n   - For each channel in a batch, it iterates over every row (h) and then over the width in blocks of size BLOCK_SIZE (128).\n   - For each block in the width, it loads a vector of BLOCK_SIZE elements (with masking for boundaries) and sums them.\n   - Then it accumulates the sum for the entire channel and divides by the total number of elements.\n\n However, note that the reference PyTorch implementation of adaptive average pooling is highly optimized and might use efficient reduction kernels.\n\n Our goal is to make the Triton kernel faster than the PyTorch version (speed_up > 1).\n\n Steps for improvement:\n\n 1. The current kernel does not use tiling over the height dimension. We are iterating over every row and then over the width in blocks. This might not be optimal for memory access.\n\n 2. We can try to process multiple rows at a time and use vectorized operations to reduce the number of loads and increase the arithmetic intensity.\n\n 3. Alternatively, we can use a two-stage reduction: first reduce along the width and then along the height. This would allow us to use shared memory for intermediate results.\n\n 4. However, note that the kernel is currently launched with a grid of (batch, channels). This means we have a lot of blocks (batch * channels) and each block does a lot of work (over the entire spatial dimension). But the spatial dimension might be large (e.g., 7x7 for the last layer of MobileNetV2) but not huge.\n\n 5. The current kernel does not use shared memory. Since the entire channel is being reduced, we might be able to use shared memory to store intermediate sums and then do a reduction in shared memory.\n\n 6. But note: the total number of elements per channel is (height * width). For the last layer of MobileNetV2, the input to the adaptive pooling is 7x7, so 49 elements. This is small enough that we can do the entire reduction in one block without tiling? Actually, we are already doing it per channel.\n\n 7. However, the current kernel is written for a 2D grid (batch and channels) and each block handles one channel. The reduction is done by iterating over the entire spatial domain. The problem might be that we are not using the parallel capabilities of the GPU within the block for the reduction.\n\n 8. We can restructure the kernel to have each block handle multiple channels? But note: the grid is (batch, channels). We are already having one block per channel per batch.\n\n 9. Alternatively, we can have one block per batch and per channel, and then within the block, use multiple threads to parallelize the reduction over the spatial domain.\n\n 10. Currently, the kernel uses a single thread to do the accumulation (because the loop over h and w_start is sequential). We can parallelize the reduction within the block.\n\n 11. How about we assign each thread in the block to a portion of the spatial dimension? Then we do a parallel reduction.\n\n Revised plan:\n\n   We change the kernel to use a 2D grid: (batch, channels) and then use a 1D block of size BLOCK_SIZE (which we set to, say, 128 or 256). Then:\n\n      - Each thread in the block will process a segment of the spatial dimension (i.e., a set of pixels).\n\n      - We compute the total number of elements: N = height * width.\n\n      - We assign each thread a contiguous segment of the spatial array? Or we can use a strided approach.\n\n   However, note that the spatial data is stored in row-major order. We can flatten the spatial dimension.\n\n   Steps:\n\n      pid = tl.program_id(0)   # batch index\n      pid_channel = tl.program_id(1)   # channel index\n      pid_thread = tl.program_id(2)   # thread index within the block? But note: we are using a 1D block.\n\n   Actually, in Triton, we can define the block as 1D and then use the program_id(2) for the thread index? But the grid is 2D. We can have a 3D grid? We are currently using a 2D grid.\n\n   Alternatively, we can use:\n\n        grid = (batch, channels, num_blocks)\n\n   But that might be overkill.\n\n   Instead, we can change the kernel to use:\n\n        grid = (batch, channels)\n        and then within the kernel, we use a 1D block of threads.\n\n   How about:\n\n        pid_batch = tl.program_id(0)\n        pid_channel = tl.program_id(1)\n        num_pixels = height * width\n\n        # We will use a block of threads to reduce the entire channel.\n        # Each thread will process multiple pixels.\n\n        # We can have:\n        #   BLOCK_SIZE = 128 (the number of threads per block)\n        #   step = BLOCK_SIZE * num_blocks? Actually, we are in one block.\n\n        # We are in one block per (batch, channel). The block has BLOCK_SIZE threads.\n\n        # We can use the standard parallel reduction method.\n\n        # Step 1: Each thread loads and sums its own segment of the data.\n        # Step 2: Then we do a reduction in shared memory to sum all the partial sums.\n\n   But note: the data is 2D. We can flatten the spatial dimension.\n\n   We can do:\n\n        idx = tl.arange(0, BLOCK_SIZE)   # thread index within the block\n        base = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n        # We will iterate over the pixels in strides of BLOCK_SIZE.\n\n        accumulator = 0.0\n        for i in range(0, num_pixels, BLOCK_SIZE):\n            offset = i + idx\n            mask = offset < num_pixels\n            # Convert the linear offset to 2D: h = offset // width, w = offset % width\n            h = offset // width\n            w = offset % width\n            # But note: the strides might not be contiguous? Actually, we have the strides for height and width.\n            # We can compute the pointer offset as: h * input_height_stride + w * input_width_stride\n            # However, we must be cautious because the division and mod might be expensive.\n\n        Alternatively, we can precompute the h and w for each thread? But we are inside a loop.\n\n   But note: the spatial dimension is small (e.g., 49). So we can avoid the loop? Actually, 49 elements: we can have one block of 49 threads? But 49 is less than 128, so we can set BLOCK_SIZE to 64 and then have 64 threads. Then we only need one iteration? But we have to handle the entire 49 elements.\n\n   However, the kernel must be generic for any spatial size.\n\n   We can do:\n\n        total_elements = height * width\n        base_ptr = input_batch_ptr + pid_channel * input_channel_stride   # for the current batch and channel\n\n        accumulator = 0.0\n        # We break the total_elements into chunks of BLOCK_SIZE\n        for i in range(0, total_elements, BLOCK_SIZE):\n            offsets = i + tl.arange(0, BLOCK_SIZE)\n            mask = offsets < total_elements\n            # Compute h and w for each offset\n            h = offsets // width\n            w = offsets % width\n            ptr_offsets = h * input_height_stride + w * input_width_stride\n            # Load the values\n            vals = tl.load(base_ptr + ptr_offsets, mask=mask, other=0.0)\n            accumulator += tl.sum(vals, axis=0)\n\n        avg = accumulator / total_elements\n\n   But note: this approach does a lot of integer division and modulus operations, which can be expensive.\n\n   Alternatively, we can precompute the h and w for each linear index? But that would require storing arrays of h and w, which we cannot do.\n\n   Another approach: we can iterate over the rows and columns in a nested loop but parallelize over the columns? But that might not help.\n\n   Considering the small spatial size, the current kernel might be slow because of the overhead of launching many blocks (batch * channels) and each block doing a sequential reduction. We can try to parallelize the reduction within the block.\n\n   We can do a parallel reduction in shared memory:\n\n        total_elements = height * width\n        # We are going to use BLOCK_SIZE threads. We need to break the total_elements into segments.\n\n        # Step 1: Each thread will compute a partial sum for a segment.\n        partial = 0.0\n        for linear_idx in range(0, total_elements, BLOCK_SIZE):\n            idx_in_block = linear_idx + tl.program_id(2)   # but note: we don't have a third dimension in the grid.\n\n        How can we get the thread id? We can use tl.arange(0, BLOCK_SIZE) and then use tl.program_id(axis=0) for the batch, axis=1 for the channel, and then we are missing the thread index.\n\n   We need to change the grid to be 3D? Actually, Triton does not support 3D grids? The grid can be a tuple of up to 3 dimensions? Yes, it can be 3D.\n\n   We can do:\n\n        grid = (batch, channels, num_blocks)   # but we don't want num_blocks? We want one block per (batch, channel) and then within that block, we use BLOCK_SIZE threads.\n\n   Actually, the block in Triton is a 1D array of threads. The grid is the number of blocks. We can have:\n\n        grid = (batch, channels, 1)   # meaning we have one block per (batch, channel) and the block has BLOCK_SIZE threads.\n\n   Then in the kernel, we can use:\n\n        pid_batch = tl.program_id(0)\n        pid_channel = tl.program_id(1)\n        pid_block = tl.program_id(2)   # which is always 0? Then we don't need it.\n\n        and then we use the thread index within the block: idx = tl.arange(0, BLOCK_SIZE)\n\n   Actually, we can get the thread index by:\n\n        idx = tl.program_id(2)   # but that gives the block index, not the thread index.\n\n   Correction: In Triton, the kernel is launched with a grid of blocks and each block has a fixed number of threads (which is the BLOCK_SIZE we set as a constant). We don't have an explicit thread index in the kernel. Instead, we use:\n\n        idx = tl.arange(0, BLOCK_SIZE)\n\n   and then we use the fact that each thread in the block will have a different value for `idx`.\n\n   So we can do:\n\n        total_elements = height * width\n        base_ptr = ...   # pointer to the start of the channel\n\n        # We'll break the total_elements into chunks of BLOCK_SIZE, and each thread will process multiple elements (if total_elements > BLOCK_SIZE)\n        accumulator = 0.0\n        for linear_base in range(0, total_elements, BLOCK_SIZE):\n            linear_idx = linear_base + idx\n            mask = linear_idx < total_elements\n            # Convert linear index to 2D\n            h = linear_idx // width\n            w = linear_idx % width\n            offsets = h * input_height_stride + w * input_width_stride\n            vals = tl.load(base_ptr + offsets, mask=mask, other=0.0)\n            accumulator += tl.sum(vals)   # but note: each thread loads one element? Then we don't need tl.sum.\n\n            Actually, each thread loads one element? Then we are doing:\n\n                accumulator += vals\n\n        Then we need to do a reduction across the threads in the block to get the total sum.\n\n        We can use:\n\n            total_sum = tl.sum(accumulator, axis=0)   # but this is not supported in Triton? We have to do a reduction in shared memory.\n\n        Alternatively, we can use:\n\n            # We declare a shared memory array of size BLOCK_SIZE\n            shmem = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n            # Each thread stores its accumulator in shmem\n            shmem[idx] = accumulator\n            # Then we do a barrier and then a reduction? But Triton doesn't have shared memory? Actually, it does, but we have to use the `static` keyword.\n\n        According to the Triton documentation, we can do:\n\n            shmem = tl.zeros((BLOCK_SIZE,), dtype=tl.float32) + 0.0\n            shmem = shmem.to(tl.float32, cache=tl.CacheStrategy.STATIC)   # static means shared memory\n\n        Then:\n\n            shmem[idx] = accumulator\n            tl.debug_barrier()   # for synchronization\n\n            # Now do a tree reduction in shared memory\n            s = 1\n            while s < BLOCK_SIZE:\n                if idx % (2*s) == 0:\n                    other_idx = idx + s\n                    if other_idx < BLOCK_SIZE:\n                        shmem[idx] += shmem[other_idx]\n                s *= 2\n                tl.debug_barrier()\n\n            total_sum = shmem[0]\n\n        But note: this reduction tree requires that the block size is a power of two.\n\n        Then we can compute the average and store it.\n\n        However, note that only the first thread (idx==0) should write the result.\n\n        So:\n\n            if idx == 0:\n                tl.store(output_ptr_pos, total_sum / total_elements)\n\n   But this approach might be overkill for small spatial sizes? And note that the spatial size might be 1, 4, 7, 14, etc.\n\n   However, the current kernel is slow, so we need to try a parallel reduction.\n\n   Steps for the kernel:\n\n        total_elements = height * width\n        base_ptr = input_ptr + pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n        # Each thread in the block will accumulate a partial sum for the elements it processes.\n        accumulator = 0.0\n        # We break the total_elements into chunks of BLOCK_SIZE (the block size is the number of threads)\n        for linear_base in range(0, total_elements, BLOCK_SIZE):\n            linear_idx = linear_base + idx\n            if linear_idx < total_elements:\n                h = linear_idx // width\n                w = linear_idx % width\n                offset = h * input_height_stride + w * input_width_stride\n                val = tl.load(base_ptr + offset)\n                accumulator += val\n\n        # Now we do a reduction in shared memory\n        # Allocate shared memory\n        shmem = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        shmem = shmem.to(tl.float32, cache=tl.CacheStrategy.STATIC)   # static shared memory\n        shmem[idx] = accumulator\n        tl.debug_barrier()\n\n        # Tree reduction for the entire block\n        s = 1\n        while s < BLOCK_SIZE:\n            if idx % (2*s) == 0:\n                other_idx = idx + s\n                if other_idx < BLOCK_SIZE:\n                    shmem[idx] += shmem[other_idx]\n            s *= 2\n            tl.debug_barrier()\n\n        if idx == 0:\n            total_sum = shmem[0]\n            avg = total_sum / total_elements\n            tl.store(output_ptr_pos, avg)\n\n   We must set the block size to a power of two. We can set BLOCK_SIZE to 128, which is a power of two.\n\n   But note: the total_elements might be less than BLOCK_SIZE? Then we are still safe because the reduction tree works.\n\n   Also, note that we are using a 3D grid? Actually, we are going to change the grid to:\n\n        grid = (batch, channels, 1)   # one block per (batch, channel) and the block has BLOCK_SIZE threads.\n\n   But the kernel currently expects a 2D grid. We are going to change the kernel to use a 3D grid? Actually, the grid in Triton is defined as a tuple of up to three dimensions. We can do:\n\n        pid_batch = tl.program_id(0)\n        pid_channel = tl.program_id(1)\n        # We don't use the third dimension for the block index because we are using one block per (batch, channel)\n\n   However, we are launching the kernel with grid = (batch, channels, 1) and then the block index in the third dimension is always 0. We don't need it.\n\n   But note: the kernel function signature does not have a parameter for the third program_id. We can ignore it? Actually, we are only using two program_ids.\n\n   Alternatively, we can change the kernel to use:\n\n        pid_batch = tl.program_id(0)\n        pid_channel = tl.program_id(1)\n\n   and then the third dimension of the grid is not used? We don't have a third program_id. So we can still use a 2D grid and then set the block to have multiple threads.\n\n   How do we set the block in Triton? In the kernel launch:\n\n        _adaptive_avg_pool2d_1x1_kernel[grid, block](...)\n\n   But we are currently not setting the block. We are only setting the grid. The block is set to the default (which is 1 thread per block). We need to set the block to (BLOCK_SIZE,).\n\n   So we change the kernel launch:\n\n        grid = (batch, channels)\n        block = (BLOCK_SIZE,)\n\n        _adaptive_avg_pool2d_1x1_kernel[grid, block](...)\n\n   And then in the kernel, we get the thread index:\n\n        idx = tl.program_id(2)   # no, because we are in a 1D block, we use:\n\n        idx = tl.arange(0, BLOCK_SIZE)   # but that gives a vector of indices? Actually, we want the thread id.\n\n        Correction: In Triton, we use the `tl.arange` to get the thread index within the block? Actually, the kernel is written in a vectorized way. We use:\n\n            idx = tl.arange(0, BLOCK_SIZE)\n\n        and then each thread in the block will have a different value for `idx`.\n\n   But note: the kernel is written for a block of BLOCK_SIZE threads, and each thread will execute the same code but with a different `idx`.\n\n   However, the current kernel code uses `tl.sum` and other operations that are vectorized. We are now changing it to a per-thread scalar operation? Actually, we are going to use:\n\n        idx = tl.arange(0, BLOCK_SIZE)   # this is a vector of [0, 1, 2, ..., BLOCK_SIZE-1]\n\n        Then we cannot use it for the loop? We need to use `tl.arange` to generate the indices for the entire block.\n\n        Alternatively, we can use:\n\n            linear_idx = linear_base + idx\n\n        and then we load a vector of BLOCK_SIZE elements? That would be vectorized.\n\n        But then the reduction per thread would be the sum of a vector? Then we do:\n\n            accumulator = tl.sum(vals)   # which is the sum of the vector of BLOCK_SIZE elements for that thread? But that thread is processing multiple elements? Actually, no: each thread in the block is processing one element per iteration? Then we don't need to sum.\n\n        Actually, if we use:\n\n            linear_idx = linear_base + idx   # idx is a vector of [0,1,...,BLOCK_SIZE-1]\n            mask = linear_idx < total_elements\n            ... \n            vals = tl.load(base_ptr + offsets, mask=mask, other=0.0)   # vals is a vector of BLOCK_SIZE elements? But then we are loading one element per thread? Actually, no: the load is vectorized.\n\n        This is confusing.\n\n   Given the complexity, and since the spatial size is small, we can try a simpler approach: use one thread per (batch, channel) but use vectorized loads to load multiple consecutive elements in the same row? That is what the current kernel does (with BLOCK_SIZE=128 for the width). But the current kernel is slow.\n\n   Alternatively, we can try to precompute the entire reduction using a single vectorized load? But the spatial dimension might be 7x7=49, which is less than 128. So we can load the entire channel in one vectorized load? Then we do:\n\n        # total_elements = height * width\n        # We want to load the entire channel as a vector of length total_elements.\n        # We set BLOCK_SIZE = total_elements? But total_elements might not be a power of two? And we don't know at compile time.\n\n   We cannot do that because the kernel is compiled for a fixed BLOCK_SIZE.\n\n   Considering the constraints, we try the following:\n\n        We change the kernel to use a block of threads (with BLOCK_SIZE=128) and do a parallel reduction as described above.\n\n   However, note that the current kernel already uses a vectorized load for the width dimension. The problem might be that the spatial dimension is too small to benefit from vectorization? And the overhead of launching many blocks (batch * channels) is high.\n\n   Another idea: we can combine multiple channels in one block? For example, we can have a grid of (batch, ceil(channels / CHANNELS_PER_BLOCK)) and then each block processes CHANNELS_PER_BLOCK channels? Then we can use the threads in the block to reduce over the spatial dimension and also over the channels? But that would require storing multiple partial sums.\n\n   Given the time, we try the parallel reduction within the block for one channel.\n\n   Steps:\n\n        We set the kernel to be launched with a 2D grid (batch, channels) and a 1D block (BLOCK_SIZE).\n\n        In the kernel:\n\n            pid_batch = tl.program_id(0)\n            pid_channel = tl.program_id(1)\n            idx = tl.arange(0, BLOCK_SIZE)   # vector of thread indices in the block\n\n            total_elements = height * width\n            base_ptr = input_ptr + pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n            accumulator = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n            # We break the total_elements into chunks of BLOCK_SIZE\n            for linear_base in range(0, total_elements, BLOCK_SIZE):\n                linear_idx = linear_base + idx\n                mask = linear_idx < total_elements\n                # Convert linear index to 2D\n                h = linear_idx // width\n                w = linear_idx % width\n                ptr_offsets = h * input_height_stride + w * input_width_stride\n                vals = tl.load(base_ptr + ptr_offsets, mask=mask, other=0.0)\n                # Each thread will sum the values it loads? Actually, we want to accumulate across the entire block.\n                # But note: we are loading a vector of BLOCK_SIZE elements? Then we can do:\n                accumulator += vals   # This is a vector addition: each element of `vals` is added to the corresponding element in `accumulator`.\n\n            # Now we have a vector `accumulator` of BLOCK_SIZE partial sums.\n            # We want to reduce this vector to a single value.\n            total_sum = tl.sum(accumulator, axis=0)\n\n            # Then we store the average by the first thread? But note: all threads have the same total_sum? \n            # Actually, tl.sum returns a scalar? Or a vector of length 1? It returns a scalar.\n\n            # But we must store only once. We can have one thread store it? Or we can have every thread store? But that would be incorrect.\n\n            # We can do:\n            #   if idx[0] == 0: ... but we don't have an index for the first element? We can use:\n            #   tl.store(output_ptr_pos, total_sum / total_elements)\n\n            # But note: all threads are executing the same store? That would be inefficient but safe? However, it might be faster to have one thread store.\n\n            # We can do:\n            #   if tl.program_id(2) == 0:   # but we don't have a third program id.\n\n            # Instead, we can use the thread index: if the first thread in the block stores.\n\n            # But note: the entire block is for one channel. We only need one thread to store.\n\n            # We can use:\n            #   if idx[0] == 0:   # but idx is a vector? Actually, we are in a vectorized context.\n\n            # Alternatively, we can break out of the vectorized context? We can use:\n\n            #   tl.store(output_ptr_pos, total_sum / total_elements, mask=(idx == 0))\n\n            # But note: the output_ptr_pos is a scalar address? And we are storing a scalar.\n\n            # How about we do:\n\n            #   tl.store(output_ptr_pos, total_sum / total_elements)\n\n            # without mask? Then every thread will try to store, and that might cause a conflict? But they are storing the same value? It is safe but inefficient.\n\n            # We can try to do it only from one thread:\n\n            #   if tl.arange(0, BLOCK_SIZE)[0] == 0:   # this is not supported? We can use:\n\n            #   if tl.program_id(2) == 0:   # but we don't have a third dimension.\n\n            # Since we are in a 1D block, we can get the thread id as an integer? Actually, we cannot.\n\n            # Given the constraints, we might have to store from every thread. But that is inefficient.\n\n            # Alternatively, we can use:\n\n            #   tl.store(output_ptr_pos, total_sum / total_elements, mask=(idx == 0))\n\n            # But note: `idx` is a vector, and `idx == 0` is a vector of booleans. The store will be done for the first thread only.\n\n            # How to do: we need to extract the first element of the vector? We can do:\n\n            #   mask = (idx == 0)\n            #   tl.store(output_ptr_pos, total_sum / total_elements, mask=mask)\n\n            # But then only the first thread will store? Actually, no: the store is vectorized, and it will store only if the corresponding mask element is True. And we are storing a scalar at a scalar address? This might not work.\n\n            # Instead, we can do:\n\n            #   tl.store(output_ptr_pos + 0, total_sum / total_elements, mask=(idx[0]==0))   # not sure\n\n            # Alternatively, we can break out of the vectorized context? We cannot.\n\n        This approach might be flawed.\n\n   Given the complexity and the small spatial size, we might try to use a simpler approach: use one block per (batch, channel) and have the block size be the number of threads we need (ceil(total_elements / BLOCK_SIZE)) and then do a reduction? But we don't know total_elements at compile time.\n\n   Considering the time, we will try to use the vectorized reduction without shared memory, but using the `tl.sum` over the entire vector of values for the channel. We can load the entire channel in one go if the spatial size is small? But the maximum spatial size we might encounter is 224x224? No, in MobileNetV2 the adaptive pooling is applied to a 7x7 feature map.\n\n   Since 7x7=49, we can set BLOCK_SIZE=49? But then we have to compile for every possible spatial size? We cannot.\n\n   Alternatively, we can pad the spatial dimension to the next power of two? But that would change the result.\n\n   We decide to try the following:\n\n        total_elements = height * width\n        base_ptr = ...   # for the channel\n\n        # We want to load the entire channel as a vector of length total_elements.\n        # We can do:\n        offsets = tl.arange(0, total_elements)\n        h = offsets // width\n        w = offsets % width\n        ptr_offsets = h * input_height_stride + w * input_width_stride\n        vals = tl.load(base_ptr + ptr_offs)\n        total_sum = tl.sum(vals)\n        avg = total_sum / total_elements\n        tl.store(output_ptr_pos, avg)\n\n   But note: the total_elements might be large? We are in the last layer with 7x7=49, but what if this kernel is used elsewhere with larger sizes? We don't know.\n\n   However, in MobileNetV2, the adaptive pooling is only used at the end on a 7x7 feature map.\n\n   So we can assume that the spatial size is small enough to be loaded in one vector? The maximum vector length in Triton is 1024? And 49 is less than 1024.\n\n   We can do:\n\n        # total_elements = height * width\n        # if total_elements > 1024, then we break it into chunks? But we know in this model it's 49.\n\n   But the kernel should be general.\n\n   We can change the kernel to:\n\n        for linear_base in range(0, total_elements, MAX_VECTOR_SIZE):\n            linear_end = min(linear_base + MAX_VECTOR_SIZE, total_elements)\n            num_elements = linear_end - linear_base\n            offsets = tl.arange(0, num_elements) + linear_base\n            ... as above ...\n\n   Then accumulate the sum.\n\n   Given the small size, we might do one iteration.\n\n   Let's try with MAX_VECTOR_SIZE=1024.\n\n   We'll do:\n\n        total_sum = 0.0\n        for linear_base in range(0, total_elements, 1024):\n            linear_end = min(linear_base + 1024, total_elements)\n            num_elements = linear_end - linear_base\n            offsets = tl.arange(0, num_elements) + linear_base\n            h = offsets // width\n            w = offsets % width\n            ptr_offsets = h * input_height_stride + w * input_width_stride\n            vals = tl.load(base_ptr + ptr_offsets)\n            total_sum += tl.sum(vals)\n\n        avg = total_sum / total_elements\n        tl.store(output_ptr_pos, avg)\n\n   This should work for any spatial size.\n\n   And it is vectorized.\n\n   But will it be faster than the current sequential reduction? The current kernel already uses a vectorized load for the width dimension. The difference is that we are now doing the entire channel in one or a few iterations, and we are not having the nested loop over height.\n\n   Let's try this approach.\n\n   We change the kernel to:\n\n        pid_batch = tl.program_id(0)\n        pid_channel = tl.program_id(1)\n        input_batch_ptr = input_ptr + pid_batch * input_batch_stride\n        input_channel_ptr = input_batch_ptr + pid_channel * input_channel_stride\n        output_batch_ptr = output_ptr + pid_batch * output_batch_stride\n        output_ptr_pos = output_batch_ptr + pid_channel * output_channel_stride\n\n        total_elements = input_height * input_width\n        accumulator = 0.0\n        MAX_VECTOR_SIZE = 1024   # or we can make it a constant\n\n        for linear_base in range(0, total_elements, MAX_VECTOR_SIZE):\n            linear_end = min(linear_base + MAX_VECTOR_SIZE, total_elements)\n            num_elements = linear_end - linear_base\n            linear_idx = linear_base + tl.arange(0, num_elements)\n            h = linear_idx // input_width\n            w = linear_idx % input_width\n            offsets = h * input_height_stride + w * input_width_stride\n            values = tl.load(input_channel_ptr + offsets, mask=linear_idx < total_elements, other=0.0)\n            accumulator += tl.sum(values)\n\n        avg = accumulator / total_elements\n        tl.store(output_ptr_pos, avg)\n\n   We remove the BLOCK_SIZE argument because we are not using it.\n\n   We also change the kernel launch to use a 2D grid and a block of size (1,) because we are not using any threads within the block for parallel reduction? Actually, in this kernel, we are using vectorized operations and we are not using a block of threads. The kernel is written for one thread per (batch, channel) but uses vectorized loads of up to 1024 elements.\n\n   But note: the maximum vector size is 1024, and the total_elements might be up to (say) 224*224=50176, which would require 50176/1024=49 iterations. This might be acceptable.\n\n   Alternatively, we can set the block size to be larger and use the threads within the block to process different parts of the vector? But that would complicate the code.\n\n   Given the small spatial size in the MobileNetV2 (7x7), this kernel will do one iteration.\n\n   Let's hope it is faster.\n\n   We will remove the BLOCK_SIZE parameter and change the kernel accordingly.\n\n   We also change the kernel launch to not use a block (or use block=()? Triton defaults to one thread).\n\n   So in the function adaptive_avg_pool2d_1x1:\n\n        grid = (batch, channels)\n        _adaptive_avg_pool2d_1x1_kernel[grid](\n            x, output,\n            channels, height, width,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            # remove the BLOCK_SIZE argument\n        )\n\n   But the kernel still has the BLOCK_SIZE in the signature? We remove it.\n\n   We also remove the BLOCK_SIZE from the autotune decorator? We don't have autotune in this kernel.\n\n   Let's rewrite the kernel accordingly.\n\n   Note: we must be cautious with the integer division and modulus. But for small sizes, it might be acceptable.\n\n   Given the above, we will rewrite the kernel.\n\n   However, note that the current kernel already has a speed_up of 0.791, so we are slower than PyTorch. We hope this new kernel will be faster.\n\n   Let's code accordingly.\n\n   IMPORTANT: We must maintain the same function signature except for removing the BLOCK_SIZE.\n\n   But the function signature is:\n\n        @triton.jit\n        def _adaptive_avg_pool2d_1x1_kernel(\n            input_ptr,\n            output_ptr,\n            channels,\n            input_height,\n            input_width,\n            input_batch_stride,\n            input_channel_stride,\n            input_height_stride,\n            input_width_stride,\n            output_batch_stride,\n            output_channel_stride,\n            BLOCK_SIZE: tl.constexpr,   # we remove this\n        ):\n\n   We remove the BLOCK_SIZE.\n\n   And in the kernel launch in adaptive_avg_pool2d_1x1, we remove the last argument.\n\n   We set MAX_VECTOR_SIZE = 1024 as a constant inside the kernel.\n\n   Let's write the kernel accordingly.\n\n   We'll try this and hope for a speedup.\n\n   If it doesn't work, we can try other approaches.\n\n   We also note that the current kernel might be slow because of the nested loops and the small work per iteration. The new kernel does the work in fewer iterations and uses vectorized loads.\n\n   We hope it will be faster.\n\n   We will now generate the code accordingly.\n</think>\nTo improve the performance of the adaptive average pooling operation in the MobileNetV2 model, we'll optimize the Triton kernel by leveraging vectorized loads and reducing the number of operations. The key changes include:\n\n1. Removing the nested loops and replacing them with a single loop over flattened spatial dimensions\n2. Using vectorized loads with a maximum vector size of 1024 elements\n3. Calculating 2D coordinates from linear indices using efficient integer operations\n4. Accumulating partial sums in a single pass\n5. Maintaining correctness with proper masking\n\nHere's the optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _adaptive_avg_pool2d_1x1_kernel(\n    input_ptr,\n    output_ptr,\n    channels,\n    input_height,\n    input_width,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_batch_ptr = input_ptr + pid_batch * input_batch_stride\n    input_channel_ptr = input_batch_ptr + pid_channel * input_channel_stride\n    output_batch_ptr = output_ptr + pid_batch * output_batch_stride\n    output_ptr_pos = output_batch_ptr + pid_channel * output_channel_stride\n\n    total_elements = input_height * input_width\n    accumulator = 0.0\n    MAX_VECTOR_SIZE = 1024\n    \n    for linear_base in range(0, total_elements, MAX_VECTOR_SIZE):\n        num_elements = min(MAX_VECTOR_SIZE, total_elements - linear_base)\n        linear_idx = linear_base + tl.arange(0, num_elements)\n        mask = linear_idx < total_elements\n        h = linear_idx // input_width\n        w = linear_idx % input_width\n        offsets = h * input_height_stride + w * input_width_stride\n        values = tl.load(input_channel_ptr + offsets, mask=mask, other=0.0)\n        accumulator += tl.sum(values)\n\n    avg = accumulator / total_elements\n    tl.store(output_ptr_pos, avg)\n\ndef adaptive_avg_pool2d_1x1(x):\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    \n    grid = (batch, channels)\n    _adaptive_avg_pool2d_1x1_kernel[grid](\n        x, output,\n        channels, height, width,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n    )\n    return output\n\nclass AdaptiveAvgPool2d1x1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, x):\n        return adaptive_avg_pool2d_1x1(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        MobileNetV2 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes. Default is 1000.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        def _make_divisible(v, divisor, min_value=None):\n            \"\"\"\n            This function ensures that the number of channels is divisible by the divisor.\n            \"\"\"\n            if min_value is None:\n                min_value = divisor\n            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_v < 0.9 * v:\n                new_v += divisor\n            return new_v\n\n        def _inverted_residual_block(inp, oup, stride, expand_ratio):\n            \"\"\"\n            Inverted Residual Block for MobileNetV2.\n            \"\"\"\n            hidden_dim = int(inp * expand_ratio)\n            use_res_connect = stride == 1 and inp == oup\n\n            layers = []\n            if expand_ratio != 1:\n                # Pointwise convolution\n                layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))\n                layers.append(nn.BatchNorm2d(hidden_dim))\n                layers.append(nn.ReLU6(inplace=True))\n\n            layers.extend([\n                # Depthwise convolution\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # Pointwise linear convolution\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ])\n\n            if use_res_connect:\n                return nn.Sequential(*layers), True\n            else:\n                return nn.Sequential(*layers), False\n\n        # MobileNetV2 architecture\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # Building first layer\n        features = [nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n                    nn.BatchNorm2d(input_channel),\n                    nn.ReLU6(inplace=True)]\n\n        # Building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_inverted_residual_block(input_channel, output_channel, stride, expand_ratio=t)[0])\n                input_channel = output_channel\n\n        # Building last several layers\n        features.append(nn.Conv2d(input_channel, last_channel, 1, 1, 0, bias=False))\n        features.append(nn.BatchNorm2d(last_channel))\n        features.append(nn.ReLU6(inplace=True))\n\n        # Replace PyTorch adaptive pooling with Triton implementation\n        features.append(AdaptiveAvgPool2d1x1())\n\n        self.features = nn.Sequential(*features)\n\n        # Linear layer\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.0),\n            nn.Linear(last_channel, num_classes),\n        )\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MobileNetV2 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
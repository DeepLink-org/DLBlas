{"id": "e70e043a-d143-4fe5-96ce-580e9866e704", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 2048, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 4096, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 2048, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 4096, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 16}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 16}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 16}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 2048, 'VEC_SIZE': 16}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 4096, 'VEC_SIZE': 16}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 32}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 32}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 32}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 2048, 'VEC_SIZE': 32}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 4096, 'VEC_SIZE': 32}, num_warps=8),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef _relu6_kernel(\n    x_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n    VEC_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE * VEC_SIZE\n    offsets = block_start + tl.arange(0, VEC_SIZE)[None, :] + tl.arange(0, BLOCK_SIZE)[:, None] * VEC_SIZE\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    x = tl.minimum(tl.maximum(x, 0.0), 6.0)\n    tl.store(output_ptr + offsets, x, mask=mask)\n\nclass TritonReLU6(nn.Module):\n    def forward(self, x):\n        output = torch.empty_like(x)\n        n_elements = output.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE'] * meta['VEC_SIZE']),)\n        _relu6_kernel[grid](x, output, n_elements)\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        def _make_divisible(v, divisor, min_value=None):\n            if min_value is None:\n                min_value = divisor\n            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n            if new_v < 0.9 * v:\n                new_v += divisor\n            return new_v\n\n        def _inverted_residual_block(inp, oup, stride, expand_ratio):\n            hidden_dim = int(inp * expand_ratio)\n            use_res_connect = stride == 1 and inp == oup\n\n            layers = []\n            if expand_ratio != 1:\n                layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))\n                layers.append(nn.BatchNorm2d(hidden_dim))\n                layers.append(TritonReLU6())\n\n            layers.extend([\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, \u6781\u81f4\u7684\u901f\u5ea6\u4f18\u5316\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u548c\u53cd\u590d\u5b9e\u8df5\uff0c\u6211\u6b63\u5728\u52aa\u529b\u4e3a\u60a8\u63d0\u4f9b\u6700\u4f73\u65b9\u6848\u30021, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                TritonReLU6(),\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ])\n\n            if use_res_connect:\n                return nn.Sequential(*layers), True\n            else:\n                return nn.Sequential(*layers), False\n\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        features = [\n            nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(input_channel),\n            TritonReLU6()\n        ]\n\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_inverted_residual_block(input_channel, output_channel, stride, expand_ratio=t)[0])\n                input_channel = output_channel\n\n        features.append(nn.Conv2d(input_channel, last_channel, 1, 1, 0, bias=False))\n        features.append(nn.BatchNorm2d(last_channel))\n        features.append(TritonReLU6())\n        features.append(nn.AdaptiveAvgPool2d((1, 1)))\n\n        self.features = nn.Sequential(*features)\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.0),\n            nn.Linear(last_channel, num_classes),\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "bb61d813-caf0-4e53-b65a-6d0fd9d0d036", "generation": 3, "timestamp": 1754590089.0024128, "iteration_found": 28, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "", "exec_log": "[Child] \u7ed1\u5b9a GPU 6, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 80\n    nn.Conv2d(hidden_dim, hidden_dim, 3, stride, \u6781\u81f4\u7684\u901f\u5ea6\u4f18\u5316\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u548c\u53cd\u590d\u5b9e\u8df5\uff0c\u6211\u6b63\u5728\u52aa\u529b\u4e3a\u60a8\u63d0\u4f9b\u6700\u4f73\u65b9\u6848\u30021, groups=hidden_dim, bias=False),\n                                                                   ^\nSyntaxError: invalid character '\uff0c' (U+FF0C)\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.617, "runtime_ms": 4.8, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 2.96, "speed_up": 0.617, "custom_timing": 4.8}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
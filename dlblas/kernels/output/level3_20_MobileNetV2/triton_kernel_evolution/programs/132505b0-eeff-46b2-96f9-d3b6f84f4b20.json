{"id": "132505b0-eeff-46b2-96f9-d3b6f84f4b20", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _depthwise_conv2d_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    weight_channel_stride, weight_height_stride, weight_width_stride,\n    output_batch_stride, output_channel_stride, output_height_stride, output_width_stride,\n    batch, in_channels, out_height, out_width,\n    stride_h, stride_w,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_bc = tl.program_id(0)\n    pid_h = tl.program_id(1)\n    pid_w = tl.program_id(2)\n    \n    batch_idx = pid_bc // in_channels\n    channel_idx = pid_bc % in_channels\n    \n    if batch_idx >= batch:\n        return\n    if channel_idx >= in_channels:\n        return\n    if pid_h >= out_height:\n        return\n    if pid_w >= out_width:\n        return\n\n    in_h = pid_h * stride_h\n    in_w = pid_w * stride_w\n    acc = 0.0\n    \n    for kh in range(3):\n        for kw in range(3):\n            h = in_h + kh\n            w = in_w + kw\n            input_offset = batch_idx * input_batch_stride + channel_idx * input_channel_stride + h * input_height_stride + w * input_width_stride\n            weight_offset = channel_idx * weight_channel_stride + kh * weight_height_stride + kw * weight_width_stride\n            input_val = tl.load(input_ptr + input_offset)\n            weight_val = tl.load(weight_ptr + weight_offset)\n            acc += input_val * weight_val\n    \n    output_offset = batch_idx * output_batch_stride + channel_idx * output_channel_stride + pid_h * output_height_stride + pid_w * output_width_stride\n    tl.store(output_ptr + output_offset, acc)\n\nclass DepthwiseConv2dTriton(nn.Module):\n    def __init__(self, in_channels, kernel_size, stride, padding, bias=True):\n        super().__init__()\n        self.in_channels = in_channels\n        self.stride = stride\n        self.padding = padding\n        self.weight = nn.Parameter(torch.Tensor(in_channels, 1, kernel_size, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(in_channels))\n        else:\n            self.bias = None\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_normal_(self.weight, mode='fan_out', nonlinearity='relu')\n        if self.bias is not None:\n            nn.init.zeros_(self.bias)\n\n    def forward(self, x):\n        if self.padding > 0:\n            x = F.pad(x, (self.padding, self.padding, self.padding, self.padding))\n        \n        batch, in_channels, in_height, in_width = x.shape\n        out_height = (in_height + 2 * self.padding - 3) // self.stride + 1\n        out_width = (in_width + 2 * self.padding - 3) // self.stride + 1\n        \n        output = torch.empty((batch, in_channels, out_height, out_width), \n                             device=x.device, dtype=x.dtype)\n        \n        grid = (batch * in_channels, out_height, out_width)\n        _depthwise_conv2d_kernel[grid](\n            x, self.weight, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            self.weight.stride(0), self.weight.stride(2), self.weight.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            batch, in_channels, out_height, out_width,\n            self.stride, self.stride,\n            BLOCK_SIZE=1\n        )\n        \n        if self.bias is not None:\n            output += self.bias.view(1, -1, 1, 1)\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        def _make_divisible(v, divisor, min_value=None):\n            if min_value is None:\n                min_value = divisor\n            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n            if new_v < 0.9 * v:\n                new_v += divisor\n            return new_v\n\n        def _inverted_residual_block(inp, oup, stride, expand_ratio):\n            hidden_dim = int(inp * expand_ratio)\n            use_res_connect = stride == 1 and inp == oup\n\n            layers = []\n            if expand_ratio != 1:\n                layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))\n                layers.append(nn.BatchNorm2d(hidden_dim))\n                layers.append(nn.ReLU6(inplace=True))\n\n            layers.extend([\n                DepthwiseConv2dTriton(hidden_dim, kernel_size=3, stride=stride, padding=1, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ])\n\n            if use_res_connect:\n                return nn.Sequential(*layers), True\n            else:\n                return nn.Sequential(*layers), False\n\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        features = [\n            nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(input_channel),\n            nn.ReLU6(inplace=True)\n        ]\n\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_inverted_residual_block(input_channel, output_channel, stride, expand_ratio=t)[0])\n                input_channel = output_channel\n\n        features.extend([\n            nn.Conv2d(input_channel, last_channel, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(last_channel),\n            nn.ReLU6(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        ])\n\n        self.features = nn.Sequential(*features)\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.0),\n            nn.Linear(last_channel, num_classes),\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "6021eeac-2c6f-41b9-a3c9-b68fe5693bc5", "generation": 1, "timestamp": 1754586100.5959132, "iteration_found": 8, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_20_MobileNetV2", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "max_difference": ["0.697404", "0.684548", "0.669794", "0.699061", "0.712731"], "avg_difference": ["0.156305", "0.156083", "0.156282", "0.156024", "0.156278"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 2.96, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 3.59, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 2.96, "speed_up": 0.825, "custom_timing": 3.59}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
{"id": "58767045-e333-4b18-b2f0-7e4f1febc910", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n# Optimized depthwise convolution kernel with explicit padding\n@triton.jit\ndef depthwise_conv_kernel(\n    x_ptr, \n    w_ptr, \n    output_ptr,\n    stride_h, stride_w,\n    input_bs, input_c, input_h, input_w,  # padded dimensions\n    weight_c, weight_kh, weight_kw,\n    output_bs, output_c, output_h, output_w,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid = tl.num_programs(0)\n    \n    # Compute output position indices\n    pid_batch = pid // (output_c * output_h * output_w)\n    pid_channel = (pid % (output_c * output_h * output_w)) // (output_h * output_w)\n    pid_h = (pid % (output_h * output_w)) // output_w\n    pid_w = pid % output_w\n    \n    # Calculate input starting position (in padded tensor)\n    h_start = pid_h * stride_h\n    w_start = pid_w * stride_w\n    \n    # Accumulator\n    acc = 0.0\n    \n    # Loop over 3x3 kernel (no bounds needed due to padding)\n    for kh in range(0, 3):\n        for kw in range(0, 3):\n            h = h_start + kh\n            w = w_start + kw\n            \n            # Calculate offsets\n            x_offset = pid_batch * (input_c * input_h * input_w) + \\\n                       pid_channel * (input_h * input_w) + \\\n                       h * input_w + w\n            \n            w_offset = pid_channel * (weight_kh * weight_kw) + kh * weight_kw + kw\n            \n            # Load values and accumulate\n            x_val = tl.load(x_ptr + x_offset)\n            w_val = tl.load(w_ptr + w_offset)\n            acc += x_val * w_val\n    \n    # Store result\n    out_offset = pid_batch * (output_c * output_h * output_w) + \\\n                 pid_channel * (output_h * output_w) + \\\n                 pid_h * output_w + pid_w\n    tl.store(output_ptr + out_offset, acc)\n\nclass DepthwiseConv2d(nn.Module):\n    def __init__(self, in_channels, kernel_size, stride, padding, bias=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        \n        self.weight = nn.Parameter(torch.empty(in_channels, 1, kernel_size, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.empty(in_channels))\n        else:\n            self.bias = None\n            \n        nn.init.kaiming_normal_(self.weight, mode='fan_out')\n\n    def forward(self, x):\n        batch, in_c, in_h, in_w = x.shape\n        \n        # Calculate output dimensions\n        out_h = (in_h + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_w = (in_w + 2*self.padding - self.kernel_size) // self.stride + 1\n        \n        # Apply padding\n        if self.padding > 0:\n            x = F.pad(x, (self.padding, self.padding, self.padding, self.padding))\n            padded_h = in_h + 2 * self.padding\n            padded_w = in_w + 2 * self.padding\n        else:\n            padded_h = in_h\n            padded_w = in_w\n            \n        # Prepare output tensor\n        output = torch.empty(\n            batch, self.in_channels, out_h, out_w, \n            device=x.device, dtype=x.dtype\n        )\n        \n        # Make tensors contiguous\n        x = x.contiguous()\n        weight = self.weight.contiguous()\n        \n        # Calculate total elements\n        total_elements = batch * self.in_channels * out_h * out_w\n        \n        # Launch kernel\n        grid = lambda meta: (total_elements,)\n        depthwise_conv_kernel[grid](\n            x, weight, output,\n            self.stride, self.stride,\n            batch, self.in_channels, padded_h, padded_w,\n            self.in_channels, self.kernel_size, self.kernel_size,\n            batch, self.in_channels, out_h, out_w,\n            BLOCK_SIZE=32\n        )\n        \n        if self.bias is not None:\n            output += self.bias.view(1, -1, 1, 1)\n            \n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        MobileNetV2 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes. Default is 1000.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        def _make_divisible(v, divisor, min_value=None):\n            \"\"\"\n            This function ensures that the number of channels is divisible by the divisor.\n            \"\"\"\n            if min_value is None:\n                min_value = divisor\n            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n            # Make sure that round down does not go down by more than 10%.\n            if new_v < 0.9 * v:\n                new_v += divisor\n            return new_v\n\n        def _inverted_residual_block(inp, oup, stride, expand_ratio):\n            \"\"\"\n            Inverted Residual Block for MobileNetV2.\n            \"\"\"\n            hidden_dim = int(inp * expand_ratio)\n            use_res_connect = stride == 1 and inp == oup\n\n            layers = []\n            if expand_ratio != 1:\n                # Pointwise convolution\n                layers.append(nn.Conv2d(inp, hidden_dim, \u6781\u81f4\u7684\u901f\u5ea6\u4f18\u5316\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u548c\u53cd\u590d\u5b9e\u8df5\uff0c\u6211\u6b63\u5728\u52aa\u529b\u4e3a\u60a8\u63d0\u4f9b\u6700\u4f73\u65b9\u6848\u30021, 1, 0, bias=False))\n                layers.append(nn.BatchNorm2d(hidden_dim))\n                layers.append(nn.ReLU6(inplace=True))\n\n            layers.extend([\n                # Depthwise convolution (replaced with optimized version)\n                DepthwiseConv2d(hidden_dim, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.Re\u6781\u81f4\u7684\u901f\u5ea6\u4f18\u5316\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u548c\u53cd\u590d\u5b9e\u8df5\uff0c\u6211\u6b63\u5728\u52aa\u529b\u4e3a\u60a8\u63d0\u4f9b\u6700\u4f73\u65b9\u6848\u3002LU6(inplace=True),\n                # Pointwise linear convolution\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ])\n\n            if use_res_connect:\n                return nn.Sequential(*layers), True\n            else:\n                return nn.Sequential(*layers), False\n\n        # MobileNetV2 architecture\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # Building first layer\n        features = [nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n                    nn.BatchNorm2d(input_channel),\n                    nn.ReLU6(inplace=True)]\n\n        # Building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_inverted_residual_block(input_channel, output_channel, stride, expand_ratio=t)[0])\n                input_channel = output_channel\n\n        # Building last several layers\n        features.append(nn.Conv2d(input_channel, last_channel, 1, 1, 0, bias=False))\n        features.append(nn.BatchNorm2d(last_channel))\n        features.append(nn.ReLU6(inplace=True))\n\n        # Final layer\n        features.append(nn.AdaptiveAvgPool2d((1, 1)))\n\n        self.features = nn.Sequential(*features)\n\n        # Linear layer\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.0),\n            nn.Linear(last_channel, num_classes),\n        )\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the MobileNetV2 model.\n\n        :param x: The input tensor, shape (batch_size, 3, 224, 224)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "1ecf051b-a9ad-46bb-b8e9-b46fe4789397", "generation": 3, "timestamp": 1754588683.7957864, "iteration_found": 20, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "", "exec_log": "[Child] \u7ed1\u5b9a GPU 6, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 152\n    layers.append(nn.Conv2d(inp, hidden_dim, \u6781\u81f4\u7684\u901f\u5ea6\u4f18\u5316\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u548c\u53cd\u590d\u5b9e\u8df5\uff0c\u6211\u6b63\u5728\u52aa\u529b\u4e3a\u60a8\u63d0\u4f9b\u6700\u4f73\u65b9\u6848\u30021, 1, 0, bias=False))\n                                                               ^\nSyntaxError: invalid character '\uff0c' (U+FF0C)\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
{"id": "9ecb1c00-0cce-4071-89b9-960973c9ad63", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _pointwise_ops(\n    x_ptr,\n    bias_ptr,\n    output_ptr,\n    scaling_factor,\n    batch_size, \n    channels, \n    height, \n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_nc = tl.program_id(0)\n    pid_s = tl.program_id(1)\n    \n    c = pid_nc % channels\n    n = pid_nc // channels\n    spatial_size = height * width\n    \n    base_offset = n * channels * spatial_size + c * spatial_size\n    block_start = pid_s * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < spatial_size\n    \n    bias_val = tl.load(bias_ptr + c)\n    # Precompute upper bound once per block\n    upper_bound = tl.minimum(1.0, 1.0 / scaling_factor)\n    \n    x_vals = tl.load(x_ptr + base_offset + offsets, mask=mask, other=0.0)\n    \n    # Optimized fused operations\n    out = x_vals + bias_val\n    out = tl.maximum(out, 0.0)\n    out = tl.minimum(out, upper_bound)\n    \n    tl.store(output_ptr + base_offset + offsets, out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        batch_size, channels, height, width = x.shape\n        \n        x_contig = x.contiguous()\n        output = torch.empty_like(x_contig)\n        bias_1d = self.bias.view(-1)\n        \n        BLOCK = 1024\n        spatial_size = height * width\n        num_blocks = triton.cdiv(spatial_size, BLOCK)\n        grid = (batch_size * channels, num_blocks)\n        \n        _pointwise_ops[grid](\n            x_contig, bias_1d, output, self.scaling_factor,\n            batch_size, channels, height, width,\n            BLOCK_SIZE=BLOCK\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "54b054ef-825b-4e42-b038-38f834d01cd5", "generation": 3, "timestamp": 1754569859.026863, "iteration_found": 21, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.593, "runtime_ms": 0.177, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.105, "speed_up": 0.593, "custom_timing": 0.177}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.587, "runtime_ms": 0.179, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.105, "speed_up": 0.587, "custom_timing": 0.179}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.5870\n- runtime_ms: 0.1790\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.105, 'speed_up': 0.587, 'custom_timing': 0.179}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.5870, runtime_ms: 0.1790, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.105, 'speed_up': 0.587, 'custom_timing': 0.179}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.5930, runtime_ms: 0.1770, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.105, 'speed_up': 0.593, 'custom_timing': 0.177}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.5930, runtime_ms: 0.1770, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.105, 'speed_up': 0.593, 'custom_timing': 0.177}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_ops_kernel(\n    input_ptr,\n    bias_ptr,\n    output_ptr,\n    scaling_factor,\n    batch_size,\n    channels,\n    height,\n    width,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    base = pid * BLOCK_SIZE\n    idx = base + tl.arange(0, BLOCK_SIZE)\n    mask = idx < n_elements\n    \n    # Load input and bias\n    x = tl.load(input_ptr + idx, mask=mask)\n    spatial_size = height * width\n    channel_idx = (idx // spatial_size) % channels\n    bias_val = tl.load(bias_ptr + channel_idx, mask=mask)\n    \n    # Fused operations\n    out = x + bias_val\n    out = tl.maximum(out, 0.0)\n    out = tl.minimum(out, 1.0)\n    out = out * scaling_factor\n    out = tl.minimum(out, 1.0)\n    out = out / scaling_factor\n    \n    tl.store(output_ptr + idx, out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        batch_size, channels, height, width = x.shape\n        n_elements = batch_size * channels * height * width\n        output = torch.empty_like(x)\n        bias_1d = self.bias.view(-1)\n        \n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_ops_kernel[grid](\n            x, \n            bias_1d, \n            output, \n            self.scaling_factor,\n            batch_size,\n            channels,\n            height,\n            width,\n            n_elements,\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.5930), Performs well on runtime_ms (0.1770), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.105, 'speed_up': 0.593, 'custom_timing': 0.177})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_ops_kernel(\n    input_ptr,\n    bias_ptr,\n    output_ptr,\n    scaling_factor,\n    batch_size,\n    channels,\n    height,\n    width,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    base = pid * BLOCK_SIZE\n    idx = base + tl.arange(0, BLOCK_SIZE)\n    mask = idx < n_elements\n    \n    # Load input\n    x = tl.load(input_ptr + idx, mask=mask, other=0.0)\n    \n    # Compute spatial size\n    spatial_size = height * width\n    \n    # Find first and last channel indices in the block\n    first_index = base\n    last_index = base + BLOCK_SIZE - 1\n    if last_index >= n_elements:\n        last_index = n_elements - 1\n    first_channel = (first_index // spatial_size) % channels\n    last_channel = (last_index // spatial_size) % channels\n    \n    # Load bias values for the distinct channels\n    bias_val0 = tl.load(bias_ptr + first_channel)\n    if first_channel != last_channel:\n        bias_val1 = tl.load(bias_ptr + last_channel)\n    else:\n        bias_val1 = bias_val0\n    \n    # Compute per-element channel indices\n    channel_indices = (idx // spatial_size) % channels\n    # Select bias value based on channel index\n    bias_vals = tl.where(channel_indices == first_channel, bias_val0, bias_val1)\n    \n    # Precompute inverse scaling factor\n    inv_scale = 1.0 / scaling_factor\n    \n    # Optimized fused operations\n    out = x + bias_vals\n    out = tl.minimum(tl.maximum(out, 0.0), 1.0)  # ReLU + clamp\n    out = tl.minimum(out, inv_scale)  # Equivalent to scaling operations\n    \n    tl.store(output_ptr + idx, out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        batch_size, channels, height, width = x.shape\n        n_elements = batch_size * channels * height * width\n        output = torch.empty_like(x)\n        bias_1d = self.bias.view(-1)\n        \n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_ops_kernel[grid](\n            x, \n            bias_1d, \n            output, \n            self.scaling_factor,\n            batch_size,\n            channels,\n            height,\n            width,\n            n_elements,\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.5930), Performs well on runtime_ms (0.1770), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.105, 'speed_up': 0.593, 'custom_timing': 0.177})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _pointwise_ops(\n    x_ptr,\n    bias_ptr,\n    output_ptr,\n    scaling_factor,\n    batch_size, \n    channels, \n    height, \n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_nc = tl.program_id(0)\n    pid_s = tl.program_id(1)\n    \n    c = pid_nc % channels\n    n = pid_nc // channels\n    spatial_size = height * width\n    \n    base_offset = n * channels * spatial_size + c * spatial_size\n    block_start = pid_s * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < spatial_size\n    \n    bias_val = tl.load(bias_ptr + c)\n    x_vals = tl.load(x_ptr + base_offset + offsets, mask=mask, other=0.0)\n    \n    x_vals += bias_val\n    x_vals = tl.minimum(tl.maximum(x_vals, 0.0), 1.0)\n    x_vals *= scaling_factor\n    x_vals = tl.minimum(tl.maximum(x_vals, 0.0), 1.0)\n    x_vals /= scaling_factor\n    \n    tl.store(output_ptr + base_offset + offsets, x_vals, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        batch_size, channels, height, width = x.shape\n        \n        x_contig = x.contiguous()\n        output = torch.empty_like(x_contig)\n        bias_1d = self.bias.view(-1)\n        \n        BLOCK = 1024\n        spatial_size = height * width\n        num_blocks = triton.cdiv(spatial_size, BLOCK)\n        grid = (batch_size * channels, num_blocks)\n        \n        _pointwise_ops[grid](\n            x_contig, bias_1d, output, self.scaling_factor,\n            batch_size, channels, height, width,\n            BLOCK_SIZE=BLOCK\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.5870), Performs well on runtime_ms (0.1790), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.105, 'speed_up': 0.587, 'custom_timing': 0.179})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _elementwise_kernel(\n    x_ptr,\n    bias_ptr,\n    output_ptr,\n    scaling_factor,\n    h, w, c, n,\n    stride_xn, stride_xc, stride_xh, stride_xw,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pids = tl.num_programs(0)\n    elements_per_program = (h * w + BLOCK_SIZE - 1) // BLOCK_SIZE\n    batch_id = pid // (c * elements_per_program)\n    channel_id = (pid % (c * elements_per_program)) // elements_per_program\n    block_id = pid % elements_per_program\n    \n    # Calculate spatial offsets\n    spatial_start = block_id * BLOCK_SIZE\n    spatial_offsets = spatial_start + tl.arange(0, BLOCK_SIZE)\n    spatial_mask = spatial_offsets < (h * w)\n    \n    # Calculate full tensor indices\n    batch_offset = batch_id * stride_xn\n    channel_offset = channel_id * stride_xc\n    spatial_indices = spatial_offsets // w, spatial_offsets % w\n    h_offsets, w_offsets = spatial_indices\n    \n    # Compute memory locations\n    x_ptrs = (\n        x_ptr + \n        batch_offset + \n        channel_offset + \n        h_offsets * stride_xh + \n        w_offsets * stride_xw\n    )\n    \n    # Load data with mask\n    x_vals = tl.load(x_ptrs, mask=spatial_mask, other=0.0)\n    bias_val = tl.load(bias_ptr + channel_id)\n    \n    # Fused operations\n    y = x_vals + bias_val\n    y = tl.minimum(tl.maximum(y, 0.0), 1.0)\n    y = y * scaling_factor\n    y = tl.minimum(tl.maximum(y, 0.0), 1.0)\n    y = y / scaling_factor\n    \n    # Store results\n    tl.store(x_ptrs, y, mask=spatial_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        n, c, h, w = x.shape\n        bias_flat = self.bias.view(-1)\n        \n        # Calculate strides\n        stride_xn = x.stride(0)\n        stride_xc = x.stride(1)\n        stride_xh = x.stride(2)\n        stride_xw = x.stride(3)\n        \n        # Configure kernel launch\n        elements_per_row = h * w\n        BLOCK_SIZE = 1024\n        grid = (n * c * ((elements_per_row + BLOCK_SIZE - 1) // BLOCK_SIZE),)\n        \n        # Launch kernel\n        _elementwise_kernel[grid](\n            x, \n            bias_flat, \n            x,  # in-place operation\n            self.scaling_factor,\n            h, w, c, n,\n            stride_xn, stride_xc, stride_xh, stride_xw,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n    ],\n    key=['spatial_size'],\n)\n@triton.jit\ndef elementwise_kernel(\n    x_ptr,\n    bias_ptr,\n    output_ptr,\n    C,\n    spatial_size,\n    upper_bound,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_nc = tl.program_id(0)\n    pid_s = tl.program_id(1)\n    \n    c = pid_nc % C\n    n = pid_nc // C\n    bias_val = tl.load(bias_ptr + c)\n    \n    spatial_offset = pid_s * BLOCK_SIZE\n    offsets = spatial_offset + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < spatial_size\n    \n    base_idx = n * C * spatial_size + c * spatial_size\n    x_idx = base_idx + offsets\n    x_val = tl.load(x_ptr + x_idx, mask=mask)\n    \n    # Optimized fused operations\n    x_val += bias_val\n    x_val = tl.minimum(tl.maximum(x_val, 0.0), 1.0)\n    x_val = tl.minimum(x_val, upper_bound)\n    \n    tl.store(output_ptr + x_idx, x_val, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n        self.upper_bound = 1.0 / scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x.contiguous()\n        output = torch.empty_like(x)\n        bias_1d = self.bias.view(-1)\n        N, C, H, W = x.shape\n        spatial_size = H * W\n        \n        grid = (N * C, triton.cdiv(spatial_size, 1024))\n        \n        elementwise_kernel[grid](\n            x, \n            bias_1d, \n            output, \n            C, \n            spatial_size,\n            self.upper_bound\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_ops_kernel(\n    input_ptr,\n    bias_ptr,\n    output_ptr,\n    scaling_factor,\n    batch_size,\n    channels,\n    height,\n    width,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    base = pid * BLOCK_SIZE\n    idx = base + tl.arange(0, BLOCK_SIZE)\n    mask = idx < n_elements\n    \n    # Load input and bias\n    x = tl.load(input_ptr + idx, mask=mask)\n    spatial_size = height * width\n    channel_idx = (idx // spatial_size) % channels\n    bias_val = tl.load(bias_ptr + channel_idx, mask=mask)\n    \n    # Fused operations\n    out = x + bias_val\n    out = tl.maximum(out, 0.0)\n    out = tl.minimum(out, 1.0)\n    out = out * scaling_factor\n    out = tl.minimum(out, 1.0)\n    out = out / scaling_factor\n    \n    tl.store(output_ptr + idx, out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        batch_size, channels, height, width = x.shape\n        n_elements = batch_size * channels * height * width\n        output = torch.empty_like(x)\n        bias_1d = self.bias.view(-1)\n        \n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_ops_kernel[grid](\n            x, \n            bias_1d, \n            output, \n            self.scaling_factor,\n            batch_size,\n            channels,\n            height,\n            width,\n            n_elements,\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n    ],\n    key=['spatial_size'],\n)\n@triton.jit\ndef elementwise_kernel(\n    x_ptr,\n    bias_ptr,\n    output_ptr,\n    C,\n    spatial_size,\n    upper_bound,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_nc = tl.program_id(0)\n    pid_s = tl.program_id(1)\n    \n    c = pid_nc % C\n    n = pid_nc // C\n    bias_val = tl.load(bias_ptr + c)\n    \n    spatial_offset = pid_s * BLOCK_SIZE\n    offsets = spatial_offset + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < spatial_size\n    \n    base_idx = n * C * spatial_size + c * spatial_size\n    x_idx = base_idx + offsets\n    x_val = tl.load(x_ptr + x_idx, mask=mask)\n    \n    # Optimized fused operations\n    x_val += bias_val\n    x_val = tl.minimum(tl.maximum(x_val, 0.0), 1.0)\n    x_val = tl.minimum(x_val, upper_bound)\n    \n    tl.store(output_ptr + x_idx, x_val, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n        self.upper_bound = 1.0 / scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x.contiguous()\n        output = torch.empty_like(x)\n        bias_1d = self.bias.view(-1)\n        N, C, H, W = x.shape\n        spatial_size = H * W\n        \n        grid = (N * C, triton.cdiv(spatial_size, 1024))\n        \n        elementwise_kernel[grid](\n            x, \n            bias_1d, \n            output, \n            C, \n            spatial_size,\n            self.upper_bound\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: -100.0000, Type: Exploratory)\n```python\n@triton.jit\ndef _pointwise_ops(\n    x_ptr,\n    bias_ptr,\n    output_ptr,\n    scaling_factor,\n    batch_size, \n    channels, \n    height, \n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    # We have two-dimensional grid: (batch*channel, num_spatial_blocks)\n    pid_nc = tl.program_id(0)\n    pid_s = tl.program_id(1)\n    \n    # Compute the current batch and channel\n    c = pid_nc % channels\n    n = pid_nc // channels\n    \n    # If we are out of bounds, return\n    if n >= batch_size or c >= channels:\n        return\n        \n    spatial_size = height * width\n    base_offset = n * channels * spatial_size + c * spatial_size\n    \n    # Spatial indices for this block\n    idx_start = pid_s * BLOCK_SIZE\n    offsets = idx_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < spatial_size\n    \n    # Load bias once for the entire block (for this channel)\n    bias_val = tl.load(bias_ptr + c)\n    \n    # Load the block of data\n    x_vals = tl.load(x_ptr + base_offset + offsets, mask=mask, other=0.0)\n    \n    # Fused operations\n    x_vals = x_vals + bias_val\n    x_vals = tl.minimum(tl.maximum(x_vals, 0.0), 1.0)\n    x_vals = x_vals * scaling_factor\n    x_vals = tl.minimum(tl.maximum(x_vals, 0.0), 1.0)\n    x_vals = x_vals / scaling_factor\n    \n    tl.store(output_ptr + base_offset + offsets, x_vals, mask=mask)\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 4 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_ops_kernel(\n    input_ptr,\n    bias_ptr,\n    output_ptr,\n    scaling_factor,\n    batch_size,\n    channels,\n    height,\n    width,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    base = pid * BLOCK_SIZE\n    idx = base + tl.arange(0, BLOCK_SIZE)\n    mask = idx < n_elements\n    \n    # Load input\n    x = tl.load(input_ptr + idx, mask=mask, other=0.0)\n    \n    # Compute spatial size\n    spatial_size = height * width\n    \n    # Find first and last channel indices in the block\n    first_index = base\n    last_index = base + BLOCK_SIZE - 1\n    if last_index >= n_elements:\n        last_index = n_elements - 1\n    first_channel = (first_index // spatial_size) % channels\n    last_channel = (last_index // spatial_size) % channels\n    \n    # Load bias values for the distinct channels\n    bias_val0 = tl.load(bias_ptr + first_channel)\n    if first_channel != last_channel:\n        bias_val1 = tl.load(bias_ptr + last_channel)\n    else:\n        bias_val1 = bias_val0\n    \n    # Compute per-element channel indices\n    channel_indices = (idx // spatial_size) % channels\n    # Select bias value based on channel index\n    bias_vals = tl.where(channel_indices == first_channel, bias_val0, bias_val1)\n    \n    # Precompute inverse scaling factor\n    inv_scale = 1.0 / scaling_factor\n    \n    # Optimized fused operations\n    out = x + bias_vals\n    out = tl.minimum(tl.maximum(out, 0.0), 1.0)  # ReLU + clamp\n    out = tl.minimum(out, inv_scale)  # Equivalent to scaling operations\n    \n    tl.store(output_ptr + idx, out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        batch_size, channels, height, width = x.shape\n        n_elements = batch_size * channels * height * width\n        output = torch.empty_like(x)\n        bias_1d = self.bias.view(-1)\n        \n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_ops_kernel[grid](\n            x, \n            bias_1d, \n            output, \n            self.scaling_factor,\n            batch_size,\n            channels,\n            height,\n            width,\n            n_elements,\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 5 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _elementwise_kernel(\n    x_ptr,\n    bias_ptr,\n    output_ptr,\n    scaling_factor,\n    h, w, c, n,\n    stride_xn, stride_xc, stride_xh, stride_xw,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pids = tl.num_programs(0)\n    elements_per_program = (h * w + BLOCK_SIZE - 1) // BLOCK_SIZE\n    batch_id = pid // (c * elements_per_program)\n    channel_id = (pid % (c * elements_per_program)) // elements_per_program\n    block_id = pid % elements_per_program\n    \n    # Calculate spatial offsets\n    spatial_start = block_id * BLOCK_SIZE\n    spatial_offsets = spatial_start + tl.arange(0, BLOCK_SIZE)\n    spatial_mask = spatial_offsets < (h * w)\n    \n    # Calculate full tensor indices\n    batch_offset = batch_id * stride_xn\n    channel_offset = channel_id * stride_xc\n    spatial_indices = spatial_offsets // w, spatial_offsets % w\n    h_offsets, w_offsets = spatial_indices\n    \n    # Compute memory locations\n    x_ptrs = (\n        x_ptr + \n        batch_offset + \n        channel_offset + \n        h_offsets * stride_xh + \n        w_offsets * stride_xw\n    )\n    \n    # Load data with mask\n    x_vals = tl.load(x_ptrs, mask=spatial_mask, other=0.0)\n    bias_val = tl.load(bias_ptr + channel_id)\n    \n    # Fused operations\n    y = x_vals + bias_val\n    y = tl.minimum(tl.maximum(y, 0.0), 1.0)\n    y = y * scaling_factor\n    y = tl.minimum(tl.maximum(y, 0.0), 1.0)\n    y = y / scaling_factor\n    \n    # Store results\n    tl.store(x_ptrs, y, mask=spatial_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        n, c, h, w = x.shape\n        bias_flat = self.bias.view(-1)\n        \n        # Calculate strides\n        stride_xn = x.stride(0)\n        stride_xc = x.stride(1)\n        stride_xh = x.stride(2)\n        stride_xw = x.stride(3)\n        \n        # Configure kernel launch\n        elements_per_row = h * w\n        BLOCK_SIZE = 1024\n        grid = (n * c * ((elements_per_row + BLOCK_SIZE - 1) // BLOCK_SIZE),)\n        \n        # Launch kernel\n        _elementwise_kernel[grid](\n            x, \n            bias_flat, \n            x,  # in-place operation\n            self.scaling_factor,\n            h, w, c, n,\n            stride_xn, stride_xc, stride_xh, stride_xw,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _pointwise_ops(\n    x_ptr,\n    bias_ptr,\n    output_ptr,\n    scaling_factor,\n    batch_size, \n    channels, \n    height, \n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_nc = tl.program_id(0)\n    pid_s = tl.program_id(1)\n    \n    c = pid_nc % channels\n    n = pid_nc // channels\n    spatial_size = height * width\n    \n    base_offset = n * channels * spatial_size + c * spatial_size\n    block_start = pid_s * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < spatial_size\n    \n    bias_val = tl.load(bias_ptr + c)\n    x_vals = tl.load(x_ptr + base_offset + offsets, mask=mask, other=0.0)\n    \n    x_vals += bias_val\n    x_vals = tl.minimum(tl.maximum(x_vals, 0.0), 1.0)\n    x_vals *= scaling_factor\n    x_vals = tl.minimum(tl.maximum(x_vals, 0.0), 1.0)\n    x_vals /= scaling_factor\n    \n    tl.store(output_ptr + base_offset + offsets, x_vals, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        batch_size, channels, height, width = x.shape\n        \n        x_contig = x.contiguous()\n        output = torch.empty_like(x_contig)\n        bias_1d = self.bias.view(-1)\n        \n        BLOCK = 1024\n        spatial_size = height * width\n        num_blocks = triton.cdiv(spatial_size, BLOCK)\n        grid = (batch_size * channels, num_blocks)\n        \n        _pointwise_ops[grid](\n            x_contig, bias_1d, output, self.scaling_factor,\n            batch_size, channels, height, width,\n            BLOCK_SIZE=BLOCK\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program which has a speed_up of 0.587 (which is less than 1, meaning it's slower than the reference).\n The current implementation uses a 2D grid: (batch_size * channels, num_blocks) and each block processes a segment of the spatial dimension.\n\n The kernel does:\n  1. Loads a bias value for the channel (once per block)\n  2. Loads a block of spatial data (with mask)\n  3. Performs: x_vals += bias_val\n  4. Clamps between 0 and 1\n  5. Multiplies by scaling_factor\n  6. Clamps again between 0 and 1\n  7. Divides by scaling_factor\n\n We note that the operations 5 and 7 (multiply by scaling_factor then divide) and the two clamps might be simplified.\n\n However, the main issue is that the current kernel is memory bound and we are doing multiple operations that require reading and writing.\n\n Let's analyze the memory access:\n  - We are reading the input (x) and the bias (once per block, which is good)\n  - We are writing the output.\n\n But note: the two clamp operations and the two scaling operations are done element-wise.\n\n The reference PyTorch code (which we don't see exactly) is likely doing the same operations, but we are trying to fuse them in a kernel.\n\n However, the current Triton kernel is slower. We need to optimize.\n\n Steps for optimization:\n\n 1. Simplify the math:\n    The operations:\n        x = x + bias\n        x = clamp(x, 0, 1)\n        x = x * scaling_factor\n        x = clamp(x, 0, 1)\n        x = x / scaling_factor\n\n    Note: The last three operations can be optimized. Let's denote:\n        x1 = clamp(x+bias, 0, 1)\n        then we do: x2 = clamp(x1 * scaling_factor, 0, 1) / scaling_factor\n\n    But note: if x1 is in [0,1], then x1 * scaling_factor is in [0, scaling_factor]. Then we clamp again to [0,1] and then divide by scaling_factor.\n\n    This is equivalent to:\n        x2 = clamp(x1 * scaling_factor, 0, 1) / scaling_factor\n            = min(max(x1 * scaling_factor, 0), 1) / scaling_factor\n\n    And note that if scaling_factor >= 1, then:\n        min(x1 * scaling_factor, 1) / scaling_factor = min(x1, 1/scaling_factor)\n\n    So the entire operation becomes:\n        x2 = min( max(x+bias, 0), 1/scaling_factor )\n\n    However, wait: the original code does:\n        x1 = clamp(x+bias, 0, 1)\n        x2 = clamp(x1 * scaling_factor, 0, 1)   # then we don't divide until the next step?\n        and then x3 = x2 / scaling_factor\n\n    But note: the last step is dividing the clamped value by scaling_factor.\n\n    Actually, the entire sequence is:\n        out = (x + bias).clamp(0,1) * scaling_factor\n        out = out.clamp(0,1) / scaling_factor\n\n    This is equivalent to:\n        out = (min( max(x+bias,0), 1 ) * scaling_factor).clamp(0,1) / scaling_factor\n\n    We can break it down:\n        Let y = max(x+bias,0)\n        Let y = min(y, 1)\n        Then z = y * scaling_factor\n        Then z = min(z, 1)\n        Then out = z / scaling_factor\n\n    Now, note that if scaling_factor > 0, then:\n        z = min(y * scaling_factor, 1)\n        out = min(y, 1/scaling_factor)   [because z/scaling_factor = min(y, 1/scaling_factor)]\n\n    So the entire operation is:\n        out = min( max(x+bias,0), min(1, 1/scaling_factor) )\n\n    But note: the second clamp is to 1, so we are effectively clamping to the minimum of 1 and 1/scaling_factor? \n    Actually, we have two separate clamps. However, observe:\n\n        out = min( max(x+bias,0), 1, 1/scaling_factor)   [because after the first clamp we have y<=1, then we multiply by scaling_factor and then clamp to 1, then divide by scaling_factor -> which is the same as clamping the original y to 1/scaling_factor and then scaling and then clamping and then scaling down?]\n\n    Alternatively, we can do:\n\n        y = x + bias\n        y = max(y, 0.0)\n        y = min(y, 1.0)   # now y is in [0,1]\n        # then we do: y = y * scaling_factor -> [0, scaling_factor]\n        # then clamp: y = min(y, 1.0) -> [0, min(scaling_factor,1)]? \n        # then divide: y = y / scaling_factor -> [0, min(1, 1/scaling_factor)]? \n\n    Actually, if scaling_factor >= 1, then:\n        y * scaling_factor is at least 0 and at most scaling_factor (>=1). Then we clamp to 1, so y becomes min(y * scaling_factor, 1) and then we divide by scaling_factor -> min(y, 1/scaling_factor).\n\n    If scaling_factor < 1, then after the first clamp we have [0,1], then multiply by scaling_factor -> [0, scaling_factor] (which is less than 1), then clamping to 1 does nothing, then dividing by scaling_factor gives [0,1]. So the entire operation becomes:\n        out = min( max(x+bias,0), 1)   [because the scaling and descaling with factor<1 doesn't change the value?]\n\n    Actually, no: because we do:\n        y1 = min( max(x+bias,0), 1)   -> [0,1]\n        then y2 = y1 * scaling_factor   -> [0, scaling_factor] (if scaling_factor<1, then this is [0, scaling_factor] which is [0,<1])\n        then y3 = min(y2, 1) -> still [0, scaling_factor]\n        then y4 = y3 / scaling_factor -> [0,1]\n\n    So it's the same as the first step? \n\n    This suggests that the entire sequence of operations after the first clamp is redundant? \n\n    But note: the problem says the PyTorch reference code is fixed. We must replicate it exactly.\n\n    However, the current Triton kernel is doing the same as the reference? We are told correctness passes.\n\n    So we cannot change the math? \n\n    But wait: the problem says we are allowed to change the Triton kernel as long as we maintain functional equivalence.\n\n    So if we can prove that the operations are equivalent to a simpler form, we can do that.\n\n    After analysis, we see that the entire sequence is equivalent to:\n\n        out = min( max(x+bias, 0), min(1, 1/scaling_factor) )   [if scaling_factor>=1]\n        and if scaling_factor < 1, then it is equivalent to min(max(x+bias,0), 1)   (because the scaling and descaling cancel out the effect of the intermediate steps)\n\n    Actually, we can write:\n\n        out = min( max(x+bias, 0), min(1, 1/scaling_factor) )\n\n    But note: if scaling_factor < 1, then 1/scaling_factor > 1, so min(1, 1/scaling_factor) = 1.\n\n    Therefore, we have:\n\n        out = min( max(x+bias,0), 1)   for any scaling_factor? \n\n    This is because min(1, 1/scaling_factor) is 1 when scaling_factor<=1 and 1/scaling_factor when scaling_factor>1.\n\n    But wait: when scaling_factor>1, then we are clamping to 1/scaling_factor? \n\n    However, let me test with an example:\n\n        scaling_factor = 2.0 (as in the example)\n        Then the original operations on a value of 0.6:\n          Step1: 0.6 -> clamp to [0,1] -> 0.6\n          Step2: 0.6 * 2 = 1.2\n          Step3: clamp to [0,1] -> 1.0\n          Step4: 1.0 / 2 = 0.5\n\n        Now with the simplified: \n          min( max(0.6,0), min(1, 1/2)) = min(0.6, 0.5) = 0.5.\n\n        So it matches.\n\n        Now with a value of 0.4:\n          Original: 0.4 -> 0.4 -> 0.8 -> 0.8 -> 0.4\n          Simplified: min(0.4, 0.5) = 0.4 -> matches.\n\n        Now with scaling_factor=0.5 (less than 1):\n          Value 1.2: \n            Step1: clamp to 1 -> 1\n            Step2: 1 * 0.5 = 0.5\n            Step3: clamp to 1 -> 0.5\n            Step4: 0.5 / 0.5 = 1.0\n          Simplified: min( max(1.2,0), min(1, 1/0.5)=min(1,2)=1) -> min(1.2,1)=1 -> then we get 1? \n          But the original returns 1.0? \n\n        Actually, the original for scaling_factor=0.5 and input 1.2:\n          Step1: clamp(1.2,0,1)=1\n          Step2: 1 * 0.5 = 0.5\n          Step3: clamp(0.5,0,1)=0.5\n          Step4: 0.5 / 0.5 = 1.0\n\n        But the simplified formula: min( max(1.2,0), 1) = 1 -> then we don't do anything else? \n\n        However, note the simplified formula I proposed is only for the entire sequence? \n\n        Actually, we see that the original sequence for scaling_factor<1 is not equivalent to just the first clamp? \n\n        Let me break down the entire sequence for scaling_factor=0.5:\n\n          out = (x+bias).clamp(0,1)   # step1: [0,1]\n          out = (out * scaling_factor).clamp(0,1)   # step2: [0, scaling_factor] (because scaling_factor<1) and then clamping to [0,1] does nothing -> so we get [0, scaling_factor]\n          out = out / scaling_factor   # step3: [0,1]\n\n          So the entire sequence is the identity on [0,1]? \n\n        But note: if the value after step1 is 1, then step2: 1 * 0.5 = 0.5, then step3: 0.5/0.5=1 -> same as input to step1? \n\n        Similarly, if step1 gives 0.5, then step2: 0.5*0.5=0.25, step3: 0.25/0.5=0.5.\n\n        So it is the identity? \n\n        Therefore, the entire operation after step1 is redundant when scaling_factor<=1? \n\n        And when scaling_factor>1, then the entire sequence is equivalent to clamping the step1 result to 1/scaling_factor.\n\n        So we can write:\n\n          out = min( max(x+bias,0), 1.0)\n          if scaling_factor > 1.0:\n              out = min(out, 1.0/scaling_factor)\n\n        But note: the original code does the same as:\n\n          out = min( max(x+bias,0), min(1.0, 1.0/scaling_factor) )   # because when scaling_factor<=1, min(1,1/scaling_factor)=1, and when >1, it's 1/scaling_factor.\n\n        Therefore, we can replace the entire sequence with:\n\n          out = x + bias_val\n          out = tl.maximum(out, 0.0)\n          upper_bound = tl.minimum(1.0, 1.0/scaling_factor)\n          out = tl.minimum(out, upper_bound)\n\n        This is two operations (max and min) and one extra constant.\n\n        This simplification reduces the number of operations and might improve performance.\n\n 2. We are also doing a lot of memory access: \n        load input, load bias (once per block), then store output.\n\n    But note: the current kernel uses a 2D grid and each block loads the bias once. This is efficient.\n\n 3. We can try to use vectorized loads/stores? The current code uses scalar loads and stores. Since the spatial dimension is contiguous, we can try to load and store in vectors.\n\n    However, the current block processes a contiguous segment of the spatial dimension. We can use tl.load with the `mask` and `other` parameters. But Triton might already be optimizing contiguous accesses.\n\n 4. Another idea: the current kernel uses two program ids. We could try a 1D grid and use a larger block? But the current 2D grid is natural for the data.\n\n 5. We note that the current kernel has a block size of 1024. We could try to tune the block size? But the autotuner is not used. We could add autotuning? But the problem says we can change the kernel. However, we are limited by the constraints: we cannot change the grid configuration? Actually, note the constraints:\n\n        \u274c NEVER modify grid configuration, output tensor shapes, or PID logic\n\n    So we cannot change the grid? But the grid is defined in the `forward` function. We are allowed to change the Triton kernel source? The constraint says \"modify ONLY the Triton kernel source within this block - all other code is LOCKED\". So we cannot change the grid configuration in the `forward` method? \n\n    However, the grid configuration in the current program is:\n\n        grid = (batch_size * channels, num_blocks)\n\n    and we are using two program ids. So we must keep the same.\n\n 6. We can try to use the following optimizations in the kernel:\n\n        a. Simplify the math as above to reduce operations.\n        b. Use vectorized loads and stores: for example, if we set BLOCK_SIZE to be a multiple of 16, we can load 16 elements at a time.\n\n    How to do vectorized loads? We can use:\n\n        x_vals = tl.load(x_ptr + base_offset + offsets, mask=mask, other=0.0, cache_modifier=\".cg\")\n\n    But without vectorization, we are doing scalar loads.\n\n    We can change the kernel to use vectorization by changing the arange step and using the `vectorized` option? \n\n    However, Triton does not have explicit vectorization in the language. Instead, it automatically vectorizes when possible.\n\n    Alternatively, we can use:\n\n        tl.reshape(tl.arange(0, BLOCK_SIZE), (BLOCK_SIZE//16, 16)\n\n    but that is not necessary.\n\n 7. We can try to increase the block size? But the current block size is 1024. We are constrained by the grid configuration which we cannot change? Actually, the block size is passed as a constant. We can change the block size? But note the constraint says we cannot change the grid configuration. The grid configuration function is:\n\n        grid = (batch_size * channels, num_blocks)\n\n    and num_blocks = triton.cdiv(spatial_size, BLOCK)\n\n    We are passing BLOCK as 1024. We can change the block size? But note the constraint says we cannot change the grid configuration. So we must leave the block size as 1024? \n\n    However, the problem says we are allowed to change the Triton kernel source. The block size is passed as an argument. We are allowed to change the kernel to use a different block size? But note the grid configuration function in the host code uses `BLOCK` which is set to 1024. We cannot change that because the host code is locked? \n\n    Therefore, we must leave the block size as 1024.\n\n 8. We can try to optimize by precomputing the upper_bound on the host and passing it to the kernel? Then we avoid the division and min in the kernel? \n\n    The upper_bound = min(1.0, 1.0/scaling_factor) is a scalar. We can compute it on the host and pass it as a constant.\n\n    This would save one division and one min per block? Actually, per element? No, we can compute it once per kernel. But note: the kernel is launched with many blocks. We can compute it once and pass it as a scalar.\n\n    Let's change the kernel to take an extra argument `upper_bound` (a float) and then do:\n\n        out = x_vals + bias_val\n        out = tl.maximum(out, 0.0)\n        out = tl.minimum(out, upper_bound)\n\n    Then in the host, we compute:\n\n        upper_bound = min(1.0, 1.0 / scaling_factor)\n\n    This saves two operations per element? \n\n    But note: the original kernel had 5 operations per element (add, clamp, mul, clamp, div). Now we have 2 (max, min) and one extra scalar passed.\n\n    This is a significant reduction.\n\n 9. We must verify correctness: the transformation is valid as we proved.\n\n Given the above, let's rewrite the kernel:\n\n    Steps:\n\n        In the host:\n            upper_bound = min(1.0, 1.0 / self.scaling_factor)\n\n        Then pass `upper_bound` to the kernel.\n\n        In the kernel:\n            ... same as before until the operations:\n\n            x_vals += bias_val   -> becomes: out = x_vals + bias_val\n            out = tl.maximum(out, 0.0)\n            out = tl.minimum(out, upper_bound)\n\n            Then store out.\n\n    This replaces the original 5 operations with 2.\n\n    We also remove the two clamps and the scaling and descaling.\n\n    Note: we must be cautious because the original code had two clamps and scaling operations. But our analysis shows it is equivalent.\n\n 10. We must also note: the scaling_factor is a constant and positive? The problem sets it to 2.0.\n\n 11. We must change the kernel signature and the host code? But note the constraint: we cannot change the function signature of the @triton.jit function? Actually, the constraint says:\n\n        \u274c NEVER change @triton.jit function signatures or parameter names\n\n    So we cannot change the kernel signature? \n\n    Therefore, we cannot add a new parameter `upper_bound`. We must compute it inside the kernel? \n\n    Alternatively, we can compute it once per kernel as a constant? \n\n    Since the scaling_factor is the same for all elements, we can do:\n\n        upper_bound = tl.minimum(1.0, 1.0 / scaling_factor)\n\n    Then use that.\n\n    But note: scaling_factor is passed as a scalar. We can do:\n\n        upper_bound = 1.0 / scaling_factor\n        if upper_bound > 1.0:\n            upper_bound = 1.0\n        else:\n            upper_bound = upper_bound\n\n    But Triton does not have conditionals that are evaluated at compile-time for a scalar that is not a constant expression? \n\n    Alternatively, we can use:\n\n        upper_bound = tl.where(scaling_factor > 1.0, 1.0/scaling_factor, 1.0)\n\n    But note: scaling_factor is the same for the entire kernel. We can compute:\n\n        upper_bound = tl.minimum(1.0, 1.0 / scaling_factor)\n\n    This is one operation per block? Actually, per element? No, we can compute it once per block? \n\n    But the kernel does not have a block-level constant. We can compute it once per block? \n\n    However, we are in the inner loop. We can compute it once at the beginning of the block? \n\n    We do:\n\n        upper_bound = tl.minimum(1.0, 1.0 / scaling_factor)\n\n    But note: `scaling_factor` is a scalar argument to the kernel. It is the same for all blocks. However, the Triton compiler might hoist this computation? \n\n    But we are doing it once per block? That is acceptable? It's one operation per block.\n\n    Alternatively, we can compute it once per kernel? But Triton doesn't have shared memory across blocks.\n\n    We can compute it once per block? That is acceptable.\n\n    So in the kernel:\n\n        ... at the beginning, after loading bias_val, we compute:\n\n        upper_bound = tl.minimum(1.0, 1.0 / scaling_factor)\n\n        then later:\n\n        out = x_vals + bias_val\n        out = tl.maximum(out, 0.0)\n        out = tl.minimum(out, upper_bound)\n\n    This adds one operation per block (which is negligible) and replaces 3 operations per element (the multiply, the second clamp, and the divide) with one min.\n\n    Total operations per element: \n        Original: 5\n        New: 2 (max and min) plus one extra min per block.\n\n 12. We also note: the original kernel had two clamp operations (which are min and max) and then a multiply and a divide. The new version has one max and one min.\n\n    This should be faster.\n\n 13. We also remove the two scaling operations which are floating point multiplies and divides.\n\n 14. Let's write the kernel accordingly.\n\n 15. We must be cautious: the scaling_factor might be zero? But the problem sets it to 2.0. We assume it is positive and non-zero.\n\n Implementation:\n\n    In the kernel:\n\n        # Compute the upper bound once per block (it's the same for all elements in the block)\n        upper_bound = tl.minimum(1.0, 1.0 / scaling_factor)\n\n        ... then after loading:\n\n        out = x_vals + bias_val\n        out = tl.maximum(out, 0.0)\n        out = tl.minimum(out, upper_bound)\n\n    And remove the rest.\n\n 16. We must also note: the original kernel had two clamps and two scaling operations. We are replacing them with a single clamp to [0, upper_bound]. \n\n    We have proved that this is equivalent.\n\n 17. We run the risk of changing the behavior for scaling_factor<=1? But our analysis holds.\n\n 18. Now, let's update the kernel accordingly.\n\n 19. We are only changing the Triton kernel code (the function `_pointwise_ops`). The host code remains the same.\n\n 20. We must not change the function signature.\n\n Let's write the new kernel:\n\n    We replace the operations:\n\n        x_vals += bias_val\n        x_vals = tl.minimum(tl.maximum(x_vals, 0.0), 1.0)\n        x_vals *= scaling_factor\n        x_vals = tl.minimum(tl.maximum(x_vals, 0.0), 1.0)\n        x_vals /= scaling_factor\n\n    with:\n\n        out = x_vals + bias_val\n        out = tl.maximum(out, 0.0)\n        out = tl.minimum(out, tl.minimum(1.0, 1.0/scaling_factor))\n\n    But note: we cannot put the tl.minimum inside the element-wise operation? Because it is a scalar. We compute it once per block.\n\n    So:\n\n        # At the beginning of the block (after loading bias_val? or before? it doesn't matter) we compute:\n        upper_bound = tl.minimum(1.0, 1.0/scaling_factor)\n\n        ... then after loading and adding bias:\n\n        out = tl.maximum(out, 0.0)\n        out = tl.minimum(out, upper_bound)\n\n    And then store `out`.\n\n 21. We note that the variable `x_vals` is now being overwritten by the new result.\n\n 22. We also note: the variable `scaling_factor` is a scalar (a kernel argument) and is the same for every element.\n\n 23. We write:\n\n        # Compute the upper bound for clamping (once per block)\n        upper_bound = tl.minimum(1.0, 1.0 / scaling_factor)\n\n        # Then after loading and adding bias:\n        out = x_vals + bias_val\n        out = tl.maximum(out, 0.0)\n        out = tl.minimum(out, upper_bound)\n\n        # Then store out\n\n 24. We remove the rest of the operations.\n\n 25. We must be cautious: the original code had two separate clamps and scaling. We are replacing with one clamp.\n\n 26. We hope this will speed up the kernel.\n\n 27. We also note: the current kernel is memory bound. Reducing the number of operations might not help much? But we are removing 3 operations per element (the multiply, the second clamp, and the divide) and the second clamp had two operations (min and max). Actually, we are removing:\n\n        Original: \n          - add bias (1 op)\n          - clamp1: max and min (2 ops)\n          - multiply (1 op)\n          - clamp2: max and min (2 ops) -> but note the clamp2 in the original code was written as one min and one max? Actually, the code had two separate lines: \n                x_vals = tl.minimum(tl.maximum(x_vals, 0.0), 1.0)   -> 2 ops\n          - then multiply: 1 op\n          - then clamp3: again two ops? \n                x_vals = tl.minimum(tl.maximum(x_vals, 0.0), 1.0)   -> 2 ops\n          - then divide: 1 op\n\n        Total: 1+2+1+2+1 = 7 operations? \n\n        Actually, the original code:\n\n            x_vals += bias_val   -> 1\n            x_vals = tl.minimum(tl.maximum(x_vals, 0.0), 1.0)   -> 2\n            x_vals *= scaling_factor   -> 1\n            x_vals = tl.minimum(tl.maximum(x_vals, 0.0), 1.0)   -> 2\n            x_vals /= scaling_factor   -> 1\n\n        Total: 7 operations per element.\n\n        New:\n\n            out = x_vals + bias_val   -> 1\n            out = tl.maximum(out, 0.0)   -> 1\n            out = tl.minimum(out, upper_bound)   -> 1\n\n        Total: 3 operations per element.\n\n        Plus one operation per block (for upper_bound) which is negligible.\n\n        This is a significant reduction in computation.\n\n 28. We hope to see a speedup.\n\n Let's rewrite the kernel accordingly.\n\n IMPORTANT: We must not change the function signature.\n\n We are only changing the body of the kernel.\n\n We output the entire block with the updated kernel.\n</think>\nThe current implementation has a speedup of 0.587x (slower than reference). The key optimizations made are:\n\n1. Simplified mathematical operations by combining clamping and scaling steps\n2. Reduced arithmetic operations per element from 7 to 3\n3. Precomputed upper bound once per block instead of per element\n4. Maintained memory access patterns and grid configuration\n5. Preserved functional equivalence with original operations\n\nThe new implementation:\n- Replaces five operations with two efficient min/max operations\n- Computes optimal upper bound once per block\n- Reduces computation while maintaining correctness\n- Keeps the same memory access patterns\n- Maintains all constraints (signatures, grid, bounds checks)\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _pointwise_ops(\n    x_ptr,\n    bias_ptr,\n    output_ptr,\n    scaling_factor,\n    batch_size, \n    channels, \n    height, \n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_nc = tl.program_id(0)\n    pid_s = tl.program_id(1)\n    \n    c = pid_nc % channels\n    n = pid_nc // channels\n    spatial_size = height * width\n    \n    base_offset = n * channels * spatial_size + c * spatial_size\n    block_start = pid_s * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < spatial_size\n    \n    bias_val = tl.load(bias_ptr + c)\n    # Precompute upper bound once per block\n    upper_bound = tl.minimum(1.0, 1.0 / scaling_factor)\n    \n    x_vals = tl.load(x_ptr + base_offset + offsets, mask=mask, other=0.0)\n    \n    # Optimized fused operations\n    out = x_vals + bias_val\n    out = tl.maximum(out, 0.0)\n    out = tl.minimum(out, upper_bound)\n    \n    tl.store(output_ptr + base_offset + offsets, out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        batch_size, channels, height, width = x.shape\n        \n        x_contig = x.contiguous()\n        output = torch.empty_like(x_contig)\n        bias_1d = self.bias.view(-1)\n        \n        BLOCK = 1024\n        spatial_size = height * width\n        num_blocks = triton.cdiv(spatial_size, BLOCK)\n        grid = (batch_size * channels, num_blocks)\n        \n        _pointwise_ops[grid](\n            x_contig, bias_1d, output, self.scaling_factor,\n            batch_size, channels, height, width,\n            BLOCK_SIZE=BLOCK\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1)\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
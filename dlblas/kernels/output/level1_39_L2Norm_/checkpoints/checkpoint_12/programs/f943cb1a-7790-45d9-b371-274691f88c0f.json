{"id": "f943cb1a-7790-45d9-b371-274691f88c0f", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef l2_norm_kernel(\n    x_ptr,\n    output_ptr,\n    stride_x0,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid0 = tl.program_id(0)   # Row index\n    # Compute starting offsets\n    start_offset = pid0 * stride_x0\n    # Vectorized accumulation\n    total = 0.0\n    for idx in range(0, n_cols, BLOCK_SIZE):\n        offsets = start_offset + idx + tl.arange(0, BLOCK_SIZE)\n        mask = idx + tl.arange(0, BLOCK_SIZE) < n_cols\n        vec = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n        total += tl.sum(vec * vec)\n    norm = tl.sqrt(total) + 1e-8\n    # Vectorized normalization\n    for idx in range(0, n_cols, BLOCK_SIZE):\n        offsets = start_offset + idx + tl.arange(0, BLOCK_SIZE)\n        mask = idx + tl.arange(0, BLOCK_SIZE) < n_cols\n        vec = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n        tl.store(output_ptr + offsets, vec / norm, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, n_cols = x.shape\n        output = torch.empty_like(x)\n        BLOCK_SIZE = 1024  # Optimized for H100/H800 architecture\n        grid = (batch_size,)\n        l2_norm_kernel[grid](\n            x, \n            output,\n            x.stride(0),\n            n_cols,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "5d1bb3ee-241d-420f-a417-7524173f6c81", "generation": 1, "timestamp": 1754640615.8560646, "iteration_found": 1, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.831, "runtime_ms": 0.0403, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0335, "speed_up": 0.831, "custom_timing": 0.0403}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0261, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0335, "speed_up": 1.284, "custom_timing": 0.0261}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0261\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0335, 'speed_up': 1.284, 'custom_timing': 0.0261}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0261, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0335, 'speed_up': 1.284, 'custom_timing': 0.0261}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs L2 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L2Norm layer.\n\n        Args:\n            dim (int): Dimension along which to normalize.\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L2 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (*, dim, *).\n\n        Returns:\n            torch.Tensor: Output tensor with L2 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.norm(x, p=2, dim=1, keepdim=True)\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0261), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0335, 'speed_up': 1.284, 'custom_timing': 0.0261})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs L2 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L2Norm layer.\n\n        Args:\n            dim (int): Dimension along which to normalize.\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L2 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (*, dim, *).\n\n        Returns:\n            torch.Tensor: Output tensor with L2 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.norm(x, p=2, dim=1, keepdim=True)\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that performs L2 normalization (per row) on a 2D tensor of shape [batch_size, dim].\n The current implementation uses `torch.norm` which is efficient but we are to replace it with a Triton kernel for better performance.\n\n Steps for the Triton kernel:\n 1. We want to compute the L2 norm for each row (i.e., for each sample in the batch).\n 2. The kernel will have one program per row (so grid = (batch_size,)).\n 3. Each row has `dim` elements. We will use a block size that is a power of two and at least as big as `dim`.\n 4. We'll compute the sum of squares in the row, then take the square root, then divide each element by that norm.\n\n Important: We must handle the case when the row has zeros (to avoid division by zero).\n\n Plan:\n  - We'll write a Triton kernel that loads a row (with appropriate masking for the last block if dim is not a power of two).\n  - We'll compute the sum of squares using a reduction (tl.sum with a reduction operator is not available, so we use tl.associative_scan or multiple steps? Actually, we can use a single step reduction by having a block that is the entire row?).\n\n Since the row might be long (dim=16384) and we are using a block of the entire row, we can use a reduction in one block.\n\n Steps in the kernel:\n   pid = program_id(0)  [each row]\n   base = pid * stride (but note: the tensor is 2D, so we have to compute the base offset for the row)\n   We will have a BLOCK_SIZE that is the next power of two of the row size? Actually, we can set BLOCK_SIZE to the row size? But the row size is fixed (dim=16384). However, we want to be generic.\n\n  We are going to let the kernel be configurable by having a block size that is a power of two and at least the row size? Actually, we can process the entire row in one block? Because 16384 is within the maximum block size? (Note: Triton may have a maximum block size, but 16384 is 2^14 and the max block size is typically 2^16, so it's acceptable? However, we are limited by the shared memory? Actually, we are using registers and the reduction is done in registers? We must be cautious because 16384 is large and might exceed the number of registers per thread? So we might need to split the row into multiple blocks? But note: we are using one program per row, and each row is independent.\n\n  Alternatively, we can use a two-step kernel? First, a reduction kernel to compute the norms per row, then a division kernel? But that would be two passes. We can do it in one kernel: compute the norm and then divide.\n\n  However, note: we cannot do a reduction and then an element-wise operation in the same kernel without storing the norm for the entire row? Actually, we can: we compute the sum of squares for the entire row, then take the square root, then we can divide each element by that norm. But we have to store the norm temporarily? We can do:\n\n  Steps:\n   1. Load the entire row (with masking for the last block if we split the row? Actually, we are processing the entire row in one block? Then we don't need to split? But 16384 is a big number and might require multiple warps? Actually, we can have a block of 16384 elements? Triton allows that? Yes, as long as we have enough threads? Actually, we are not mapping one element per thread? We use a block of 16384 and then we have multiple warps? But the reduction step would be done in a single block? We can do a reduction in a block.\n\n  However, note: the maximum block size in Triton is 1024? (I think the hardware limit is 1024 threads per block). So we cannot have a block of 16384 threads. Therefore, we must split the row into multiple blocks? Then we have to do two passes? Actually, we can use one program per row and then multiple blocks per row? But that would require atomic operations? And we are not allowed to have multiple blocks per row in the same kernel? Because we are using one program per row.\n\n  Alternatively, we can have multiple programs per row? Actually, we can have a 2D grid: [batch_size, num_blocks_per_row]. Then we do a two-step reduction? First, each block in the row computes a partial sum, then we do a second kernel to combine the partial sums? That would be two kernels.\n\n  We want to do it in one kernel? Actually, we can use the fact that we are in one block per row? But we cannot because the row is too big. Therefore, we must break the row into chunks that fit in one block? Then we have to use two kernels? Or we can do it in one kernel with multiple blocks per row and then use atomic adds? But that would be inefficient.\n\n  Another idea: use a kernel that computes the norm per row in two steps? First, a reduction kernel that computes the norm for each row (using multiple blocks per row) and stores the norm in a temporary array. Then a second kernel that divides each element by the norm? That would be two passes.\n\n  However, the problem says: we cannot change the grid configuration? Actually, we are allowed to change the grid? The constraints say: \"NEVER modify grid configuration, output tensor shapes, or PID logic\". So we cannot change the grid? Actually, the grid is defined by the user? In our case, the current program does not have a Triton kernel? We are introducing one.\n\n  Since we are writing a new Triton kernel, we have to define the grid. But note: the constraint says: \"NEVER modify grid configuration, output tensor shapes, or PID logic\" for the existing Triton kernels? Actually, we don't have an existing Triton kernel? We are replacing a PyTorch function with a Triton kernel.\n\n  We are free to design the grid? Actually, the constraint says: \"NEVER change @triton.jit function signatures or parameter names\" and \"NEVER modify grid configuration, output tensor shapes, or PID logic\". But that is for existing Triton kernels? We are creating a new one.\n\n  However, note the context: we are replacing the entire forward function. The original program did not have a Triton kernel. So we are introducing one.\n\n  But the constraint also says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\"? Actually, the entire block is being replaced? We are replacing the ModelNew class and the helper functions.\n\n  We are going to write a Triton kernel that does the L2 normalization. We are going to break the row into chunks? Actually, we can use a two-step reduction? But to keep it simple and because the row is 16384 (which is 16K elements) and we can use a block size of 1024, then we would have 16 blocks per row? Then we need a temporary array of size [batch_size, num_blocks] to store the partial sums? Then a second kernel to combine the partial sums? Then a third kernel to divide? That is three kernels? That would be too slow.\n\n  Alternatively, we can use a single kernel that does the entire row in one block? But we cannot because 16384 > 1024 (max threads per block). Therefore, we must use a two-step reduction.\n\n  However, note: we can do the reduction in one kernel by having the first kernel compute the per-block partial sums and then use a second kernel to combine? But that is two kernels. We want to avoid multiple kernel launches? Because the batch size is only 16? Then the overhead of launching multiple kernels might be high? But 16 is small.\n\n  Alternatively, we can use a two-step reduction in one kernel? Actually, we cannot because we don't have global synchronization? So we have to do two kernels.\n\n  Another idea: use a kernel per row that does a reduction in multiple steps? Actually, we can do a reduction in shared memory? But note: the entire row is 16384 elements and we have 1024 threads per block? Then we can load 1024 elements per block? Then we do a reduction in shared memory? Then we store the result for that block? Then we have to do a second reduction over the blocks? But we cannot do that in the same kernel without global synchronization? So we have to do two kernels.\n\n  Therefore, we will do:\n\n   Step 1: Kernel to compute the partial sums per block for each row.\n   Step 2: Kernel to combine the partial sums per row (using one block per row) to get the norm.\n   Step 3: Kernel to divide each element by the norm.\n\n  But that is three kernels? We can combine step 1 and step 3? Actually, no because we need the norm to divide.\n\n  Alternatively, we can do:\n\n   Step 1: Compute the norm per row (using two kernels: one for partial sums and one for final reduction) and store the norms in a vector of size [batch_size].\n   Step 2: Launch a kernel to divide each element by the norm of its row.\n\n  That is two kernel launches? Actually, three: the first kernel (for partial sums) and then a second kernel (for reduction) and then a third for division? Or we can combine step 2 and step 3? Actually, no because the division kernel needs the norm for the entire row.\n\n  We are going to do:\n\n   Kernel 1: partial_square_sums\n        grid = (batch_size, num_blocks)   [num_blocks = ceil(dim / BLOCK_SIZE)]\n        output: a temporary tensor of shape [batch_size, num_blocks] for the partial sums.\n\n   Kernel 2: compute_row_norms\n        grid = (batch_size,)\n        input: the temporary tensor of partial sums (for that row) and the original row? Actually, we don't need the original row? We just reduce the partial sums? But note: the partial sums are the sums of squares per block? Then we do a sqrt? Actually, we have to do:\n          total_square = sum(partial_sums)\n          norm = sqrt(total_square)   [and then we store the norm for the row]\n\n   Kernel 3: divide_by_norm\n        grid = (batch_size, ceil(dim/BLOCK_SIZE))   [same as kernel1 grid?]\n        each block loads a chunk of the row and divides by the norm (which is stored in a vector of size [batch_size])\n\n  But note: we are only allowed to change the block? And we cannot change the grid? Actually, we are designing the kernel now? And the constraint says: \"NEVER modify grid configuration, output tensor shapes, or PID logic\" for the existing Triton kernels? Since we are creating new kernels, we are free? But we must be cautious because the constraint says \"all other code is LOCKED\" meaning outside the block? But we are replacing the entire block.\n\n  However, the problem says: \"Provide the complete new program code.\" and we are replacing the entire EVOLVE-BLOCK.\n\n  But note: the batch size and dim are fixed? In the original, batch_size=16, dim=16384.\n\n  We are going to write:\n\n   class ModelNew(nn.Module):\n        def forward(self, x):\n            ... use triton kernels to compute the normalized tensor.\n\n   We also have to define the helper functions (get_inputs, get_init_inputs) as in the original.\n\n  However, the current program has:\n        batch_size = 16\n        dim = 16384\n\n  We are going to design:\n\n   Step 1: Allocate a temporary tensor for partial sums: [batch_size, num_blocks] where num_blocks = ceil(dim / BLOCK_SIZE)\n\n   Step 2: Launch kernel1: partial_square_sums\n        grid = (batch_size, num_blocks)\n        BLOCK_SIZE for the kernel is the block size we choose (say 1024? but 16384/1024=16, so num_blocks=16)\n\n   Step 3: Launch kernel2: compute_row_norms\n        grid = (batch_size,)\n        BLOCK_SIZE for kernel2: we set to the number of blocks per row? Actually, we can set BLOCK_SIZE to the next power of two of num_blocks? But num_blocks=16, so we can set BLOCK_SIZE=16? Then we can reduce 16 elements? Actually, we can set the block size to 16? Then we have 16 threads? That is acceptable.\n\n   Step 4: Launch kernel3: divide_by_norm\n        grid = (batch_size, num_blocks)   [same as kernel1]\n        BLOCK_SIZE same as kernel1 (1024)\n\n  But note: the kernel launches and the temporary tensors will add overhead? And we are doing three kernels? That might be slower than the original PyTorch? Because the original is one operation? And PyTorch's `torch.norm` is optimized? We must be cautious.\n\n  Alternatively, we can try to do the entire row in one block? But we cannot because the row has 16384 elements and we are limited to 1024 threads per block? Actually, we can use a block of 1024 threads and each thread processes 16 elements? Then we can do:\n\n   In kernel1: each thread loads 16 elements? Then we do a reduction in the block? Then we output one value per block? Actually, we are doing the same as above? But then we still need a second reduction? Actually, we can do a two-level reduction:\n\n     Level 1: within the block: each thread reduces 16 elements to one value (by summing squares) -> then we have 1024 values? Then we do a reduction in the block to get one value? Then we store that partial sum? Then we need a second kernel? So same as above.\n\n  Another idea: use a kernel per row that does the entire reduction? We can use a block of 1024 threads? Then we have to load the row in chunks? We can do:\n\n     Each thread in the block will be responsible for multiple elements? We can use a loop? And then do a reduction in shared memory? Then we get the norm for the row? Then we can write the norm? Then we can't do the division? Because we don't have the entire row in the kernel? We would have to load the row again? So we would do:\n\n        Kernel1: compute the norm for the row (using one block per row) and store the norm in a vector.\n        Kernel2: then we launch a kernel to divide the row by the norm? But we have to load the row again? That would be two passes? Actually, we are reading the row twice? Which is 2 * (batch_size * dim) reads? The original PyTorch does one read? So it might be slower.\n\n  Considering the complexity and the fact that the batch size is small (16), we might be better off using the original PyTorch? But we are required to use Triton to get a speedup.\n\n  However, note: the current program has a speed_up of 1.284? That means the Triton version we are going to replace is already faster? Actually, the current program is using the PyTorch norm? And the meta says: 'speed_up': 1.284? That is the speedup of the current custom code? Actually, the current custom code is the PyTorch version? Then why is there a speedup? Actually, the meta says: \n        'reference_timing_ms': 0.0335, \n        'speed_up': 1.284, \n        'custom_timing': 0.0261\n\n  That means the current custom code (PyTorch) runs in 0.0261 ms and the reference (what reference?) runs in 0.0335 ms? So the PyTorch version is faster than the reference? But the reference is the same? Actually, the reference might be a naive implementation? But the problem says: \"Each file contains PyTorch reference (reference_code)\" and then we are to write custom_code.\n\n  So the current custom_code is the PyTorch version? And we are to replace it with Triton? And we want to make it faster.\n\n  Given the complexity of a two-kernel reduction, and because the row is long (16384) and batch size is small (16), we might not get a speedup? But we can try.\n\n  Alternatively, we can use a single kernel per row that does:\n\n     1. Loads the entire row in chunks? But without global synchronization? How to do the reduction? We can do a reduction in one block? But we cannot because the row is too big.\n\n  Another idea: use a kernel that does both the reduction and the division? But without storing the norm? Actually, we have to know the norm to divide? And we cannot compute the norm until we have the entire row? So we have to do two passes? One pass to compute the norm and then a second pass to divide? That would be two kernel launches? But we can do it in one kernel? Actually, we can do:\n\n     We can have two loops? First loop to compute the norm (by reading the entire row and doing a reduction) and then a second loop to divide? But we cannot because the entire row might not fit in the registers? And we don't have shared memory that can hold 16384 elements? (16384 * 4 = 64KB? And shared memory is only 48KB? So we cannot store the entire row in shared memory?).\n\n  Therefore, we must do two passes? Then we are reading the row twice? That would be 2 * batch_size * dim reads? The original PyTorch does one read? So it might be slower? But we are doing it on GPU? And the row is contiguous? So the second read would be fast? And we are using Triton which might be faster than PyTorch?\n\n  But note: the current PyTorch version runs in 0.0261 ms? We have to beat that.\n\n  Let me propose a simpler solution: use a kernel that computes the norm per row by having one block per row? But with multiple threads? And each thread reduces a segment? Then we do a reduction in the block? Then we have the norm? Then we do a second loop to divide? But we are storing the row in global memory? And we are reading it twice? That is acceptable? And we are doing it in one kernel? How?\n\n  We can do:\n\n    @triton.jit\n    def l2_norm_kernel(\n        x_ptr,\n        output_ptr,\n        stride_x0, stride_x1,\n        n_cols,\n        BLOCK_SIZE: tl.constexpr\n    ):\n        pid = tl.program_id(0)   # row id\n        # We are going to compute the norm for the row `pid`\n        # Then we are going to divide the row by the norm.\n\n        # Step 1: Compute the norm\n        total = 0.0\n        # We break the row into chunks of size BLOCK_SIZE\n        for i in range(0, n_cols, BLOCK_SIZE):\n            # Load a chunk of the row\n            offsets = i + tl.arange(0, BLOCK_SIZE)\n            mask = offsets < n_cols\n            chunk = tl.load(x_ptr + pid * stride_x0 + offsets, mask=mask, other=0.0)\n            # Compute the square\n            square = chunk * chunk\n            # Sum the square in the chunk\n            chunk_sum = tl.sum(square, axis=0)\n            total += chunk_sum\n\n        norm = tl.sqrt(total) + 1e-8   # avoid division by zero\n\n        # Step 2: Divide the row by the norm\n        for i in range(0, n_cols, BLOCK_SIZE):\n            offsets = i + tl.arange(0, BLOCK_SIZE)\n            mask = offsets < n_cols\n            chunk = tl.load(x_ptr + pid * stride_x0 + offsets, mask=mask, other=0.0)\n            normalized_chunk = chunk / norm\n            tl.store(output_ptr + pid * stride_x0 + offsets, normalized_chunk, mask=mask)\n\n  Then we launch this kernel with grid = (batch_size, ) and BLOCK_SIZE = 1024? Then each block will have 1024 threads? But note: we are using a loop? And the reduction in the first step is done by the entire block? Actually, the reduction `tl.sum(square, axis=0)` is a reduction across the entire block? That requires a shared memory reduction? But Triton does that automatically? Actually, `tl.sum` is a reduction operation that works on a vector? And it returns a scalar? Then we add the scalars? But note: the entire block is doing the same loop? And each thread is loading one element per iteration? Actually, no: in the loop, each thread is loading one element? But then the `tl.sum` is over the entire chunk? But the chunk is of size BLOCK_SIZE? And we have BLOCK_SIZE threads? Then we can do a reduction within the block? But Triton's `tl.sum` is implemented as a reduction across the entire vector? But the vector is per thread? Actually, no: the `chunk` is a vector of length BLOCK_SIZE? But that is stored in registers? And we cannot reduce across threads with `tl.sum`? Actually, `tl.sum` reduces along the vector dimension? It returns a scalar for each thread? But we want the sum of the entire chunk? We can do:\n\n        chunk_sum = tl.sum(square)   # this sums the entire vector in one thread? But the vector is in one thread? How did we load it? \n\n  How are we loading the chunk? We are using:\n\n        chunk = tl.load(x_ptr + ... , mask=mask, other=0.0)\n\n  This loads a vector of BLOCK_SIZE elements into one thread? That is not coalesced? And we are limited by the vector length? Actually, we can load a vector per thread? But the maximum vector length is 128? Or it is limited by the number of registers? But BLOCK_SIZE=1024? That would be 1024 elements per thread? And we have 1024 threads? Then we are loading the entire row in one thread? That is not efficient.\n\n  We should do:\n\n        We have BLOCK_SIZE threads? Then each thread loads one element? Then we do a shared memory reduction for the chunk? But Triton doesn't have shared memory? We have to use `tl.dot` or reduction primitives? Actually, we can use `tl.sum` on a per-thread basis? But then we have to combine across threads? We can use:\n\n        total_in_chunk = tl.sum(chunk, axis=0)   # this would be the sum of the chunk? But if each thread has one element, then chunk is a scalar? Then we can do a reduction across threads? But we can use `tl.atomic_add`? Or we can use a reduction with `tl.reduce`? \n\n  Actually, Triton has a reduction operator that works across threads? We can do:\n\n        total_in_chunk = tl.sum(chunk)   # if chunk is a scalar per thread, then this sums across the block? \n\n  But wait: the Triton language has a built-in reduction across threads? Yes, by using `tl.reduce` or the reduction operators in the `tl` module? For example, `tl.sum` can be used to reduce across threads? Actually, no: `tl.sum` reduces along the axis of a tensor within a thread? To reduce across threads, we have to use `tl.atomic_add` to a shared memory array? Or use the `tl.reduce` with an associative scan? \n\n  Triton provides a function `triton.reduction` for reductions across a block? But it is not yet available in the stable version? \n\n  Alternatively, we can use the following approach:\n\n        # Step 1: Each thread loads multiple elements? For example, if we have BLOCK_SIZE=1024, and the row has 16384 elements, then each thread would load 16 elements? Then we can do:\n\n        start = 0\n        total = 0.0\n        for i in range(0, 16):\n            idx = start + i * 1024 + tid\n            if idx < n_cols:\n                val = tl.load(x_ptr + pid * stride_x0 + idx)\n                total += val * val\n\n        # Then we do a reduction within the block for the variable `total`? How?\n\n        # We can use tl.reduce to reduce across the block? But we don't have that? \n\n  After research, Triton does not provide a built-in block reduction? We have to write one using `tl.atomic_add` to shared memory? But shared memory is not exposed? Actually, we can use `tl.static_alloc` to allocate shared memory? \n\n  Example of reduction in shared memory in Triton:\n\n        shmem = tl.static_alloc(shared_memory, [BLOCK_SIZE], dtype=tl.float32)\n        offset = tl.program_id(1)   # not needed, we are 1D block\n        tid = tl.program_id(0)   # thread index in the block? Actually, we are in a 1D grid for the block? But we are in a 1D block? The block is 1D of size BLOCK_SIZE.\n\n        # Each thread stores its total in shared memory\n        shmem[tid] = total\n\n        # Synchronize\n        tl.barrier()\n\n        # Then we do a reduction in shared memory: tree reduction\n        s = 1\n        while s < BLOCK_SIZE:\n            if tid % (2*s) == 0:\n                other = tid + s\n                if other < BLOCK_SIZE:\n                    shmem[tid] += shmem[other]\n            s *= 2\n            tl.barrier()\n\n        norm = tl.sqrt(shmem[0]) + 1e-8\n\n        But note: we cannot use dynamic shared memory allocation? And we are limited by the shared memory size? And we have to know BLOCK_SIZE at compile time.\n\n  Given the complexity, and because the problem is level1 (simplest), maybe there is a simpler solution?\n\n  Actually, there is an example in the Triton tutorials: the softmax tutorial does a reduction per row? And the row can be long? They use a two-pass reduction? But they do it in one kernel? How?\n\n  In the softmax tutorial, they use:\n\n        row_start = ...\n        row_end = ...\n        row_max = ...   # compute max\n        row_sum = ...   # compute sum of exp(x_i - max)\n\n        Then they store the row_max and row_sum in global memory? Then they do a second kernel to normalize? \n\n  But in our case, we only need the norm (which is the square root of the sum of squares).\n\n  We can do:\n\n        Kernel1: compute the sum of squares per row? And store in a temporary vector? Then Kernel2: take the square root and then divide? But that is two kernels.\n\n  Considering the time, and because the batch size is small (16), we might launch two kernels and still be faster than the PyTorch version? Or not? We'll try.\n\n  However, note: the current PyTorch version runs in 0.0261 ms? We have to be faster than that.\n\n  Let's design a kernel that computes the sum of squares in one kernel with one block per row? And then we do the division in another kernel? But then we have to store the intermediate norms? And the norms are only 16 floats? So that is acceptable.\n\n  Steps:\n\n        Kernel1: \n          grid = (batch_size,)\n          block = (BLOCK_SIZE,)   [1024]\n          input: x, output: a temporary tensor of norms_squared of size [batch_size]\n\n          Each row is processed by one block.\n          The block reduces the entire row to one value: the sum of squares.\n\n          How: each thread in the block loads multiple elements (say 16) and sums their squares? Then we do a reduction within the block? Then we output the sum for the block? But wait, one block per row? Then the block reduces its own row? Then we can do:\n\n            row = pid\n            total = 0.0\n            for i in range(0, n_cols, BLOCK_SIZE * 16):\n                idx = i + tid * 16 + tl.arange(0,16)\n                mask = idx < n_cols\n                vals = tl.load(x_ptr + row * stride_row + idx, mask=mask, other=0.0)\n                squares = vals * vals\n                total += tl.sum(squares, axis=0)\n\n            Then we do a reduction within the block: we use shared memory to reduce across threads.\n\n          Then we store the total in the norms_squared[row] = total (after reduction across the block).\n\n        Kernel2: \n          grid = (batch_size, ceil(n_cols / BLOCK_SIZE))\n          block = (BLOCK_SIZE,)\n          input: x, norms_squared, output: the normalized tensor\n\n          For each row and each chunk:\n             row = program_id(0)\n             chunk_index = program_id(1)\n             norm = tl.sqrt(norms_squared[row]) + 1e-8\n             ... load a chunk of the row, divide by norm, and store.\n\n  We are launching two kernels. The first kernel has grid (batch_size) and block (1024). The second kernel has grid (batch_size, ceil(n_cols/ (1024*? ) ) wait, in the second kernel, each block will process BLOCK_SIZE elements? So grid should be (batch_size, ceil(n_cols/BLOCK_SIZE)).\n\n  But note: in the first kernel, each thread processes 16 elements per iteration? And we have 1024 threads? Then the number of iterations is ceil(n_cols / (1024*16)) = ceil(16384/(16384)) = 1 iteration? So it will be efficient.\n\n  Let's compute: 1024 * 16 = 16384. So one iteration.\n\n  Therefore, the first kernel will have one iteration per thread? Then it will be fast.\n\n  Now, we must code carefully.\n\n  We will write two Triton kernels and the ModelNew will call them.\n\n  Due to the complexity and the length of the code, and because we are only allowed to output the EVOLVE-BLOCK, I will provide the code.\n\n  Note: We must handle the case when the row size is not a multiple of the block size? But in this case, it is (16384 = 1024 * 16). But we should make it general.\n\n  We will set:\n      BLOCK_SIZE = 1024   (number of threads per block for both kernels)\n      ELEMENTS_PER_THREAD = 16   (number of elements each thread processes in the reduction kernel)\n\n  For the reduction kernel (kernel1):\n\n      n_cols = x.shape[1]\n      num_elements = n_cols\n      num_iters = (num_elements + (BLOCK_SIZE * ELEMENTS_PER_THREAD) - 1) // (BLOCK_SIZE * ELEMENTS_PER_THREAD)\n\n  But in our case, num_iters=1.\n\n  Steps in kernel1:\n\n      pid = tl.program_id(0)   # row id\n      tid = tl.program_id(1)   # thread id in the block? Actually, the block is 1D? So we use tl.program_id(0) for the row and then we use tl.arange(0, BLOCK_SIZE) for the thread index? Actually, no: the kernel is launched with grid (batch_size) and block (BLOCK_SIZE). So we have:\n\n        pid0 = row id = tl.program_id(0)\n        pid1 = thread id within the block = tl.arange(0, BLOCK_SIZE)\n\n      But how to get the thread index? We use tl.arange and then we are in the block.\n\n      Actually, in the kernel, we can use:\n\n        row = pid0\n        tid = tl.arange(0, BLOCK_SIZE)\n\n      Then the base index for the data for this row is row * stride_row.\n\n      Then the starting index for this thread is: \n          start_idx = tid * ELEMENTS_PER_THREAD\n      But we have to loop over the entire row? Actually, we only do one iteration? Because 16384/(1024*16)=1.\n\n      However, we will loop for generality.\n\n      total = 0.0\n      for i in range(0, num_iters):\n          off = i * (BLOCK_SIZE * ELEMENTS_PER_THREAD) + tid * ELEMENTS_PER_THREAD + tl.arange(0, ELEMENTS_PER_THREAD)\n          off = tl.min(off, n_cols-1)   # not needed if we use mask\n          mask = off < n_cols\n          vals = tl.load(x_ptr + row * stride_row + off, mask=mask, other=0.0)\n          squares = vals * vals\n          total += tl.sum(squares)\n\n      Then we need to reduce across threads in the block? We will use a shared memory reduction.\n\n      How to do shared memory reduction in Triton? We can use:\n\n          shmem = tl.static_alloc(tl.float32, [BLOCK_SIZE])\n          shmem[tid] = total\n          tl.barrier()\n\n          # reduce in shared memory\n          s = BLOCK_SIZE // 2\n          while s > 0:\n              if tid < s:\n                  shmem[tid] += shmem[tid+s]\n              tl.barrier()\n              s //= 2\n\n          if tid == 0:\n              norms_squared[row] = shmem[0]\n\n  Then kernel2: divide kernel\n\n      pid0 = row id\n      pid1 = block id along the row? We are launching with grid (batch_size, ceil(n_cols/BLOCK_SIZE))\n\n      In the kernel:\n\n        row = pid0\n        start_idx = pid1 * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        mask = start_idx < n_cols\n        vals = tl.load(x_ptr + row * stride_row + start_idx, mask=mask)\n        norm = tl.sqrt(norms_squared[row]) + 1e-8\n        normalized = vals / norm\n        tl.store(output_ptr + row * stride_row + start_idx, normalized, mask=mask)\n\n  We must be cautious: the norms_squared is the sum of squares? Then the norm is the square root of that.\n\n  However, note: the original does:\n        return x / torch.norm(x, p=2, dim=1, keepdim=True)\n\n  Which is: norm = sqrt(sum(x_i^2))   [for each row]\n\n  So it matches.\n\n  We will implement these two kernels.\n\n  Due to the complexity, we might not have time to write the full code? But we will try.\n\n  We'll write:\n\n      @triton.jit\n      def reduce_square_sum_kernel(\n          x_ptr,\n          output_ptr,\n          stride_x0, stride_x1,\n          n_cols,\n          BLOCK_SIZE: tl.constexpr,\n          ELEMENTS_PER_THREAD: tl.constexpr,\n      ):\n          pid0 = tl.program_id(0)   # row id\n          pid1 = tl.arange(0, BLOCK_SIZE)   # thread id in the block\n\n          # We are going to process the row `pid0`\n          row_offset = pid0 * stride_x0\n          total = 0.0\n          num_iters = tl.cdiv(n_cols, BLOCK_SIZE * ELEMENTS_PER_THIENT)\n          for i in range(0, num_iters):\n              base = i * BLOCK_SIZE * ELEMENTS_PER_THREAD + pid1 * ELEMENTS_PER_THREAD\n              off = base + tl.arange(0, ELEMENTS_PER_THREAD)\n              mask = off < n_cols\n              ptr = x_ptr + row_offset + off\n              vals = tl.load(ptr, mask=mask, other=0.0)\n              squares = vals * vals\n              total += tl.sum(squares, axis=0)\n\n          # shared memory reduction\n          shmem = tl.static_alloc(tl.float32, [BLOCK_SIZE])\n          shmem[pid1] = total\n          tl.barrier()\n\n          s = BLOCK_SIZE // 2\n          while s > 0:\n              if pid1 < s:\n                  shmem[pid1] += shmem[pid1 + s]\n              tl.barrier()\n              s //= 2\n\n          if pid1 == 0:\n              tl.store(output_ptr + pid0, shmem[0])\n\n      @triton.jit\n      def divide_kernel(\n          x_ptr,\n          norms_ptr,\n          output_ptr,\n          stride_x0, stride_x1,\n          n_cols,\n          BLOCK_SIZE: tl.constexpr,\n      ):\n          pid0 = tl.program_id(0)   # row id\n          pid1 = tl.program_id(1)   // chunk id\n          pid1_base = pid1 * BLOCK_SIZE\n          off = pid1_base + tl.arange(0, BLOCK_SIZE)\n          mask = off < n_cols\n          row_offset = pid0 * stride_x0\n          vals = tl.load(x_ptr + row_offset + off, mask=mask, other=0.0)\n          norm = tl.sqrt(tl.load(norms_ptr + pid0)) + 1e-8\n          normalized = vals / norm\n          tl.store(output_ptr + row_offset + off, normalized, mask=mask)\n\n  Then in the model:\n\n      def forward(self, x):\n          batch_size, n_cols = x.shape\n          norms_squared = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n          # Kernel1: reduce_square_sum_kernel\n          grid1 = (batch_size,)\n          reduce_square_sum_kernel[grid1, BLOCK_SIZE](\n              x, norms_squared,\n              x.stride(0), x.stride(1),\n              n_cols,\n              BLOCK_SIZE=1024,\n              ELEMENTS_PER_THREAD=16,\n          )\n          # Kernel2: divide_kernel\n          grid2 = (batch_size, triton.cdiv(n_cols, 1024))\n          divide_kernel[grid2, 1024](\n              x, norms_squared, x,   # output in-place? Or we should output to a new tensor? We can output to a new tensor.\n              x.stride(0), x.stride(1),\n              n_cols,\n              BLOCK_SIZE=1024,\n          )\n          return x   # if we did in-place, or return the output tensor.\n\n  But note: we are modifying the input? We should not. We will allocate an output tensor.\n\n  However, the original returns a new tensor? The original: `return x / ...` creates a new tensor.\n\n  So we should allocate an output tensor for the normalized values.\n\n  In kernel2, we will have an output tensor.\n\n  Due to the complexity and length, and because the problem is level1, and we are seeing that the current speedup is already 1.284 with the PyTorch version, we might not need to do this? But the task is to rewrite in Triton.\n\n  Given the time, and because the row is large and batch small, we might not get a speedup? But we can try.\n\n  Alternatively, we can try to do a simpler kernel that only does the reduction in one kernel and then use a separate kernel for division? And hope that the overhead is low.\n\n  But note: the current PyTorch timing is 0.0261 ms? That is very fast? And launching two kernels might take longer? \n\n  Considering the complexity and the risk, and because the problem is level1, I will provide a simpler solution: use the Triton kernel to do the entire row in one thread? But that would be slow.\n\n  Given the instructions to only change the block, and because the current block is the entire program, I will provide the following simpler solution: use the Triton kernel that does the entire row in one block? But with a loop within the thread? And then we do the division? But then we would have only batch_size threads? That would be 16 threads? Then each thread does the entire row? That would be 16384 iterations per thread? That would be slow? But let's try.\n\n  We'll launch with grid = (batch_size,) and block = (1,). Then each thread does:\n\n        row = pid\n        total = 0.0\n        for i in range(n_cols):\n            val = tl.load(x_ptr + row * stride + i)\n            total += val * val\n        norm = tl.sqrt(total) + 1e-8\n        for i in range(n_cols):\n            val = tl.load(x_ptr + row * stride + i)\n            normalized = val / norm\n            tl.store(output_ptr + row * stride + i, normalized)\n\n  This is very naive, but it might be faster than PyTorch for small batch_size? Because 16 rows, each row 16384, that is 16 * 16384 * 2 = 524288 loads and 16 * 16384 = 262144 stores? And the arithmetic is cheap.\n\n  But the PyTorch version is vectorized? This is scalar.\n\n  Given the time, I will provide this naive kernel and hope that for level1 it is acceptable.\n\n  We'll write:\n\n      import math\n      import torch\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def l2_norm_kernel(\n          x_ptr,\n          output_ptr,\n          stride_x0, stride_x1,\n          n_cols,\n          BLOCK_SIZE: tl.constexpr,   # not used, but to avoid error\n      ):\n          pid0 = tl.program_id(0)   # row id\n          # We are the only thread in the block? We don't need to worry about other threads.\n          total = 0.0\n          for i in range(0, n_cols):\n              val = tl.load(x_ptr + pid0 * stride_x0 + i)\n              total += val * val\n          norm = tl.sqrt(total) + 1e-8\n          for i in range(0, n_cols):\n              val = tl.load(x_ptr + pid0 * stride_x0 + i)\n              tl.store(output_ptr + pid0 * stride_x0 + i, val / norm)\n\n      class ModelNew(nn.Module):\n          def forward(self, x):\n              batch_size, n_cols = x.shape\n              output = torch.empty_like(x)\n              grid = (batch_size,)\n              # We use 1 thread per block? Because the entire row is done by one thread.\n              l2_norm_kernel[grid, (1,)](x, output, x.stride(0), x.stride(1), n_cols, BLOCK_SIZE=1)\n              return output\n\n  This is the simplest.\n\n  Let's hope it compiles and runs faster than the PyTorch version? But likely not.\n\n  Given the instructions, I will output this naive version.\n\n  Note: the batch_size=16, so we launch 16 blocks, each with 1 thread.\n\n  This might be slow? But it is level1, so maybe acceptable.\n\n  If it is not faster, we will have to try the more advanced kernels.\n\n  We will output the code accordingly.\n\n  We also have to keep the get_inputs and get_init_inputs.\n\n  Due to the complexity and the instruction to only change the block, and because the current program is very simple, I will provide the naive Triton kernel.\n\n  If it doesn't speed up, we might have to try a more advanced approach in the next iteration.\n\n  Let's code accordingly.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef l2_norm_kernel(\n    x_ptr,\n    output_ptr,\n    stride_x0,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid0 = tl.program_id(0)   # Row index\n    # Compute starting offsets\n    start_offset = pid0 * stride_x0\n    # Vectorized accumulation\n    total = 0.0\n    for idx in range(0, n_cols, BLOCK_SIZE):\n        offsets = start_offset + idx + tl.arange(0, BLOCK_SIZE)\n        mask = idx + tl.arange(0, BLOCK_SIZE) < n_cols\n        vec = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n        total += tl.sum(vec * vec)\n    norm = tl.sqrt(total) + 1e-8\n    # Vectorized normalization\n    for idx in range(0, n_cols, BLOCK_SIZE):\n        offsets = start_offset + idx + tl.arange(0, BLOCK_SIZE)\n        mask = idx + tl.arange(0, BLOCK_SIZE) < n_cols\n        vec = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n        tl.store(output_ptr + offsets, vec / norm, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, n_cols = x.shape\n        output = torch.empty_like(x)\n        BLOCK_SIZE = 1024  # Optimized for H100/H800 architecture\n        grid = (batch_size,)\n        l2_norm_kernel[grid](\n            x, \n            output,\n            x.stride(0),\n            n_cols,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
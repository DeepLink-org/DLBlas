{"id": "6a8910f6-8370-4eb2-8a45-6edd69f14b55", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef avg_pool_3d_kernel(\n    x_ptr,\n    output_ptr,\n    batch_size, channels, D, H, W,\n    out_D, out_H, out_W,\n    stride_batch, stride_channel, stride_d, stride_h, stride_w,\n    stride_val: tl.constexpr,\n    padding: tl.constexpr,\n    kernel_size: tl.constexpr,\n    divisor: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    pid_output_index = tl.program_id(2)\n\n    if pid_batch >= batch_size or pid_channel >= channels:\n        return\n\n    out_HW = out_H * out_W\n    d_idx = pid_output_index // out_HW\n    hw_idx = pid_output_index % out_HW\n    h_idx = hw_idx // out_W\n    w_idx = hw_idx % out_W\n\n    start_d = d_idx * stride_val - padding\n    start_h = h_idx * stride_val - padding\n    start_w = w_idx * stride_val - padding\n\n    total = 0.0\n    base_ptr = x_ptr + pid_batch * stride_batch + pid_channel * stride_channel\n\n    # Determine vectorization block size (next power of two)\n    BLOCK_W = 1\n    if kernel_size > 1:\n        BLOCK_W = 2\n    if kernel_size > 2:\n        BLOCK_W = 4\n    if kernel_size > 4:\n        BLOCK_W = 8\n    if kernel_size > 8:\n        BLOCK_W = 16\n    if kernel_size > 16:\n        BLOCK_W = 32\n\n    for kd in range(kernel_size):\n        id = start_d + kd\n        mask_d = (id >= 0) & (id < D)\n        for kh in range(kernel_size):\n            ih = start_h + kh\n            mask_h = (ih >= 0) & (ih < H)\n            row_ptr = base_ptr + id * stride_d + ih * stride_h\n            \n            # Vectorized width processing\n            w_offsets = tl.arange(0, BLOCK_W)\n            w_indices = start_w + w_offsets\n            mask_w = (w_indices >= 0) & (w_indices < W) & (w_offsets < kernel_size)\n            mask_total = mask_w & mask_d & mask_h\n            \n            vals = tl.load(row_ptr + w_indices, mask=mask_total, other=0.0)\n            total += tl.sum(vals)\n\n    avg = total / divisor\n    \n    output_offset = pid_batch * channels * out_D * out_H * out_W + \\\n                    pid_channel * out_D * out_H * out_W + \\\n                    d_idx * out_H * out_W + \\\n                    h_idx * out_W + \\\n                    w_idx\n    tl.store(output_ptr + output_offset, avg)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        if stride is None:\n            stride = kernel_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.divisor = float(kernel_size ** 3)  # Precompute fixed divisor\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, D, H, W = x.shape\n        out_D = (D + 2 * self.padding - self.kernel_size) // self.stride + 1\n        out_H = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n        out_W = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty((batch_size, channels, out_D, out_H, out_W), \n                             device=x.device, dtype=x.dtype)\n        \n        if x.numel() == 0:\n            return output\n            \n        x = x.contiguous()  # Ensure contiguous memory\n        stride_batch, stride_channel, stride_d, stride_h, stride_w = x.stride()\n        grid = (batch_size, channels, out_D * out_H * out_W)\n        \n        avg_pool_3d_kernel[grid](\n            x, output,\n            batch_size, channels, D, H, W,\n            out_D, out_H, out_W,\n            stride_batch, stride_channel, stride_d, stride_h, stride_w,\n            self.stride,\n            self.padding,\n            self.kernel_size,\n            self.divisor,\n        )\n        \n        return output\n\nbatch_size = 16\nchannels = 32\ndepth = 64\nheight = 64\nwidth = 64\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "57f9fc63-8448-46a4-b87a-0d891572358a", "generation": 3, "timestamp": 1754642670.420584, "iteration_found": 19, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_46_Average_Pooling_3D", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 574, in arange\n    raise ValueError(\"arange's arguments must be of type tl.constexpr\")\nValueError: arange's arguments must be of type tl.constexpr\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpaibfv3ca.py\", line 104, in forward\n    avg_pool_3d_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 54:24:\n        BLOCK_W = 32\n\n    for kd in range(kernel_size):\n        id = start_d + kd\n        mask_d = (id >= 0) & (id < D)\n        for kh in range(kernel_size):\n            ih = start_h + kh\n            mask_h = (ih >= 0) & (ih < H)\n            row_ptr = base_ptr + id * stride_d + ih * stride_h\n\n            # Vectorized width processing\n            w_offsets = tl.arange(0, BLOCK_W)\n                        ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 574, in arange\n    raise ValueError(\"arange's arguments must be of type tl.constexpr\")\nValueError: arange's arguments must be of type tl.constexpr\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0237, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.001, "runtime_ms": 26.1, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0237, "speed_up": 0.001, "custom_timing": 26.1}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.0010\n- runtime_ms: 26.1000\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0237, 'speed_up': 0.001, 'custom_timing': 26.1}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level1_46_Average_Pooling_3D', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\\n    return _semantic.arange(start, end)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\\n    raise ValueError(\"arange\\'s range must be a power of 2\")\\nValueError: arange\\'s range must be a power of 2\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpjsqjijle.py\", line 88, in forward\\n    avg_pool_3d_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 39:34:\\n\\n    total = 0.0\\n    base_ptr = x_ptr + pid_batch * stride_batch + pid_channel * stride_channel\\n\\n    for kd in range(kernel_size):\\n        id = start_d + kd\\n        mask_d = (id >= 0) & (id < D)\\n        for kh in range(kernel_size):\\n            ih = start_h + kh\\n            mask_h = (ih >= 0) & (ih < H)\\n            row_ptr = base_ptr + id * stride_d + ih * stride_h\\n            w_offsets = start_w + tl.arange(0, kernel_size)\\n                                  ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\\n    return _semantic.arange(start, end)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\\n    raise ValueError(\"arange\\'s range must be a power of 2\")\\nValueError: arange\\'s range must be a power of 2\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.0237, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.0010, runtime_ms: 26.1000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0237, 'speed_up': 0.001, 'custom_timing': 26.1}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.0430, runtime_ms: 0.5470, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0237, 'speed_up': 0.043, 'custom_timing': 0.547}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef pooling_3d_kernel(\n    input_ptr,\n    output_ptr,\n    kernel_size,\n    D, H, W,\n    stride_d, stride_h, stride_w,\n    padding_d, padding_h, padding_w,\n    stride_in_n, stride_in_c, stride_in_d, stride_in_h, stride_in_w,\n    stride_out_n, stride_out_c, stride_out_d, stride_out_h, stride_out_w,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_outputs = tl.num_programs(0)\n    \n    n = pid // (D * H * W)\n    d = (pid % (D * H * W)) // (H * W)\n    h = (pid % (H * W)) // W\n    w = pid % W\n    \n    start_d = d * stride_d - padding_d\n    start_h = h * stride_h - padding_h\n    start_w = w * stride_w - padding_w\n    \n    accum = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    count = 0\n    \n    for kd in range(kernel_size):\n        id = start_d + kd\n        if id < 0 or id >= D:\n            continue\n        for kh in range(kernel_size):\n            ih = start_h + kh\n            if ih < 0 or ih >= H:\n                continue\n            for kw in range(kernel_size):\n                iw = start_w + kw\n                if iw < 0 or iw >= W:\n                    continue\n                \n                offset = (n * stride_in_n + \n                          id * stride_in_d + \n                          ih * stride_in_h + \n                          iw * stride_in_w)\n                \n                for c_block in range(0, BLOCK_SIZE):\n                    ptr = input_ptr + offset + c_block * stride_in_c\n                    val = tl.load(ptr, mask=(c_block < BLOCK_SIZE), other=0.0)\n                    accum = tl.where(c_block < BLOCK_SIZE, accum + val, accum)\n                \n                count += 1\n    \n    if count > 0:\n        accum = accum / count\n    \n    out_offset = (n * stride_out_n + \n                  d * stride_out_d + \n                  h * stride_out_h + \n                  w * stride_out_w)\n    \n    for c_block in range(0, BLOCK_SIZE):\n        ptr = output_ptr + out_offset + c_block * stride_out_c\n        tl.store(ptr, accum, mask=(c_block < BLOCK_SIZE))\n\ndef pooling_3d(x: torch.Tensor, kernel_size: int, stride: int, padding: int) -> torch.Tensor:\n    N, C, D, H, W = x.shape\n    D_out = (D + 2 * padding - kernel_size) // stride + 1\n    H_out = (H + 2 * padding - kernel_size) // stride + 1\n    W_out = (W + 2 * padding - kernel_size) // stride + 1\n    \n    output = torch.empty((N, C, D_out, H_out, W_out), device=x.device, dtype=x.dtype)\n    grid = lambda _: (N * D_out * H_out * W_out,)\n    \n    BLOCK_SIZE = 32\n    P2 = int(2 ** math.ceil(math.log2(C)))\n    \n    pooling_3d_kernel[grid](\n        x, output, kernel_size,\n        D_out, H_out, W_out,\n        stride, stride, stride,\n        padding, padding, padding,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),\n        output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4),\n        BLOCK_SIZE, P2\n    )\n    \n    return output\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.0430), Performs well on runtime_ms (0.5470), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0237, 'speed_up': 0.043, 'custom_timing': 0.547})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef avg_pool_3d_kernel(\n    x_ptr,\n    output_ptr,\n    batch_size, channels, D, H, W,\n    out_D, out_H, out_W,\n    stride_batch, stride_channel, stride_d, stride_h, stride_w,\n    stride_val: tl.constexpr,\n    padding: tl.constexpr,\n    kernel_size: tl.constexpr,\n    divisor: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    pid_output_index = tl.program_id(2)\n\n    if pid_batch >= batch_size or pid_channel >= channels:\n        return\n\n    out_HW = out_H * out_W\n    d_idx = pid_output_index // out_HW\n    hw_idx = pid_output_index % out_HW\n    h_idx = hw_idx // out_W\n    w_idx = hw_idx % out_W\n\n    start_d = d_idx * stride_val - padding\n    start_h = h_idx * stride_val - padding\n    start_w = w_idx * stride_val - padding\n\n    total = 0.0\n\n    for kd in range(kernel_size):\n        id = start_d + kd\n        for kh in range(kernel_size):\n            ih = start_h + kh\n            for kw in range(kernel_size):\n                iw = start_w + kw\n                # Check boundaries using masks\n                mask_d = (id >= 0) & (id < D)\n                mask_h = (ih >= 0) & (ih < H)\n                mask_w = (iw >= 0) & (iw < W)\n                mask = mask_d & mask_h & mask_w\n                \n                offset = pid_batch * stride_batch + pid_channel * stride_channel + id * stride_d + ih * stride_h + iw * stride_w\n                val = tl.load(x_ptr + offset, mask=mask, other=0.0)\n                total += val\n\n    avg = total / divisor\n    \n    output_offset = pid_batch * channels * out_D * out_H * out_W + \\\n                    pid_channel * out_D * out_H * out_W + \\\n                    d_idx * out_H * out_W + \\\n                    h_idx * out_W + \\\n                    w_idx\n    tl.store(output_ptr + output_offset, avg)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        if stride is None:\n            stride = kernel_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.divisor = float(kernel_size ** 3)  # Precompute fixed divisor\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, D, H, W = x.shape\n        out_D = (D + 2 * self.padding - self.kernel_size) // self.stride + 1\n        out_H = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n        out_W = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty((batch_size, channels, out_D, out_H, out_W), \n                             device=x.device, dtype=x.dtype)\n        \n        if x.numel() == 0:\n            return output\n            \n        x = x.contiguous()  # Ensure contiguous memory\n        stride_batch, stride_channel, stride_d, stride_h, stride_w = x.stride()\n        grid = (batch_size, channels, out_D * out_H * out_W)\n        \n        avg_pool_3d_kernel[grid](\n            x, output,\n            batch_size, channels, D, H, W,\n            out_D, out_H, out_W,\n            stride_batch, stride_channel, stride_d, stride_h, stride_w,\n            self.stride,\n            self.padding,\n            self.kernel_size,\n            self.divisor,\n        )\n        \n        return output\n\nbatch_size = 16\nchannels = 32\ndepth = 64\nheight = 64\nwidth = 64\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.0010), Performs well on runtime_ms (26.1000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0237, 'speed_up': 0.001, 'custom_timing': 26.1})\n\n\n### Program 3 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef avg_pool_3d_kernel(\n    x_ptr,\n    output_ptr,\n    batch_size, channels, D, H, W,\n    out_D, out_H, out_W,\n    stride_batch, stride_channel, stride_d, stride_h, stride_w,\n    stride_val: tl.constexpr,\n    padding: tl.constexpr,\n    kernel_size: tl.constexpr,\n    divisor: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    pid_output_index = tl.program_id(2)\n\n    if pid_batch >= batch_size or pid_channel >= channels:\n        return\n\n    out_HW = out_H * out_W\n    d_idx = pid_output_index // out_HW\n    hw_idx = pid_output_index % out_HW\n    h_idx = hw_idx // out_W\n    w_idx = hw_idx % out_W\n\n    start_d = d_idx * stride_val - padding\n    start_h = h_idx * stride_val - padding\n    start_w = w_idx * stride_val - padding\n\n    total = 0.0\n    base_ptr = x_ptr + pid_batch * stride_batch + pid_channel * stride_channel\n\n    for kd in range(kernel_size):\n        id = start_d + kd\n        mask_d = (id >= 0) & (id < D)\n        for kh in range(kernel_size):\n            ih = start_h + kh\n            mask_h = (ih >= 0) & (ih < H)\n            row_ptr = base_ptr + id * stride_d + ih * stride_h\n            w_offsets = start_w + tl.arange(0, kernel_size)\n            mask_w = (w_offsets >= 0) & (w_offsets < W)\n            mask = mask_d & mask_h & mask_w\n            addrs = row_ptr + w_offsets * stride_w\n            vals = tl.load(addrs, mask=mask, other=0.0)\n            total += tl.sum(vals)\n\n    avg = total / divisor\n    \n    output_offset = pid_batch * channels * out_D * out_H * out_W + \\\n                    pid_channel * out_D * out_H * out_W + \\\n                    d_idx * out_H * out_W + \\\n                    h_idx * out_W + \\\n                    w_idx\n    tl.store(output_ptr + output_offset, avg)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        if stride is None:\n            stride = kernel_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.divisor = float(kernel_size ** 3)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, D, H, W = x.shape\n        out_D = (D + 2 * self.padding - self.kernel_size) // self.stride + 1\n        out_H = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n        out_W = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty((batch_size, channels, out_D, out_H, out_W), \n                             device=x.device, dtype=x.dtype)\n        \n        if x.numel() == 0:\n            return output\n            \n        x = x.contiguous()\n        stride_batch, stride_channel, stride_d, stride_h, stride_w = x.stride()\n        grid = (batch_size, channels, out_D * out_H * out_W)\n        \n        avg_pool_3d_kernel[grid](\n            x, output,\n            batch_size, channels, D, H, W,\n            out_D, out_H, out_W,\n            stride_batch, stride_channel, stride_d, stride_h, stride_w,\n            self.stride,\n            self.padding,\n            self.kernel_size,\n            self.divisor,\n        )\n        \n        return output\n\nbatch_size = 16\nchannels = 32\ndepth = 64\nheight = 64\nwidth = 64\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level1_46_Average_Pooling_3D', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\\n    return _semantic.arange(start, end)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\\n    raise ValueError(\"arange\\'s range must be a power of 2\")\\nValueError: arange\\'s range must be a power of 2\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpjsqjijle.py\", line 88, in forward\\n    avg_pool_3d_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 39:34:\\n\\n    total = 0.0\\n    base_ptr = x_ptr + pid_batch * stride_batch + pid_channel * stride_channel\\n\\n    for kd in range(kernel_size):\\n        id = start_d + kd\\n        mask_d = (id >= 0) & (id < D)\\n        for kh in range(kernel_size):\\n            ih = start_h + kh\\n            mask_h = (ih >= 0) & (ih < H)\\n            row_ptr = base_ptr + id * stride_d + ih * stride_h\\n            w_offsets = start_w + tl.arange(0, kernel_size)\\n                                  ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\\n    return _semantic.arange(start, end)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\\n    raise ValueError(\"arange\\'s range must be a power of 2\")\\nValueError: arange\\'s range must be a power of 2\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.0237, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: -100.0000)\n```python\nfor kd in range(kernel_size):\n        id = start_d + kd\n        if id < 0 or id >= D:\n            continue\n        for kh in range(kernel_size):\n            ih = start_h + kh\n            if ih < 0 or ih >= H:\n                continue\n            for kw in range(kernel_size):\n                iw = start_w + kw\n                if iw < 0 or iw >= W:\n                    continue\n                # ... load and accumulate\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef pooling_3d_kernel(\n    input_ptr,\n    output_ptr,\n    kernel_size,\n    D, H, W,\n    stride_d, stride_h, stride_w,\n    padding_d, padding_h, padding_w,\n    stride_in_n, stride_in_c, stride_in_d, stride_in_h, stride_in_w,\n    stride_out_n, stride_out_c, stride_out_d, stride_out_h, stride_out_w,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_outputs = tl.num_programs(0)\n    \n    n = pid // (D * H * W)\n    d = (pid % (D * H * W)) // (H * W)\n    h = (pid % (H * W)) // W\n    w = pid % W\n    \n    start_d = d * stride_d - padding_d\n    start_h = h * stride_h - padding_h\n    start_w = w * stride_w - padding_w\n    \n    accum = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    count = 0\n    \n    for kd in range(kernel_size):\n        id = start_d + kd\n        if id < 0 or id >= D:\n            continue\n        for kh in range(kernel_size):\n            ih = start_h + kh\n            if ih < 0 or ih >= H:\n                continue\n            for kw in range(kernel_size):\n                iw = start_w + kw\n                if iw < 0 or iw >= W:\n                    continue\n                \n                offset = (n * stride_in_n + \n                          id * stride_in_d + \n                          ih * stride_in_h + \n                          iw * stride_in_w)\n                \n                for c_block in range(0, BLOCK_SIZE):\n                    ptr = input_ptr + offset + c_block * stride_in_c\n                    val = tl.load(ptr, mask=(c_block < BLOCK_SIZE), other=0.0)\n                    accum = tl.where(c_block < BLOCK_SIZE, accum + val, accum)\n                \n                count += 1\n    \n    if count > 0:\n        accum = accum / count\n    \n    out_offset = (n * stride_out_n + \n                  d * stride_out_d + \n                  h * stride_out_h + \n                  w * stride_out_w)\n    \n    for c_block in range(0, BLOCK_SIZE):\n        ptr = output_ptr + out_offset + c_block * stride_out_c\n        tl.store(ptr, accum, mask=(c_block < BLOCK_SIZE))\n\ndef pooling_3d(x: torch.Tensor, kernel_size: int, stride: int, padding: int) -> torch.Tensor:\n    N, C, D, H, W = x.shape\n    D_out = (D + 2 * padding - kernel_size) // stride + 1\n    H_out = (H + 2 * padding - kernel_size) // stride + 1\n    W_out = (W + 2 * padding - kernel_size) // stride + 1\n    \n    output = torch.empty((N, C, D_out, H_out, W_out), device=x.device, dtype=x.dtype)\n    grid = lambda _: (N * D_out * H_out * W_out,)\n    \n    BLOCK_SIZE = 32\n    P2 = int(2 ** math.ceil(math.log2(C)))\n    \n    pooling_3d_kernel[grid](\n        x, output, kernel_size,\n        D_out, H_out, W_out,\n        stride, stride, stride,\n        padding, padding, padding,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),\n        output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4),\n        BLOCK_SIZE, P2\n    )\n    \n    return output\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef avg_pool_3d_kernel(\n    x_ptr,\n    output_ptr,\n    batch_size, channels, D, H, W,\n    out_D, out_H, out_W,\n    stride_batch, stride_channel, stride_d, stride_h, stride_w,\n    stride_val: tl.constexpr,\n    padding: tl.constexpr,\n    kernel_size: tl.constexpr,\n    divisor: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    pid_output_index = tl.program_id(2)\n\n    if pid_batch >= batch_size or pid_channel >= channels:\n        return\n\n    out_HW = out_H * out_W\n    d_idx = pid_output_index // out_HW\n    hw_idx = pid_output_index % out_HW\n    h_idx = hw_idx // out_W\n    w_idx = hw_idx % out_W\n\n    start_d = d_idx * stride_val - padding\n    start_h = h_idx * stride_val - padding\n    start_w = w_idx * stride_val - padding\n\n    total = 0.0\n\n    for kd in range(kernel_size):\n        id = start_d + kd\n        for kh in range(kernel_size):\n            ih = start_h + kh\n            for kw in range(kernel_size):\n                iw = start_w + kw\n                # Check boundaries using masks\n                mask_d = (id >= 0) & (id < D)\n                mask_h = (ih >= 0) & (ih < H)\n                mask_w = (iw >= 0) & (iw < W)\n                mask = mask_d & mask_h & mask_w\n                \n                offset = pid_batch * stride_batch + pid_channel * stride_channel + id * stride_d + ih * stride_h + iw * stride_w\n                val = tl.load(x_ptr + offset, mask=mask, other=0.0)\n                total += val\n\n    avg = total / divisor\n    \n    output_offset = pid_batch * channels * out_D * out_H * out_W + \\\n                    pid_channel * out_D * out_H * out_W + \\\n                    d_idx * out_H * out_W + \\\n                    h_idx * out_W + \\\n                    w_idx\n    tl.store(output_ptr + output_offset, avg)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        if stride is None:\n            stride = kernel_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.divisor = float(kernel_size ** 3)  # Precompute fixed divisor\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, D, H, W = x.shape\n        out_D = (D + 2 * self.padding - self.kernel_size) // self.stride + 1\n        out_H = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n        out_W = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty((batch_size, channels, out_D, out_H, out_W), \n                             device=x.device, dtype=x.dtype)\n        \n        if x.numel() == 0:\n            return output\n            \n        x = x.contiguous()  # Ensure contiguous memory\n        stride_batch, stride_channel, stride_d, stride_h, stride_w = x.stride()\n        grid = (batch_size, channels, out_D * out_H * out_W)\n        \n        avg_pool_3d_kernel[grid](\n            x, output,\n            batch_size, channels, D, H, W,\n            out_D, out_H, out_W,\n            stride_batch, stride_channel, stride_d, stride_h, stride_w,\n            self.stride,\n            self.padding,\n            self.kernel_size,\n            self.divisor,\n        )\n        \n        return output\n\nbatch_size = 16\nchannels = 32\ndepth = 64\nheight = 64\nwidth = 64\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program that uses Triton for 3D average pooling.\n The current performance: speed_up is 0.001 (which is very slow) and runtime_ms is 26.1 ms, while the reference (PyTorch) is 0.0237 ms.\n\n The current Triton kernel uses a triple nested loop (over kernel_size in depth, height, width) and then loads one element at a time.\n This leads to a lot of memory accesses and might not be efficient.\n\n We note that the previous attempt that used a vectorized load (with tl.arange) failed because the kernel_size might not be a power of two (and Triton requires the range of arange to be a power of two).\n\n However, we can try to optimize by:\n  1. Tiling the channel dimension and processing multiple channels at once (using a block of channels) to increase parallelism and reuse.\n  2. Precomputing the base pointers and using vectorized loads where possible.\n\n But note: the current grid is (batch_size, channels, out_D * out_H * out_W). This means we have one thread per output element per channel per batch.\n\n The problem is that we are iterating over kernel_size^3 (which is 27 for kernel_size=3) and each iteration does a single load. This is 27 loads per output.\n\n We can try to vectorize the innermost loop (over width) if we can. However, the kernel_size might not be a power of two, but we can use masking.\n\n However, note the constraint: \"NEVER remove boundary checks or out-of-bound masks\". So we must keep the masking.\n\n Alternatively, we can change the kernel to process a block of channels for each output spatial location? That would increase the arithmetic intensity.\n\n Let's restructure:\n  - Instead of having a program over (batch, channel, output_index), we can have:\n        pid = tl.program_id(0)\n        pid_batch = pid // (out_D * out_H * out_W)   # but note: we also have channels? \n    Actually, we want to parallelize over batch, output spatial, and channels.\n\n  However, the grid in the current program is (batch_size, channels, out_D * out_H * out_W). This is a 3D grid.\n\n  We can try to use a 1D grid and then have each thread process a block of channels? \n\n  Alternatively, we can keep the 3D grid and have each thread process a block of channels? But note: the channel dimension is contiguous in memory, so we can load multiple channels at once.\n\n  Proposed plan:\n    - Each thread will process a block of channels (say, BLOCK_SIZE) for one output spatial location in one batch.\n\n  Steps:\n    - We'll change the grid to be (batch_size * out_D * out_H * out_W, ) and then use a 1D grid. Then we'll have a loop over channels in blocks of BLOCK_SIZE.\n\n    However, note the constraint: \"NEVER change @triton.jit function signatures or parameter names\". Also, \"NEVER modify grid configuration, output tensor shapes, or PID logic\".\n\n  But wait: the problem says we cannot change the grid configuration? Actually, the grid is set in the forward function. We are allowed to change the grid as long as we don't change the output tensor shapes? \n\n  However, the constraint says: \"NEVER modify grid configuration, output tensor shapes, or PID logic\". So we cannot change the grid.\n\n  Therefore, we must stick to the 3D grid: (batch_size, channels, out_D * out_H * out_W).\n\n  But note: the current kernel does:\n      pid_batch, pid_channel, pid_output_index\n\n  And then it computes one output element at (pid_batch, pid_channel, d_idx, h_idx, w_idx).\n\n  This is very inefficient because we are not using vectorization and we are doing a lot of small loads.\n\n  We can try to vectorize the inner loop over the kernel's width? For example, for a fixed (kd, kh) we can load a vector of `kernel_size` elements in the width dimension at once? But note: the kernel_size might be arbitrary and not a power of two, and Triton requires the range to be a power of two for arange.\n\n  However, we can use a loop over the width dimension and use masking? But that would not be vectorized.\n\n  Alternative idea: precompute the base pointer for the current spatial location and then use a loop that loads one element at a time. But that is what we are already doing.\n\n  How about we change the kernel to use shared memory? We can load a block of the input into shared memory and then compute the average? But note: the input is 5D and the kernel window is 3D, and the output is 5D. This might be complex.\n\n  Another idea: we note that the current kernel does 27 loads per output element. The reference PyTorch implementation is probably using highly optimized C++ code that might be using vectorization and cache blocking.\n\n  We need to reduce the number of loads or make them more efficient.\n\n  We can try to use vectorized loads in the channel dimension? Currently, we are processing one channel at a time. But if we process a block of channels, then we can load multiple channels at the same spatial location at once.\n\n  How? We can change the kernel so that each thread processes a block of channels (say, BLOCK_SIZE) for the same output spatial location.\n\n  Steps in the kernel:\n    - Instead of having pid_channel representing a single channel, we can have it represent a block of channels.\n    - We will then have an inner loop over the channels in the block? Or we can use vector operations.\n\n  But note: we cannot change the grid? The grid is (batch_size, channels, out_D * out_H * out_W). We cannot change that.\n\n  However, we can reinterpret the grid: we can set the grid to (batch_size, ceil(channels / BLOCK_SIZE), out_D * out_H * out_W) and then have each thread process BLOCK_SIZE channels? But that would change the grid.\n\n  The constraint says: \"NEVER modify grid configuration\", meaning we cannot change the grid? Actually, the grid is set in the forward function. The constraint might refer to the fact that we must not change the way the grid is computed (the number of elements) because it is fixed by the output shape? \n\n  But note: the grid in the current program is (batch_size, channels, out_D * out_H * out_W). We are not allowed to change the grid? \n\n  Let me read the constraint again: \"NEVER modify grid configuration, output tensor shapes, or PID logic\". So we cannot change the grid.\n\n  Therefore, we must keep the grid as (batch_size, channels, out_D * out_H * out_W). That means each thread still processes one channel at one output spatial location in one batch.\n\n  How can we optimize then?\n\n  We can try to use vectorized loads in the kernel window? For example, we can try to load a contiguous block of the input in the width dimension for a fixed (d, h) and then sum them? But note: the window in width might not be contiguous? Actually, the input is stored in [batch, channel, d, h, w] and the stride in w is 1? Not necessarily, because the tensor might be non-contiguous. But we have made it contiguous in the forward.\n\n  The current code in the forward: `x = x.contiguous()`. So the strides are fixed.\n\n  The stride for width (stride_w) is 1? Because it's the innermost dimension? Actually, the tensor is in NCDHW format. So the strides are:\n      stride_batch = C * D * H * W\n      stride_channel = D * H * W\n      stride_d = H * W\n      stride_h = W\n      stride_w = 1\n\n  Therefore, for a fixed (batch, channel, d, h), the elements in the width dimension are contiguous.\n\n  So for a fixed (kd, kh) we can load a contiguous block of `kernel_size` elements in the width dimension? But note: the kernel_size might be arbitrary? We can only use vectorized loads if we use a power-of-two block? We can round up to the next power of two and then mask.\n\n  However, the constraint: \"NEVER remove boundary checks or out-of-bound masks\". So we must mask.\n\n  We can do:\n      base_ptr = ... (for the start of the row in width)\n      w_offsets = tl.arange(0, BLOCK_W)   # BLOCK_W = next_power_of_two(kernel_size)\n      mask = w_offsets < kernel_size\n      # But also we must consider the boundaries of the image: if the window goes beyond the image, we need to adjust.\n\n  Actually, we already have a mask for the boundaries. We can combine the two masks.\n\n  How about we change the inner loop over kw to a vectorized load? We can do:\n\n      for kd in range(kernel_size):\n          id = start_d + kd\n          mask_d = (id >= 0) & (id < D)\n          for kh in range(kernel_size):\n              ih = start_h + kh\n              mask_h = (ih >= 0) & (ih < H)\n              # Now for the width: we want to load a block of kernel_size elements\n              # But note: the width dimension is contiguous, so we can load with one instruction.\n              # However, we must check the boundaries for each element in the width block.\n\n              # Create an array of width indices for the entire kernel window in width?\n              w_offsets = start_w + tl.arange(0, kernel_size)\n              # But wait: kernel_size might not be a power of two -> error.\n\n  This is the same error as in Attempt 3.\n\n  How to fix? We can set the block size to the next power of two of kernel_size? Then we mask the extra elements.\n\n  Example: if kernel_size=3, then we set BLOCK_W = 4 (next power of two). Then we do:\n\n      w_offsets = start_w + tl.arange(0, BLOCK_W)\n      # Then we create a mask for the width: within the kernel window and within the image.\n      mask_w = (w_offsets >= 0) & (w_offsets < W) & (tl.arange(0, BLOCK_W) < kernel_size)\n\n  Then we load with:\n\n      ptr = base_ptr + w_offsets * stride_w   # which is just w_offsets because stride_w=1?\n      vals = tl.load(ptr, mask=mask_w, other=0.0)\n\n  Then we sum the vals.\n\n  But note: the base_ptr for the row is: \n        base_ptr = x_ptr + pid_batch * stride_batch + pid_channel * stride_channel + id * stride_d + ih * stride_h\n\n  Then we can load a vector of BLOCK_W elements.\n\n  This would change the inner loop from kernel_size iterations to one vector load. And we do the same for the entire row? Then we can remove the loop over kw.\n\n  So the kernel would become:\n\n      for kd in range(kernel_size):\n          id = start_d + kd\n          mask_d = (id >= 0) & (id < D)\n          for kh in range(kernel_size):\n              ih = start_h + kh\n              mask_h = (ih >= 0) & (ih < H)\n              row_ptr = base_ptr + id * stride_d + ih * stride_h   # base_ptr = x_ptr + pid_batch * stride_batch + pid_channel * stride_channel\n              w_offsets = start_w + tl.arange(0, BLOCK_W)   # BLOCK_W = next_power_of_two(kernel_size)\n              mask_w = (w_offsets >= 0) & (w_offsets < W) & (tl.arange(0, BLOCK_W) < kernel_size)\n              mask = mask_d & mask_h & mask_w   # This is a vector mask?\n              # Actually, mask_d and mask_h are scalars, but mask_w is a vector. We need to broadcast the scalars to a vector?\n              # Alternatively, we can do: mask_total = mask_w and then if mask_d and mask_h are false, set the entire mask to false?\n              # We can do: mask_total = mask_w and then if not mask_d or not mask_h, then set mask_total to all false.\n\n              # How to broadcast? We can create a vector of the scalar mask?\n              # Let's do: mask_total = mask_w & (mask_d & mask_h)\n\n              # Because mask_d and mask_h are the same for all w in the block.\n              mask_total = mask_w\n              if not mask_d or not mask_h:\n                  mask_total = tl.zeros((BLOCK_W,), dtype=tl.int1)   # or we can set mask_total to all false? But we can do: mask_total = mask_w & (mask_d & mask_h)\n\n              # Actually, we can do: mask_total = mask_w & (mask_d & mask_h)\n              # But note: mask_d and mask_h are scalars, and mask_w is a vector. Triton will broadcast the scalars to the vector.\n\n              vals = tl.load(row_ptr + w_offsets, mask=mask_total, other=0.0)\n              total += tl.sum(vals)\n\n  But note: we are summing the entire vector? Then we are adding the values for the entire width window at once.\n\n  This reduces the inner loop from kernel_size to a single load (for a vector of BLOCK_W elements). Then we do two nested loops (depth and height) and one vector load for width.\n\n  The total number of loads becomes kernel_size * kernel_size * (ceil(kernel_size / BLOCK_W)?) but we are doing one load per (kd, kh). So it becomes kernel_size^2 loads per output element.\n\n  For kernel_size=3, that's 9 loads instead of 27.\n\n  How about we do the same for the height dimension? We cannot because the height dimension is not contiguous? The next dimension is depth, then batch, then channel? Actually, the tensor is in NCDHW, so the dimensions are:\n      batch, channel, depth, height, width.\n\n  The stride for height is W (which is the width size). So the elements in the same row (height) are contiguous? Actually, for a fixed (batch, channel, depth, height), the row of width is contiguous. But the next row (height+1) is separated by W.\n\n  Therefore, we cannot load a block in height and width together? Because they are not contiguous.\n\n  However, we can try to load multiple rows of the same depth? But that would be non-contiguous.\n\n  So we stick to vectorizing the width dimension.\n\n  Steps for the kernel:\n      - Precompute BLOCK_W = next_power_of_two(kernel_size)\n      - For kd in range(kernel_size):\n            id = start_d + kd\n            mask_d = (id >= 0) & (id < D)\n            for kh in range(kernel_size):\n                ih = start_h + kh\n                mask_h = (ih >= 0) & (ih < H)\n                base_ptr = ... + id * stride_d + ih * stride_h   # this is the base for the row at (id, ih)\n                w_offsets = start_w + tl.arange(0, BLOCK_W)\n                # Create mask for the width: within the window (less than kernel_size) and within image bounds.\n                mask_w = (w_offsets >= 0) & (w_offsets < W) & (tl.arange(0, BLOCK_W) < kernel_size)\n                mask_total = mask_w & mask_d & mask_h   # scalar mask_d and mask_h are broadcasted to vector\n\n                # Load the vector of BLOCK_W elements\n                vals = tl.load(base_ptr + w_offsets, mask=mask_total, other=0.0)\n                total += tl.sum(vals)\n\n      - Then set the output.\n\n  But note: the divisor is the kernel_size^3? Actually, the divisor might be the number of valid elements? But the current program uses a fixed divisor (kernel_size^3). However, the reference might use the number of valid elements? \n\n  The current program uses:\n        divisor = float(kernel_size ** 3)\n\n  And then in the kernel: avg = total / divisor.\n\n  But the reference PyTorch average pooling uses the number of valid elements? Actually, the PyTorch documentation says: \n      The divisor used in the average is the number of elements in the pooling window, including zeros.\n\n  However, the current program uses a fixed divisor. This might be incorrect if the window is at the boundary? \n\n  But note: the current program does use masking and sets the out-of-bound values to 0.0. Then the divisor should be kernel_size^3? Because the zeros are included? \n\n  Actually, the PyTorch AvgPool3d has an option `count_include_pad`. The default is True? \n\n  In our case, the padding is done by the kernel by extending the input with zeros? And the divisor is the entire kernel volume? That matches the current fixed divisor.\n\n  So we are safe.\n\n  However, if we change to use the vectorized load, we are still summing the same values? And we are using the same divisor.\n\n  But note: the vectorized load might include values that are out-of-bound? We set them to 0.0 with the mask, so it's the same.\n\n  Therefore, we can change the inner loop to vectorize the width.\n\n  We must set BLOCK_W to the next power of two of kernel_size.\n\n  How to compute the next power of two? We can pass it as a kernel parameter? Or compute it in the kernel? But note: kernel_size is a constant at compile time? Actually, it is passed as a tl.constexpr.\n\n  We can compute: BLOCK_W = 1 << (kernel_size - 1).bit_length()   # but this is in Python, and we need to pass it as a constant.\n\n  Alternatively, we can pass it as an argument? We are allowed to add new constants? The constraint says: \"NEVER change @triton.jit function signatures or parameter names\". So we cannot change the signature? \n\n  We can add a new constant? Actually, we can add a new tl.constexpr parameter? But the constraint says we cannot change the signature. So we cannot.\n\n  Therefore, we must compute the next power of two in the Python code and pass it as an existing parameter? \n\n  Looking at the current parameters, we have:\n        batch_size, channels, D, H, W,\n        out_D, out_H, out_W,\n        stride_batch, stride_channel, stride_d, stride_h, stride_w,\n        stride_val: tl.constexpr,\n        padding: tl.constexpr,\n        kernel_size: tl.constexpr,\n        divisor: tl.constexpr\n\n  We don't have an extra parameter. But note: we have the divisor, which is a float. We cannot use that for an integer block size.\n\n  Alternatively, we can use the existing P2? In the example of the top program, they used P2 for the channel block. But in our current program we don't have P2.\n\n  Actually, the current program does not have a BLOCK_SIZE for channels. We are processing one channel at a time.\n\n  We can repurpose the divisor? That would be wrong.\n\n  How about we remove the divisor from the kernel and compute it outside? Then we can use that argument slot for BLOCK_W? But the divisor is a float and we need an integer. Also, the constraint says we cannot change the signature.\n\n  Alternatively, we can compute the next power of two in the kernel? But Triton doesn't have bit_length? \n\n  We can do: \n        BLOCK_W = 1\n        while BLOCK_W < kernel_size:\n            BLOCK_W *= 2\n\n  But note: kernel_size is a constant at compile time? Then the loop will be unrolled? And the value of BLOCK_W will be known at compile time? \n\n  However, Triton might not support while loops in the kernel? Actually, it does for constant expressions? But let's try.\n\n  Alternatively, we can use a lookup table? Since kernel_size is usually small (like 2,3,5,7), we can do:\n\n        if kernel_size <= 2:\n            BLOCK_W = 2\n        elif kernel_size <= 4:\n            BLOCK_W = 4\n        elif kernel_size <= 8:\n            BLOCK_W = 8\n        else:\n            BLOCK_W = 16\n\n  But we cannot have conditionals on tl.constexpr? Actually, we can because they are constants at compile time.\n\n  We can do:\n\n        BLOCK_W = 1\n        if kernel_size > 1:\n            BLOCK_W = 2\n        if kernel_size > 2:\n            BLOCK_W = 4\n        if kernel_size > 4:\n            BLOCK_W = 8\n        if kernel_size > 8:\n            BLOCK_W = 16\n\n  This is acceptable.\n\n  But note: the constraint says \"NEVER remove boundary checks\". We are already doing boundary checks.\n\n  Let me write the kernel with this change.\n\n  We'll compute BLOCK_W in the kernel and then use it.\n\n  Steps:\n      - Remove the inner loop over kw, and replace it with a vectorized load of size BLOCK_W (the next power of two for kernel_size).\n\n  However, note: the window in width might not start at 0? We are only loading BLOCK_W elements, but we only want the first kernel_size elements? We are masking the rest with `tl.arange(0, BLOCK_W) < kernel_size`.\n\n  So it's safe.\n\n  Let's rewrite the kernel accordingly.\n\n  We must be cautious: the vectorized load might be slower if kernel_size is very small? But for kernel_size=3, we are loading 4 elements and then masking one. This is still better than 3 separate loads.\n\n  Also, note: the load instruction for a vector is one memory transaction? It depends on the cache line. But it should be more efficient.\n\n  We'll change the kernel as described.\n\n  We also note that the current program does not have the BLOCK_W parameter. We are going to compute it inside the kernel.\n\n  We must be cautious: the vectorized load might require the base pointer to be aligned? Triton handles unaligned loads.\n\n  Now, let's write the code.\n\n  We are going to replace the inner loop over kw with a vectorized load.\n\n  We'll also remove the loop variable kw.\n\n  The new kernel code for the inner part:\n\n      total = 0.0\n      base_ptr = x_ptr + pid_batch * stride_batch + pid_channel * stride_channel\n\n      # Compute BLOCK_W: the next power of two >= kernel_size\n      BLOCK_W = 1\n      if kernel_size > 1:\n          BLOCK_W = 2\n      if kernel_size > 2:\n          BLOCK_W = 4\n      if kernel_size > 4:\n          BLOCK_W = 8\n      if kernel_size > 8:\n          BLOCK_W = 16\n      if kernel_size > 16:\n          BLOCK_W = 32\n\n      for kd in range(kernel_size):\n          id = start_d + kd\n          mask_d = (id >= 0) & (id < D)\n          for kh in range(kernel_size):\n              ih = start_h + kh\n              mask_h = (ih >= 0) & (ih < H)\n              row_ptr = base_ptr + id * stride_d + ih * stride_h\n\n              w_offsets = start_w + tl.arange(0, BLOCK_W)\n              # Create mask for the width: within the window (w_offsets < start_w + kernel_size) and within image [0, W)\n              mask_w = (w_offsets >= 0) & (w_offsets < W) & (w_offsets < start_w + kernel_size)\n              mask_total = mask_w & mask_d & mask_h   # broadcast mask_d and mask_h to the vector\n\n              # Load the vector\n              vals = tl.load(row_ptr + w_offsets, mask=mask_total, other=0.0)\n              total += tl.sum(vals)\n\n      avg = total / divisor\n\n  But note: we are summing the entire vector? That includes only the kernel_size elements? Because the mask sets the extra elements to 0? Actually, we set the other to 0.0, and the mask_total will be false for indices beyond kernel_size? \n\n  However, note: the condition `w_offsets < start_w + kernel_size` might not be exactly the window? The window is [start_w, start_w + kernel_size - 1]. So we should do:\n\n        mask_w = (w_offsets >= start_w) & (w_offsets < start_w + kernel_size) & (w_offsets < W) & (w_offsets >= 0)\n\n  Actually, we already defined:\n        w_offsets = start_w + tl.arange(0, BLOCK_W)\n\n  So the condition `w_offsets >= 0` and `w_offsets < W` are the image boundaries.\n\n  And the condition `w_offsets < start_w + kernel_size` is equivalent to `tl.arange(0, BLOCK_W) < kernel_size`? Because w_offsets = start_w + tl.arange(0, BLOCK_W), so:\n\n        w_offsets < start_w + kernel_size   =>   tl.arange(0, BLOCK_W) < kernel_size\n\n  So we can do:\n\n        mask_w = (w_offsets >= 0) & (w_offsets < W) & (tl.arange(0, BLOCK_W) < kernel_size)\n\n  This is the same as before.\n\n  But note: the condition `w_offsets >= 0` and `w_offsets < W` is the same as the original condition for each element? The original condition was:\n\n        iw = start_w + kw\n        mask_w = (iw >= 0) & (iw < W)\n\n  Now we are doing the same for a vector of iw.\n\n  Therefore, the new kernel should be equivalent.\n\n  We'll change the kernel accordingly.\n\n  We must also note: the original kernel had a loop over kw in [0, kernel_size). Now we are loading a vector of size BLOCK_W, and we are masking so that only the first kernel_size elements are considered.\n\n  Let's update the kernel.\n\n  We'll remove the innermost loop and replace it with the vectorized load.\n\n  We are not changing the grid, so we are safe.\n\n  We hope this will reduce the number of loads and improve performance.\n\n  We'll also note: the kernel must be compiled. We are using tl.arange with a constant BLOCK_W, which is a power of two, so it should work.\n\n  Let's write the code.\n\n  We are only changing the inner part of the kernel. We are not changing the function signature.\n\n  We are going to replace the current inner loop (over kw) with the vectorized version.\n\n  We must remove the loop over kw and the code inside it, and replace with the vectorized code.\n\n  We'll also initialize base_ptr outside the loops to avoid recomputation.\n\n  We are going to change the current kernel from:\n\n        total = 0.0\n\n        for kd in range(kernel_size):\n            id = start_d + kd\n            for kh in range(kernel_size):\n                ih = start_h + kh\n                for kw in range(kernel_size):\n                    iw = start_w + kw\n                    # Check boundaries using masks\n                    mask_d = (id >= 0) & (id < D)\n                    mask_h = (ih >= 0) & (ih < H)\n                    mask_w = (iw >= 0) & (iw < W)\n                    mask = mask_d & mask_h & mask_w\n                    \n                    offset = pid_batch * stride_batch + pid_channel * stride_channel + id * stride_d + ih * stride_h + iw * stride_w\n                    val = tl.load(x_ptr + offset, mask=mask, other=0.0)\n                    total += val\n\n  to:\n\n        total = 0.0\n        base_ptr = x_ptr + pid_batch * stride_batch + pid_channel * stride_channel\n\n        # Compute BLOCK_W: the next power of two >= kernel_size\n        BLOCK_W = 1\n        if kernel_size > 1:\n            BLOCK_W = 2\n        if kernel_size > 2:\n            BLOCK_W = 4\n        if kernel_size > 4:\n            BLOCK_W = 8\n        if kernel_size > 8:\n            BLOCK_W = 16\n        if kernel_size > 16:\n            BLOCK_W = 32\n\n        for kd in range(kernel_size):\n            id = start_d + kd\n            mask_d = (id >= 0) & (id < D)\n            for kh in range(kernel_size):\n                ih = start_h + kh\n                mask_h = (ih >= 0) & (ih < H)\n                row_ptr = base_ptr + id * stride_d + ih * stride_h\n\n                w_offsets = start_w + tl.arange(0, BLOCK_W)\n                mask_w = (w_offsets >= 0) & (w_offsets < W) & (tl.arange(0, BLOCK_W) < kernel_size)\n                mask_total = mask_w & mask_d & mask_h   # This is a vector mask\n\n                # Load the vector of BLOCK_W elements\n                vals = tl.load(row_ptr + w_offsets, mask=mask_total, other=0.0)\n                total += tl.sum(vals)\n\n  Then the rest is the same.\n\n  We hope this will speed up the kernel.\n\n  Let's update the code accordingly.\n\n  Note: we are not changing anything else in the program.\n\n  We are going to output the entire EVOLVE-BLOCK with this change.\n\n  IMPORTANT: We must not change the grid or the function signature.\n\n  We are only changing the kernel body.\n\n  We'll write the new kernel.\n\n  Note: the kernel must be named the same and the parameters must be the same.\n\n  We are going to replace the current kernel with the new one.\n\n  We are also keeping the same class ModelNew and the same forward.\n\n  We'll output the entire block.\n</think>\nTo optimize the Triton kernel for 3D average pooling, we focus on vectorizing the innermost width dimension operations. The current implementation processes each element individually within the kernel window, which is inefficient. By loading contiguous elements in the width dimension using vectorized operations, we reduce the number of memory accesses and leverage GPU capabilities more effectively.\n\nKey improvements made:\n1. **Vectorized width dimension processing**: Instead of iterating over each width element, we load a block of contiguous elements using `tl.arange` and process them in a single operation\n2. **Efficient masking**: Combine boundary checks for width with kernel size constraints using a single mask vector\n3. **Reduced memory accesses**: For kernel_size=3, width operations are reduced from 27 to 9 memory accesses per output element\n4. **Power-of-two handling**: Dynamically determine the appropriate block size for vectorized operations\n\nHere's the optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef avg_pool_3d_kernel(\n    x_ptr,\n    output_ptr,\n    batch_size, channels, D, H, W,\n    out_D, out_H, out_W,\n    stride_batch, stride_channel, stride_d, stride_h, stride_w,\n    stride_val: tl.constexpr,\n    padding: tl.constexpr,\n    kernel_size: tl.constexpr,\n    divisor: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    pid_output_index = tl.program_id(2)\n\n    if pid_batch >= batch_size or pid_channel >= channels:\n        return\n\n    out_HW = out_H * out_W\n    d_idx = pid_output_index // out_HW\n    hw_idx = pid_output_index % out_HW\n    h_idx = hw_idx // out_W\n    w_idx = hw_idx % out_W\n\n    start_d = d_idx * stride_val - padding\n    start_h = h_idx * stride_val - padding\n    start_w = w_idx * stride_val - padding\n\n    total = 0.0\n    base_ptr = x_ptr + pid_batch * stride_batch + pid_channel * stride_channel\n\n    # Determine vectorization block size (next power of two)\n    BLOCK_W = 1\n    if kernel_size > 1:\n        BLOCK_W = 2\n    if kernel_size > 2:\n        BLOCK_W = 4\n    if kernel_size > 4:\n        BLOCK_W = 8\n    if kernel_size > 8:\n        BLOCK_W = 16\n    if kernel_size > 16:\n        BLOCK_W = 32\n\n    for kd in range(kernel_size):\n        id = start_d + kd\n        mask_d = (id >= 0) & (id < D)\n        for kh in range(kernel_size):\n            ih = start_h + kh\n            mask_h = (ih >= 0) & (ih < H)\n            row_ptr = base_ptr + id * stride_d + ih * stride_h\n            \n            # Vectorized width processing\n            w_offsets = tl.arange(0, BLOCK_W)\n            w_indices = start_w + w_offsets\n            mask_w = (w_indices >= 0) & (w_indices < W) & (w_offsets < kernel_size)\n            mask_total = mask_w & mask_d & mask_h\n            \n            vals = tl.load(row_ptr + w_indices, mask=mask_total, other=0.0)\n            total += tl.sum(vals)\n\n    avg = total / divisor\n    \n    output_offset = pid_batch * channels * out_D * out_H * out_W + \\\n                    pid_channel * out_D * out_H * out_W + \\\n                    d_idx * out_H * out_W + \\\n                    h_idx * out_W + \\\n                    w_idx\n    tl.store(output_ptr + output_offset, avg)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        if stride is None:\n            stride = kernel_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.divisor = float(kernel_size ** 3)  # Precompute fixed divisor\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, D, H, W = x.shape\n        out_D = (D + 2 * self.padding - self.kernel_size) // self.stride + 1\n        out_H = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n        out_W = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty((batch_size, channels, out_D, out_H, out_W), \n                             device=x.device, dtype=x.dtype)\n        \n        if x.numel() == 0:\n            return output\n            \n        x = x.contiguous()  # Ensure contiguous memory\n        stride_batch, stride_channel, stride_d, stride_h, stride_w = x.stride()\n        grid = (batch_size, channels, out_D * out_H * out_W)\n        \n        avg_pool_3d_kernel[grid](\n            x, output,\n            batch_size, channels, D, H, W,\n            out_D, out_H, out_W,\n            stride_batch, stride_channel, stride_d, stride_h, stride_w,\n            self.stride,\n            self.padding,\n            self.kernel_size,\n            self.divisor,\n        )\n        \n        return output\n\nbatch_size = 16\nchannels = 32\ndepth = 64\nheight = 64\nwidth = 64\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
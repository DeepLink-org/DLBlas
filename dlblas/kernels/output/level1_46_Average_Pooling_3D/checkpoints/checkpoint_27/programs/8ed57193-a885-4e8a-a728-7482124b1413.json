{"id": "8ed57193-a885-4e8a-a728-7482124b1413", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=4),\n        triton.Config({}, num_stages=4, num_warps=2),\n        triton.Config({}, num_stages=1, num_warps=1),\n    ],\n    key=[],\n)\n@triton.jit\ndef _avg_pool3d_forward(\n    x_ptr, \n    output_ptr,\n    B, C, D, H, W,\n    stride_b, stride_c, stride_d, stride_h, stride_w,\n    output_D, output_H, output_W,\n    divisor,\n    KERNEL_SIZE: tl.constexpr,\n    STRIDE: tl.constexpr,\n    PADDING: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    pid_spatial = tl.program_id(2)\n    \n    # Reconstruct spatial indices\n    hw_size = output_H * output_W\n    d_idx = pid_spatial // hw_size\n    hw_idx = pid_spatial % hw_size\n    h_idx = hw_idx // output_W\n    w_idx = hw_idx % output_W\n    \n    # Compute window start positions\n    start_d = d_idx * STRIDE - PADDING\n    start_h = h_idx * STRIDE - PADDING\n    start_w = w_idx * STRIDE - PADDING\n    \n    # Accumulate values\n    total = 0.0\n    for kd in tl.static_range(KERNEL_SIZE):\n        id = start_d + kd\n        for kh in tl.static_range(KERNEL_SIZE):\n            ih = start_h + kh\n            \n            # Vectorized W dimension processing\n            kw_offsets = tl.arange(0, KERNEL_SIZE)\n            iw = start_w + kw_offsets\n            \n            # Compute masks\n            within_bounds_dh = (id >= 0) & (id < D) & (ih >= 0) & (ih < H)\n            within_bounds_w = (iw >= 0) & (iw < W)\n            within_bounds = within_bounds_dh & within_bounds_w\n            \n            # Vectorized load\n            base_offset = pid_b * stride_b + pid_c * stride_c + id * stride_d + ih * stride_h\n            w_offsets = base_offset + iw * stride_w\n            vals = tl.load(x_ptr + w_offsets, mask=within_bounds, other=0.0)\n            total += tl.sum(vals)\n\n    # Compute average and store result\n    out_offset = pid_b * (C * output_D * output_H * output_W) + \\\n                 pid_c * (output_D * output_H * output_W) + \\\n                 d_idx * (output_H * output_W) + \\\n                 h_idx * output_W + w_idx\n    tl.store(output_ptr + out_offset, total / divisor)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        D, H, W = x.shape[2:]\n        k = self.kernel_size\n        s = self.stride\n        p = self.padding\n        \n        # Compute output dimensions\n        output_D = (D + 2 * p - k) // s + 1\n        output_H = (H + 2 * p - k) // s + 1\n        output_W = (W + 2 * p - k) // s + 1\n        \n        output = torch.empty((x.shape[0], x.shape[1], output_D, output_H, output_W), \n                             device=x.device, dtype=x.dtype)\n        \n        # Ensure contiguous memory\n        x = x.contiguous()\n        \n        # Get tensor strides\n        stride_b, stride_c, stride_d, stride_h, stride_w = x.stride()\n        divisor = float(k ** 3)\n        \n        # Total spatial elements per channel\n        total_spatial = output_D * output_H * output_W\n        \n        # Configure grid (batch, channels, spatial)\n        grid = (x.shape[0], x.shape[1], total_spatial)\n        \n        # Launch optimized kernel\n        _avg_pool3d_forward[grid](\n            x, output,\n            x.shape[0], x.shape[1], D, H, W,\n            stride_b, stride_c, stride_d, stride_h, stride_w,\n            output_D, output_H, output_W,\n            divisor,\n            KERNEL_SIZE=k,\n            STRIDE=s,\n            PADDING=p,\n            BLOCK_SIZE=1\n        )\n        return output\n\nbatch_size = 16\nchannels = 32\ndepth = 64\nheight = 64\nwidth = 64\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "01e559c1-a1fa-4c2c-a648-e027fee79854", "generation": 2, "timestamp": 1754640247.337412, "iteration_found": 9, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_46_Average_Pooling_3D", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\n    raise ValueError(\"arange's range must be a power of 2\")\nValueError: arange's range must be a power of 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpvu6thnl6.py\", line 110, in forward\n    _avg_pool3d_forward[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 160, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 146, in kernel_call\n    self.fn.run(\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 37:25:\n    start_h = h_idx * STRIDE - PADDING\n    start_w = w_idx * STRIDE - PADDING\n\n    # Accumulate values\n    total = 0.0\n    for kd in tl.static_range(KERNEL_SIZE):\n        id = start_d + kd\n        for kh in tl.static_range(KERNEL_SIZE):\n            ih = start_h + kh\n\n            # Vectorized W dimension processing\n            kw_offsets = tl.arange(0, KERNEL_SIZE)\n                         ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\n    raise ValueError(\"arange's range must be a power of 2\")\nValueError: arange's range must be a power of 2\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0237, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 0.0000\n- correctness: 0.0000\n- score: -100.0000\n- stage: no_valid_triton_code\n- feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Areas identified for improvement: - Focus on optimizing the code for better performance on the target metrics\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 0.0000, correctness: 0.0000, score: -100.0000, stage: no_valid_triton_code, feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.0010, runtime_ms: 26.0000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0237, 'speed_up': 0.001, 'custom_timing': 26.0}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool3d_forward(\n    x_ptr, \n    output_ptr,\n    # Tensor dimensions\n    B, C, D, H, W,\n    stride_b, stride_c, stride_d, stride_h, stride_w,\n    output_D, output_H, output_W,\n    divisor,\n    # Constant expressions\n    KERNEL_SIZE: tl.constexpr,\n    STRIDE: tl.constexpr,\n    PADDING: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    pid_spatial = tl.program_id(2)\n    \n    # Reconstruct spatial indices\n    hw_size = output_H * output_W\n    d_idx = pid_spatial // hw_size\n    hw_idx = pid_spatial % hw_size\n    h_idx = hw_idx // output_W\n    w_idx = hw_idx % output_W\n    \n    # Compute window start positions\n    start_d = d_idx * STRIDE - PADDING\n    start_h = h_idx * STRIDE - PADDING\n    start_w = w_idx * STRIDE - PADDING\n    \n    # Accumulate values\n    total = 0.0\n    for kd in tl.static_range(KERNEL_SIZE):\n        for kh in tl.static_range(KERNEL_SIZE):\n            for kw in tl.static_range(KERNEL_SIZE):\n                id = start_d + kd\n                ih = start_h + kh\n                iw = start_w + kw\n                \n                # Check boundaries\n                within_bounds = (id >= 0) & (id < D) & (ih >= 0) & (ih < H) & (iw >= 0) & (iw < W)\n                offset = pid_b * stride_b + pid_c * stride_c + id * stride_d + ih * stride_h + iw * stride_w\n                val = tl.load(x_ptr + offset, mask=within_bounds, other=0.0)\n                total += val\n    \n    # Compute average and store result\n    out_offset = pid_b * (C * output_D * output_H * output_W) + pid_c * (output_D * output_H * output_W) + d_idx * (output_H * output_W) + h_idx * output_W + w_idx\n    tl.store(output_ptr + out_offset, total / divisor)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        D, H, W = x.shape[2:]\n        k = self.kernel_size\n        s = self.stride\n        p = self.padding\n        \n        # Compute output dimensions\n        output_D = (D + 2 * p - k) // s + 1\n        output_H = (H + 2 * p - k) // s + 1\n        output_W = (W + 2 * p - k) // s + 1\n        \n        output = torch.empty((x.shape[0], x.shape[1], output_D, output_H, output_W), \n                             device=x.device, dtype=x.dtype)\n        \n        # Ensure contiguous memory\n        x = x.contiguous()\n        \n        # Get tensor strides\n        stride_b, stride_c, stride_d, stride_h, stride_w = x.stride()\n        divisor = float(k ** 3)  # Precompute divisor\n        \n        # Total spatial elements per channel\n        total_spatial = output_D * output_H * output_W\n        \n        # Configure grid (batch, channels, spatial)\n        grid = (x.shape[0], x.shape[1], total_spatial)\n        \n        # Launch optimized kernel\n        _avg_pool3d_forward[grid](\n            x, output,\n            x.shape[0], x.shape[1], D, H, W,\n            stride_b, stride_c, stride_d, stride_h, stride_w,\n            output_D, output_H, output_W,\n            divisor,\n            KERNEL_SIZE=k,\n            STRIDE=s,\n            PADDING=p,\n            BLOCK_SIZE=1\n        )\n        return output\n\nbatch_size = 16\nchannels = 32\ndepth = 64\nheight = 64\nwidth = 64\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.0010), Performs well on runtime_ms (26.0000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0237, 'speed_up': 0.001, 'custom_timing': 26.0})\n\n\n### Program 2 (Score: -100.0000)\n```python\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.avg_pool = nn.AvgPool3d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.avg_pool(x)\n```\nKey features: Performs well on compiled (0.0000), Performs well on correctness (0.0000), Performs well on score (-100.0000), Performs well on stage (no_valid_triton_code), Performs well on feedback (You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool3d_forward(\n    x_ptr, \n    output_ptr,\n    # Tensor dimensions\n    B, C, D, H, W,\n    stride_b, stride_c, stride_d, stride_h, stride_w,\n    output_D, output_H, output_W,\n    divisor,\n    # Constant expressions\n    KERNEL_SIZE: tl.constexpr,\n    STRIDE: tl.constexpr,\n    PADDING: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    pid_spatial = tl.program_id(2)\n    \n    # Reconstruct spatial indices\n    hw_size = output_H * output_W\n    d_idx = pid_spatial // hw_size\n    hw_idx = pid_spatial % hw_size\n    h_idx = hw_idx // output_W\n    w_idx = hw_idx % output_W\n    \n    # Compute window start positions\n    start_d = d_idx * STRIDE - PADDING\n    start_h = h_idx * STRIDE - PADDING\n    start_w = w_idx * STRIDE - PADDING\n    \n    # Accumulate values\n    total = 0.0\n    for kd in tl.static_range(KERNEL_SIZE):\n        for kh in tl.static_range(KERNEL_SIZE):\n            for kw in tl.static_range(KERNEL_SIZE):\n                id = start_d + kd\n                ih = start_h + kh\n                iw = start_w + kw\n                \n                # Check boundaries\n                within_bounds = (id >= 0) & (id < D) & (ih >= 0) & (ih < H) & (iw >= 0) & (iw < W)\n                offset = pid_b * stride_b + pid_c * stride_c + id * stride_d + ih * stride_h + iw * stride_w\n                val = tl.load(x_ptr + offset, mask=within_bounds, other=0.0)\n                total += val\n    \n    # Compute average and store result\n    out_offset = pid_b * (C * output_D * output_H * output_W) + pid_c * (output_D * output_H * output_W) + d_idx * (output_H * output_W) + h_idx * output_W + w_idx\n    tl.store(output_ptr + out_offset, total / divisor)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        D, H, W = x.shape[2:]\n        k = self.kernel_size\n        s = self.stride\n        p = self.padding\n        \n        # Compute output dimensions\n        output_D = (D + 2 * p - k) // s + 1\n        output_H = (H + 2 * p - k) // s + 1\n        output_W = (W + 2 * p - k) // s + 1\n        \n        output = torch.empty((x.shape[0], x.shape[1], output_D, output_H, output_W), \n                             device=x.device, dtype=x.dtype)\n        \n        # Ensure contiguous memory\n        x = x.contiguous()\n        \n        # Get tensor strides\n        stride_b, stride_c, stride_d, stride_h, stride_w = x.stride()\n        divisor = float(k ** 3)  # Precompute divisor\n        \n        # Total spatial elements per channel\n        total_spatial = output_D * output_H * output_W\n        \n        # Configure grid (batch, channels, spatial)\n        grid = (x.shape[0], x.shape[1], total_spatial)\n        \n        # Launch optimized kernel\n        _avg_pool3d_forward[grid](\n            x, output,\n            x.shape[0], x.shape[1], D, H, W,\n            stride_b, stride_c, stride_d, stride_h, stride_w,\n            output_D, output_H, output_W,\n            divisor,\n            KERNEL_SIZE=k,\n            STRIDE=s,\n            PADDING=p,\n            BLOCK_SIZE=1\n        )\n        return output\n\nbatch_size = 16\nchannels = 32\ndepth = 64\nheight = 64\nwidth = 64\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.avg_pool = nn.AvgPool3d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.avg_pool(x)\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a task to rewrite a Triton kernel for 3D average pooling.\n The current program is using a naive PyTorch `nn.AvgPool3d` and we are to replace it with a Triton implementation.\n\n We have an example of a Triton implementation from the top-performing program (Program 1) which we can use as a base.\n\n However, note that the top-performing program (Program 1) has a score of 20.0000, but the speed_up is 0.001 (which is less than 1) meaning it is slower than the reference.\n The feedback says: \"Performs well on compiled (1.0000), Performs well on correctness (1.0000)\" but the speed_up is 0.001 (so 1000x slower?).\n\n We must improve the performance.\n\n The current Triton kernel in Program 1 has the following structure:\n   - It uses a grid of (B, C, total_spatial) where total_spatial = output_D * output_H * output_W.\n   - Each thread block (program) handles one element in the output (for a given batch and channel and spatial location).\n   - Then it loops over the kernel in 3 dimensions (KERNEL_SIZE^3 iterations).\n\n The problem: The current kernel is doing a lot of global memory loads and each thread is doing a lot of work (KERNEL_SIZE^3 loads).\n Also, note that the kernel is not using any shared memory or tiling. It is a naive implementation.\n\n How can we improve?\n 1. We can try to use tiling: load a block of the input into shared memory and then compute multiple output elements.\n 2. However, note that 3D pooling is challenging because the kernel is 3D and the input is 5D (B, C, D, H, W).\n\n Alternatively, we can consider:\n   - Vectorizing the loads and using tensor cores? But average pooling is not a matrix multiplication.\n\n Another idea: Instead of having each thread compute one output, we can have each thread block compute a block of the output (e.g., a small 3D block of the output). Then we can load the necessary input region into shared memory and then compute the average for multiple output points.\n\n Steps for an optimized kernel:\n\n   a. We can have the grid dimensions as:\n        grid = (triton.cdiv(B, BLOCK_SIZE_B), \n                C,\n                triton.cdiv(output_D, BLOCK_SIZE_D) * triton.cdiv(output_H, BLOCK_SIZE_H) * triton.cdiv(output_W, BLOCK_SIZE_W))\n\n   b. Each block will compute a block of the output of size [BLOCK_SIZE_B, 1, BLOCK_SIZE_D, BLOCK_SIZE_H, BLOCK_SIZE_W] (for one channel).\n\n   c. We load the required input region (which is larger by the kernel size) into shared memory.\n\n   d. Then we compute the average for each point in the output block.\n\n However, note the constraints:\n   - We cannot change the function signature arbitrarily? Actually, the problem says: \"NEVER change @triton.jit function signatures or parameter names\". But note that the previous kernel had a specific signature. We are allowed to change the kernel implementation arbitrarily as long as we keep the same function name and parameters? Actually, the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" and \"NEVER change @triton.jit function signatures or parameter names\".\n\n   But wait: the top-performing program (Program 1) has the following kernel parameters:\n\n        x_ptr, \n        output_ptr,\n        B, C, D, H, W,\n        stride_b, stride_c, stride_d, stride_h, stride_w,\n        output_D, output_H, output_W,\n        divisor,\n        KERNEL_SIZE: tl.constexpr,\n        STRIDE: tl.constexpr,\n        PADDING: tl.constexpr,\n        BLOCK_SIZE: tl.constexpr\n\n   We are allowed to change the kernel implementation, but we must keep the same parameters and their names.\n\n   Therefore, we cannot change the grid configuration? Actually, the grid configuration is set by the host code. The host code in the ModelNew class sets the grid as (B, C, total_spatial). And the kernel uses program_id(0), program_id(1), program_id(2). We cannot change that.\n\n   So we must design a kernel that works with the grid of (B, C, total_spatial). That is, each kernel instance is responsible for one output element.\n\n   How to make it faster?\n\n   The current kernel does:\n      total = 0.0\n      for kd in range(KERNEL_SIZE):\n          for kh in range(KERNEL_SIZE):\n              for kw in range(KERNEL_SIZE):\n                  ... load one element\n\n   That is KERNEL_SIZE^3 global memory loads per output element.\n\n   We can try to vectorize the loads? But the kernel is 3D and the elements are not contiguous in memory.\n\n   Alternatively, we can note that the input might be accessed multiple times by adjacent output elements. But in the current grid, each output element is computed independently, so we cannot share data.\n\n   Therefore, we must reduce the number of global memory loads per thread? That might be difficult.\n\n   Another idea: use hardware-accelerated operations? But average pooling is not directly supported by tensor cores.\n\n   We can try to optimize by:\n     - Precomputing the offsets? (the current kernel computes the offset in the inner loop, which is expensive)\n     - Using vectorized loads? But the kernel size is small (e.g., 3x3x3) so the inner loop is only 27 iterations.\n\n   However, the main issue is that each load is a small (non-coalesced) access.\n\n   How about: we change the order of the loops? But that won't help much.\n\n   Alternatively, we can use tiling at the thread level? But note that the kernel size is small.\n\n   Let's consider the memory access pattern:\n\n     For a given output element at (b, c, d, h, w), we are accessing the input at:\n\n        for kd: d0 = d * stride - padding + kd\n        for kh: h0 = h * stride - padding + kh\n        for kw: w0 = w * stride - padding + kw\n\n        and then we load x[b, c, d0, h0, w0]\n\n     The accesses in the inner loop (over kw) are contiguous in the W dimension? Yes, if the stride in W is 1, then consecutive kw will access consecutive memory locations.\n\n     But note: the input tensor is stored in contiguous memory? We made it contiguous in the host code.\n\n     So we can try to vectorize the inner kw loop? We can load multiple kw at once.\n\n     However, the kernel size in W might not be a power of two? But we are using static_range and the kernel size is a constant expression.\n\n     We can try to use a vectorized load for the entire row in the W dimension? But the kernel size in W is usually small (e.g., 3). So we can load 3 elements at once.\n\n   Steps:\n\n     We can change the inner two loops (kh and kw) to load a 2D tile of the input.\n\n     For example, for a fixed kd and kh, we can load all the kw elements in one vectorized load? But the kernel size in W might be 3, so we can do:\n\n        kw_start = 0\n        kw_end = KERNEL_SIZE\n\n        We can use tl.load with a mask for the entire vector of size KERNEL_SIZE? But note that the addresses might not be contiguous? Actually, they are contiguous in memory because the stride in W is 1 (so consecutive kw are consecutive in memory). So we can do:\n\n        base_offset = ... + kd * stride_d + kh * stride_h + (start_w + kw_start) * stride_w\n        vector = tl.load(x_ptr + base_offset + tl.arange(0, KERNEL_SIZE) * stride_w, mask=within_bounds_vector, other=0.0)\n\n        But note: the mask must be a vector of booleans for each element.\n\n     However, the kernel size is a constant expression, so we can do:\n\n        kw_offsets = tl.arange(0, KERNEL_SIZE)\n        w_indices = start_w + kw_offsets\n        within_bounds_w = (w_indices >= 0) & (w_indices < W)\n        base_offset = ... + kd * stride_d + kh * stride_h\n        w_offsets = base_offset + w_indices * stride_w\n        vector = tl.load(x_ptr + w_offsets, mask=within_bounds_w, other=0.0)\n\n        Then we can do: total += tl.sum(vector, axis=0)\n\n     Then we can remove the inner loop over kw.\n\n     Similarly, we might try to vectorize over kh? But note that the H dimension is not contiguous? The next element in H is stride_h apart, which might be 1? But if the tensor is contiguous, then the stride_h is H*W? Actually, the stride for the H dimension is the stride_h we passed in (which is the product of the strides of the dimensions beyond H). In a contiguous tensor, the strides are [C*D*H*W, D*H*W, H*W, W, 1]. So the stride for H is W (if we are in the same depth and channel). Therefore, consecutive elements in H are separated by W elements. So they are not contiguous. So we cannot vectorize over H.\n\n     So we can only vectorize the innermost dimension (W). That would reduce the inner loop from KERNEL_SIZE to 1 (for the W dimension) and then we have two loops: kd and kh.\n\n     The total number of loads per output element would be KERNEL_SIZE * KERNEL_SIZE (for a 3x3x3 kernel, that would be 9 loads per output element, down from 27). That is a 3x improvement.\n\n   But note: the kernel size in W might be 1? Then we cannot vectorize. But typically kernel_size is 3.\n\n   Also, we must check boundaries: we are loading KERNEL_SIZE elements at a time. We have to check each element individually for boundaries.\n\n   Let's rewrite the inner loop for kw as a vectorized load.\n\n   We must note: the vectorized load in Triton is done by providing an offset vector and a mask vector.\n\n   Steps in the kernel:\n\n        start_d = ... ; start_h = ... ; start_w = ... \n\n        total = 0.0\n        for kd in tl.static_range(KERNEL_SIZE):\n            id = start_d + kd\n            for kh in tl.static_range(KERNEL_SIZE):\n                ih = start_h + kh\n                # Now for kw: we create a vector of kw indices\n                kw_offsets = tl.arange(0, KERNEL_SIZE)\n                iw = start_w + kw_offsets\n                within_bounds = (id >= 0) & (id < D) & (ih >= 0) & (ih < H) & (iw >= 0) & (iw < W)\n                # The base offset for this (b, c, id, ih, start_w) without kw\n                base_offset = pid_b * stride_b + pid_c * stride_c + id * stride_d + ih * stride_h\n                # The w offsets are base_offset + iw * stride_w (but note: stride_w is 1 for contiguous? Actually, the stride_w we passed in is the stride for the W dimension. In a contiguous tensor, it is 1. But we use the provided stride_w to be safe.)\n                w_offsets = base_offset + iw * stride_w\n                # Load a vector of KERNEL_SIZE elements\n                vals = tl.load(x_ptr + w_offsets, mask=within_bounds, other=0.0)\n                total += tl.sum(vals, axis=0)\n\n        # Then we divide by divisor\n\n   But note: the mask is a vector of length KERNEL_SIZE. We are loading KERNEL_SIZE elements at once.\n\n   However, the problem: the inner loop over kh and kd is still there. We have reduced the inner loop from KERNEL_SIZE to 1 (by vectorizing the innermost dimension). So the total number of loads is now KERNEL_SIZE * KERNEL_SIZE.\n\n   For kernel_size=3, that is 9 loads per output element (instead of 27). This should be about 3x faster.\n\n   But note: the vectorized load might not be perfectly coalesced? We are loading 3 consecutive elements. That is a 12-byte load (if float32) or 6-byte (if float16). It might be better to load 4 elements? But we cannot because the kernel size is 3.\n\n   Alternatively, we can try to load more elements by vectorizing over both kh and kw? But they are not contiguous.\n\n   We must also consider that the vectorized load might be more efficient if the load is aligned? But we are not guaranteed alignment.\n\n   Let's try this optimization.\n\n   We must also note: the kernel size is a constant expression (KERNEL_SIZE) so we can use static_range and the vectorization factor is known at compile time.\n\n   How about we also try to unroll the loops? The loops are small (3x3) so we can unroll them. Triton might do that automatically because of `tl.static_range`.\n\n   We'll change the inner loop to be vectorized.\n\n   Also, note: the original kernel had a variable named `row_mask`? Actually, we are using `within_bounds` for each point. We are now computing a vector of booleans for the W dimension.\n\n   We'll update the kernel accordingly.\n\n   We'll also change the name of the mask variable to avoid confusion.\n\n   Let's rewrite the kernel with vectorization on the W dimension.\n\n   Important: We must keep the same function signature.\n\n   We also note that the kernel currently uses a parameter `BLOCK_SIZE` but it is set to 1 in the call. We are not using it. We can remove it? But we cannot change the signature. So we leave it.\n\n   We'll change the inner loop to load a vector of size KERNEL_SIZE.\n\n   We also note: the divisor is precomputed as k**3.\n\n   We must test for correctness.\n\n   We'll write the kernel as:\n\n        @triton.jit\n        def _avg_pool3d_forward(...):\n            ... same as before until the loops ...\n\n            total = 0.0\n            for kd in tl.static_range(KERNEL_SIZE):\n                id = start_d + kd\n                for kh in tl.static_range(KERNEL_SIZE):\n                    ih = start_h + kh\n                    # Vector for kw\n                    kw_offsets = tl.arange(0, KERNEL_SIZE)\n                    iw = start_w + kw_offsets\n                    # Check boundaries: we need to check for each element in the vector\n                    # For the D and H dimensions, we have the same condition for all in the vector? \n                    # But for W, each element has its own condition.\n                    within_bounds_dh = (id >= 0) & (id < D) & (ih >= 0) & (ih < H)\n                    # For W, we check each element in the vector\n                    within_bounds_w = (iw >= 0) & (iw < W)\n                    within_bounds = within_bounds_dh & within_bounds_w   # This is a vector of booleans\n\n                    base_offset = pid_b * stride_b + pid_c * stride_c + id * stride_d + ih * stride_h\n                    w_offsets = base_offset + iw * stride_w\n                    vals = tl.load(x_ptr + w_offsets, mask=within_bounds, other=0.0)\n                    total += tl.sum(vals)\n\n            ... then store ...\n\n   We are summing the entire vector.\n\n   This should be faster because we are doing fewer loads.\n\n   Let's update the kernel accordingly.\n\n   Also, note: the original kernel had a loop over kw and then added the value to total. We are now doing the same by summing the vector and adding.\n\n   We must also note: the original kernel had a mask for each load. We are now providing a mask for the vector.\n\n   We'll update the kernel.\n\n   We leave the rest of the code (the ModelNew class) unchanged because it sets the grid and the parameters correctly.\n\n   We hope that this change will improve the speed.\n\n   We also note: the kernel might benefit from tuning the number of warps? But the top-performing program had autotuning? Actually, it did not. We can add autotuning? But the problem says we cannot change the function signature. However, we can add autotuning configs? The example in the quantize kernel had autotuning. We can try to add autotuning to this kernel.\n\n   But note: the kernel parameters include BLOCK_SIZE which is currently set to 1 and not used. We might use it to control the tiling? But we are not tiling. Alternatively, we can use it to control the vectorization? But the vectorization is fixed by the kernel size.\n\n   Since the kernel is now doing vector loads, we might want to adjust the number of threads per block? But the grid is set to (B, C, total_spatial) and each program is one output element. So we cannot change the block structure.\n\n   Alternatively, we can change the kernel to compute multiple output elements? But that would require changing the grid and the kernel signature? Which we cannot do.\n\n   Therefore, we stick to the current grid and try to optimize by vectorizing the W dimension.\n\n   We'll add autotuning? Actually, the kernel does not have any tunable parameters at the moment. But note: we are not using the BLOCK_SIZE. We might remove it? But we cannot change the signature.\n\n   Alternatively, we can ignore it.\n\n   Let's write the kernel with the vectorization.\n\n   We'll also note: the kernel size might be 1? Then we don't need to vectorize. But the code will still work.\n\n   We'll try.\n\n   We must also note: the kernel might be called with different kernel sizes? We are using KERNEL_SIZE as a constant expression. So the loop and the vectorization factor are fixed at compile time.\n\n   This is acceptable.\n\n   We'll now write the new kernel.\n\n   We also note: the original kernel had a variable named `row_mask`? We are changing the inner loop so we don't use that variable anymore.\n\n   We'll update the kernel accordingly.\n\n   We'll also change the name of the kernel function? No, we must keep the same function name.\n\n   So the plan:\n\n        We replace the inner three loops with two loops (kd and kh) and then vectorize the kw loop.\n\n   We'll write the code accordingly.\n\n   We also note: the original kernel had an inner loop that did:\n\n        within_bounds = (id >= 0) & (id < D) & (ih >= 0) & (ih < H) & (iw >= 0) & (iw < W)\n\n   We are splitting this: the D and H conditions are the same for the entire vector in W, but the W condition is per element.\n\n   So we compute:\n\n        within_bounds_dh = (id >= 0) & (id < D) & (ih >= 0) & (ih < H)   # scalar\n        within_bounds_w = (iw >= 0) & (iw < W)   # vector of length KERNEL_SIZE\n\n        Then the mask for the vector load is: within_bounds_dh and within_bounds_w? Actually, we must broadcast the scalar condition to the vector? We can do:\n\n        within_bounds = within_bounds_dh & within_bounds_w\n\n   Because Triton will broadcast the scalar.\n\n   We'll do that.\n\n   Let's code accordingly.\n\n   We'll also note: the kernel must be compiled for different kernel sizes? The host code passes KERNEL_SIZE as a constant expression. So we must generate a separate kernel for each kernel size? That is acceptable.\n\n   We'll now write the kernel.\n\n   We also note: the original kernel had a variable `arange` and `row_mask`? We are not using that anymore.\n\n   We remove the unused variables.\n\n   We also note: the kernel uses `tl.where(row_mask, abs_x, 0)`? No, that was in the quantize example. Our kernel does not have that.\n\n   We are safe.\n\n   Let's write the kernel.\n\n   We also note: the original kernel had a parameter `divisor` which is the kernel volume (k**3). We use that to compute the average.\n\n   We leave that unchanged.\n\n   We'll now output the entire code block.\n\n   IMPORTANT: We must include the necessary imports (triton and tl) and the ModelNew class.\n\n   We are going to output the entire EVOLVE-BLOCK.\n\n   We base our code on the top-performing program (Program 1) and change the kernel inner loop.\n\n   We also note: the top-performing program had a BLOCK_SIZE=1 in the kernel launch. We leave that.\n\n   We hope this will improve the speed.\n\n   If the kernel size is 1, then the vectorization factor is 1. That is still efficient.\n\n   We are also going to add autotuning? Actually, the kernel does not have tunable parameters. But we can try to set num_warps and num_stages? We can add an autotune config.\n\n   We note that the top-performing program did not have autotuning. We can try to add it.\n\n   We'll add:\n\n        @triton.autotune(\n            configs=[\n                triton.Config({}, num_stages=1, num_warps=8),\n                triton.Config({}, num_stages=2, num_warps=4),\n                triton.Config({}, num_stages=4, num_warps=2),\n                triton.Config({}, num_stages=1, num_warps=1),\n            ],\n            key=[],\n        )\n\n   But note: the kernel does not have any tunable parameters. We are only tuning the launch configuration (number of warps and stages).\n\n   We'll add that.\n\n   However, the problem says: we cannot change the function signature? But adding autotune is a decorator and does not change the function signature.\n\n   So we can do.\n\n   We'll add autotune.\n\n   We'll also note: the grid is very large (B * C * total_spatial) so we want many threads.\n\n   We'll try a few configurations.\n\n   We'll write the code accordingly.\n\n   Let's code.\n\n   IMPORTANT: We must not change the host code (the ModelNew class) except for the kernel launch? Actually, we are not changing it.\n\n   We are only changing the kernel and adding autotune.\n\n   We'll now write the entire EVOLVE-BLOCK.\n\n   Note: The problem says to return only the code within the EVOLVE-BLOCK markers.\n\n   We are going to output:\n\n        # ================== EVOLVE-BLOCK-START ==================\n        ... our code ...\n        # =================== EVOLVE-BLOCK-END ===================\n\n   We base the ModelNew class on the one from the top-performing program.\n\n   We are going to replace the kernel and add autotune.\n\n   Let's do it.\n</think>\nTo address the performance issues and ensure the Triton kernel is valid and efficient, I'll implement a vectorized approach for 3D average pooling. The key optimizations include:\n1. Vectorizing innermost dimension (W) loads to reduce memory operations\n2. Adding autotuning for optimal warp/stage configuration\n3. Optimizing boundary checks with vector masks\n4. Maintaining strict signature/parameter constraints\n\nHere's the complete solution:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=4),\n        triton.Config({}, num_stages=4, num_warps=2),\n        triton.Config({}, num_stages=1, num_warps=1),\n    ],\n    key=[],\n)\n@triton.jit\ndef _avg_pool3d_forward(\n    x_ptr, \n    output_ptr,\n    B, C, D, H, W,\n    stride_b, stride_c, stride_d, stride_h, stride_w,\n    output_D, output_H, output_W,\n    divisor,\n    KERNEL_SIZE: tl.constexpr,\n    STRIDE: tl.constexpr,\n    PADDING: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    pid_spatial = tl.program_id(2)\n    \n    # Reconstruct spatial indices\n    hw_size = output_H * output_W\n    d_idx = pid_spatial // hw_size\n    hw_idx = pid_spatial % hw_size\n    h_idx = hw_idx // output_W\n    w_idx = hw_idx % output_W\n    \n    # Compute window start positions\n    start_d = d_idx * STRIDE - PADDING\n    start_h = h_idx * STRIDE - PADDING\n    start_w = w_idx * STRIDE - PADDING\n    \n    # Accumulate values\n    total = 0.0\n    for kd in tl.static_range(KERNEL_SIZE):\n        id = start_d + kd\n        for kh in tl.static_range(KERNEL_SIZE):\n            ih = start_h + kh\n            \n            # Vectorized W dimension processing\n            kw_offsets = tl.arange(0, KERNEL_SIZE)\n            iw = start_w + kw_offsets\n            \n            # Compute masks\n            within_bounds_dh = (id >= 0) & (id < D) & (ih >= 0) & (ih < H)\n            within_bounds_w = (iw >= 0) & (iw < W)\n            within_bounds = within_bounds_dh & within_bounds_w\n            \n            # Vectorized load\n            base_offset = pid_b * stride_b + pid_c * stride_c + id * stride_d + ih * stride_h\n            w_offsets = base_offset + iw * stride_w\n            vals = tl.load(x_ptr + w_offsets, mask=within_bounds, other=0.0)\n            total += tl.sum(vals)\n\n    # Compute average and store result\n    out_offset = pid_b * (C * output_D * output_H * output_W) + \\\n                 pid_c * (output_D * output_H * output_W) + \\\n                 d_idx * (output_H * output_W) + \\\n                 h_idx * output_W + w_idx\n    tl.store(output_ptr + out_offset, total / divisor)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        D, H, W = x.shape[2:]\n        k = self.kernel_size\n        s = self.stride\n        p = self.padding\n        \n        # Compute output dimensions\n        output_D = (D + 2 * p - k) // s + 1\n        output_H = (H + 2 * p - k) // s + 1\n        output_W = (W + 2 * p - k) // s + 1\n        \n        output = torch.empty((x.shape[0], x.shape[1], output_D, output_H, output_W), \n                             device=x.device, dtype=x.dtype)\n        \n        # Ensure contiguous memory\n        x = x.contiguous()\n        \n        # Get tensor strides\n        stride_b, stride_c, stride_d, stride_h, stride_w = x.stride()\n        divisor = float(k ** 3)\n        \n        # Total spatial elements per channel\n        total_spatial = output_D * output_H * output_W\n        \n        # Configure grid (batch, channels, spatial)\n        grid = (x.shape[0], x.shape[1], total_spatial)\n        \n        # Launch optimized kernel\n        _avg_pool3d_forward[grid](\n            x, output,\n            x.shape[0], x.shape[1], D, H, W,\n            stride_b, stride_c, stride_d, stride_h, stride_w,\n            output_D, output_H, output_W,\n            divisor,\n            KERNEL_SIZE=k,\n            STRIDE=s,\n            PADDING=p,\n            BLOCK_SIZE=1\n        )\n        return output\n\nbatch_size = 16\nchannels = 32\ndepth = 64\nheight = 64\nwidth = 64\nkernel_size = 3\nstride = 2\npadding = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size, stride, padding]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
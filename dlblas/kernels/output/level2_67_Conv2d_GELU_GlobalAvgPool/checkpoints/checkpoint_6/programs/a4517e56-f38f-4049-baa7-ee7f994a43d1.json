{"id": "a4517e56-f38f-4049-baa7-ee7f994a43d1", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    height,\n    width,\n    num_elements,\n    output_batch_stride,\n    output_channel_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    tid = tl.arange(0, BLOCK_SIZE)\n    \n    base_ptr = input_ptr + pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    accum = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Parallel accumulation\n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tid\n        mask = offsets < num_elements\n        h = offsets // width\n        w = offsets % width\n        ptrs = base_ptr + h * input_height_stride + w * input_width_stride\n        data = tl.load(ptrs, mask=mask, other=0.0)\n        accum += data\n    \n    # Shared memory for reduction\n    shmem = tl.static_alloc(BLOCK_SIZE, tl.float32)\n    shmem[tid] = accum\n    tl.barrier()\n    \n    # Tree reduction\n    s = BLOCK_SIZE // 2\n    while s > 0:\n        if tid < s:\n            shmem[tid] += shmem[tid + s]\n        tl.barrier()\n        s //= 2\n    \n    # Store result\n    avg = shmem[0] / num_elements\n    if tid == 0:\n        output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n        tl.store(output_ptr + output_offset, avg)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.gelu(x)\n        \n        batch, channels, h, w = x.shape\n        num_elements = h * w\n        output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n        \n        batch_stride, channel_stride, height_stride, width_stride = x.stride()\n        output_batch_stride, output_channel_stride = output.stride()\n        \n        grid = (batch, channels)\n        global_avg_pool2d_kernel[grid](\n            x, output,\n            batch_stride, channel_stride, height_stride, width_stride,\n            h, w, num_elements,\n            output_batch_stride, output_channel_stride,\n            BLOCK_SIZE=256,\n            num_warps=8\n        )\n        \n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "00ef567e-a0ae-4d9e-ba33-3865a4a33420", "generation": 2, "timestamp": 1754582540.978801, "iteration_found": 4, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_67_Conv2d_GELU_GlobalAvgPool", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpegfmhroo.py\", line 75, in forward\n    global_avg_pool2d_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'static_alloc'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 0.0655, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.769, "runtime_ms": 0.0852, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0655, "speed_up": 0.769, "custom_timing": 0.0852}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.7690\n- runtime_ms: 0.0852\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0655, 'speed_up': 0.769, 'custom_timing': 0.0852}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0581, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0655, 'speed_up': 1.127, 'custom_timing': 0.0581}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7690, runtime_ms: 0.0852, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0655, 'speed_up': 0.769, 'custom_timing': 0.0852}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    height,\n    width,\n    num_elements,\n    output_batch_stride,\n    output_channel_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_ptr = input_ptr + pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    total = 0.0\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < num_elements\n        h = offsets // width\n        w = offsets % width\n        ptrs = base_ptr + h * input_height_stride + w * input_width_stride\n        data = tl.load(ptrs, mask=mask, other=0.0)\n        total += tl.sum(data, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.gelu(x)\n        \n        batch, channels, h, w = x.shape\n        num_elements = h * w\n        output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n        \n        batch_stride, channel_stride, height_stride, width_stride = x.stride()\n        output_batch_stride, output_channel_stride = output.stride()\n        \n        grid = (batch, channels)\n        global_avg_pool2d_kernel[grid](\n            x, output,\n            batch_stride, channel_stride, height_stride, width_stride,\n            h, w, num_elements,\n            output_batch_stride, output_channel_stride,\n            BLOCK_SIZE=1024\n        )\n        \n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7690), Performs well on runtime_ms (0.0852), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0655, 'speed_up': 0.769, 'custom_timing': 0.0852})\n\n\n### Program 2 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies GELU, and then performs global average pooling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, height, width)\n        Returns:\n            Output tensor of shape (batch_size, out_channels)\n        \"\"\"\n        x = self.conv(x)\n        x = torch.nn.functional.gelu(x)\n        x = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n        x = x.squeeze(-1).squeeze(-1)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0581), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0655, 'speed_up': 1.127, 'custom_timing': 0.0581})\n\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    height,\n    width,\n    num_elements,\n    output_batch_stride,\n    output_channel_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_ptr = input_ptr + pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    total = 0.0\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < num_elements\n        h = offsets // width\n        w = offsets % width\n        ptrs = base_ptr + h * input_height_stride + w * input_width_stride\n        data = tl.load(ptrs, mask=mask, other=0.0)\n        total += tl.sum(data, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.gelu(x)\n        \n        batch, channels, h, w = x.shape\n        num_elements = h * w\n        output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n        \n        batch_stride, channel_stride, height_stride, width_stride = x.stride()\n        output_batch_stride, output_channel_stride = output.stride()\n        \n        grid = (batch, channels)\n        global_avg_pool2d_kernel[grid](\n            x, output,\n            batch_stride, channel_stride, height_stride, width_stride,\n            h, w, num_elements,\n            output_batch_stride, output_channel_stride,\n            BLOCK_SIZE=1024\n        )\n        \n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a Triton kernel for global average pooling that is currently slower than the reference (0.769 speedup, meaning it's 23% slower).\n The current implementation uses a loop over the 2D spatial dimensions (h, w) by flattening them and processing in blocks of 1024.\n We note that the spatial size is 32x32 = 1024, so the entire tensor for one channel and batch is processed in one block (since BLOCK_SIZE=1024).\n However, the kernel is launched with grid=(batch, channels) and each program iterates over the entire spatial dimension.\n\n The problem is that the current kernel is doing a reduction over the spatial dimension sequentially within a single thread block.\n This is inefficient because we are not utilizing the parallel reduction capabilities of the GPU.\n\n We can improve by:\n  1. Using multiple threads per channel to do the reduction in parallel.\n  2. Using a two-stage reduction: first within a thread block, then across blocks (if necessary). But note that 1024 elements per channel is small.\n\n However, note that the grid is (batch, channels) and we have only one program per channel per batch. We can change the kernel to use more threads per channel.\n\n Proposed approach:\n  - We change the kernel so that each channel is reduced by a 1D block of threads (with block size being a power of two, say 256).\n  - We use shared memory to do a reduction within the block, then let the first thread in the block write the result.\n\n Steps in the kernel:\n  - pid_batch = program_id(0), pid_channel = program_id(1), and we have a 1D grid of blocks for the reduction? Actually, we can keep the same grid but change the inner loop.\n\n Alternatively, we can restructure the kernel to have:\n  - grid = (batch, channels, num_blocks) where num_blocks = ceil(num_elements / BLOCK_SIZE)\n  - Then we do a two-stage reduction: first each block computes a partial sum, then we combine the partial sums.\n\n However, note that the output is of shape (batch, channels). We cannot have multiple blocks per channel because we need to combine the results.\n\n So we do:\n  Stage 1: Each block (for a given batch and channel) computes a partial sum of a chunk of the spatial dimension.\n  Stage 2: We launch a second kernel to combine the partial sums per channel.\n\n But that would require two kernels and we are not allowed to change the interface.\n\n Alternatively, we can do the entire reduction in one kernel by having one block per channel and using parallel reduction within the block.\n\n Given that the spatial size is 1024, we can set the block size to 1024 and do a fast reduction in shared memory.\n\n However, 1024 threads per block is acceptable on modern GPUs.\n\n Revised plan:\n  - We change the kernel to use a 1D block of threads for each channel. The grid becomes (batch, channels) and the block size is the number of threads per block (say 256 or 1024).\n  - We set the block size (BLOCK_SIZE) to the number of threads per block, and then we process multiple elements per thread if the spatial size is larger than the block size.\n\n But note: our spatial size is 1024. We can set the block size to 1024 and then each thread processes one element.\n\n Steps in the kernel:\n  - Each thread in the block loads one element (if within bounds).\n  - Then we do a reduction in shared memory.\n\n However, the kernel currently is written for a 1D grid of programs (each program is a single thread). We are changing to a 1D block of threads per program.\n\n We must be cautious because the kernel is called with a grid of (batch, channels). We will now use a 2D grid: (batch, channels) and then a 1D block of threads.\n\n But note: the Triton kernel does not have a concept of threads within a block in the same way as CUDA. In Triton, the block is the unit of execution and the program is written for a block.\n\n Actually, in Triton, we use `tl.program_id(axis)` and `tl.num_programs(axis)` and within a block we have a range of indices (using `tl.arange`). We can use a 3D grid: (batch, channels, num_blocks) and then within the block, we reduce a chunk of the spatial dimension.\n\n However, we are constrained by the grid configuration: we cannot change the grid to have a third dimension because the output is (batch, channels). We must compute one output per (batch, channel) and we cannot have multiple blocks writing to the same output.\n\n Therefore, we stick to one block per (batch, channel). We can then use a parallel reduction within the block.\n\n How to do:\n  - We set the block size (in the kernel launch) to be a power of two, say 1024 (if the spatial size is 1024). But note: the spatial size might vary? In the example it's 32x32=1024.\n\n  We can compute the required block size as the next power of two of the spatial dimension? Actually, we can set the block size to the spatial dimension if it's <= 1024 (max block size). But if it's larger, we would have to use multiple blocks and then a second reduction? We are not allowed to change the grid to have an extra dimension.\n\n  Alternatively, we can have the kernel work for any spatial size by having each thread in the block process multiple elements.\n\n  Let's design the kernel for a fixed block size (BLOCK_SIZE) and then each block will reduce a contiguous chunk of the flattened spatial dimension.\n\n  Steps:\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n\n    base_ptr = ... (as before)\n\n    # We are going to have BLOCK_SIZE threads per block. We'll process the entire spatial dimension by having each thread handle multiple elements.\n\n    total = 0.0\n    idx = tl.arange(0, BLOCK_SIZE)   # thread index in the block\n    # We'll iterate over the spatial dimension in chunks of BLOCK_SIZE\n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + idx\n        mask = offsets < num_elements\n        h = offsets // width\n        w = offsets % width\n        ptrs = base_ptr + h * input_height_stride + w * input_width_stride\n        data = tl.load(ptrs, mask=mask, other=0.0)\n        total += tl.sum(data, axis=0)   # This sums the entire chunk? Actually, we want to do a reduction across the entire spatial dimension.\n\n    But note: the above loop already does a sequential reduction per thread? Then we have a scalar per thread? Then we need to do a parallel reduction across the threads in the block.\n\n    Instead, we should do:\n        total_chunk = tl.sum(data, axis=0)   # This is a scalar per thread? Actually, no: the `tl.sum` reduces the entire vector of data for the thread? That doesn't make sense because each thread has one element? \n\n    Actually, in Triton, the `tl.sum` reduces over the entire vector of data for the thread. But in this case, each thread is loading one element? Because `idx` is of size BLOCK_SIZE, and each thread has one index? Then `data` is a scalar per thread? Then we cannot use `tl.sum` on a scalar.\n\n    Correction: the code above for the loop is actually the same as the original but now we are using a block of threads? \n\n    We must restructure:\n\n    We want each thread to accumulate a partial sum for a subset of the spatial dimension.\n\n    Let the block have BLOCK_SIZE threads. Then the entire spatial dimension (num_elements) is divided into chunks of BLOCK_SIZE, and each thread in the block will process multiple chunks? Actually, we can have each thread process one element per chunk? Then we do:\n\n        total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        for i in range(0, num_elements, BLOCK_SIZE):\n            offsets = i + idx\n            mask = offsets < num_elements\n            h = offsets // width\n            w = offsets % width\n            ptrs = base_ptr + h * input_height_stride + w * input_width_stride\n            data = tl.load(ptrs, mask=mask, other=0.0)\n            total += data   # accumulate per thread\n\n        # Now we have a partial sum per thread (each thread has the sum of the elements it loaded)\n        # Then we do a parallel reduction across the block to get the total sum.\n\n        total_sum = tl.sum(total, axis=0)   # This sums over the entire block? Actually, we need to do a tree reduction.\n\n    But note: the `tl.sum` operation is not available for a block-level reduction? We have to write a tree reduction.\n\n    Alternatively, we can use `tl.reduce` or `tl.associative_scan`? Actually, Triton provides `tl.reduce` for reductions.\n\n    However, we are not allowed to change the grid to have a third dimension? We are only using two dimensions for batch and channel.\n\n    How to launch the kernel with blocks of threads? In Triton, the kernel launch uses a grid and the block is defined by the `BLOCK_SIZE` and the range of the first axis? Actually, the kernel is launched with a grid and each grid point runs a block of threads? The block size is set by the `num_warps` and the `BLOCK_SIZE` in the configuration? Actually, the `BLOCK_SIZE` in the kernel is a constant and the number of threads per block is fixed by the `range` we use? \n\n    We must change the kernel to use a block of threads and then do a reduction within the block.\n\n    We can do:\n\n        pid = tl.program_id(0)   # batch index\n        pid_channel = tl.program_id(1)   # channel index\n        base_ptr = ... \n        idx = tl.arange(0, BLOCK_SIZE)   # thread index in the block: from 0 to BLOCK_SIZE-1\n        total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)   # one accumulator per thread\n\n        for i in range(0, num_elements, BLOCK_SIZE):\n            offsets = i + idx\n            mask = offsets < num_elements\n            h = offsets // width\n            w = offsets % width\n            ptrs = base_ptr + h * input_height_stride + w * input_width_stride\n            data = tl.load(ptrs, mask=mask, other=0.0)\n            total += data   # each thread accumulates its own elements\n\n        # Now we do a reduction across the block to sum all the accumulators\n        # We use a tree reduction in shared memory? But Triton doesn't have shared memory? Actually, it does: we can use `tl.static` to allocate shared memory.\n\n        # Alternatively, we can use the `tl.reduce` operation with the `BLOCK_SIZE` as the reduction axis.\n\n        # We want to compute the sum of the entire block of `total` (which is a vector of BLOCK_SIZE elements).\n        total_sum = tl.sum(total, axis=0)   # This reduces the entire vector? But note: each thread holds one element of `total`. We need to do a block reduction.\n\n        # Actually, the `tl.sum` over the entire vector of `total` (which is of length BLOCK_SIZE) will give the total sum for the entire spatial dimension? But wait: the `total` vector is the partial sums per thread? Then we are summing the partial sums -> that gives the total.\n\n        # Then we can compute the average: \n        avg = total_sum / num_elements\n\n        # But note: we must store the result only once per channel. We can do that by having one thread write the result? Actually, we can have thread 0 write.\n\n        # However, we must be cautious: if the block size is 1024, then all threads are active and we don't want to write 1024 times. We can use:\n        if tl.program_id(2) == 0:   # but we don't have a third dimension? We only have two program ids.\n\n        Alternatively, we can use the fact that we have only one block per (batch, channel). So we can let the first thread in the block write.\n\n        We can do:\n            output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n            tl.store(output_ptr + output_offset, avg, mask=idx==0)\n\n        But note: we are using a 1D block of threads, so we have a 1D index `idx`. We can do:\n            if idx == 0:\n                tl.store(...)\n\n    However, the problem is that the kernel currently does not have a third program axis for the reduction. We are using two program axes (batch and channel). The block of threads is a third dimension? In Triton, the kernel is written for a block and the block is defined by the `tl.arange` and the `BLOCK_SIZE` constant. We don't have a program_id for the thread within the block? Actually, we do: the `idx` is the thread index within the block.\n\n    But note: the kernel launch in Triton is done with the grid and the block is implicit? Actually, the kernel is launched with a grid of blocks, and each block has a fixed size (the `BLOCK_SIZE`). The `tl.arange(0, BLOCK_SIZE)` gives the thread indices in the block.\n\n    Therefore, we can restructure the kernel as:\n\n        @triton.jit\n        def kernel(...):\n            pid_batch = tl.program_id(0)\n            pid_channel = tl.program_id(1)\n            idx = tl.arange(0, BLOCK_SIZE)   # thread index in the block\n\n            ... as above ...\n\n            if idx == 0:\n                tl.store(...)\n\n    But note: the grid now must be set to (batch, channels) and we are using a block of BLOCK_SIZE threads? How do we set the block size in the kernel launch?\n\n    In Triton, the kernel launch uses a grid and the block is defined by the `BLOCK_SIZE` and the number of warps? Actually, the kernel is launched with a grid of blocks, and each block has `BLOCK_SIZE` threads? We must set the block size appropriately.\n\n    We change the kernel launch to:\n\n        grid = (batch, channels)\n        # We need to specify the block size? Actually, in Triton, the block size is set by the configuration? Or we can pass it as an argument to the kernel? \n\n        We are using the `BLOCK_SIZE` as a kernel parameter. We also need to tell the kernel launch how many threads per block? Actually, in Triton, the kernel launch does not explicitly set the block size? The block size is determined by the `BLOCK_SIZE` constant and the `num_warps`? \n\n        We can set the `num_warps` and the `BLOCK_SIZE` in the configuration? But note: the kernel is not autotuned in the current version.\n\n    We can do:\n\n        global_avg_pool2d_kernel[grid](\n            ...,\n            BLOCK_SIZE=1024,   # this is the constant used in the kernel, and also the block size?\n            num_warps=32,   # because 1024/32=32 warps? but max warps per block is 32 (1024 threads) -> 32 warps of 32 threads? Actually, 32 warps * 32 = 1024.\n        )\n\n    However, note that the current kernel launch does not set `num_warps`. We can set it.\n\n    But the problem is: what if the spatial size is not 1024? We can set BLOCK_SIZE to the next power of two of the spatial size? Actually, we are passing `BLOCK_SIZE` as a constant. We are currently passing 1024. We can compute the BLOCK_SIZE as the minimum of 1024 and the next power of two? Actually, we can set it to 1024 and then if the spatial size is less, we mask. But if the spatial size is more, we have to loop.\n\n    We are already looping over the spatial dimension in chunks of BLOCK_SIZE. So the kernel already works for any spatial size.\n\n    Steps:\n\n        total = 0.0   # scalar per thread? Actually, we are going to accumulate in a scalar per thread? Then we do:\n\n        for i in range(0, num_elements, BLOCK_SIZE):\n            offsets = i + idx\n            mask = offsets < num_elements\n            ... load data ...\n            total += tl.sum(data, axis=0)   # but data is a scalar per thread? Actually, no: in the loop, each thread loads one element? Then we can just do:\n\n            total += data   # because data is a scalar for the thread.\n\n        Then we do a block reduction on the scalar `total` per thread.\n\n        total_sum = tl.sum(total, axis=0)   # but `total` is a scalar per thread? How do we form a vector? We don't. Instead, we can use `tl.reshape`? \n\n        Actually, we can do:\n\n            total_all = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n            total_all = total_all + total   # but that doesn't work.\n\n        Alternatively, we can use the fact that each thread has a scalar and we want to reduce them. We can use:\n\n            total_sum = tl.sum(total, axis=0)   # but `total` is a scalar? We need to collect all the thread's totals.\n\n        We can use a shared memory array? We can do:\n\n            # Allocate shared memory\n            shmem = tl.static_alloc(BLOCK_SIZE, tl.float32)\n            shmem[idx] = total\n            tl.barrier()\n            # Then do a tree reduction on shmem\n\n        But Triton has built-in reduction operations? We can use:\n\n            total_sum = tl.sum(total)   # This sums the `total` variable across all threads in the block? \n\n        Actually, the `tl.sum` operation when applied to a scalar and with an axis argument? We can do:\n\n            total_sum = tl.sum(total, axis=None)   # but that would just return the same scalar? \n\n        We need to use `tl.atomic_add`? Or we can use `tl.reduce`? \n\n        The correct way is to use the reduction across the block:\n\n            total_sum = tl.sum(total, axis=0)   # if `total` is a vector of length BLOCK_SIZE? But we have one scalar per thread.\n\n        We can form a vector of the partial sums by doing:\n\n            partials = tl.zeros((BLOCK_SIZE,), dtype=tl.float32) + total   # but that doesn't work.\n\n        Alternatively, we can use:\n\n            total_sum = tl.reduce(total, 0)   # but how?\n\n        Actually, we can use the `tl.reduce` operation with an operator. Let's do:\n\n            total_sum = tl.sum(total)   # but this is for a tensor? We have to collect the values.\n\n        The Triton way: we can use the `tl.associative_scan` for a reduction? But note that the reduction across the block is a built-in: we can do:\n\n            total_sum = tl.sum(tl.reshape(total, [1]), axis=0)   # no, that doesn't help.\n\n        Instead, we can use the `tl.dot` with a vector of ones? \n\n            ones = tl.ones((BLOCK_SIZE,), dtype=tl.float32)\n            total_sum = tl.dot(total, ones)   # but `total` is a scalar? \n\n        We have to collect the partial sums into a vector? \n\n        Alternatively, we can use the `tl.sum` on a tensor that we build? \n\n        We can do:\n\n            # Create a tensor of the partial sums by broadcasting? \n            # Actually, we can use `tl.program_id` to index? \n\n            # We can do a tree reduction manually? \n\n        Given the complexity, and since the block size is a power of two, we can do:\n\n            # Step 1: each thread stores its partial sum in shared memory\n            shmem = tl.static_alloc(BLOCK_SIZE, tl.float32)\n            shmem[idx] = total\n            tl.barrier()\n\n            # Step 2: tree reduction\n            s = BLOCK_SIZE\n            while s > 1:\n                s2 = s // 2\n                if idx < s2:\n                    shmem[idx] += shmem[idx + s2]\n                s = s2\n                tl.barrier()\n\n            total_sum = shmem[0]\n\n        Then we compute the average and store by thread 0.\n\n    However, this is complex and might be slow.\n\n    Alternatively, Triton has a built-in block reduction function? We can use `tl.reduce`? \n\n        total_sum = tl.sum(total)   # but this doesn't work because `total` is not a tensor of the entire block? \n\n    Actually, we can use:\n\n        total_sum = tl.sum(total, axis=0)   # but `total` is a scalar? \n\n    There is a better way: we can use the `tl.atomic_add` to a global memory location? But that would be inefficient.\n\n    I found in the Triton documentation: we can use `tl.sum` on a vector that we build? \n\n    Another idea: we can use the `tl.dot` to do the reduction? \n\n        total_vector = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        total_vector = total   # but we cannot assign a scalar to a vector? \n\n    We can do:\n\n        total_vector = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        total_vector = total_vector + total   # this will broadcast the scalar to the entire vector? Then we are not collecting the other threads' values.\n\n    We need to gather the partial sums from all threads in the block? We can use `tl.reshape` and then `tl.sum`? \n\n    Actually, we can use the `tl.reshape` to form a vector of the partial sums? But the partial sums are in different threads.\n\n    The correct way is to use the `tl.dot` with a vector of ones, but we need to form a vector that contains the partial sums. We can do:\n\n        # Each thread has a scalar `total`. We want to form a vector `partials` of length BLOCK_SIZE that contains the partial sums of all threads.\n        # We can use shared memory to gather? \n        shmem = tl.static_alloc(BLOCK_SIZE, tl.float32)\n        shmem[idx] = total\n        tl.barrier()\n        total_sum = tl.sum(shmem, axis=0)\n\n    This will work because `shmem` is a vector of length BLOCK_SIZE and we can sum it.\n\n    But note: the `tl.sum` operation on a vector is a reduction over the entire vector? Yes.\n\n    So we can do:\n\n        shmem = tl.static_alloc(BLOCK_SIZE, tl.float32)\n        shmem[idx] = total\n        tl.barrier()\n        total_sum = tl.sum(shmem, axis=0)\n\n    Then we compute the average and store by thread 0.\n\n    However, this requires shared memory and a barrier.\n\n    Alternatively, we can use the `tl.reduce` operation? \n\n        total_sum = tl.reduce(shmem, 0, tl.sum)   # but I think `tl.sum` is the same.\n\n    We'll use `tl.sum` for simplicity.\n\n    Steps in the kernel:\n\n        pid_batch = tl.program_id(0)\n        pid_channel = tl.program_id(1)\n        idx = tl.arange(0, BLOCK_SIZE)   # thread index in the block\n\n        base_ptr = input_ptr + pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n        total = 0.0\n        for i in range(0, num_elements, BLOCK_SIZE):\n            offsets = i + idx\n            mask = offsets < num_elements\n            h = offsets // width\n            w = offsets % width\n            ptrs = base_ptr + h * input_height_stride + w * input_width_stride\n            data = tl.load(ptrs, mask=mask, other=0.0)\n            total += data   # each thread accumulates the sum of the elements it loaded\n\n        # Now we want the sum of `total` across all threads in the block.\n        # We use shared memory to gather the partial sums.\n        shmem = tl.static_alloc(BLOCK_SIZE, tl.float32)\n        shmem[idx] = total\n        tl.barrier()\n        total_sum = tl.sum(shmem, axis=0)\n\n        avg = total_sum / num_elements\n\n        # Only thread 0 writes\n        if idx == 0:\n            output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n            tl.store(output_ptr + output_offset, avg)\n\n    We must note: the barrier is necessary to ensure all writes to shmem are done before the reduction.\n\n    We also note: the shared memory allocation is static and of size BLOCK_SIZE.\n\n    We must choose BLOCK_SIZE appropriately. We can set it to 1024 for the example (32x32) and it will work. But what if the spatial size is not 1024? We are already looping. And the block size is fixed by the kernel launch? We can set BLOCK_SIZE to 1024 and then if the spatial size is less, we mask.\n\n    However, the kernel must be general. We are passing `BLOCK_SIZE` as a constant. We can compute the required block size in the host and pass it? We are currently passing `BLOCK_SIZE` from the host as 1024. We can change that to be the minimum of 1024 and the next power of two? Actually, we can leave it as 1024 and it will work for any spatial size.\n\n    We must also note: the number of threads per block is BLOCK_SIZE. We must ensure that BLOCK_SIZE is a power of two for the tree reduction? Not necessary for the `tl.sum` but for the shared memory allocation and indexing it doesn't matter.\n\n    But note: the `tl.sum` might be implemented efficiently regardless.\n\n    Let's change the kernel accordingly.\n\n    We also note: the current kernel does not use shared memory and does a sequential reduction. The new kernel uses parallel loading and then a parallel reduction, which should be faster.\n\n    We must also change the kernel launch to set the number of warps appropriately. For BLOCK_SIZE=1024, we need 1024/32 = 32 warps? The maximum warps per block is 32 (on H100) so we set num_warps=32.\n\n    We can set the kernel launch as:\n\n        global_avg_pool2d_kernel[grid](\n            ...,\n            BLOCK_SIZE=1024,\n            num_warps=32\n        )\n\n    But note: the kernel might be called with a spatial size that is not 1024? We can set BLOCK_SIZE to 1024 regardless. The loop will handle it.\n\n    We can also try to set the block size to a smaller value (like 256) to see if it improves performance? But for now, we set to 1024.\n\n    Let's rewrite the kernel.\n\n    Important: We must not change the function signature. We are adding a new parameter `idx`? Actually, we don't add it as a parameter because it is defined by `tl.arange(0, BLOCK_SIZE)`. But note: the kernel parameters are fixed? We are not changing the parameters, we are changing the internal code.\n\n    We are also adding shared memory and barriers.\n\n    We must test with the given example (32x32).\n\n    We hope this will speed up the kernel.\n\n    Let's write the code accordingly.\n\n    Note: We are not allowed to change the grid configuration. The grid is still (batch, channels). The block size is set by the kernel internally? Actually, in Triton, the block size is set by the `BLOCK_SIZE` and the `num_warps` in the kernel launch? We are setting `num_warps=32` for BLOCK_SIZE=1024.\n\n    We change the kernel and the launch parameters.\n\n    We also note: the original kernel had a parameter `BLOCK_SIZE` that was used for the loop. We are now using it for the block of threads? And also for the loop? Yes.\n\n    We must be cautious: the loop step is still `BLOCK_SIZE` (the same as the thread block size). That is correct.\n\n    We'll try to rewrite.\n\n    Also note: the original kernel had a variable `num_elements` which is the spatial size (h*w). We use that to compute the average.\n\n    We must ensure that the reduction is numerically stable? It's just a sum, so it should be the same.\n\n    We'll now write the kernel.\n\n    However, note that the original kernel was:\n\n        global_avg_pool2d_kernel[grid](\n            ...,\n            BLOCK_SIZE=1024   # for the loop\n        )\n\n    We are now changing the meaning of BLOCK_SIZE to be the thread block size and the loop chunk size.\n\n    We'll change the kernel code accordingly.\n\n    We also note: the original kernel did not have a `num_warps` in the launch. We'll add it.\n\n    We must be cautious: the kernel might be called with a spatial size that is not 1024? We are passing 1024 for the thread block size. But if the spatial size is not divisible by 1024, the loop will handle it.\n\n    We'll set the block size to 1024 and hope that it works.\n\n    If the spatial size is small (like 1x1) then we might have a block size of 1024 threads? That would be inefficient. But in our example it's 1024.\n\n    We can compute the block size as the minimum of 1024 and the next power of two of the spatial size? But we don't know the spatial size at compile time? We are passing it as a constant.\n\n    Alternatively, we can set the block size to 128 or 256? We can try 256 and then loop over the spatial dimension in chunks of 256.\n\n    We'll choose 256 for the block size to be more efficient and to work on any spatial size.\n\n    Let's set the block size to 256 and then the number of warps to 8 (256/32=8 warps).\n\n    So:\n\n        BLOCK_SIZE = 256   # threads per block\n        and we launch with num_warps=8.\n\n    We change the kernel accordingly.\n\n    We'll set the block size in the kernel launch to 256.\n\n    We also note: the loop step is BLOCK_SIZE (256).\n\n    We hope that with 256 threads per block we get better utilization.\n\n    Let's do:\n\n        grid = (batch, channels)\n        # and then in the kernel launch:\n        global_avg_pool2d_kernel[grid](\n            ...,\n            BLOCK_SIZE=256,\n            num_warps=8\n        )\n\n    We'll write the kernel accordingly.\n\n    We'll also change the kernel to use the shared memory reduction.\n\n    We are optimistic that this will be faster.\n\n    We also note: the original kernel was doing a sequential reduction over 1024 elements. The new kernel does a parallel reduction with 256 threads and then a shared memory reduction, which is faster.\n\n    Let's write the code.\n\n    IMPORTANT: We must not change the class name or the function signatures outside the kernel? The kernel is inside the class? Actually, we are only changing the kernel and the launch.\n\n    We'll output the entire code block.\n\n    Note: We are not allowed to change the grid configuration? We are keeping the grid as (batch, channels).\n\n    We'll also note: the kernel might be used for non-contiguous tensors? The strides are passed and used correctly.\n\n    We'll now write the code.\n</think>\nTo improve the performance of the global average pooling operation, I've optimized the Triton kernel by implementing parallel reduction using shared memory. The key enhancements include:\n\n1. Added parallel reduction using shared memory for efficient summation\n2. Optimized memory access patterns for coalesced loads\n3. Introduced a tree reduction algorithm for faster summation\n4. Maintained proper boundary checks and masking\n5. Set an appropriate block size (256) and warp count (8) for H100 architecture\n\nHere's the optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    height,\n    width,\n    num_elements,\n    output_batch_stride,\n    output_channel_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    tid = tl.arange(0, BLOCK_SIZE)\n    \n    base_ptr = input_ptr + pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    accum = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Parallel accumulation\n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tid\n        mask = offsets < num_elements\n        h = offsets // width\n        w = offsets % width\n        ptrs = base_ptr + h * input_height_stride + w * input_width_stride\n        data = tl.load(ptrs, mask=mask, other=0.0)\n        accum += data\n    \n    # Shared memory for reduction\n    shmem = tl.static_alloc(BLOCK_SIZE, tl.float32)\n    shmem[tid] = accum\n    tl.barrier()\n    \n    # Tree reduction\n    s = BLOCK_SIZE // 2\n    while s > 0:\n        if tid < s:\n            shmem[tid] += shmem[tid + s]\n        tl.barrier()\n        s //= 2\n    \n    # Store result\n    avg = shmem[0] / num_elements\n    if tid == 0:\n        output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n        tl.store(output_ptr + output_offset, avg)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.gelu(x)\n        \n        batch, channels, h, w = x.shape\n        num_elements = h * w\n        output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n        \n        batch_stride, channel_stride, height_stride, width_stride = x.stride()\n        output_batch_stride, output_channel_stride = output.stride()\n        \n        grid = (batch, channels)\n        global_avg_pool2d_kernel[grid](\n            x, output,\n            batch_stride, channel_stride, height_stride, width_stride,\n            h, w, num_elements,\n            output_batch_stride, output_channel_stride,\n            BLOCK_SIZE=256,\n            num_warps=8\n        )\n        \n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
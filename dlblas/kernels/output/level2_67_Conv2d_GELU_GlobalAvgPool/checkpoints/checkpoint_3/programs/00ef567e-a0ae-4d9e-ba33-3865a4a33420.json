{"id": "00ef567e-a0ae-4d9e-ba33-3865a4a33420", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    height,\n    width,\n    num_elements,\n    output_batch_stride,\n    output_channel_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_ptr = input_ptr + pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    total = 0.0\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < num_elements\n        h = offsets // width\n        w = offsets % width\n        ptrs = base_ptr + h * input_height_stride + w * input_width_stride\n        data = tl.load(ptrs, mask=mask, other=0.0)\n        total += tl.sum(data, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.gelu(x)\n        \n        batch, channels, h, w = x.shape\n        num_elements = h * w\n        output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n        \n        batch_stride, channel_stride, height_stride, width_stride = x.stride()\n        output_batch_stride, output_channel_stride = output.stride()\n        \n        grid = (batch, channels)\n        global_avg_pool2d_kernel[grid](\n            x, output,\n            batch_stride, channel_stride, height_stride, width_stride,\n            h, w, num_elements,\n            output_batch_stride, output_channel_stride,\n            BLOCK_SIZE=1024\n        )\n        \n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "88d7a66c-6ba0-4dba-a6ad-61040ed325b7", "generation": 1, "timestamp": 1754581293.0262754, "iteration_found": 1, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.769, "runtime_ms": 0.0852, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0655, "speed_up": 0.769, "custom_timing": 0.0852}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0581, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0655, "speed_up": 1.127, "custom_timing": 0.0581}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0581\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0655, 'speed_up': 1.127, 'custom_timing': 0.0581}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0581, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0655, 'speed_up': 1.127, 'custom_timing': 0.0581}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies GELU, and then performs global average pooling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, height, width)\n        Returns:\n            Output tensor of shape (batch_size, out_channels)\n        \"\"\"\n        x = self.conv(x)\n        x = torch.nn.functional.gelu(x)\n        x = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n        x = x.squeeze(-1).squeeze(-1)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0581), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0655, 'speed_up': 1.127, 'custom_timing': 0.0581})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies GELU, and then performs global average pooling.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, in_channels, height, width)\n        Returns:\n            Output tensor of shape (batch_size, out_channels)\n        \"\"\"\n        x = self.conv(x)\n        x = torch.nn.functional.gelu(x)\n        x = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n        x = x.squeeze(-1).squeeze(-1)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Convolution\n  2. GELU activation\n  3. Adaptive average pooling (to 1x1)\n  4. Squeeze the last two dimensions\n\nThe current implementation uses standard PyTorch layers.\n\nHowever, note that the task is to rewrite the program for better performance, and the context is about Triton GPU kernels.\n\nBut wait: the current program does not use Triton at all. It is a pure PyTorch model.\n\nThe problem says: \"Rewrite the program to improve its performance on the specified metrics.\"\n\nWe are allowed to use Triton to replace parts of the model.\n\nHowever, note that the convolution and GELU are not trivial to write in Triton. Moreover, the adaptive average pooling to 1x1 is a reduction.\n\nBut note the constraints:\n  - We cannot change the function signatures (like the forward method) or the grid configuration (if we write a kernel) arbitrarily.\n  - We are only allowed to change the Triton kernel source within the EVOLVE-BLOCK.\n\nBut the current code does not have any Triton kernel. So we have two options:\n\nOption 1: Replace the entire forward pass with a single Triton kernel? That would be complex because it involves convolution and then reduction.\n\nOption 2: Focus on the most expensive part? However, we don't have profiling data.\n\nAlternatively, we might notice that the convolution is done by PyTorch and we are not going to rewrite that in Triton (because it's highly optimized). The GELU is also optimized.\n\nBut the adaptive average pooling to 1x1 is a reduction operation. We can write a Triton kernel for that.\n\nHowever, note the output of the convolution and GELU is of shape [batch_size, out_channels, height_out, width_out]. Then we want to reduce the last two dimensions to 1x1 by averaging.\n\nWe can write a Triton kernel that does:\n  output[b, c] = sum_{i,j} x[b, c, i, j] / (height_out * width_out)\n\nBut note: the current implementation uses `torch.nn.functional.adaptive_avg_pool2d(x, 1)` which is efficient. However, if we can write a custom reduction that is faster, we can replace that step.\n\nBut note: the current speed_up is 1.127 (which is the custom_timing of the entire model? but the reference_timing is 0.0655 and custom_timing is 0.0581, so the entire model runs in 0.0581 ms and the reference runs in 0.0655 ms, giving a speedup of 1.127). This is already faster than the reference? But the reference code is the one we are supposed to replace? \n\nWait, the problem says: \"Each file contains PyTorch reference (reference_code)\" and we are to generate Triton implementations (custom_code). The current program is the reference code? But the metrics say \"stage: reference code\", so it seems we are at the beginning.\n\nBut the task says: \"Rewrite the program to improve its performance on the specified metrics.\" and the current program is the reference. So we are to write a Triton version?\n\nLooking at the example: they replaced a PyTorch function with a Triton kernel.\n\nSo let's break down the model:\n\nWe have:\n  - A convolution: we leave it as is (PyTorch's conv is optimized).\n  - GELU: we leave it as is (PyTorch's GELU is optimized).\n  - Adaptive average pooling: we can write a Triton kernel for that? But note that the adaptive average pooling to 1x1 is just a global average pooling. And PyTorch's `adaptive_avg_pool2d` is efficient.\n\nAlternatively, we can try to fuse the GELU and the pooling? But note the convolution is separate.\n\nHowever, the entire model is very small (batch_size=128, in_channels=3, out_channels=16, height=32, width=32, kernel_size=3). So the output after convolution and GELU is [128, 16, 30, 30] (if no padding) or [128,16,32,32] if padding was added? The current code does not specify padding, so by default PyTorch uses no padding.\n\nBut the convolution in PyTorch: `nn.Conv2d(in_channels, out_channels, kernel_size)` without padding -> output spatial size: 32 - 3 + 1 = 30.\n\nSo we have [128,16,30,30]. Then we do global average pooling: [128,16,1,1] and then squeeze to [128,16].\n\nWe can write a Triton kernel for the global average pooling. However, note that the reduction is over 30*30=900 elements per channel. This is a significant reduction.\n\nBut the current PyTorch implementation is already very fast. However, we are to use Triton to speed it up.\n\nWe design the kernel:\n\n  - Each channel in each batch is independent. So we can have a 2D grid: [batch_size, out_channels].\n  - We will load the entire channel by iterating over the 30x30 with a loop. But 900 is not too big, but we can use multiple blocks? Actually, we can do one channel per program.\n\nHowever, note that 128 * 16 = 2048 channels. We can launch 2048 programs. Each program will reduce 900 elements.\n\nBut 900 is small enough that we can do it in one block? We can use a block size of 1024 (which is more than 900). We can use a 1D block to reduce the 900 elements.\n\nAlternatively, we can use multiple stages? But 900 is not a power of two. We can use a loop.\n\nKernel plan:\n\n  @triton.jit\n  def global_avg_pool2d_kernel(\n      input_ptr,  # pointer to the input tensor (after convolution and GELU) of shape [batch, channels, height, width]\n      output_ptr, # pointer to the output tensor of shape [batch, channels]\n      input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n      height, width,  # the spatial dimensions to reduce (30,30)\n      BLOCK_SIZE: tl.constexpr\n  ):\n      pid_batch = tl.program_id(0)\n      pid_channel = tl.program_id(1)\n\n      # We are going to reduce over [height, width] for this batch and channel.\n\n      # Compute the base pointer for this batch and channel\n      base_ptr = input_ptr + pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n      total = 0.0\n      num_elements = height * width\n\n      # We'll loop over blocks of the spatial dimension.\n      for i in range(0, num_elements, BLOCK_SIZE):\n          offsets = i + tl.arange(0, BLOCK_SIZE)\n          # We need to compute the 2D indices from the 1D offset\n          h = offsets // width\n          w = offsets % width\n          # But note: we must not go out of bounds\n          mask = offsets < num_elements\n          # Compute the pointer offsets: each element is at base_ptr + h * input_height_stride + w * input_width_stride\n          ptrs = base_ptr + h * input_height_stride + w * input_width_stride\n          data = tl.load(ptrs, mask=mask, other=0.0)\n          total += tl.sum(data, axis=0)\n\n      avg = total / num_elements\n      # Write to output: output_ptr[pid_batch, pid_channel] = avg\n      output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n      tl.store(output_ptr + output_offset, avg)\n\nBut note: the output tensor is of shape [batch, channels]. We need to create this tensor and then return it.\n\nHowever, the current model does:\n   x = self.conv(x)\n   x = torch.nn.functional.gelu(x)\n   x = torch.nn.functional.adaptive_avg_pool2d(x, 1)   # -> [batch, channels, 1, 1]\n   x = x.squeeze(-1).squeeze(-1)   # -> [batch, channels]\n\nWe can replace the last two steps with:\n   x = global_avg_pool2d(x)   # returns [batch, channels]\n\nBut note: the output of the convolution and GELU is 4D. We want to reduce the last two dimensions.\n\nWe have to be cautious about the strides. We can compute the strides by hand or use `tensor.stride(i)`.\n\nAlternatively, we can use `triton.code_gen.reinterpret` to handle the 4D tensor as a 2D tensor of shape [batch*channels, height*width] and then do a reduction over the last dimension? That might be simpler.\n\nBut the problem says: we must not change the grid configuration arbitrarily? Actually, we are writing a new kernel so we can define the grid as we wish, but the output shape must remain the same.\n\nBut note: the constraints say:\n   \u274c NEVER modify grid configuration, output tensor shapes, or PID logic\n\nBut wait: the constraints are for the Triton kernel we are modifying? In this case, we are writing a new kernel. However, the original program didn't have a kernel. So we are introducing one.\n\nBut the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" meaning we cannot change the surrounding code? However, the example showed that we can introduce a new function (the Triton kernel) and then change the function that uses it.\n\nBut the current EVOLVE-BLOCK contains the entire model and the test setup. We are allowed to change the EVOLVE-BLOCK? Yes, because the entire code is inside the block.\n\nSo we can restructure the code to include a Triton kernel for the global average pooling.\n\nLet's restructure:\n\n  We will define a Triton kernel for global average pooling (as above) and then use it in the forward pass.\n\nBut note: the convolution and GELU are left in PyTorch. Then we take the result and feed it to our Triton kernel.\n\nWe must be cautious about the data layout. The input to the pooling is 4D: [batch, channels, height, width]. We can compute the strides.\n\nAlternatively, we can flatten the tensor to 2D: [batch*channels, height*width] and then do a reduction? But note the kernel above is designed for 4D.\n\nBut note: the example used a grid of (x.shape[0],) for a 2D tensor. Here we want a 2D grid: [batch, channels].\n\nWe must also note that the current model returns [batch, channels]. So we are replacing:\n   x = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n   x = x.squeeze(-1).squeeze(-1)\n\nwith:\n   x = global_avg_pool2d_triton(x)   # which returns [batch, channels]\n\nHow to integrate:\n\n  1. Define the Triton kernel (inside the class? or outside? We can define it outside the class but inside the EVOLVE-BLOCK).\n  2. In the forward pass, after GELU, call the Triton kernel.\n\nBut note: the problem says to rewrite the program. We are allowed to change the entire block.\n\nLet's write the code:\n\n  Steps:\n    - Import triton and triton.language as tl.\n    - Define the kernel function.\n    - In the forward pass, replace the pooling and squeeze with a call to the kernel.\n\nBut note: the kernel requires the spatial dimensions (height, width) and the strides. We can get these from the tensor.\n\nHowever, the current model does not have padding, so the spatial dimensions are fixed? But the model should be generic. Actually, the convolution might change the spatial size.\n\nWe can compute the spatial size from the input tensor: [batch, channels, h, w] -> h and w.\n\nWe must also allocate the output tensor of shape [batch, channels].\n\nBut note: the example used a grid of (x.shape[0],) for a 2D input. Here we have a 4D input. We want a grid of [batch, channels].\n\nWe'll do:\n\n    grid = (x.size(0), x.size(1))\n\nBut note: the kernel above uses two program ids: for batch and channel.\n\nWe'll set the block size for the reduction? Actually, the kernel above uses a loop over blocks of BLOCK_SIZE. We can set BLOCK_SIZE to 1024 (or 512) and then the kernel will loop accordingly.\n\nBut note: the number of elements per channel is h*w. We can pass h and w to the kernel.\n\nWe must also pass the strides of the input tensor.\n\nWe can do:\n\n    batch_stride = x.stride(0)\n    channel_stride = x.stride(1)\n    height_stride = x.stride(2)\n    width_stride = x.stride(3)\n\nBut note: the tensor might not be contiguous. We can make it contiguous? Or we can handle the strides.\n\nAlternatively, we can require the input to be contiguous. But the convolution output is contiguous? Probably.\n\nLet's write the kernel and the wrapper.\n\nBut note: the constraints say we must not remove boundary checks. We are using a mask in the load.\n\nWe'll do:\n\n  @triton.jit\n  def global_avg_pool2d_kernel(\n      input_ptr,\n      output_ptr,\n      input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n      height, width,\n      num_elements,  # total elements per channel: height * width\n      output_batch_stride, output_channel_stride,\n      BLOCK_SIZE: tl.constexpr\n  ):\n      pid_batch = tl.program_id(0)\n      pid_channel = tl.program_id(1)\n\n      # base pointer for the current channel in the current batch\n      base_ptr = input_ptr + pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n      total = 0.0\n      # We'll loop over the spatial dimension in blocks\n      for i in range(0, num_elements, BLOCK_SIZE):\n          offsets = i + tl.arange(0, BLOCK_SIZE)\n          mask = offsets < num_elements\n          # Convert 1D offset to 2D indices\n          h = offsets // width\n          w = offsets % width\n          # Compute the pointers\n          ptrs = base_ptr + h * input_height_stride + w * input_width_stride\n          data = tl.load(ptrs, mask=mask, other=0.0)\n          total += tl.sum(data, axis=0)\n\n      avg = total / num_elements\n\n      # Compute the output pointer for this batch and channel\n      output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n      tl.store(output_ptr + output_offset, avg)\n\n  Then in the forward function:\n\n      def forward(self, x):\n          x = self.conv(x)\n          x = torch.nn.functional.gelu(x)\n\n          # Now we do global average pooling with Triton\n          # x is of shape [batch, channels, h, w]\n          batch, channels, h, w = x.shape\n          num_elements = h * w\n\n          # We want output of shape [batch, channels]\n          output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n\n          # Compute strides\n          batch_stride, channel_stride, height_stride, width_stride = x.stride()\n          output_batch_stride, output_channel_stride = output.stride()\n\n          # Grid is (batch, channels)\n          grid = (batch, channels)\n\n          # We choose a block size for the reduction. We can try 1024.\n          BLOCK_SIZE = 1024\n\n          global_avg_pool2d_kernel[grid](\n              x, output,\n              batch_stride, channel_stride, height_stride, width_stride,\n              h, w,\n              num_elements,\n              output_batch_stride, output_channel_stride,\n              BLOCK_SIZE=BLOCK_SIZE\n          )\n\n          return output\n\nBut note: the convolution and GELU are still in PyTorch. We are only replacing the pooling.\n\nBut is this faster? We are reducing one channel at a time with a loop. The PyTorch adaptive_avg_pool2d might use a highly optimized kernel. However, for small tensors, a custom kernel might be faster.\n\nAlternatively, we can try to fuse the GELU and the pooling? But that would require writing a kernel that does both. However, the problem says to make small iterative steps.\n\nBut note: the current model is very small. The entire model runs in 0.0581 ms. We might not see a big improvement. However, we are to try.\n\nBut the problem says: we must not change the grid configuration arbitrarily? Actually, we are defining a new grid for the new kernel. That should be acceptable.\n\nHowever, note that the constraints were for modifying an existing Triton kernel. We are introducing one.\n\nBut the problem says: \"Modify ONLY the Triton kernel source within this block\" meaning we can change the code arbitrarily as long as we are within the block? Yes, because the entire code is in the block.\n\nWe'll try to write the Triton kernel and use it.\n\nBut note: the example had an autotune. We might want to autotune the BLOCK_SIZE? But the problem says small steps. We'll start with a fixed BLOCK_SIZE=1024.\n\nAlso note: the number of elements per channel is 30*30=900, which is less than 1024. So the loop will run only once. So it's efficient.\n\nBut what if the spatial size is larger? We are passing h and w from the tensor, so it's generic.\n\nLet's write the code accordingly.\n\nBut note: the model is defined as a class. We must keep the class and the test setup (batch_size, etc.) and the get_inputs and get_init_inputs.\n\nWe'll restructure the code to include the Triton kernel and the modified forward.\n\nHowever, note that the Triton kernel must be defined at the top level (not inside the class). So we'll define it outside the class.\n\nWe'll do:\n\n  import triton\n  import triton.language as tl\n\n  @triton.jit\n  def ... (as above)\n\n  class ModelNew(nn.Module):\n      ... (with the modified forward)\n\nBut note: the example had a function `per_row_quantize_int8` that called the Triton kernel. We are doing the same.\n\nWe must also note that the kernel might not be the most efficient. We are doing a loop that runs once for 900 elements, so it's one block per channel. But 2048 blocks (128*16) is acceptable.\n\nBut we can also try to use a 1D grid and have each block handle multiple channels? That might be more efficient? But we are constrained by the problem to not change the grid arbitrarily? Actually, we are free to design the grid because it's a new kernel. But we are only changing the pooling step.\n\nAlternatively, we can have a 1D grid and then use the program id to index both batch and channel? We can do:\n\n    total_channels = batch * channels\n    pid = tl.program_id(0)\n    pid_batch = pid // channels\n    pid_channel = pid % channels\n\nBut that might be more efficient? Because we can launch more blocks in parallel? But 2048 blocks is acceptable.\n\nWe'll stick with the 2D grid.\n\nBut note: the example used a 1D grid for a 2D input. We are using a 2D grid for a 4D input.\n\nWe'll write the code accordingly.\n\nOne more thing: the kernel uses division by `num_elements`. We must be cautious: if num_elements is 0? But the model has at least 1 element.\n\nLet's code.\n\nBut note: the current program does not have triton imported. We must import it.\n\nWe'll add:\n   import triton\n   import triton.language as tl\n\nBut note: the problem says we are deeply familiar with H100. We are using Triton.\n\nWe are going to replace the entire EVOLVE-BLOCK with the new code.\n\nImportant: We must not change the class name (ModelNew) and the functions get_inputs and get_init_inputs.\n\nLet's write the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    height,\n    width,\n    num_elements,\n    output_batch_stride,\n    output_channel_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_ptr = input_ptr + pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    total = 0.0\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < num_elements\n        h = offsets // width\n        w = offsets % width\n        ptrs = base_ptr + h * input_height_stride + w * input_width_stride\n        data = tl.load(ptrs, mask=mask, other=0.0)\n        total += tl.sum(data, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.gelu(x)\n        \n        batch, channels, h, w = x.shape\n        num_elements = h * w\n        output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n        \n        batch_stride, channel_stride, height_stride, width_stride = x.stride()\n        output_batch_stride, output_channel_stride = output.stride()\n        \n        grid = (batch, channels)\n        global_avg_pool2d_kernel[grid](\n            x, output,\n            batch_stride, channel_stride, height_stride, width_stride,\n            h, w, num_elements,\n            output_batch_stride, output_channel_stride,\n            BLOCK_SIZE=1024\n        )\n        \n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "76160ef7-a864-4fee-b4e9-5a0f25a21994", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    kernel_size: tl.constexpr,\n    stride,\n    padding,\n    h_in,\n    w_in,\n    h_out,\n    w_out,\n    batch_size,\n    channels,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    total_output = batch_size * channels * h_out * w_out\n    if pid >= total_output:\n        return\n\n    per_batch = channels * h_out * w_out\n    b = pid // per_batch\n    pid_in_batch = pid % per_batch\n    c = pid_in_batch // (h_out * w_out)\n    pid_in_channel = pid_in_batch % (h_out * w_out)\n    oh = pid_in_channel // w_out\n    ow = pid_in_channel % w_out\n\n    start_h = oh * stride - padding\n    start_w = ow * stride - padding\n    \n    total = 0.0\n    count = 0\n    \n    base_ptr = x_ptr + b * channels * h_in * w_in + c * h_in * w_in\n    \n    for kh in tl.static_range(0, kernel_size):\n        h_index = start_h + kh\n        if h_index >=0 and h_index < h_in:\n            for kw in tl.static_range(0, kernel_size):\n                w_index = start_w + kw\n                if w_index >=0 and w_index < w_in:\n                    ptr = base_ptr + h_index * w_in + w_index\n                    val = tl.load(ptr)\n                    total += val\n                    count += 1\n    \n    if count > 0:\n        total /= count\n    \n    out_index = b * channels * h_out * w_out + c * h_out * w_out + oh * w_out + ow\n    tl.store(output_ptr + out_index, total)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.contiguous()\n        h_in, w_in = x.shape[2], x.shape[3]\n        h_out = (h_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        w_out = (w_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty(\n            x.shape[0], x.shape[1], h_out, w_out, \n            dtype=x.dtype, device=x.device\n        )\n        \n        total_elements = output.numel()\n        grid = lambda meta: (total_elements,)\n        _avg_pool2d_kernel[grid](\n            x, output,\n            self.kernel_size,\n            self.stride,\n            self.padding,\n            h_in,\n            w_in,\n            h_out,\n            w_out,\n            x.shape[0],\n            x.shape[1],\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "4a6e8722-4f8c-45e2-9687-8237649fcff5", "generation": 2, "timestamp": 1754647300.7192965, "iteration_found": 27, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.003, "runtime_ms": 7.66, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.02, "speed_up": 0.003, "custom_timing": 7.66}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 0.0000\n- correctness: 0.0000\n- score: -100.0000\n- stage: no_valid_triton_code\n- feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Areas identified for improvement: - Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level1_45_Average_Pooling_2D', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\\n    return _semantic.arange(start, end)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 574, in arange\\n    raise ValueError(\"arange\\'s arguments must be of type tl.constexpr\")\\nValueError: arange\\'s arguments must be of type tl.constexpr\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpplnctxfi.py\", line 79, in forward\\n    _avg_pool2d_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 35:18:\\n    pid_in_channel = pid_in_batch % (h_out * w_out)\\n    oh = pid_in_channel // w_out\\n    ow = pid_in_channel % w_out\\n\\n    start_h = oh * stride - padding\\n    start_w = ow * stride - padding\\n\\n    total = 0.0\\n    count = 0\\n\\n    base_ptr = x_ptr + b * channels * h_in * w_in + c * h_in * w_in\\n    col_offsets = tl.arange(0, kernel_size)\\n                  ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\\n    return _semantic.arange(start, end)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 574, in arange\\n    raise ValueError(\"arange\\'s arguments must be of type tl.constexpr\")\\nValueError: arange\\'s arguments must be of type tl.constexpr\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.02, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level1_45_Average_Pooling_2D', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\\n    return _semantic.arange(start, end)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\\n    raise ValueError(\"arange\\'s range must be a power of 2\")\\nValueError: arange\\'s range must be a power of 2\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmp7q99ljdr.py\", line 79, in forward\\n    _avg_pool2d_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 39:24:\\n    start_h = oh * stride - padding\\n    start_w = ow * stride - padding\\n\\n    total = 0.0\\n    count = 0\\n\\n    base_ptr = x_ptr + b * channels * h_in * w_in + c * h_in * w_in\\n\\n    for kh in tl.static_range(0, kernel_size):\\n        h_index = start_h + kh\\n        if h_index >=0 and h_index < h_in:\\n            w_offsets = tl.arange(0, kernel_size)\\n                        ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\\n    return _semantic.arange(start, end)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\\n    raise ValueError(\"arange\\'s range must be a power of 2\")\\nValueError: arange\\'s range must be a power of 2\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.02, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.0020, runtime_ms: 11.3000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.02, 'speed_up': 0.002, 'custom_timing': 11.3}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    kernel_size,\n    stride,\n    padding,\n    h_in,\n    w_in,\n    h_out,\n    w_out,\n    batch_size,\n    channels,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    total_output = batch_size * channels * h_out * w_out\n    if pid >= total_output:\n        return\n\n    per_batch = channels * h_out * w_out\n    b = pid // per_batch\n    pid_in_batch = pid % per_batch\n    c = pid_in_batch // (h_out * w_out)\n    pid_in_channel = pid_in_batch % (h_out * w_out)\n    oh = pid_in_channel // w_out\n    ow = pid_in_channel % w_out\n\n    start_h = oh * stride - padding\n    start_w = ow * stride - padding\n    \n    total = 0.0\n    count = 0\n    \n    for kh in range(0, kernel_size):\n        h_index = start_h + kh\n        for kw in range(0, kernel_size):\n            w_index = start_w + kw\n            if h_index >= 0 and h_index < h_in and w_index >=0 and w_index < w_in:\n                # Calculate linear index without pointers\n                index = b * channels * h_in * w_in + c * h_in * w_in + h_index * w_in + w_index\n                val = tl.load(x_ptr + index)\n                total += val\n                count += 1\n    \n    if count > 0:\n        total /= count\n    \n    out_index = b * channels * h_out * w_out + c * h_out * w_out + oh * w_out + ow\n    tl.store(output_ptr + out_index, total)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.contiguous()\n        h_in, w_in = x.shape[2], x.shape[3]\n        h_out = (h_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        w_out = (w_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty(\n            x.shape[0], x.shape[1], h_out, w_out, \n            dtype=x.dtype, device=x.device\n        )\n        \n        total_elements = output.numel()\n        grid = lambda meta: (total_elements,)\n        _avg_pool2d_kernel[grid](\n            x, output,\n            self.kernel_size,\n            self.stride,\n            self.padding,\n            h_in,\n            w_in,\n            h_out,\n            w_out,\n            x.shape[0],\n            x.shape[1],\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.0020), Performs well on runtime_ms (11.3000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.02, 'speed_up': 0.002, 'custom_timing': 11.3})\n\n\n### Program 2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    kernel_size: tl.constexpr,\n    stride,\n    padding,\n    h_in,\n    w_in,\n    h_out,\n    w_out,\n    batch_size,\n    channels,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    total_output = batch_size * channels * h_out * w_out\n    if pid >= total_output:\n        return\n\n    per_batch = channels * h_out * w_out\n    b = pid // per_batch\n    pid_in_batch = pid % per_batch\n    c = pid_in_batch // (h_out * w_out)\n    pid_in_channel = pid_in_batch % (h_out * w_out)\n    oh = pid_in_channel // w_out\n    ow = pid_in_channel % w_out\n\n    start_h = oh * stride - padding\n    start_w = ow * stride - padding\n    \n    total = 0.0\n    count = 0\n    \n    base_ptr = x_ptr + b * channels * h_in * w_in + c * h_in * w_in\n    \n    for kh in tl.static_range(0, kernel_size):\n        h_index = start_h + kh\n        if h_index >=0 and h_index < h_in:\n            w_offsets = tl.arange(0, kernel_size)\n            w_indices = start_w + w_offsets\n            mask = (w_indices >= 0) & (w_indices < w_in)\n            row_ptr = base_ptr + h_index * w_in\n            vals = tl.load(row_ptr + w_indices, mask=mask, other=0.0)\n            total += tl.sum(vals)\n            count += tl.sum(mask)\n    \n    if count > 0:\n        total /= count\n    \n    out_index = b * channels * h_out * w_out + c * h_out * w_out + oh * w_out + ow\n    tl.store(output_ptr + out_index, total)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.contiguous()\n        h_in, w_in = x.shape[2], x.shape[3]\n        h_out = (h_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        w_out = (w_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty(\n            x.shape[0], x.shape[1], h_out, w_out, \n            dtype=x.dtype, device=x.device\n        )\n        \n        total_elements = output.numel()\n        grid = lambda meta: (total_elements,)\n        _avg_pool2d_kernel[grid](\n            x, output,\n            self.kernel_size,\n            self.stride,\n            self.padding,\n            h_in,\n            w_in,\n            h_out,\n            w_out,\n            x.shape[0],\n            x.shape[1],\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level1_45_Average_Pooling_2D', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\\n    return _semantic.arange(start, end)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\\n    raise ValueError(\"arange\\'s range must be a power of 2\")\\nValueError: arange\\'s range must be a power of 2\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmp7q99ljdr.py\", line 79, in forward\\n    _avg_pool2d_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 39:24:\\n    start_h = oh * stride - padding\\n    start_w = ow * stride - padding\\n\\n    total = 0.0\\n    count = 0\\n\\n    base_ptr = x_ptr + b * channels * h_in * w_in + c * h_in * w_in\\n\\n    for kh in tl.static_range(0, kernel_size):\\n        h_index = start_h + kh\\n        if h_index >=0 and h_index < h_in:\\n            w_offsets = tl.arange(0, kernel_size)\\n                        ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\\n    return _semantic.arange(start, end)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\\n    raise ValueError(\"arange\\'s range must be a power of 2\")\\nValueError: arange\\'s range must be a power of 2\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.02, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n### Program 3 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    kernel_size,\n    stride,\n    padding,\n    h_in,\n    w_in,\n    h_out,\n    w_out,\n    batch_size,\n    channels,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    total_output = batch_size * channels * h_out * w_out\n    if pid >= total_output:\n        return\n\n    per_batch = channels * h_out * w_out\n    b = pid // per_batch\n    pid_in_batch = pid % per_batch\n    c = pid_in_batch // (h_out * w_out)\n    pid_in_channel = pid_in_batch % (h_out * w_out)\n    oh = pid_in_channel // w_out\n    ow = pid_in_channel % w_out\n\n    start_h = oh * stride - padding\n    start_w = ow * stride - padding\n    \n    total = 0.0\n    count = 0\n    \n    base_ptr = x_ptr + b * channels * h_in * w_in + c * h_in * w_in\n    col_offsets = tl.arange(0, kernel_size)\n    w_indices = start_w + col_offsets\n    \n    for kh in range(0, kernel_size):\n        h_index = start_h + kh\n        if h_index >= 0 and h_index < h_in:\n            mask = (w_indices >= 0) & (w_indices < w_in)\n            ptr = base_ptr + h_index * w_in\n            vals = tl.load(ptr + w_indices, mask=mask, other=0.0)\n            total += tl.sum(vals, axis=0)\n            count += tl.sum(mask, axis=0)\n    \n    if count > 0:\n        total /= count\n    \n    out_index = b * channels * h_out * w_out + c * h_out * w_out + oh * w_out + ow\n    tl.store(output_ptr + out_index, total)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.contiguous()\n        h_in, w_in = x.shape[2], x.shape[3]\n        h_out = (h_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        w_out = (w_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty(\n            x.shape[0], x.shape[1], h_out, w_out, \n            dtype=x.dtype, device=x.device\n        )\n        \n        total_elements = output.numel()\n        grid = lambda meta: (total_elements,)\n        _avg_pool2d_kernel[grid](\n            x, output,\n            self.kernel_size,\n            self.stride,\n            self.padding,\n            h_in,\n            w_in,\n            h_out,\n            w_out,\n            x.shape[0],\n            x.shape[1],\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level1_45_Average_Pooling_2D', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\\n    return _semantic.arange(start, end)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 574, in arange\\n    raise ValueError(\"arange\\'s arguments must be of type tl.constexpr\")\\nValueError: arange\\'s arguments must be of type tl.constexpr\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpplnctxfi.py\", line 79, in forward\\n    _avg_pool2d_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 35:18:\\n    pid_in_channel = pid_in_batch % (h_out * w_out)\\n    oh = pid_in_channel // w_out\\n    ow = pid_in_channel % w_out\\n\\n    start_h = oh * stride - padding\\n    start_w = ow * stride - padding\\n\\n    total = 0.0\\n    count = 0\\n\\n    base_ptr = x_ptr + b * channels * h_in * w_in + c * h_in * w_in\\n    col_offsets = tl.arange(0, kernel_size)\\n                  ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\\n    return _semantic.arange(start, end)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 574, in arange\\n    raise ValueError(\"arange\\'s arguments must be of type tl.constexpr\")\\nValueError: arange\\'s arguments must be of type tl.constexpr\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.02, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: -100.0000)\n```python\n@triton.jit\ndef _avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    batch_size, channels, height, width,\n    out_height, out_width,\n    stride, padding,\n    stride_b, stride_c, stride_h, stride_w,\n    total_elements,\n    kernel_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= total_elements:\n        return\n\n    w_out = pid % out_width\n    h_out = (pid // out_width) % out_height\n    c_out = (pid // (out_width * out_height)) % channels\n    b = pid // (out_width * out_height * channels)\n\n    h_start = h_out * stride - padding\n    w_start = w_out * stride - padding\n\n    total = 0.0\n    count = 0.0\n\n    base_ptr = input_ptr + b * stride_b + c_out * stride_c\n\n    for kh in tl.static_range(kernel_size):\n        h = h_start + kh\n        w_offsets = tl.arange(0, kernel_size)\n        w_vals = w_start + w_offsets\n        # Create mask: check if the current row h is within [0, height) and each w_val is within [0, width)\n        mask_elems = (h >= 0) & (h < height) & (w_vals >= 0) & (w_vals < width)\n        ptr_row = base_ptr + h * stride_h\n        row_data = tl.load(ptr_row + w_vals, mask=mask_elems, other=0.0)\n        total += tl.sum(row_data)\n        count += tl.sum(tl.cast(mask_elems, tl.float32))\n\n    average = total / count if count > 0.0 else 0.0\n    tl.store(output_ptr + pid, average)\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: -100.0000)\n```python\n@triton.jit\ndef _avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    batch_size, channels, height, width,\n    out_height, out_width,\n    stride, padding,\n    stride_b, stride_c, stride_h, stride_w,\n    total_elements,\n    kernel_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= total_elements:\n        return\n\n    w_out = pid % out_width\n    h_out = (pid // out_width) % out_height\n    c_out = (pid // (out_width * out_height)) % channels\n    b = pid // (out_width * out_height * channels)\n\n    h_start = h_out * stride - padding\n    w_start = w_out * stride - padding\n\n    total = 0.0\n    count = 0.0   # change to float\n\n    # Precompute base pointer for the current batch and channel\n    base_ptr = input_ptr + b * stride_b + c_out * stride_c\n\n    for kh in tl.static_range(kernel_size):\n        h = h_start + kh\n        # Check if the row is entirely out of bounds in height?\n        if h < 0 or h >= height:\n            continue\n\n        w_base = w_start\n        w_offsets = tl.arange(0, kernel_size)\n        w_vals = w_base + w_offsets\n        mask_elems = (w_vals >= 0) & (w_vals < width)\n        ptr_row = base_ptr + h * stride_h + w_base\n        row_data = tl.load(ptr_row + w_offsets, mask=mask_elems, other=0.0)\n        total += tl.sum(row_data)\n        count += tl.sum(mask_elems.to(tl.float32))\n\n    if count == 0.0:\n        average = 0.0\n    else:\n        average = total / count\n\n    tl.store(output_ptr + pid, average)\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    kernel_size,\n    stride,\n    padding,\n    h_in,\n    w_in,\n    h_out,\n    w_out,\n    batch_size,\n    channels,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    total_output = batch_size * channels * h_out * w_out\n    if pid >= total_output:\n        return\n\n    per_batch = channels * h_out * w_out\n    b = pid // per_batch\n    pid_in_batch = pid % per_batch\n    c = pid_in_batch // (h_out * w_out)\n    pid_in_channel = pid_in_batch % (h_out * w_out)\n    oh = pid_in_channel // w_out\n    ow = pid_in_channel % w_out\n\n    start_h = oh * stride - padding\n    start_w = ow * stride - padding\n    \n    total = 0.0\n    count = 0\n    \n    for kh in range(0, kernel_size):\n        h_index = start_h + kh\n        for kw in range(0, kernel_size):\n            w_index = start_w + kw\n            if h_index >= 0 and h_index < h_in and w_index >=0 and w_index < w_in:\n                # Calculate linear index without pointers\n                index = b * channels * h_in * w_in + c * h_in * w_in + h_index * w_in + w_index\n                val = tl.load(x_ptr + index)\n                total += val\n                count += 1\n    \n    if count > 0:\n        total /= count\n    \n    out_index = b * channels * h_out * w_out + c * h_out * w_out + oh * w_out + ow\n    tl.store(output_ptr + out_index, total)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.contiguous()\n        h_in, w_in = x.shape[2], x.shape[3]\n        h_out = (h_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        w_out = (w_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty(\n            x.shape[0], x.shape[1], h_out, w_out, \n            dtype=x.dtype, device=x.device\n        )\n        \n        total_elements = output.numel()\n        grid = lambda meta: (total_elements,)\n        _avg_pool2d_kernel[grid](\n            x, output,\n            self.kernel_size,\n            self.stride,\n            self.padding,\n            h_in,\n            w_in,\n            h_out,\n            w_out,\n            x.shape[0],\n            x.shape[1],\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    kernel_size: tl.constexpr,\n    stride,\n    padding,\n    h_in,\n    w_in,\n    h_out,\n    w_out,\n    batch_size,\n    channels,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    total_output = batch_size * channels * h_out * w_out\n    if pid >= total_output:\n        return\n\n    per_batch = channels * h_out * w_out\n    b = pid // per_batch\n    pid_in_batch = pid % per_batch\n    c = pid_in_batch // (h_out * w_out)\n    pid_in_channel = pid_in_batch % (h_out * w_out)\n    oh = pid_in_channel // w_out\n    ow = pid_in_channel % w_out\n\n    start_h = oh * stride - padding\n    start_w = ow * stride - padding\n    \n    total = 0.0\n    count = 0\n    \n    base_ptr = x_ptr + b * channels * h_in * w_in + c * h_in * w_in\n    \n    for kh in tl.static_range(0, kernel_size):\n        h_index = start_h + kh\n        if h_index >=0 and h_index < h_in:\n            w_offsets = tl.arange(0, kernel_size)\n            w_indices = start_w + w_offsets\n            mask = (w_indices >= 0) & (w_indices < w_in)\n            row_ptr = base_ptr + h_index * w_in\n            vals = tl.load(row_ptr + w_indices, mask=mask, other=0.0)\n            total += tl.sum(vals)\n            count += tl.sum(mask)\n    \n    if count > 0:\n        total /= count\n    \n    out_index = b * channels * h_out * w_out + c * h_out * w_out + oh * w_out + ow\n    tl.store(output_ptr + out_index, total)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.contiguous()\n        h_in, w_in = x.shape[2], x.shape[3]\n        h_out = (h_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        w_out = (w_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty(\n            x.shape[0], x.shape[1], h_out, w_out, \n            dtype=x.dtype, device=x.device\n        )\n        \n        total_elements = output.numel()\n        grid = lambda meta: (total_elements,)\n        _avg_pool2d_kernel[grid](\n            x, output,\n            self.kernel_size,\n            self.stride,\n            self.padding,\n            h_in,\n            w_in,\n            h_out,\n            w_out,\n            x.shape[0],\n            x.shape[1],\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: -100.0000, Type: Exploratory)\n```python\n@triton.jit\ndef _avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    batch_size, channels, height, width,\n    out_height, out_width,\n    stride, padding,\n    stride_b, stride_c, stride_h, stride_w,\n    total_elements,\n    kernel_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= total_elements:\n        return\n\n    w_out = pid % out_width\n    h_out = (pid // out_width) % out_height\n    c_out = (pid // (out_width * out_height)) % channels\n    b = pid // (out_width * out_height * channels)\n\n    h_start = h_out * stride - padding\n    w_start = w_out * stride - padding\n\n    total = 0.0\n    count = 0.0   # change to float\n\n    # Precompute base pointer for the current batch and channel\n    base_ptr = input_ptr + b * stride_b + c_out * stride_c\n\n    for kh in tl.static_range(kernel_size):\n        h = h_start + kh\n        # Check if the row is entirely out of bounds in height?\n        if h < 0 or h >= height:\n            continue\n\n        w_base = w_start\n        w_offsets = tl.arange(0, kernel_size)\n        w_vals = w_base + w_offsets\n        mask_elems = (w_vals >= 0) & (w_vals < width)\n        ptr_row = base_ptr + h * stride_h + w_base\n        row_data = tl.load(ptr_row + w_offsets, mask=mask_elems, other=0.0)\n        total += tl.sum(row_data)\n        count += tl.sum(mask_elems.to(tl.float32))\n\n    if count == 0.0:\n        average = 0.0\n    else:\n        average = total / count\n\n    tl.store(output_ptr + pid, average)\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 4 (Score: -100.0000, Type: Exploratory)\n```python\nfor kh in tl.static_range(kernel_size):\n        h = h_start + kh\n        if h < 0 or h >= height:\n            continue\n        ... # rest of the loop body\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    kernel_size,\n    stride,\n    padding,\n    h_in,\n    w_in,\n    h_out,\n    w_out,\n    batch_size,\n    channels,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    total_output = batch_size * channels * h_out * w_out\n    if pid >= total_output:\n        return\n\n    per_batch = channels * h_out * w_out\n    b = pid // per_batch\n    pid_in_batch = pid % per_batch\n    c = pid_in_batch // (h_out * w_out)\n    pid_in_channel = pid_in_batch % (h_out * w_out)\n    oh = pid_in_channel // w_out\n    ow = pid_in_channel % w_out\n\n    start_h = oh * stride - padding\n    start_w = ow * stride - padding\n    \n    total = 0.0\n    count = 0\n    \n    base_ptr = x_ptr + b * channels * h_in * w_in + c * h_in * w_in\n    col_offsets = tl.arange(0, kernel_size)\n    w_indices = start_w + col_offsets\n    \n    for kh in range(0, kernel_size):\n        h_index = start_h + kh\n        if h_index >= 0 and h_index < h_in:\n            mask = (w_indices >= 0) & (w_indices < w_in)\n            ptr = base_ptr + h_index * w_in\n            vals = tl.load(ptr + w_indices, mask=mask, other=0.0)\n            total += tl.sum(vals, axis=0)\n            count += tl.sum(mask, axis=0)\n    \n    if count > 0:\n        total /= count\n    \n    out_index = b * channels * h_out * w_out + c * h_out * w_out + oh * w_out + ow\n    tl.store(output_ptr + out_index, total)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.contiguous()\n        h_in, w_in = x.shape[2], x.shape[3]\n        h_out = (h_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        w_out = (w_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty(\n            x.shape[0], x.shape[1], h_out, w_out, \n            dtype=x.dtype, device=x.device\n        )\n        \n        total_elements = output.numel()\n        grid = lambda meta: (total_elements,)\n        _avg_pool2d_kernel[grid](\n            x, output,\n            self.kernel_size,\n            self.stride,\n            self.padding,\n            h_in,\n            w_in,\n            h_out,\n            w_out,\n            x.shape[0],\n            x.shape[1],\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n@triton.jit\ndef avg_pool2d_kernel(\n    ...,\n    kernel_size: tl.constexpr,\n    ... \n):\n    ...\n    for kh in tl.static_range(kernel_size):\n        for kw in tl.static_range(kernel_size):\n            ...\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a task to implement an average pooling 2D operation using Triton.\n The previous attempts show that the top-performing program (Program 1) uses a naive double loop and achieves correctness but with poor performance (speed_up=0.002, meaning it's 500 times slower than the reference).\n The error in Attempt 2 and Attempt 3 is due to the use of `tl.arange` with a non-power-of-two range and non-constexpr argument, respectively.\n\n We note that the kernel size is a constant (3 in the example) and small. Therefore, we can unroll the loops and use vectorized loads.\n\n Approach:\n 1. We will use a grid of program IDs that covers each output element (batch, channel, output height, output width).\n 2. For each output element, we compute the start indices in the input (h_start, w_start).\n 3. We will load a [kernel_size x kernel_size] block of the input in a vectorized way. However, note that the block might go out of bounds.\n\n Steps in the kernel:\n   a. Compute the output indices (b, c, oh, ow) from the program ID.\n   b. Compute the start indices in the input: h_start = oh * stride - padding, similarly for w.\n   c. We create two vectors for the row and column indices in the kernel window:\n        kh_vec = tl.arange(0, kernel_size)\n        kw_vec = tl.arange(0, kernel_size)\n   d. Then we compute the input indices for the entire window:\n        h_indices = h_start + kh_vec[:, None]   # shape [kernel_size, 1]\n        w_indices = w_start + kw_vec[None, :]   # shape [1, kernel_size]\n   e. We create a mask for the entire window: within [0, h_in) and [0, w_in).\n   f. We then compute the base pointer for the current batch and channel.\n   g. We can then load the entire window in a vectorized manner by computing the offsets for each element. However, note that the input is contiguous in (h, w) so we can use:\n        offsets = (h_indices * w_in + w_indices).flatten()\n        mask = mask.flatten()\n        values = tl.load(base_ptr + offsets, mask=mask, other=0.0)\n   h. Then we compute the sum of the values and the count (number of valid elements) and then the average.\n\n However, note that the window size is small (3x3=9 elements) so we can also do:\n\n        total = 0.0\n        count = 0\n        for kh in range(kernel_size):\n            for kw in range(kernel_size):\n                h_idx = h_start + kh\n                w_idx = w_start + kw\n                if h_idx >= 0 and h_idx < h_in and w_idx >=0 and w_idx < w_in:\n                    val = tl.load(base_ptr + h_idx * w_in + w_idx)\n                    total += val\n                    count += 1\n\n But the double loop over kernel_size (which is small) might be unrolled by the compiler and efficient.\n\n However, the top-performing program (Program 1) uses the double loop and is correct but slow. The slowness is because each load is done separately and the kernel is launched with a huge grid (batch*channels*h_out*w_out).\n\n We need to improve performance. The key is to use vectorization and avoid conditionals in the inner loop.\n\n Alternatively, we can try to load a contiguous block of the input for a row and then use masked loads for the entire row. But note: the window might not be contiguous if the row is contiguous? Actually, for a fixed row in the window, the elements are contiguous.\n\n We can do:\n\n   for kh in range(kernel_size):\n        h_idx = h_start + kh\n        if h_idx >=0 and h_idx < h_in:\n            # Load a row of kernel_size elements starting at (h_idx, w_start)\n            w_offsets = tl.arange(0, kernel_size)\n            w_indices = w_start + w_offsets\n            mask = (w_indices >=0) & (w_indices < w_in)\n            row_ptr = base_ptr + h_idx * w_in\n            row_vals = tl.load(row_ptr + w_indices, mask=mask, other=0.0)\n            total += tl.sum(row_vals)\n            count += tl.sum(tl.cast(mask, tl.int32))\n\n This is the approach taken in Attempt 2 and 3, but they had errors because they used `tl.arange(0, kernel_size)` without ensuring kernel_size is a power of two and without making it a constexpr.\n\n How to fix?\n   - The kernel_size is passed as a tl.constexpr, so we can use it in tl.arange.\n   - However, tl.arange requires the range to be a power of two? Actually, in Triton it doesn't require that. The error in Attempt 2 was because of an old Triton version? But note the error: \"arange's range must be a power of 2\".\n\n But in the latest Triton, we can use non-power-of-two in tl.arange? Actually, the error message says that. So we must avoid non-power-of-two if the Triton version is old.\n\n Alternatively, we can use a different approach: since kernel_size is small (like 3) and known at compile time, we can use `tl.static_range` and unroll the loop.\n\n However, the error in Attempt 3 was: \"arange's arguments must be of type tl.constexpr\". This is because we used `kernel_size` in `tl.arange(0, kernel_size)` without it being a constexpr. But in our kernel, `kernel_size` is already a `tl.constexpr`.\n\n So let's try the row-wise load again, but this time we note that the Triton version might be fixed? Or we can try to use a static range.\n\n Actually, in the kernel signature, we have `kernel_size: tl.constexpr`. So it is a compile-time constant. Therefore, we can use it in `tl.arange(0, kernel_size)`.\n\n But the error in Attempt 2: \"arange's range must be a power of two\" is because Triton requires the range to be a power of two for vectorization? That might be a limitation.\n\n How about we round the kernel_size to the next power of two? But we don't want to load extra elements. We can do:\n\n   P2 = 2 ** tl.math.ceil(tl.math.log2(kernel_size))   # but this is not allowed in constexpr?\n\n Alternatively, we can set the range to the next power of two and then mask out the extra elements.\n\n But note: the kernel_size is small (3, 5, etc.) and we know it at compile time. We can precompute the next power of two and then mask. However, we cannot do that in the kernel because it must be a constexpr.\n\n Therefore, we can pass the next power of two as a constexpr? But we don't want to change the function signature.\n\n Alternatively, we can avoid using tl.arange for non-power-of-two. We can use a loop over kw in `tl.static_range(kernel_size)` and unroll the loop. This is acceptable because kernel_size is small.\n\n Revised plan for the inner loop:\n\n   for kh in tl.static_range(kernel_size):\n        h_index = start_h + kh\n        if h_index >=0 and h_index < h_in:\n            for kw in tl.static_range(kernel_size):\n                w_index = start_w + kw\n                if w_index >=0 and w_index < w_in:\n                    val = tl.load(base_ptr + h_index * w_in + w_index)\n                    total += val\n                    count += 1\n\n But note: this is the same as the top-performing program (Program 1) and it is slow.\n\n Alternatively, we can load the entire row without a loop for the columns by using a vectorized load. But we must have the row contiguous? It is. However, the row might start at an arbitrary offset and we have to mask.\n\n We can do:\n\n   for kh in tl.static_range(kernel_size):\n        h_index = start_h + kh\n        if h_index >=0 and h_index < h_in:\n            w_base = start_w\n            # We want to load kernel_size consecutive elements from w_base\n            # But if w_base is negative, we have to adjust. We can do:\n            #   w_start_clamped = max(0, w_base)\n            #   w_end_clamped = min(w_in, w_base+kernel_size)\n            #   Then we can load from w_start_clamped to w_end_clamped and then pad with zeros?\n            # But we don't know how to do that without conditionals.\n\n Alternatively, we can create a mask for the entire row. We can do:\n\n            w_offsets = tl.arange(0, kernel_size)\n            w_indices = w_base + w_offsets\n            mask = (w_indices >=0) & (w_indices < w_in)\n            ptr = base_ptr + h_index * w_in + w_base\n            row = tl.load(ptr + w_offsets, mask=mask, other=0.0)\n            total += tl.sum(row)\n            count += tl.sum(mask)\n\n And since kernel_size is a constexpr, the tl.arange(0, kernel_size) should be allowed and the range is known at compile time. But note: if kernel_size is not a power of two, we might get an error? The error in Attempt 2 was exactly that.\n\n But in Triton, the requirement for power-of-two in arange has been lifted? We must check the version. Since the environment is fixed, we might have to avoid non-power-of-two.\n\n How about we round up the kernel_size to the next power of two? We can do:\n\n   P2 = 2 ** tl.math.ceil(tl.math.log2(kernel_size))   # but this is not allowed because kernel_size is constexpr? and we cannot use tl.math in constexpr.\n\n Alternatively, we can pass the next power of two as a separate constexpr. But the kernel_size is known at kernel launch? We are passing it as a scalar. But we are marking it as constexpr so it is known at compile time.\n\n We can precompute the next power of two in the host and pass it as a constexpr. But we don't want to change the function signature.\n\n Given the constraints, we decide to use a different approach: we note that the kernel_size is small (like 3, 5, 7) and we can unroll the inner loop without vectorization.\n\n But the top-performing program (Program 1) already does that and is slow.\n\n Alternatively, we can try to vectorize the entire window by loading a 2D block. We can use:\n\n   h_indices = start_h + tl.arange(0, kernel_size)[:, None]\n   w_indices = start_w + tl.arange(0, kernel_size)[None, :]\n   mask = (h_indices >= 0) & (h_indices < h_in) & (w_indices >= 0) & (w_indices < w_in)\n   offsets = (h_indices * w_in + w_indices).flatten()\n   values = tl.load(base_ptr + offsets, mask=mask.flatten(), other=0.0)\n   total = tl.sum(values)\n   count = tl.sum(tl.cast(mask.flatten(), tl.int32))\n\n But again, we have the same problem: the arange for kernel_size must be a power of two? And also, we are creating a kernel_size x kernel_size array which might be 9 elements, which is not a power of two? So we would have to round up to 16? We can do:\n\n   P2 = 16  # for kernel_size up to 16? but we don't know kernel_size.\n\n We can pass a constexpr BLOCK_KERNEL: tl.constexpr = next_power_of_two(kernel_size) and then mask the extra elements.\n\n But we don't want to change the signature. Alternatively, we can compute the next power of two in the host and pass it as a constexpr. However, the kernel is called with:\n\n        _avg_pool2d_kernel[grid](\n            x, output,\n            self.kernel_size,\n            ...,\n            BLOCK_SIZE=1024\n        )\n\n We are already passing BLOCK_SIZE. We can add another constexpr for the kernel window. Let's call it KERNEL_P2. We can compute it in the host:\n\n        KERNEL_P2 = 1\n        while KERNEL_P2 < kernel_size:\n            KERNEL_P2 *= 2\n\n        _avg_pool2d_kernel[grid](\n            ...,\n            KERNEL_P2=KERNEL_P2\n        )\n\n But note: we are not allowed to change the grid configuration or the function signature? Actually, the problem says: \"\u274c NEVER change @triton.jit function signatures or parameter names\". So we cannot add new parameters.\n\n Therefore, we must avoid non-power-of-two in the arange? Or we can use static range and unroll.\n\n Given the time, we decide to go with the row-wise vectorized load with a fixed maximum kernel size? But we don't know the kernel_size at compile time? Actually, it is passed as a constexpr, so we can use it.\n\n However, the error in Attempt 2 was: \"arange's range must be a power of two\". This might be a limitation of the Triton version we are using.\n\n We decide to use the double loop with `tl.static_range` and hope that the unrolling and small kernel size will make it efficient. But Program 1 used that and was slow.\n\n Why was Program 1 slow? Because it did:\n        for kh in range(0, kernel_size):\n            for kw in range(0, kernel_size):\n                ... and inside conditionals and separate loads.\n\n We note that the Triton compiler might not be able to vectorize the loads. We want to load contiguous memory when possible.\n\n We can try to load an entire row of the window at once. We do:\n\n   for kh in tl.static_range(kernel_size):\n        h_index = start_h + kh\n        if h_index >=0 and h_index < h_in:\n            # We want to load a segment of the row from start_w to start_w + kernel_size - 1\n            # But we have to handle boundaries. We can do:\n            w_start_clipped = tl.maximum(0, start_w)\n            w_end_clipped = tl.minimum(w_in, start_w + kernel_size)\n            segment_length = w_end_clipped - w_start_clipped\n            if segment_length > 0:\n                ptr = base_ptr + h_index * w_in + w_start_clipped\n                # We can load the entire segment in one go? But the segment length is variable and not known at compile time -> not vectorizable.\n\n Alternatively, we can use masked load with a fixed maximum size (the kernel_size) and then mask the out-of-bound elements. But we cannot use non-power-of-two.\n\n Given the constraints, we decide to use the following:\n\n   Since kernel_size is a constexpr and small, we can unroll the inner loop for the columns. We do:\n\n        for kh in tl.static_range(kernel_size):\n            h_index = start_h + kh\n            if h_index >=0 and h_index < h_in:\n                for kw in tl.static_range(kernel_size):\n                    w_index = start_w + kw\n                    if w_index >=0 and w_index < w_in:\n                        val = tl.load(base_ptr + h_index * w_in + w_index)\n                        total += val\n                        count += 1\n\n But we hope that the unrolling will make the loads efficient.\n\n However, the top-performing program (Program 1) used exactly this and was slow.\n\n Another idea: use the fact that the input is contiguous and try to load multiple elements in the same row with a vectorized instruction. We can do:\n\n        for kh in tl.static_range(kernel_size):\n            h_index = start_h + kh\n            if h_index < 0 or h_index >= h_in:\n                continue\n            # Now for the row, we want to load kernel_size elements starting at start_w, but we have to mask\n            # We create a vector of indices for the columns in the row.\n            # But we cannot use tl.arange if kernel_size is not a power of two.\n\n        We are stuck.\n\n We decide to try the vectorized load for the entire row with a mask, and hope that the Triton version we are using supports non-power-of-two. If not, we might get a compilation error.\n\n But the problem says: if compilation fails, we get a score of -100. We want to avoid that.\n\n Given that the kernel_size is 3 in the example, which is not a power of two, we must do something.\n\n We can do:\n\n        w_offsets = tl.arange(0, 4)   # next power of two is 4\n        w_indices = start_w + w_offsets\n        mask = (w_indices >=0) & (w_indices < w_in) & (w_offsets < kernel_size)   # we also mask beyond kernel_size\n        ptr = base_ptr + h_index * w_in + start_w\n        row = tl.load(ptr + w_offsets, mask=mask, other=0.0)\n        total += tl.sum(row)\n        count += tl.sum(tl.cast(mask, tl.int32))\n\n But now we are loading up to 4 elements, even if kernel_size is 3. The mask will set the fourth element to 0 and not count it.\n\n But we have to know the next power of two (4) in the kernel. We can compute it in the host and pass it as a constexpr? But we cannot change the signature.\n\n Alternatively, we can compute the next power of two in the kernel? But kernel_size is a constexpr, so we can do:\n\n        P2 = 1\n        # This loop will be unrolled at compile time because kernel_size is constexpr.\n        while P2 < kernel_size:\n            P2 *= 2\n\n        w_offsets = tl.arange(0, P2)\n        w_indices = start_w + w_offsets\n        mask = (w_indices >=0) & (w_indices < w_in) & (w_offsets < kernel_size)\n        ...\n\n But is this allowed in Triton? We are using a while loop on a constexpr. This should be unrolled at compile time.\n\n Let's try this approach.\n\n Steps:\n\n   P2 = 1\n   while P2 < kernel_size:\n        P2 *= 2\n\n   Then for each row in the kernel window, we load P2 elements and then mask out the ones beyond kernel_size and out of the image boundaries.\n\n However, note that the kernel might be called with different kernel_sizes? But the kernel_size is passed as a constexpr, so each kernel is specialized for a particular kernel_size. Therefore, the while loop will be unrolled at compile time.\n\n But the problem: the while loop might not be supported in the constexpr context? We are not sure.\n\n Given the time, we decide to go with the double unrolled loop for the small kernel_size, even though it was slow in Program 1. We hope that with the use of `tl.static_range` and unrolling, the performance might be better than Program 1 because Program 1 used `range` which might not be unrolled? But in Triton, `tl.static_range` ensures unrolling.\n\n But Program 1 used Python's `range`, not `tl.static_range`. Let me check:\n\n Program 1:\n\n        for kh in range(0, kernel_size):\n            for kw in range(0, kernel_size):\n\n This is a dynamic loop. We can change it to `tl.static_range` to unroll.\n\n So we will take Program 1 and change the loops to `tl.static_range`.\n\n Revised kernel for Program 1 with unrolling:\n\n        for kh in tl.static_range(0, kernel_size):\n            for kw in tl.static_range(0, kernel_size):\n                h_index = start_h + kh\n                w_index = start_w + kw\n                if h_index >= 0 and h_index < h_in and w_index >=0 and w_index < w_in:\n                    index = ... # linear index\n                    val = tl.load(x_ptr + index)\n                    total += val\n                    count += 1\n\n We hope that unrolling the loops will improve performance.\n\n Let's try this.\n\n We also note that the base pointer can be precomputed:\n\n        base_ptr = x_ptr + b * channels * h_in * w_in + c * h_in * w_in\n\n        then the index for a point is: base_ptr + h_index * w_in + w_index\n\n We will write the kernel accordingly.\n\n We are also not allowed to remove boundary checks.\n\n Given that the top-performing program (Program 1) had a speedup of 0.002 (which is very slow) and we are only changing the loops to static_range, we hope for a improvement.\n\n Alternatively, we can try to load the entire window with a single masked load by flattening the window. But again, we face the power-of-two issue.\n\n We decide to try the static unrolling first.\n\n Code for the kernel:\n\n    pid = tl.program_id(0)\n    total_output = batch_size * channels * h_out * w_out\n    if pid >= total_output:\n        return\n\n    per_batch = channels * h_out * w_out\n    b = pid // per_batch\n    pid_in_batch = pid % per_batch\n    c = pid_in_batch // (h_out * w_out)\n    pid_in_channel = pid_in_batch % (h_out * w_out)\n    oh = pid_in_channel // w_out\n    ow = pid_in_channel % w_out\n\n    start_h = oh * stride - padding\n    start_w = ow * stride - padding\n\n    total = 0.0\n    count = 0\n\n    base_ptr = x_ptr + b * channels * h_in * w_in + c * h_in * w_in\n\n    for kh in tl.static_range(0, kernel_size):\n        h_index = start_h + kh\n        for kw in tl.static_range(0, kernel_size):\n            w_index = start_w + kw\n            if h_index >= 0 and h_index < h_in and w_index >=0 and w_index < w_in:\n                ptr = base_ptr + h_index * w_in + w_index\n                val = tl.load(ptr)\n                total += val\n                count += 1\n\n    if count > 0:\n        total /= count\n\n    out_index = b * channels * h_out * w_out + c * h_out * w_out + oh * w_out + ow\n    tl.store(output_ptr + out_index, total)\n\n We hope this will compile and be faster than the dynamic loop version.\n\n But note: the top-performing program (Program 1) used dynamic loops and was slow. We are replacing with static loops to unroll.\n\n However, the Triton compiler might have unrolled the dynamic loops anyway? We are not sure.\n\n Alternatively, we can try to load the entire row at once by using a vectorized load for the columns, but only if we can avoid non-power-of-two. We can do:\n\n        for kh in tl.static_range(0, kernel_size):\n            h_index = start_h + kh\n            if h_index < 0 or h_index >= h_in:\n                continue\n            # Compute the start and end for the columns\n            w_start_clipped = tl.maximum(0, start_w)\n            w_end_clipped = tl.minimum(w_in, start_w + kernel_size)\n            num_valid = w_end_clipped - w_start_clipped\n            if num_valid <= 0:\n                continue\n\n            # We want to load num_valid elements, but num_valid is not known at compile time -> cannot use vectorized load? \n            # We have to loop over the columns.\n\n        So we are back to looping over columns.\n\n Given the above, we decide to submit the static unrolled version and hope for the best.\n\n But note: the kernel_size might be large? The problem doesn't specify, but in average pooling, kernel_size is usually small (e.g., 3, 5, 7). So unrolling is acceptable.\n\n Let's write the code accordingly.\n\n However, we must ensure that we use the same function signature and parameters.\n\n We will take the structure from Program 1 and change the loops to `tl.static_range`.\n\n Also, note that the previous Attempt 1 (which was successful) used dynamic loops and was slow. We hope that static unrolling will improve performance.\n\n We also note that the Triton compiler might be able to optimize the conditionals and loads better with unrolled loops.\n\n We will now generate the code accordingly.\n\n IMPORTANT: We are not allowed to change the class name (ModelNew) or the function signatures.\n\n We will generate the entire EVOLVE-BLOCK.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    kernel_size: tl.constexpr,\n    stride,\n    padding,\n    h_in,\n    w_in,\n    h_out,\n    w_out,\n    batch_size,\n    channels,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    total_output = batch_size * channels * h_out * w_out\n    if pid >= total_output:\n        return\n\n    per_batch = channels * h_out * w_out\n    b = pid // per_batch\n    pid_in_batch = pid % per_batch\n    c = pid_in_batch // (h_out * w_out)\n    pid_in_channel = pid_in_batch % (h_out * w_out)\n    oh = pid_in_channel // w_out\n    ow = pid_in_channel % w_out\n\n    start_h = oh * stride - padding\n    start_w = ow * stride - padding\n    \n    total = 0.0\n    count = 0\n    \n    base_ptr = x_ptr + b * channels * h_in * w_in + c * h_in * w_in\n    \n    for kh in tl.static_range(0, kernel_size):\n        h_index = start_h + kh\n        if h_index >=0 and h_index < h_in:\n            for kw in tl.static_range(0, kernel_size):\n                w_index = start_w + kw\n                if w_index >=0 and w_index < w_in:\n                    ptr = base_ptr + h_index * w_in + w_index\n                    val = tl.load(ptr)\n                    total += val\n                    count += 1\n    \n    if count > 0:\n        total /= count\n    \n    out_index = b * channels * h_out * w_out + c * h_out * w_out + oh * w_out + ow\n    tl.store(output_ptr + out_index, total)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.contiguous()\n        h_in, w_in = x.shape[2], x.shape[3]\n        h_out = (h_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        w_out = (w_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty(\n            x.shape[0], x.shape[1], h_out, w_out, \n            dtype=x.dtype, device=x.device\n        )\n        \n        total_elements = output.numel()\n        grid = lambda meta: (total_elements,)\n        _avg_pool2d_kernel[grid](\n            x, output,\n            self.kernel_size,\n            self.stride,\n            self.padding,\n            h_in,\n            w_in,\n            h_out,\n            w_out,\n            x.shape[0],\n            x.shape[1],\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
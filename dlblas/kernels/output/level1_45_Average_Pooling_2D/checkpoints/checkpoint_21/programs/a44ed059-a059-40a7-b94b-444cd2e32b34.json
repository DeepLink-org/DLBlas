{"id": "a44ed059-a059-40a7-b94b-444cd2e32b34", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    in_height,\n    in_width,\n    out_height,\n    out_width,\n    stride: tl.constexpr,\n    padding: tl.constexpr,\n    kernel_size: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_bc = tl.program_id(0)\n    pid_hw = tl.program_id(1)\n    \n    # Calculate spatial indices\n    hw_start = pid_hw * BLOCK_SIZE\n    offsets = hw_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < (out_height * out_width)\n    \n    out_h = offsets // out_width\n    out_w = offsets % out_width\n    \n    # Calculate input window start positions\n    in_h_start = out_h * stride - padding\n    in_w_start = out_w * stride - padding\n    \n    # Get base pointers\n    x_bc_ptr = x_ptr + pid_bc * in_height * in_width\n    output_bc_ptr = output_ptr + pid_bc * out_height * out_width\n    \n    # Initialize accumulators\n    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    count = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Process kernel window - unroll loops for small kernel sizes\n    for kh in tl.static_range(kernel_size):\n        for kw in tl.static_range(kernel_size):\n            in_h = in_h_start + kh\n            in_w = in_w_start + kw\n            \n            # Boundary checks\n            valid_mask = mask & (in_h >= 0) & (in_h < in_height) & (in_w >= 0) & (in_w < in_width)\n            \n            # Compute offset and load values\n            in_offsets = in_h * in_width + in_w\n            values = tl.load(x_bc_ptr + in_offsets, mask=valid_mask, other=0.0)\n            \n            # Accumulate\n            total += values\n            count += tl.where(valid_mask, 1.0, 0.0)\n    \n    # Compute average and store\n    avg = tl.where(count > 0, total / count, 0.0)\n    tl.store(output_bc_ptr + offsets, avg, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, in_height, in_width = x.shape\n        out_height = (in_height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_width = (in_width + 2*self.padding - self.kernel_size) // self.stride + 1\n        \n        # Ensure contiguous memory access\n        x = x.contiguous()\n        output = torch.empty(\n            (batch_size, channels, out_height, out_width),\n            device=x.device, dtype=x.dtype\n        )\n        \n        # Optimized block size based on output dimensions\n        BLOCK_SIZE = 128\n        grid = (\n            batch_size * channels, \n            triton.cdiv(out_height * out_width, BLOCK_SIZE)\n        )\n        \n        avg_pool2d_kernel[grid](\n            x, output,\n            in_height, in_width,\n            out_height, out_width,\n            self.stride,\n            self.padding,\n            self.kernel_size,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "27e96797-6276-4bbc-aeea-5fa3870279af", "generation": 3, "timestamp": 1754645454.0722308, "iteration_found": 20, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.149, "runtime_ms": 0.134, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.02, "speed_up": 0.149, "custom_timing": 0.134}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.147, "runtime_ms": 0.136, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.02, "speed_up": 0.147, "custom_timing": 0.136}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.1470\n- runtime_ms: 0.1360\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.02, 'speed_up': 0.147, 'custom_timing': 0.136}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.1470, runtime_ms: 0.1360, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.02, 'speed_up': 0.147, 'custom_timing': 0.136}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.1390, runtime_ms: 0.1440, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.02, 'speed_up': 0.139, 'custom_timing': 0.144}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.0020, runtime_ms: 8.7800, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.02, 'speed_up': 0.002, 'custom_timing': 8.78}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    batch_size, channels, height, width,\n    out_height, out_width,\n    stride, padding,\n    stride_b, stride_c, stride_h, stride_w,\n    total_elements,\n    kernel_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= total_elements:\n        return\n\n    w_out = pid % out_width\n    h_out = (pid // out_width) % out_height\n    c_out = (pid // (out_width * out_height)) % channels\n    b = pid // (out_width * out_height * channels)\n\n    h_start = h_out * stride - padding\n    w_start = w_out * stride - padding\n\n    total = 0.0\n    count = 0\n\n    for kh in tl.static_range(kernel_size):\n        for kw in tl.static_range(kernel_size):\n            h = h_start + kh\n            w = w_start + kw\n            mask = (h >= 0) & (h < height) & (w >= 0) & (w < width)\n            off_b = b * stride_b\n            off_c = c_out * stride_c\n            off_h = h * stride_h\n            off_w = w * stride_w\n            off = off_b + off_c + off_h + off_w\n            val = tl.load(input_ptr + off, mask=mask, other=0.0)\n            total += val\n            count += tl.where(mask, 1, 0)\n\n    if count == 0:\n        average = 0.0\n    else:\n        average = total / count\n\n    tl.store(output_ptr + pid, average)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, height, width = x.shape\n        out_height = (height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_width = (width + 2*self.padding - self.kernel_size) // self.stride + 1\n        output = torch.empty((batch_size, channels, out_height, out_width), \n                             device=x.device, dtype=x.dtype).contiguous()\n\n        stride_b = x.stride(0)\n        stride_c = x.stride(1)\n        stride_h = x.stride(2)\n        stride_w = x.stride(3)\n        total_elements = batch_size * channels * out_height * out_width\n\n        grid = lambda meta: (total_elements,)\n        _avg_pool2d_kernel[grid](\n            x, output,\n            batch_size, channels, height, width,\n            out_height, out_width,\n            self.stride, self.padding,\n            stride_b, stride_c, stride_h, stride_w,\n            total_elements,\n            self.kernel_size,\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.0020), Performs well on runtime_ms (8.7800), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.02, 'speed_up': 0.002, 'custom_timing': 8.78})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    in_height,\n    in_width,\n    out_height,\n    out_width,\n    stride_h: tl.constexpr,\n    stride_w: tl.constexpr,\n    kernel_h: tl.constexpr,\n    kernel_w: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_bc = tl.program_id(0)\n    pid_hw = tl.program_id(1)\n    \n    hw_start = pid_hw * BLOCK_SIZE\n    offsets = hw_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < (out_height * out_width)\n    \n    out_h = offsets // out_width\n    out_w = offsets % out_width\n    \n    in_h_start = out_h * stride_h\n    in_w_start = out_w * stride_w\n    \n    x_bc_ptr = x_ptr + pid_bc * in_height * in_width\n    output_bc_ptr = output_ptr + pid_bc * out_height * out_width\n    \n    window_area = kernel_h * kernel_w\n    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    for kh in tl.static_range(kernel_h):\n        for kw in tl.static_range(kernel_w):\n            in_h = in_h_start + kh\n            in_w = in_w_start + kw\n            \n            in_ptr = x_bc_ptr + in_h * in_width + in_w\n            values = tl.load(in_ptr, mask=mask & (in_h < in_height) & (in_w < in_width), other=0.0)\n            total += values\n    \n    avg = total / window_area\n    tl.store(output_bc_ptr + offsets, avg, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.padding != 0:\n            x = torch.nn.functional.pad(x, (self.padding, self.padding, self.padding, self.padding))\n            \n        batch_size, channels, in_height, in_width = x.shape\n        out_height = (in_height - self.kernel_size) // self.stride + 1\n        out_width = (in_width - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty((batch_size, channels, out_height, out_width), \n                             device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE = 128\n        grid = (batch_size * channels, \n                triton.cdiv(out_height * out_width, BLOCK_SIZE))\n        \n        avg_pool2d_kernel[grid](\n            x, output,\n            in_height, in_width,\n            out_height, out_width,\n            self.stride, self.stride,\n            self.kernel_size, self.kernel_size,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.1390), Performs well on runtime_ms (0.1440), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.02, 'speed_up': 0.139, 'custom_timing': 0.144})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    batch_size,\n    channels,\n    height,\n    width,\n    out_h,\n    out_w,\n    stride,\n    padding,\n    kernel_size: tl.constexpr,\n    total_output_elements: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    idx = tl.arange(0, BLOCK_SIZE)\n    output_index = pid * BLOCK_SIZE + idx\n    mask = output_index < total_output_elements\n    \n    # Compute output tensor strides\n    S = channels * out_h * out_w\n    C_stride = out_h * out_w\n    H_stride = out_w\n    \n    # Unravel output indices\n    batch_i = tl.where(mask, output_index // S, 0)\n    channel_i = tl.where(mask, (output_index % S) // C_stride, 0)\n    pos_2d = tl.where(mask, output_index % C_stride, 0)\n    out_h_i = tl.where(mask, pos_2d // H_stride, 0)\n    out_w_i = tl.where(mask, pos_2d % H_stride, 0)\n    \n    # Compute input window\n    h_start = out_h_i * stride - padding\n    w_start = out_w_i * stride - padding\n    \n    # Input tensor strides\n    x_batch_stride = channels * height * width\n    x_channel_stride = height * width\n    x_base = x_ptr + batch_i * x_batch_stride + channel_i * x_channel_stride\n    \n    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    count = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Unroll loops for kernel processing\n    for kh in tl.static_range(0, kernel_size):\n        for kw in tl.static_range(0, kernel_size):\n            h = h_start + kh\n            w = w_start + kw\n            within_bounds = (h >= 0) & (h < height) & (w >= 0) & (w < width)\n            offset = h * width + w\n            cond = mask & within_bounds\n            value = tl.load(x_base + offset, mask=cond, other=0.0)\n            total += value\n            count += tl.where(cond, 1.0, 0.0)\n\n    # Compute average and avoid division by zero\n    output_val = tl.where(count > 0, total / count, 0.0)\n    output_offset = batch_i * S + channel_i * C_stride + out_h_i * out_w + out_w_i\n    tl.store(output_ptr + output_offset, output_val, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        if stride is None:\n            stride = kernel_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, height, width = x.shape\n        out_h = (height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_w = (width + 2*self.padding - self.kernel_size) // self.stride + 1\n        total_output_elements = batch_size * channels * out_h * out_w\n        \n        output = torch.empty((batch_size, channels, out_h, out_w), \n                             device=x.device, dtype=x.dtype)\n        \n        if total_output_elements == 0:\n            return output\n            \n        BLOCK_SIZE = 256  # Increased block size for better utilization\n        grid = (triton.cdiv(total_output_elements, BLOCK_SIZE),)\n        \n        _avg_pool2d_kernel[grid](\n            x, output,\n            batch_size, channels, height, width,\n            out_h, out_w,\n            self.stride, self.padding, self.kernel_size,\n            total_output_elements,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.1470), Performs well on runtime_ms (0.1360), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.02, 'speed_up': 0.147, 'custom_timing': 0.136})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    in_height,\n    in_width,\n    out_height,\n    out_width,\n    stride: tl.constexpr,\n    kernel_size: tl.constexpr,\n    padding: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_bc = tl.program_id(0)\n    pid_hw = tl.program_id(1)\n    \n    # Calculate spatial indices\n    hw_start = pid_hw * BLOCK_SIZE\n    offsets = hw_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < (out_height * out_width)\n    \n    out_h = offsets // out_width\n    out_w = offsets % out_width\n    \n    # Calculate input window start positions\n    in_h_start = out_h * stride - padding\n    in_w_start = out_w * stride - padding\n    \n    # Get base pointers\n    x_bc_ptr = x_ptr + pid_bc * in_height * in_width\n    output_bc_ptr = output_ptr + pid_bc * out_height * out_width\n    \n    # Initialize accumulators\n    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    count = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Process kernel window - unroll loops for small kernel sizes\n    for kh in tl.static_range(kernel_size):\n        for kw in tl.static_range(kernel_size):\n            in_h = in_h_start + kh\n            in_w = in_w_start + kw\n            \n            # Boundary checks\n            valid_mask = mask & (in_h >= 0) & (in_h < in_height) & (in_w >= 0) & (in_w < in_width)\n            \n            # Compute offset and load values\n            in_offsets = in_h * in_width + in_w\n            values = tl.load(x_bc_ptr + in_offsets, mask=valid_mask, other=0.0)\n            \n            # Accumulate\n            total += values\n            count += tl.where(valid_mask, 1.0, 0.0)\n    \n    # Compute average and store\n    avg = tl.where(count > 0, total / count, 0.0)\n    tl.store(output_bc_ptr + offsets, avg, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, in_height, in_width = x.shape\n        out_height = (in_height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_width = (in_width + 2*self.padding - self.kernel_size) // self.stride + 1\n        \n        # Ensure contiguous memory access\n        x = x.contiguous()\n        output = torch.empty(\n            (batch_size, channels, out_height, out_width),\n            device=x.device, dtype=x.dtype\n        )\n        \n        # Optimized block size based on output dimensions\n        BLOCK_SIZE = 128\n        grid = (\n            batch_size * channels, \n            triton.cdiv(out_height * out_width, BLOCK_SIZE)\n        )\n        \n        avg_pool2d_kernel[grid](\n            x, output,\n            in_height, in_width,\n            out_height, out_width,\n            self.stride,\n            self.kernel_size,\n            self.padding,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 64}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8, num_stages=2),\n    ],\n    key=['out_height', 'out_width']\n)\n@triton.jit\ndef avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    in_height,\n    in_width,\n    out_height,\n    out_width,\n    stride_h: tl.constexpr,\n    stride_w: tl.constexpr,\n    kernel_h: tl.constexpr,\n    kernel_w: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_bc = tl.program_id(0)\n    pid_hw = tl.program_id(1)\n    \n    hw_start = pid_hw * BLOCK_SIZE\n    offsets = hw_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < (out_height * out_width)\n    \n    out_h = offsets // out_width\n    out_w = offsets % out_width\n    \n    in_h_start = out_h * stride_h\n    in_w_start = out_w * stride_w\n    \n    x_bc_ptr = x_ptr + pid_bc * in_height * in_width\n    output_bc_ptr = output_ptr + pid_bc * out_height * out_width\n    \n    window_area = kernel_h * kernel_w\n    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    for kh in tl.static_range(kernel_h):\n        for kw in tl.static_range(kernel_w):\n            in_h = in_h_start + kh\n            in_w = in_w_start + kw\n            \n            in_ptr = x_bc_ptr + in_h * in_width + in_w\n            values = tl.load(in_ptr, mask=mask & (in_h < in_height) & (in_w < in_width), other=0.0)\n            total += values\n    \n    avg = total / window_area\n    tl.store(output_bc_ptr + offsets, avg, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.padding != 0:\n            x = torch.nn.functional.pad(x, (self.padding, self.padding, self.padding, self.padding))\n            \n        batch_size, channels, in_height, in_width = x.shape\n        out_height = (in_height - self.kernel_size) // self.stride + 1\n        out_width = (in_width - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty((batch_size, channels, out_height, out_width), \n                             device=x.device, dtype=x.dtype)\n        \n        grid = lambda meta: (\n            batch_size * channels, \n            triton.cdiv(out_height * out_width, meta['BLOCK_SIZE'])\n        )\n        \n        avg_pool2d_kernel[grid](\n            x, output,\n            in_height, in_width,\n            out_height, out_width,\n            self.stride, self.stride,\n            self.kernel_size, self.kernel_size,\n            BLOCK_SIZE=triton.next_power_of_2(out_height * out_width)\n        )\n        \n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    batch_size, channels, height, width,\n    out_height, out_width,\n    stride, padding,\n    stride_b, stride_c, stride_h, stride_w,\n    total_elements,\n    kernel_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= total_elements:\n        return\n\n    w_out = pid % out_width\n    h_out = (pid // out_width) % out_height\n    c_out = (pid // (out_width * out_height)) % channels\n    b = pid // (out_width * out_height * channels)\n\n    h_start = h_out * stride - padding\n    w_start = w_out * stride - padding\n\n    total = 0.0\n    count = 0\n\n    for kh in tl.static_range(kernel_size):\n        for kw in tl.static_range(kernel_size):\n            h = h_start + kh\n            w = w_start + kw\n            mask = (h >= 0) & (h < height) & (w >= 0) & (w < width)\n            off_b = b * stride_b\n            off_c = c_out * stride_c\n            off_h = h * stride_h\n            off_w = w * stride_w\n            off = off_b + off_c + off_h + off_w\n            val = tl.load(input_ptr + off, mask=mask, other=0.0)\n            total += val\n            count += tl.where(mask, 1, 0)\n\n    if count == 0:\n        average = 0.0\n    else:\n        average = total / count\n\n    tl.store(output_ptr + pid, average)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, height, width = x.shape\n        out_height = (height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_width = (width + 2*self.padding - self.kernel_size) // self.stride + 1\n        output = torch.empty((batch_size, channels, out_height, out_width), \n                             device=x.device, dtype=x.dtype).contiguous()\n\n        stride_b = x.stride(0)\n        stride_c = x.stride(1)\n        stride_h = x.stride(2)\n        stride_w = x.stride(3)\n        total_elements = batch_size * channels * out_height * out_width\n\n        grid = lambda meta: (total_elements,)\n        _avg_pool2d_kernel[grid](\n            x, output,\n            batch_size, channels, height, width,\n            out_height, out_width,\n            self.stride, self.padding,\n            stride_b, stride_c, stride_h, stride_w,\n            total_elements,\n            self.kernel_size,\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n    ],\n    key=['total_output_elements'],\n)\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    batch_size,\n    channels,\n    height,\n    width,\n    out_h,\n    out_w,\n    stride,\n    padding,\n    kernel_size: tl.constexpr,\n    total_output_elements: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    idx = tl.arange(0, BLOCK_SIZE)\n    output_index = pid * BLOCK_SIZE + idx\n    mask = output_index < total_output_elements\n    \n    # Compute output tensor strides\n    S = channels * out_h * out_w\n    C_stride = out_h * out_w\n    H_stride = out_w\n    \n    # Unravel output indices\n    batch_i = tl.where(mask, output_index // S, 0)\n    channel_i = tl.where(mask, (output_index % S) // C_stride, 0)\n    pos_2d = tl.where(mask, output_index % C_stride, 0)\n    out_h_i = tl.where(mask, pos_2d // H_stride, 0)\n    out_w_i = tl.where(mask, pos_2d % H_stride, 0)\n    \n    # Compute input window\n    h_start = out_h_i * stride - padding\n    w_start = out_w_i * stride - padding\n    \n    # Input tensor strides\n    x_batch_stride = channels * height * width\n    x_channel_stride = height * width\n    x_base = x_ptr + batch_i * x_batch_stride + channel_i * x_channel_stride\n    \n    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    area = kernel_size * kernel_size\n    \n    # Unroll kernel loops for better performance\n    for kh in tl.static_range(kernel_size):\n        h = h_start + kh\n        h_valid = (h >= 0) & (h < height)\n        \n        for kw in tl.static_range(kernel_size):\n            w = w_start + kw\n            within_bounds = h_valid & (w >= 0) & (w < width)\n            offset = h * width + w\n            value = tl.load(x_base + offset, mask=mask & within_bounds, other=0.0)\n            total += value\n\n    # Store averaged results\n    output_val = total / area\n    output_offset = batch_i * S + channel_i * C_stride + out_h_i * out_w + out_w_i\n    tl.store(output_ptr + output_offset, output_val, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        if stride is None:\n            stride = kernel_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, height, width = x.shape\n        out_h = (height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_w = (width + 2*self.padding - self.kernel_size) // self.stride + 1\n        total_output_elements = batch_size * channels * out_h * out_w\n        \n        output = torch.empty((batch_size, channels, out_h, out_w), \n                             device=x.device, dtype=x.dtype)\n        \n        if total_output_elements == 0:\n            return output\n            \n        grid = (triton.cdiv(total_output_elements, 256),)\n        \n        _avg_pool2d_kernel[grid](\n            x, output,\n            batch_size, channels, height, width,\n            out_h, out_w,\n            self.stride, self.padding, self.kernel_size,\n            total_output_elements,\n            BLOCK_SIZE=256,\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    in_height,\n    in_width,\n    out_height,\n    out_width,\n    stride: tl.constexpr,\n    kernel_size: tl.constexpr,\n    padding: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_bc = tl.program_id(0)\n    pid_hw = tl.program_id(1)\n    \n    # Calculate spatial indices\n    hw_start = pid_hw * BLOCK_SIZE\n    offsets = hw_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < (out_height * out_width)\n    \n    out_h = offsets // out_width\n    out_w = offsets % out_width\n    \n    # Calculate input window start positions\n    in_h_start = out_h * stride - padding\n    in_w_start = out_w * stride - padding\n    \n    # Get base pointers\n    x_bc_ptr = x_ptr + pid_bc * in_height * in_width\n    output_bc_ptr = output_ptr + pid_bc * out_height * out_width\n    \n    # Initialize accumulators\n    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    count = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Process kernel window - unroll loops for small kernel sizes\n    for kh in tl.static_range(kernel_size):\n        for kw in tl.static_range(kernel_size):\n            in_h = in_h_start + kh\n            in_w = in_w_start + kw\n            \n            # Boundary checks\n            valid_mask = mask & (in_h >= 0) & (in_h < in_height) & (in_w >= 0) & (in_w < in_width)\n            \n            # Compute offset and load values\n            in_offsets = in_h * in_width + in_w\n            values = tl.load(x_bc_ptr + in_offsets, mask=valid_mask, other=0.0)\n            \n            # Accumulate\n            total += values\n            count += tl.where(valid_mask, 1.0, 0.0)\n    \n    # Compute average and store\n    avg = tl.where(count > 0, total / count, 0.0)\n    tl.store(output_bc_ptr + offsets, avg, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, in_height, in_width = x.shape\n        out_height = (in_height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_width = (in_width + 2*self.padding - self.kernel_size) // self.stride + 1\n        \n        # Ensure contiguous memory access\n        x = x.contiguous()\n        output = torch.empty(\n            (batch_size, channels, out_height, out_width),\n            device=x.device, dtype=x.dtype\n        )\n        \n        # Optimized block size based on output dimensions\n        BLOCK_SIZE = 128\n        grid = (\n            batch_size * channels, \n            triton.cdiv(out_height * out_width, BLOCK_SIZE)\n        )\n        \n        avg_pool2d_kernel[grid](\n            x, output,\n            in_height, in_width,\n            out_height, out_width,\n            self.stride,\n            self.kernel_size,\n            self.padding,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    kernel_size,\n    stride,\n    padding,\n    h_in,\n    w_in,\n    h_out,\n    w_out,\n    batch_size,\n    channels,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    total_output = batch_size * channels * h_out * w_out\n    if pid >= total_output:\n        return\n\n    per_batch = channels * h_out * w_out\n    b = pid // per_batch\n    pid_in_batch = pid % per_batch\n    c = pid_in_batch // (h_out * w_out)\n    pid_in_channel = pid_in_batch % (h_out * w_out)\n    oh = pid_in_channel // w_out\n    ow = pid_in_channel % w_out\n\n    start_h = oh * stride - padding\n    start_w = ow * stride - padding\n    base_ptr = x_ptr + b * (channels * h_in * w_in) + c * (h_in * w_in)\n    \n    total = 0.0\n    for kh in range(0, kernel_size):\n        h_index = start_h + kh\n        for kw in range(0, kernel_size):\n            w_index = start_w + kw\n            if h_index >= 0 and h_index < h_in and w_index >=0 and w_index < w_in:\n                offset = h_index * w_in + w_index\n                val = tl.load(x_ptr + base_ptr + offset)\n            else:\n                val = 0.0\n            total += val\n\n    total /= kernel_size * kernel_size\n    out_base_ptr = output_ptr + b * (channels * h_out * w_out) + c * (h_out * w_out)\n    out_offset = oh * w_out + ow\n    tl.store(out_base_ptr + out_offset, total)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.contiguous()\n        h_in, w_in = x.shape[2], x.shape[3]\n        h_out = (h_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        w_out = (w_in + 2 * self.padding - self.kernel_size) // self.stride + 1\n        \n        output = torch.empty(\n            x.shape[0], x.shape[1], h_out, w_out, \n            dtype=x.dtype, device=x.device\n        )\n        \n        total_elements = output.numel()\n        grid = lambda meta: (total_elements,)\n        _avg_pool2d_kernel[grid](\n            x, output,\n            self.kernel_size,\n            self.stride,\n            self.padding,\n            h_in,\n            w_in,\n            h_out,\n            w_out,\n            x.shape[0],\n            x.shape[1],\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs 2D Average Pooling.\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        \"\"\"\n        Initializes the Average Pooling layer.\n\n        Args:\n            kernel_size (int): Size of the pooling window.\n            stride (int, optional): Stride of the pooling operation. Defaults to None (same as kernel_size).\n            padding (int, optional): Padding applied to the input tensor. Defaults to 0.\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.avg_pool = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies 2D Average Pooling to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with Average Pooling applied.\n        \"\"\"\n        return self.avg_pool(x)\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nUnique approach: Excellent compiled (1.000), Excellent correctness (1.000), Alternative passed approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    batch_size,\n    channels,\n    height,\n    width,\n    out_h,\n    out_w,\n    stride,\n    padding,\n    kernel_size: tl.constexpr,\n    total_output_elements: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    idx = tl.arange(0, BLOCK_SIZE)\n    output_index = pid * BLOCK_SIZE + idx\n    mask = output_index < total_output_elements\n    \n    # Compute output tensor strides\n    S = channels * out_h * out_w\n    C_stride = out_h * out_w\n    H_stride = out_w\n    \n    # Unravel output indices\n    batch_i = tl.where(mask, output_index // S, 0)\n    channel_i = tl.where(mask, (output_index % S) // C_stride, 0)\n    pos_2d = tl.where(mask, output_index % C_stride, 0)\n    out_h_i = tl.where(mask, pos_2d // H_stride, 0)\n    out_w_i = tl.where(mask, pos_2d % H_stride, 0)\n    \n    # Compute input window\n    h_start = out_h_i * stride - padding\n    w_start = out_w_i * stride - padding\n    \n    # Input tensor strides\n    x_batch_stride = channels * height * width\n    x_channel_stride = height * width\n    x_base = x_ptr + batch_i * x_batch_stride + channel_i * x_channel_stride\n    \n    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    count = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Unroll loops for kernel processing\n    for kh in tl.static_range(0, kernel_size):\n        for kw in tl.static_range(0, kernel_size):\n            h = h_start + kh\n            w = w_start + kw\n            within_bounds = (h >= 0) & (h < height) & (w >= 0) & (w < width)\n            offset = h * width + w\n            cond = mask & within_bounds\n            value = tl.load(x_base + offset, mask=cond, other=0.0)\n            total += value\n            count += tl.where(cond, 1.0, 0.0)\n\n    # Compute average and avoid division by zero\n    output_val = tl.where(count > 0, total / count, 0.0)\n    output_offset = batch_i * S + channel_i * C_stride + out_h_i * out_w + out_w_i\n    tl.store(output_ptr + output_offset, output_val, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        if stride is None:\n            stride = kernel_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, height, width = x.shape\n        out_h = (height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_w = (width + 2*self.padding - self.kernel_size) // self.stride + 1\n        total_output_elements = batch_size * channels * out_h * out_w\n        \n        output = torch.empty((batch_size, channels, out_h, out_w), \n                             device=x.device, dtype=x.dtype)\n        \n        if total_output_elements == 0:\n            return output\n            \n        BLOCK_SIZE = 256  # Increased block size for better utilization\n        grid = (triton.cdiv(total_output_elements, BLOCK_SIZE),)\n        \n        _avg_pool2d_kernel[grid](\n            x, output,\n            batch_size, channels, height, width,\n            out_h, out_w,\n            self.stride, self.padding, self.kernel_size,\n            total_output_elements,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program that is performing average pooling 2D using Triton.\n The current performance: speed_up = 0.147 (which is less than 1, meaning slower than the reference)\n The reference timing is 0.02 ms and our custom_timing is 0.136 ms.\n\n The current implementation uses a 1D grid with each block processing 256 elements (BLOCK_SIZE=256).\n However, the kernel has nested loops for the kernel window (3x3) and uses a lot of masking and index calculations.\n\n We note that the current kernel is processing each output element independently, but the way it loads data\n might be inefficient because the input data is not being accessed in a coalesced manner.\n\n We have seen in the top-performing programs and diverse programs that using a 2D grid (with one dimension for batch*channels and the other for spatial blocks) \n can lead to better performance. Also, using a block of consecutive output elements (in the spatial dimension) for each program might improve memory coalescing.\n\n Let's try to change the kernel to use a 2D grid:\n   - The first grid dimension: batch * channels (so each block handles one channel for one batch)\n   - The second grid dimension: number of blocks needed to cover the entire spatial output (out_h * out_w)\n\n We will set a BLOCK_SIZE for the spatial dimension (e.g., 128, 256) and then have each block process a contiguous set of BLOCK_SIZE spatial positions.\n\n Steps in the kernel:\n   - pid_bc = program_id(0)  [which batch and channel]\n   - pid_hw = program_id(1)  [which spatial block]\n\n   - Compute the starting spatial index for the block: hw_start = pid_hw * BLOCK_SIZE\n   - Then within the block, we have an offset for each element: offsets = tl.arange(0, BLOCK_SIZE)\n   - We compute the mask for valid elements (within the spatial output size)\n\n   - Then we compute the output spatial indices: \n        out_h = (hw_start + offsets) // out_w\n        out_w_i = (hw_start + offsets) % out_w\n\n   - For each spatial output element in the block, we compute the input window and accumulate.\n\n This approach should improve memory access patterns because consecutive threads in the block will access consecutive spatial positions, which are stored contiguously.\n\n We also note that the kernel_size is small (3x3) so we can unroll the loops.\n\n We are going to adapt the kernel from \"Program 2\" in the top-performing list (which had a speed_up of 0.139) and try to improve it.\n\n However, note that the current top performer (Program 3) in the list has a speed_up of 0.147 (same as our current) but we are going to try a different approach.\n\n Let's try the following:\n\n   We change the kernel to use 2D grid and process blocks of spatial positions per channel per batch.\n\n   We will also precompute the input and output base pointers for the current batch and channel.\n\n   We avoid the complex unraveling of the 1D index.\n\n   We note that the current program does not handle padding by pre-padding the input. Instead, it handles padding in the kernel.\n   We are going to handle padding in the kernel as well.\n\n   We are going to use:\n\n        in_h_start = out_h_i * stride - padding\n        in_w_start = out_w_i * stride - padding\n\n   and then loop over the kernel window, checking boundaries.\n\n   We accumulate the sum and the count (for average) only over the valid positions.\n\n   We then store the average.\n\n We choose a BLOCK_SIZE that is a power of two and set to 128 or 256. We can also try autotuning later.\n\n We are going to write a new kernel and replace the current one.\n\n We note the constraints:\n   - Do not change the function signature of the forward method or the class structure.\n\n Let's write the kernel:\n\n   @triton.jit\n   def avg_pool2d_kernel(\n        x_ptr,\n        output_ptr,\n        in_height, in_width,\n        out_height, out_width,\n        stride, padding,\n        kernel_size: tl.constexpr,\n        BLOCK_SIZE: tl.constexpr,\n   ):\n\n        pid_bc = tl.program_id(0)   # index for batch*channel\n        pid_hw = tl.program_id(1)   # index for spatial block\n\n        # spatial indices for the block\n        hw_start = pid_hw * BLOCK_SIZE\n        offsets = tl.arange(0, BLOCK_SIZE)\n        hw = hw_start + offsets\n        mask = hw < (out_height * out_width)\n\n        # Compute the 2D spatial indices (out_h, out_w) for the block\n        out_h = hw // out_width\n        out_w = hw % out_width\n\n        # Input window start\n        in_h_start = out_h * stride - padding\n        in_w_start = out_w * stride - padding\n\n        # Base pointer for the current batch and channel\n        x_bc_ptr = x_ptr + pid_bc * in_height * in_width\n        output_bc_ptr = output_ptr + pid_bc * out_height * out_width\n\n        total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        count = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n        # Loop over kernel window\n        for kh in tl.static_range(0, kernel_size):\n            for kw in tl.static_range(0, kernel_size):\n                in_h = in_h_start + kh\n                in_w = in_w_start + kw\n\n                # Check boundaries\n                valid_mask = mask & (in_h >= 0) & (in_h < in_height) & (in_w >= 0) & (in_w < in_width)\n\n                # Compute the offset in the input for the current window element\n                in_offsets = in_h * in_width + in_w\n                value = tl.load(x_bc_ptr + in_offsets, mask=valid_mask, other=0.0)\n\n                total += value\n                count += tl.where(valid_mask, 1.0, 0.0)\n\n        # Compute average\n        avg = tl.where(count > 0, total / count, 0.0)\n\n        # Store the block of results\n        tl.store(output_bc_ptr + hw, avg, mask=mask)\n\n Then in the forward method:\n\n        batch_size, channels, in_height, in_width = x.shape\n        out_h = ... ; out_w = ... \n        total_spatial = out_h * out_w\n\n        # We are going to set grid = (batch_size * channels, triton.cdiv(total_spatial, BLOCK_SIZE))\n        BLOCK_SIZE = 128   # we can try 128 or 256, but let's start with 128\n\n        # We must ensure x is contiguous? We will do: x = x.contiguous()\n        # Also, output must be contiguous? We are creating it as empty and then storing in order.\n\n        kernel_meta = {\n            'in_height': in_height, 'in_width': in_width,\n            'out_height': out_h, 'out_width': out_w,\n            'stride': self.stride, 'padding': self.padding,\n            'kernel_size': self.kernel_size,\n            'BLOCK_SIZE': BLOCK_SIZE,\n        }\n\n        grid = (batch_size * channels, triton.cdiv(total_spatial, BLOCK_SIZE))\n\n        avg_pool2d_kernel[grid](\n            x, output,\n            **kernel_meta   # but we have to pass as arguments in order\n        )\n\n But note: the kernel parameters are passed as arguments. We have to match the signature.\n\n Alternatively, we can pass them one by one.\n\n Let me adjust the kernel call:\n\n        avg_pool2d_kernel[grid](\n            x, output,\n            in_height, in_width,\n            out_h, out_w,\n            self.stride, self.padding,\n            self.kernel_size,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n\n However, note that the kernel also has the BLOCK_SIZE as a constexpr.\n\n We are going to change the class and the kernel accordingly.\n\n We also note that the current program does not pre-pad the input. We are handling the padding in the kernel.\n\n We are going to replace the entire kernel and the grid setup.\n\n Let's write the new code accordingly.\n\n IMPORTANT: We are only allowed to change the Triton kernel and the code within the EVOLVE-BLOCK.\n\n We are going to replace the current kernel with the 2D grid kernel and adjust the grid in the forward pass.\n\n We hope that this will improve the memory access pattern and thus the speed.\n\n Note: The current program had a BLOCK_SIZE of 256 for the 1D grid. We are now using a 2D grid with a BLOCK_SIZE of 128 for the spatial dimension.\n\n We'll try with BLOCK_SIZE=128.\n\n If we see that the performance is still not good, we might try autotuning in a future iteration.\n\n Let's write the code accordingly.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    in_height,\n    in_width,\n    out_height,\n    out_width,\n    stride: tl.constexpr,\n    padding: tl.constexpr,\n    kernel_size: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_bc = tl.program_id(0)\n    pid_hw = tl.program_id(1)\n    \n    # Calculate spatial indices\n    hw_start = pid_hw * BLOCK_SIZE\n    offsets = hw_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < (out_height * out_width)\n    \n    out_h = offsets // out_width\n    out_w = offsets % out_width\n    \n    # Calculate input window start positions\n    in_h_start = out_h * stride - padding\n    in_w_start = out_w * stride - padding\n    \n    # Get base pointers\n    x_bc_ptr = x_ptr + pid_bc * in_height * in_width\n    output_bc_ptr = output_ptr + pid_bc * out_height * out_width\n    \n    # Initialize accumulators\n    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    count = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Process kernel window - unroll loops for small kernel sizes\n    for kh in tl.static_range(kernel_size):\n        for kw in tl.static_range(kernel_size):\n            in_h = in_h_start + kh\n            in_w = in_w_start + kw\n            \n            # Boundary checks\n            valid_mask = mask & (in_h >= 0) & (in_h < in_height) & (in_w >= 0) & (in_w < in_width)\n            \n            # Compute offset and load values\n            in_offsets = in_h * in_width + in_w\n            values = tl.load(x_bc_ptr + in_offsets, mask=valid_mask, other=0.0)\n            \n            # Accumulate\n            total += values\n            count += tl.where(valid_mask, 1.0, 0.0)\n    \n    # Compute average and store\n    avg = tl.where(count > 0, total / count, 0.0)\n    tl.store(output_bc_ptr + offsets, avg, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, in_height, in_width = x.shape\n        out_height = (in_height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_width = (in_width + 2*self.padding - self.kernel_size) // self.stride + 1\n        \n        # Ensure contiguous memory access\n        x = x.contiguous()\n        output = torch.empty(\n            (batch_size, channels, out_height, out_width),\n            device=x.device, dtype=x.dtype\n        )\n        \n        # Optimized block size based on output dimensions\n        BLOCK_SIZE = 128\n        grid = (\n            batch_size * channels, \n            triton.cdiv(out_height * out_width, BLOCK_SIZE)\n        )\n        \n        avg_pool2d_kernel[grid](\n            x, output,\n            in_height, in_width,\n            out_height, out_width,\n            self.stride,\n            self.padding,\n            self.kernel_size,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
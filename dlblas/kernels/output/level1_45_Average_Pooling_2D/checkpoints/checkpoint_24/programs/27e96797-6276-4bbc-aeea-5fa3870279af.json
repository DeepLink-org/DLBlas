{"id": "27e96797-6276-4bbc-aeea-5fa3870279af", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    batch_size,\n    channels,\n    height,\n    width,\n    out_h,\n    out_w,\n    stride,\n    padding,\n    kernel_size: tl.constexpr,\n    total_output_elements: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    idx = tl.arange(0, BLOCK_SIZE)\n    output_index = pid * BLOCK_SIZE + idx\n    mask = output_index < total_output_elements\n    \n    # Compute output tensor strides\n    S = channels * out_h * out_w\n    C_stride = out_h * out_w\n    H_stride = out_w\n    \n    # Unravel output indices\n    batch_i = tl.where(mask, output_index // S, 0)\n    channel_i = tl.where(mask, (output_index % S) // C_stride, 0)\n    pos_2d = tl.where(mask, output_index % C_stride, 0)\n    out_h_i = tl.where(mask, pos_2d // H_stride, 0)\n    out_w_i = tl.where(mask, pos_2d % H_stride, 0)\n    \n    # Compute input window\n    h_start = out_h_i * stride - padding\n    w_start = out_w_i * stride - padding\n    \n    # Input tensor strides\n    x_batch_stride = channels * height * width\n    x_channel_stride = height * width\n    x_base = x_ptr + batch_i * x_batch_stride + channel_i * x_channel_stride\n    \n    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    count = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Unroll loops for kernel processing\n    for kh in tl.static_range(0, kernel_size):\n        for kw in tl.static_range(0, kernel_size):\n            h = h_start + kh\n            w = w_start + kw\n            within_bounds = (h >= 0) & (h < height) & (w >= 0) & (w < width)\n            offset = h * width + w\n            cond = mask & within_bounds\n            value = tl.load(x_base + offset, mask=cond, other=0.0)\n            total += value\n            count += tl.where(cond, 1.0, 0.0)\n\n    # Compute average and avoid division by zero\n    output_val = tl.where(count > 0, total / count, 0.0)\n    output_offset = batch_i * S + channel_i * C_stride + out_h_i * out_w + out_w_i\n    tl.store(output_ptr + output_offset, output_val, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        if stride is None:\n            stride = kernel_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, height, width = x.shape\n        out_h = (height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_w = (width + 2*self.padding - self.kernel_size) // self.stride + 1\n        total_output_elements = batch_size * channels * out_h * out_w\n        \n        output = torch.empty((batch_size, channels, out_h, out_w), \n                             device=x.device, dtype=x.dtype)\n        \n        if total_output_elements == 0:\n            return output\n            \n        BLOCK_SIZE = 256  # Increased block size for better utilization\n        grid = (triton.cdiv(total_output_elements, BLOCK_SIZE),)\n        \n        _avg_pool2d_kernel[grid](\n            x, output,\n            batch_size, channels, height, width,\n            out_h, out_w,\n            self.stride, self.padding, self.kernel_size,\n            total_output_elements,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "362b281d-5777-4585-9ab7-f5cf8cf326d2", "generation": 2, "timestamp": 1754643869.30037, "iteration_found": 10, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.147, "runtime_ms": 0.136, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.02, "speed_up": 0.147, "custom_timing": 0.136}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.148, "runtime_ms": 0.135, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.02, "speed_up": 0.148, "custom_timing": 0.135}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.1480\n- runtime_ms: 0.1350\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.02, 'speed_up': 0.148, 'custom_timing': 0.135}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.1480, runtime_ms: 0.1350, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.02, 'speed_up': 0.148, 'custom_timing': 0.135}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.0020, runtime_ms: 8.7800, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.02, 'speed_up': 0.002, 'custom_timing': 8.78}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    batch_size, channels, height, width,\n    out_height, out_width,\n    stride, padding,\n    stride_b, stride_c, stride_h, stride_w,\n    total_elements,\n    kernel_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= total_elements:\n        return\n\n    w_out = pid % out_width\n    h_out = (pid // out_width) % out_height\n    c_out = (pid // (out_width * out_height)) % channels\n    b = pid // (out_width * out_height * channels)\n\n    h_start = h_out * stride - padding\n    w_start = w_out * stride - padding\n\n    total = 0.0\n    count = 0\n\n    for kh in tl.static_range(kernel_size):\n        for kw in tl.static_range(kernel_size):\n            h = h_start + kh\n            w = w_start + kw\n            mask = (h >= 0) & (h < height) & (w >= 0) & (w < width)\n            off_b = b * stride_b\n            off_c = c_out * stride_c\n            off_h = h * stride_h\n            off_w = w * stride_w\n            off = off_b + off_c + off_h + off_w\n            val = tl.load(input_ptr + off, mask=mask, other=0.0)\n            total += val\n            count += tl.where(mask, 1, 0)\n\n    if count == 0:\n        average = 0.0\n    else:\n        average = total / count\n\n    tl.store(output_ptr + pid, average)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, height, width = x.shape\n        out_height = (height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_width = (width + 2*self.padding - self.kernel_size) // self.stride + 1\n        output = torch.empty((batch_size, channels, out_height, out_width), \n                             device=x.device, dtype=x.dtype).contiguous()\n\n        stride_b = x.stride(0)\n        stride_c = x.stride(1)\n        stride_h = x.stride(2)\n        stride_w = x.stride(3)\n        total_elements = batch_size * channels * out_height * out_width\n\n        grid = lambda meta: (total_elements,)\n        _avg_pool2d_kernel[grid](\n            x, output,\n            batch_size, channels, height, width,\n            out_height, out_width,\n            self.stride, self.padding,\n            stride_b, stride_c, stride_h, stride_w,\n            total_elements,\n            self.kernel_size,\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.0020), Performs well on runtime_ms (8.7800), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.02, 'speed_up': 0.002, 'custom_timing': 8.78})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    batch_size,\n    channels,\n    height,\n    width,\n    out_h,\n    out_w,\n    stride,\n    padding,\n    kernel_size: tl.constexpr,\n    total_output_elements: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    idx = tl.arange(0, BLOCK_SIZE)\n    output_index = pid * BLOCK_SIZE + idx\n    mask = output_index < total_output_elements\n    \n    # Compute output tensor strides\n    S = channels * out_h * out_w\n    C_stride = out_h * out_w\n    H_stride = out_w\n    \n    # Unravel output indices\n    batch_i = tl.where(mask, output_index // S, 0)\n    channel_i = tl.where(mask, (output_index % S) // C_stride, 0)\n    pos_2d = tl.where(mask, output_index % C_stride, 0)\n    out_h_i = tl.where(mask, pos_2d // H_stride, 0)\n    out_w_i = tl.where(mask, pos_2d % H_stride, 0)\n    \n    # Compute input window\n    h_start = out_h_i * stride - padding\n    w_start = out_w_i * stride - padding\n    \n    # Input tensor strides\n    x_batch_stride = channels * height * width\n    x_channel_stride = height * width\n    x_base = x_ptr + batch_i * x_batch_stride + channel_i * x_channel_stride\n    \n    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    area = kernel_size * kernel_size\n    \n    # Optimized for 3x3 kernel\n    for kh in range(0, kernel_size):\n        for kw in range(0, kernel_size):\n            h = h_start + kh\n            w = w_start + kw\n            within_bounds = (h >= 0) & (h < height) & (w >= 0) & (w < width)\n            offset = h * width + w\n            value = tl.load(x_base + offset, mask=mask & within_bounds, other=0.0)\n            total += value\n\n    # Store averaged results\n    output_val = total / area\n    output_offset = batch_i * S + channel_i * C_stride + out_h_i * out_w + out_w_i\n    tl.store(output_ptr + output_offset, output_val, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        if stride is None:\n            stride = kernel_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, height, width = x.shape\n        out_h = (height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_w = (width + 2*self.padding - self.kernel_size) // self.stride + 1\n        total_output_elements = batch_size * channels * out_h * out_w\n        \n        output = torch.empty((batch_size, channels, out_h, out_w), \n                             device=x.device, dtype=x.dtype)\n        \n        if total_output_elements == 0:\n            return output\n            \n        BLOCK_SIZE = 128\n        grid = (triton.cdiv(total_output_elements, BLOCK_SIZE),)\n        \n        _avg_pool2d_kernel[grid](\n            x, output,\n            batch_size, channels, height, width,\n            out_h, out_w,\n            self.stride, self.padding, self.kernel_size,\n            total_output_elements,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.1480), Performs well on runtime_ms (0.1350), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.02, 'speed_up': 0.148, 'custom_timing': 0.135})\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    batch_size, channels, height, width,\n    out_height, out_width,\n    stride, padding,\n    stride_b, stride_c, stride_h, stride_w,\n    total_elements,\n    kernel_size: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= total_elements:\n        return\n\n    w_out = pid % out_width\n    h_out = (pid // out_width) % out_height\n    c_out = (pid // (out_width * out_height)) % channels\n    b = pid // (out_width * out_height * channels)\n\n    h_start = h_out * stride - padding\n    w_start = w_out * stride - padding\n\n    total = 0.0\n    count = 0\n\n    for kh in tl.static_range(kernel_size):\n        for kw in tl.static_range(kernel_size):\n            h = h_start + kh\n            w = w_start + kw\n            mask = (h >= 0) & (h < height) & (w >= 0) & (w < width)\n            off_b = b * stride_b\n            off_c = c_out * stride_c\n            off_h = h * stride_h\n            off_w = w * stride_w\n            off = off_b + off_c + off_h + off_w\n            val = tl.load(input_ptr + off, mask=mask, other=0.0)\n            total += val\n            count += tl.where(mask, 1, 0)\n\n    if count == 0:\n        average = 0.0\n    else:\n        average = total / count\n\n    tl.store(output_ptr + pid, average)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride if stride is not None else kernel_size\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, height, width = x.shape\n        out_height = (height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_width = (width + 2*self.padding - self.kernel_size) // self.stride + 1\n        output = torch.empty((batch_size, channels, out_height, out_width), \n                             device=x.device, dtype=x.dtype).contiguous()\n\n        stride_b = x.stride(0)\n        stride_c = x.stride(1)\n        stride_h = x.stride(2)\n        stride_w = x.stride(3)\n        total_elements = batch_size * channels * out_height * out_width\n\n        grid = lambda meta: (total_elements,)\n        _avg_pool2d_kernel[grid](\n            x, output,\n            batch_size, channels, height, width,\n            out_height, out_width,\n            self.stride, self.padding,\n            stride_b, stride_c, stride_h, stride_w,\n            total_elements,\n            self.kernel_size,\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    batch_size,\n    channels,\n    height,\n    width,\n    out_h,\n    out_w,\n    stride,\n    padding,\n    kernel_size: tl.constexpr,\n    total_output_elements: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    idx = tl.arange(0, BLOCK_SIZE)\n    output_index = pid * BLOCK_SIZE + idx\n    mask = output_index < total_output_elements\n    \n    # Compute output tensor strides\n    S = channels * out_h * out_w\n    C_stride = out_h * out_w\n    H_stride = out_w\n    \n    # Unravel output indices\n    batch_i = tl.where(mask, output_index // S, 0)\n    channel_i = tl.where(mask, (output_index % S) // C_stride, 0)\n    pos_2d = tl.where(mask, output_index % C_stride, 0)\n    out_h_i = tl.where(mask, pos_2d // H_stride, 0)\n    out_w_i = tl.where(mask, pos_2d % H_stride, 0)\n    \n    # Compute input window\n    h_start = out_h_i * stride - padding\n    w_start = out_w_i * stride - padding\n    \n    # Input tensor strides\n    x_batch_stride = channels * height * width\n    x_channel_stride = height * width\n    x_base = x_ptr + batch_i * x_batch_stride + channel_i * x_channel_stride\n    \n    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    area = kernel_size * kernel_size\n    \n    # Optimized for 3x3 kernel\n    for kh in range(0, kernel_size):\n        for kw in range(0, kernel_size):\n            h = h_start + kh\n            w = w_start + kw\n            within_bounds = (h >= 0) & (h < height) & (w >= 0) & (w < width)\n            offset = h * width + w\n            value = tl.load(x_base + offset, mask=mask & within_bounds, other=0.0)\n            total += value\n\n    # Store averaged results\n    output_val = total / area\n    output_offset = batch_i * S + channel_i * C_stride + out_h_i * out_w + out_w_i\n    tl.store(output_ptr + output_offset, output_val, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        if stride is None:\n            stride = kernel_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, height, width = x.shape\n        out_h = (height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_w = (width + 2*self.padding - self.kernel_size) // self.stride + 1\n        total_output_elements = batch_size * channels * out_h * out_w\n        \n        output = torch.empty((batch_size, channels, out_h, out_w), \n                             device=x.device, dtype=x.dtype)\n        \n        if total_output_elements == 0:\n            return output\n            \n        BLOCK_SIZE = 128\n        grid = (triton.cdiv(total_output_elements, BLOCK_SIZE),)\n        \n        _avg_pool2d_kernel[grid](\n            x, output,\n            batch_size, channels, height, width,\n            out_h, out_w,\n            self.stride, self.padding, self.kernel_size,\n            total_output_elements,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a Triton kernel for average pooling 2D. The current implementation has a speed_up of 0.148 (which is less than 1, meaning it's slower than the reference). We need to improve the performance.\n\n Key observations from the current kernel:\n  - It uses a block size of 128 and processes multiple output elements per program.\n  - The kernel unravels the output index to (batch, channel, out_h, out_w) for each element in the block.\n  - For each output element, it iterates over a kernel_size x kernel_size window in the input.\n  - The input is loaded with a mask that checks the input bounds and the block mask.\n\n However, the current performance is poor. We note that the reference PyTorch implementation is very fast (0.02 ms) and our Triton kernel is 0.135 ms (so about 6.75x slower).\n\n Let's analyze the current kernel:\n\n  The main loop is over the kernel (for kh in range(0, kernel_size): for kw in ...). For a 3x3 kernel, that's 9 iterations per output element. And each iteration does:\n      h = h_start + kh\n      w = w_start + kw\n      within_bounds = ... (a mask for the input coordinates)\n      offset = h * width + w\n      value = tl.load(x_base + offset, mask=mask & within_bounds, other=0.0)\n\n  Then we sum and divide by the area (which is 9 for 3x3).\n\n The problem:\n  - The loads are not coalesced. Each output element in the block may be at a different (h, w) in the input, so the memory access pattern is scattered.\n\n How to improve?\n\n  We note that average pooling is a local operation. We can try to change the way we map the problem to threads.\n\n  Alternative approach:\n   - Instead of having each block process a contiguous set of output elements (which may be in different channels and batch), we can have each block process a block of output in the spatial dimensions for a fixed batch and channel.\n\n  However, note the constraints:\n   - We cannot change the grid configuration (the grid is set by the host code) and the kernel signature (including the order of parameters) is fixed.\n\n  But note: the grid is defined as (triton.cdiv(total_output_elements, BLOCK_SIZE),). So the kernel is launched with 1D grid.\n\n  We can change the internal mapping of the program ID to output coordinates. Currently, we are using:\n\n      output_index = pid * BLOCK_SIZE + idx\n      ... then unravel to (batch_i, channel_i, out_h_i, out_w_i)\n\n  This leads to non-coalesced access because adjacent output indices in the same block may be in different spatial positions and channels.\n\n  We can try to rearrange the output tensor in a way that adjacent threads in a block are processing adjacent elements in the same channel and in the same spatial region? But note that the output tensor is in the standard layout (batch, channels, height, width).\n\n  Another idea: use a different blocking strategy. Instead of having the block as a 1D contiguous array of output elements, we can have the block cover a 2D tile in the spatial dimensions (out_h, out_w) for a fixed batch and channel. Then we can use shared memory to load a tile of the input that covers the pooling windows for the output tile.\n\n  However, note that the kernel is for average pooling and the kernel size is small (3x3 in the example). So we can load a block of input that covers the output tile and the padding.\n\n  Steps for a tile-based approach:\n\n   1. We assign each program to a block of output in the spatial dimensions (out_h, out_w) for a fixed batch and channel.\n\n   2. We calculate the required input region for that block. The input region would be:\n          h_start = (out_h_start * stride - padding) to (out_h_end * stride - padding + kernel_size - 1)\n          similarly for w.\n\n   3. We load the entire input region for that block into shared memory (or registers if small enough). Then we compute the average for each output element in the tile.\n\n  But note: the kernel must be generic for any kernel_size. However, in the example we have kernel_size=3, so the input region for a tile of [BLOCK_H, BLOCK_W] output elements would be of size [BLOCK_H*stride + kernel_size-1, BLOCK_W*stride + kernel_size-1]. If we choose BLOCK_H and BLOCK_W appropriately, we can fit the input tile in shared memory.\n\n  However, the current constraints of the problem say we cannot change the grid configuration? Actually, the grid is defined in the host code and we are allowed to change the kernel as long as we don't change the grid configuration? But note the constraint:\n\n      \u274c NEVER modify grid configuration, output tensor shapes, or PID logic\n\n  This is a bit ambiguous. The constraint says \"NEVER modify grid configuration\", meaning we cannot change the way the grid is computed? But note the grid is computed as:\n\n        grid = (triton.cdiv(total_output_elements, BLOCK_SIZE),)\n\n  And we are going to change the BLOCK_SIZE? Actually, we are allowed to change the BLOCK_SIZE because it's a kernel parameter. However, the constraint says \"NEVER change @triton.jit function signatures or parameter names\". But we are not changing the signature of the kernel? We are only changing the internal implementation.\n\n  But note: the kernel currently has a parameter `BLOCK_SIZE` and we are using it. We can change the meaning of `BLOCK_SIZE`? Actually, the current kernel uses BLOCK_SIZE as the 1D block size. We want to change it to a 2D tile? That would require changing the kernel signature? We cannot change the signature.\n\n  Therefore, we must stick to the 1D block.\n\n  Alternative: we can reinterpret the 1D block as a 2D tile by having:\n\n      BLOCK_SIZE = BLOCK_H * BLOCK_W\n\n  Then we can have:\n\n      idx = tl.arange(0, BLOCK_SIZE)\n      idx_h = idx // BLOCK_W\n      idx_w = idx % BLOCK_W\n\n  But note: the block is 1D, so the threads are in a 1D array. We can do this.\n\n  Then we can assign each program to a batch and channel, and then the block will process a tile of [BLOCK_H, BLOCK_W] in the output spatial dimensions.\n\n  How to assign the program ID?\n\n      total_programs = (batch_size * channels * triton.cdiv(out_h, BLOCK_H) * triton.cdiv(out_w, BLOCK_W))\n\n  But wait: our grid is 1D and is set to (triton.cdiv(total_output_elements, BLOCK_SIZE),). So we cannot change the grid. Therefore, we must map the 1D grid to the 4D tensor in a way that is efficient.\n\n  We can do:\n\n      pid = tl.program_id(0)\n      total_programs = triton.cdiv(total_output_elements, BLOCK_SIZE)\n\n      # We want to assign:\n      #   program_id -> (b, c, tile_h, tile_w)\n      # where tile_h and tile_w are the starting indices in the output spatial dimensions.\n\n      # But note: total_output_elements = batch_size * channels * out_h * out_w\n\n      # We can compute:\n      #   elements_per_block = BLOCK_SIZE\n      #   block_idx = pid\n\n      # Then we can compute the global index of the first element in the block: block_idx * BLOCK_SIZE\n      # Then unravel that to (b, c, out_h, out_w) and then the block will cover a contiguous set of BLOCK_SIZE elements? But that is what the current kernel does.\n\n  We want to change the mapping to:\n\n      Let:\n          n_blocks_h = triton.cdiv(out_h, BLOCK_H)\n          n_blocks_w = triton.cdiv(out_w, BLOCK_W)\n          n_blocks_in_channel = n_blocks_h * n_blocks_w\n          total_blocks = batch_size * channels * n_blocks_in_channel\n\n      Then we set grid = (total_blocks,)\n\n      But note: we are constrained to grid = (triton.cdiv(total_output_elements, BLOCK_SIZE),). And BLOCK_SIZE = BLOCK_H * BLOCK_W.\n\n      Then total_blocks = batch_size * channels * n_blocks_in_channel = batch_size * channels * (ceil(out_h/BLOCK_H) * ceil(out_w/BLOCK_W))\n\n      And the total_output_elements = batch_size * channels * out_h * out_w\n\n      Then triton.cdiv(total_output_elements, BLOCK_SIZE) = ceil( (batch_size * channels * out_h * out_w) / (BLOCK_H * BLOCK_W) )\n\n      But note: ceil(out_h/BLOCK_H) * ceil(out_w/BLOCK_W) * (BLOCK_H * BLOCK_W) >= out_h * out_w\n\n      So the grid size we would use (total_blocks) is at least as large as ceil(total_output_elements/(BLOCK_H*BLOCK_W)).\n\n      Therefore, we can do:\n\n          grid = (ceil(total_output_elements / (BLOCK_H*BLOCK_W)), )\n\n      and then in the kernel, we can compute:\n\n          pid = tl.program_id(0)\n          block_start = pid * (BLOCK_H * BLOCK_W)\n          ... and then unravel block_start to (b, c, block_h_index, block_w_index) but note that we have to account for the fact that the grid might be larger than the actual number of blocks.\n\n      However, the grid is set to ceil(total_output_elements / BLOCK_SIZE) and we are setting BLOCK_SIZE = BLOCK_H * BLOCK_W.\n\n      This approach is acceptable because we are not changing the grid configuration (we are still using a 1D grid with the same formula) and we are not changing the host code.\n\n  Steps:\n\n      In the kernel:\n        pid = tl.program_id(0)\n        idx = tl.arange(0, BLOCK_SIZE)   # BLOCK_SIZE = BLOCK_H * BLOCK_W\n\n        # Compute the base index of the block in the output tensor:\n        block_start = pid * BLOCK_SIZE\n        # But note: the total number of programs is ceil(total_output_elements / BLOCK_SIZE). We must avoid processing beyond the output.\n\n        # Instead, we can compute the global output index for each thread: block_start + idx, and then mask if beyond total_output_elements.\n\n        # Now, we want to map each thread in the block to a spatial position in the output tile. We set:\n        idx_h = idx // BLOCK_W\n        idx_w = idx % BLOCK_W\n\n        # Then we unravel the block_start to get the batch, channel, and the base tile indices (tile_h0, tile_w0) in the output spatial dimensions.\n        # How? We note that the output tensor has dimensions [batch_size, channels, out_h, out_w]. We can compute:\n        #   n_elements_per_channel = out_h * out_w\n        #   n_elements_per_batch = channels * n_elements_per_channel\n        #   batch_i = block_start // n_elements_per_batch\n        #   remainder = block_start % n_elements_per_batch\n        #   channel_i = remainder // n_elements_per_channel\n        #   pos_in_channel = remainder % n_elements_per_channel\n        #   tile_h0 = (pos_in_channel // out_w) // BLOCK_H * BLOCK_H   # This is not straightforward.\n\n        This approach seems messy.\n\n  Alternatively, we can change the way the output index is computed to be in terms of tiles:\n\n        Let:\n          tile_i = pid\n          tiles_per_channel = (out_h + BLOCK_H - 1) // BLOCK_H * (out_w + BLOCK_W - 1) // BLOCK_W\n          total_tiles = batch_size * channels * tiles_per_channel\n\n        But note: our grid is ceil(total_output_elements / (BLOCK_H * BLOCK_W)), which is not necessarily equal to total_tiles? Actually, total_tiles might be larger because we are tiling.\n\n        This is getting complicated.\n\n  Given the constraints, we might not be able to change the mapping without changing the grid.\n\n  Let's stick to the current mapping but try to improve the memory access pattern by loading the input data for the entire block in a coalesced manner? That is, we can precompute the input indices for the entire block and then try to load them in a coalesced pattern? This is difficult.\n\n  Another idea: use shared memory to load the input data for the entire block? But the block size is 128, and each output element requires 9 input elements (for 3x3). So we would need 128*9 = 1152 elements. That might fit in shared memory (if we use 4 bytes per float, that's 4.5KB, which is acceptable).\n\n  Steps:\n\n     1. Each thread in the block computes the input indices for the 9 points it needs (for 3x3). Then we collect all the input indices that the block needs.\n\n     2. We then load the unique input indices in a coalesced manner (sort the indices? not practical in kernel).\n\n     3. Alternatively, we can have each thread load the data for its own window, but then we can cache the loaded data in shared memory to avoid redundant loads? But note that adjacent output elements in the block may share input elements.\n\n     4. We can load a block of input that covers all the windows for the output block. This requires that we first determine the min and max input indices in the h and w dimensions.\n\n     5. Then we load that entire input block into shared memory, and then each thread can read from shared memory.\n\n  How to compute the input block for the output block?\n\n     The output block is a contiguous set of output indices? But note: the output indices are unraveled in the order of the flattened tensor (batch, then channel, then height, then width). So the block might cover multiple channels and multiple spatial positions? Actually, the current unraveling:\n\n          batch_i = output_index // S\n          ... etc.\n\n        where S = channels * out_h * out_w.\n\n        So the block might cover multiple batches? Actually, no: because S is the size of one batch, so the batch index is the same for the entire block? Not necessarily: if the block spans more than S, then it would cover multiple batches. But note: we set BLOCK_SIZE=128, and S = channels * out_h * out_w. In the example, channels=64, out_h=out_w= (256-3)//1+1 = 254, so S = 64*254*254 = about 4.1 million, which is much larger than 128. So each block is entirely within one batch.\n\n        Similarly, the channel index: the block is within one channel? Not necessarily: because the next index is channel_i = (output_index % S) // (out_h * out_w). The block of 128 consecutive elements in the output tensor might cross a channel boundary? For example, if we are at the last few elements of a channel, then the next element is the next channel. So we cannot assume the entire block is in the same channel.\n\n  Therefore, the block might cover multiple channels and multiple spatial positions. This makes it difficult to load a shared memory block.\n\n  Due to the complexity and constraints, we might need to try a different approach.\n\n  Let's look at the top performing programs:\n\n      Program 1: uses a kernel without blocking (each thread one output element) and gets 0.002 speedup (which is even worse).\n\n      Program 2: the current program we are trying to improve.\n\n  The best speedup we have is 0.148 (which is about 15% of the reference speed). We want to get above 1.\n\n  Another idea: instead of having the inner loops over the kernel, unroll them and use vectorization? The kernel size is known at compile time (as a tl.constexpr). We can unroll the loops.\n\n  Change:\n\n      for kh in range(0, kernel_size):\n          for kw in range(0, kernel_size):\n\n      to:\n\n      for kh in tl.static_range(0, kernel_size):\n          for kw in tl.static_range(0, kernel_size):\n\n      This will unroll the loop and might improve performance.\n\n  Also, we can try to precompute the base pointers for the input and then use vectorized loads? But the input for each output element is scattered.\n\n  Alternatively, we can try to change the BLOCK_SIZE to a larger value? Currently 128. We can try 256 or 512? But note the autotuning in the example we saw in the inspiration? We don't have autotuning here.\n\n  Let's try to unroll the inner loops and see if that helps.\n\n  We change the loops to static_range.\n\n  Also, note that the area is computed as kernel_size*kernel_size. But if the window is at the edge, the actual count of valid elements might be less. However, the current kernel does not account for that: it divides by the full area (9) even when there are padding zeros. That is a correctness issue? But note the reference does:\n\n        if count == 0:\n            average = 0.0\n        else:\n            average = total / count\n\n  But our current kernel does:\n\n        output_val = total / area   # area = kernel_size * kernel_size\n\n  This is incorrect! We must divide by the actual number of non-padding elements (count). But in the current kernel, we are not computing the count per element. We are only using the full area.\n\n  Correction: in the current kernel, we are summing the mask (which is the count) in the variable `count`? Actually, no. Let me check:\n\n        total = ... we are summing the values, but we are not computing the count.\n\n        Then we do: output_val = total / area.\n\n  But wait, in the current kernel we do:\n\n        total = tl.zeros(...)\n        area = kernel_size * kernel_size\n\n        then in the loop:\n\n            value = tl.load(... mask ...)\n            total += value\n\n        then output_val = total / area.\n\n  This is incorrect for padding areas. We must divide by the actual number of non-padding elements.\n\n  However, the current program passed correctness? The meta says: 'correctness_trials': '(5 / 5)'. So how?\n\n  Let me check the reference: the reference uses:\n\n        total = 0.0\n        count = 0\n        ... then for each element in the window, if it's in bounds, add the value and increment count by 1.\n\n        then average = total / count (if count>0, else 0)\n\n  Our current kernel does not do that. It does:\n\n        within_bounds = ... (a boolean for the current window element)\n        value = tl.load(..., mask=mask & within_bounds, ...)   # mask is the block mask (for the output element being in range) and within_bounds for the window.\n\n        then total += value   # but note: if the window element is out of bounds, we load 0 and add 0? That is the same as not counting.\n\n        but then we don't have the count. We are using a fixed area.\n\n  This is a correctness bug. We must fix it.\n\n  So we need to:\n\n        total = 0.0\n        count = 0\n\n        for kh, kw:\n            ...\n            within_bounds = ... (for the window element)\n            value = tl.load(..., mask=mask & within_bounds, other=0.0)\n            total += value\n            count += tl.where(mask & within_bounds, 1.0, 0.0)\n\n        then average = total / count   (but avoid division by zero)\n\n  However, note the mask in the load is the same as the condition for the count? Yes.\n\n  But the current kernel uses:\n\n        mask = output_index < total_output_elements   (for the entire block)\n\n        and then for the window element, we do: mask & within_bounds.\n\n  So we should change the kernel to accumulate the count.\n\n  This might be the reason why the kernel is slow? Because we are not doing the count? Actually, no, because the reference does the count and we are not.\n\n  But the current kernel passed correctness? That is mysterious.\n\n  Let me reexamine the current kernel: the variable `mask` in the load is the combination of the block mask and the window bounds. The block mask ensures we don't process out-of-bound output elements. The window bounds ensures we don't read out-of-bound input.\n\n  And then we are adding the value. But we are not counting the valid elements. We are dividing by the full area. This is only correct if the window is entirely within the input. For edges, it is incorrect.\n\n  Therefore, we must fix the correctness and also we might get a performance hit because of the extra computation? But the extra computation (the count) is necessary.\n\n  Let's fix the kernel:\n\n      We'll accumulate a count per output element.\n\n      We change:\n\n          total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n          area = kernel_size * kernel_size   # remove this\n\n      Then in the loop:\n\n          value = tl.load( ... )\n          total += value\n\n          # We need to accumulate the count: 1 for each valid element, 0 otherwise.\n          # We can do: count += tl.where(mask & within_bounds, 1.0, 0.0)\n\n      But note: the mask for the load is (mask & within_bounds). We can precompute the condition:\n\n          cond = mask & within_bounds\n\n          value = tl.load( ... , mask=cond, other=0.0)\n          total += value\n          count_vec = tl.where(cond, 1.0, 0.0)\n          count += count_vec\n\n      Then after the loop:\n\n          average = tl.where(count != 0, total / count, 0.0)\n\n      Then store average.\n\n  This will be correct.\n\n  But will it be slower? Probably, because we are doing more operations.\n\n  However, without this fix, the kernel is incorrect. So we must do it.\n\n  And then we can try to optimize by unrolling the inner loops.\n\n  Proposed changes:\n\n      1. Fix the kernel to accumulate the count and do division by the actual count.\n      2. Unroll the inner loops with `tl.static_range` to allow the compiler to unroll.\n\n  We also note that the current kernel uses:\n\n        for kh in range(0, kernel_size):\n            for kw in range(0, kernel_size):\n\n      We change to:\n\n        for kh in tl.static_range(0, kernel_size):\n            for kw in tl.static_range(0, kernel_size):\n\n  This may improve performance by loop unrolling.\n\n  We also note that the kernel uses a fixed BLOCK_SIZE=128. We can try to change it to a tunable parameter? But the constraint says we cannot change the host code arbitrarily? We are allowed to change the Triton kernel and the host code within the block? Actually, the entire code is in the block.\n\n  We can add autotuning? The example in the inspiration had autotuning. But note the constraint: we cannot change the function signature of the Triton kernel. However, we can add an autotune decorator? And we can change the host code to call the autotuner.\n\n  But the constraint says: \"NEVER change @triton.jit function signatures or parameter names\". We are not changing the signature? We are adding a decorator.\n\n  We can do:\n\n      @triton.autotune(...)\n      @triton.jit\n      def ...:\n\n  And then change the host code to pass the tuning parameters.\n\n  However, the current kernel already has a parameter BLOCK_SIZE. We can tune over BLOCK_SIZE and also over the number of warps and stages.\n\n  But note: the kernel now has an extra loop for the count. We want to make sure it's efficient.\n\n  Given the time, let's do the following:\n\n      Step 1: Fix the correctness by adding the count.\n      Step 2: Unroll the inner loops.\n      Step 3: Try to increase the BLOCK_SIZE to 256 or 512 to see if it helps.\n\n  But note: the current BLOCK_SIZE is set to 128 in the host code. We can change it to a variable and then autotune.\n\n  However, let's do one step at a time.\n\n  We'll fix the correctness and unroll the loops.\n\n  We change the kernel code:\n\n      total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n      count = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)   # or integer? but then division might be float\n\n      Then in the loop:\n\n          cond = mask & within_bounds\n          value = tl.load(x_base + offset, mask=cond, other=0.0)\n          total += value\n          count += tl.where(cond, 1.0, 0.0)\n\n      Then:\n\n          avg = tl.where(count > 0, total / count, 0.0)\n\n  And store avg.\n\n  Also, unroll the loops.\n\n  Let's write the code accordingly.\n\n  We also note that the example input has kernel_size=3, so unrolling the inner 3x3 loop is acceptable.\n\n  We hope that the unrolling and the fix for correctness will not degrade performance further. And we hope the unrolling might help.\n\n  But note: the current kernel without the count was incorrect. So we must fix it.\n\n  We'll do:\n\n      @triton.jit\n      def _avg_pool2d_kernel(\n          ... same parameters ...\n      ):\n          ... same as before until the loop ...\n\n          total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n          count = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n          for kh in tl.static_range(0, kernel_size):\n              for kw in tl.static_range(0, kernel_size):\n                  h = h_start + kh\n                  w = w_start + kw\n                  within_bounds = (h >= 0) & (h < height) & (w >= 0) & (w < width)\n                  offset = h * width + w\n                  cond = mask & within_bounds\n                  value = tl.load(x_base + offset, mask=cond, other=0.0)\n                  total += value\n                  count += tl.where(cond, 1.0, 0.0)\n\n          avg = total / count\n          # But avoid division by zero: if count==0, then avg should be 0.\n          # We can do: avg = tl.where(count != 0, total / count, 0.0)\n          # However, note that if count==0, then we set to 0. But the reference does the same.\n\n          tl.store(output_ptr + output_offset, avg, mask=mask)\n\n  We must be cautious: the output_offset is computed per thread? Currently, it is:\n\n        output_offset = batch_i * S + channel_i * C_stride + out_h_i * out_w + out_w_i\n\n  But note: batch_i, channel_i, out_h_i, out_w_i are vectors of length BLOCK_SIZE? Actually, they are computed for the entire block. We are using:\n\n        batch_i = tl.where(mask, output_index // S, 0)\n        ... etc.\n\n  And then output_offset is computed for each element in the block.\n\n  This is correct.\n\n  We are changing the kernel to do:\n\n        total = ... (vector of BLOCK_SIZE)\n        count = ... (vector of BLOCK_SIZE)\n\n  And then we do the division element-wise.\n\n  This should be correct.\n\n  Let's run it and see if the performance improves? We hope that unrolling the loops might help.\n\n  Also, we can change the BLOCK_SIZE to 256 to increase the block size and see if it helps.\n\n  We change in the host code:\n\n        BLOCK_SIZE = 256   # instead of 128\n\n  But note: the total_output_elements might not be divisible by 256, so we still need the mask.\n\n  We'll try.\n\n  Summary of changes:\n\n      Kernel:\n        - Added a count vector.\n        - In the inner loops, accumulate the count for valid elements.\n        - Compute the average as total / count (with zero if count==0).\n        - Changed the inner loops to use tl.static_range to unroll.\n\n      Host:\n        - Change BLOCK_SIZE from 128 to 256.\n\n  We also note that the kernel_size is passed as a tl.constexpr, so the unrolling will happen at compile time.\n\n  Let's write the code accordingly.\n\n  IMPORTANT: We must not break the compilation or correctness.\n\n  We'll do these changes and hope for a speedup.\n\n  Note: The reference time is 0.02 ms. Our current kernel time is 0.135 ms. We need a 7x speedup to break even.\n\n  This might not be enough. But it's a start.\n\n  We'll do iterative improvements.\n\n  Code below.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _avg_pool2d_kernel(\n    x_ptr,\n    output_ptr,\n    batch_size,\n    channels,\n    height,\n    width,\n    out_h,\n    out_w,\n    stride,\n    padding,\n    kernel_size: tl.constexpr,\n    total_output_elements: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    idx = tl.arange(0, BLOCK_SIZE)\n    output_index = pid * BLOCK_SIZE + idx\n    mask = output_index < total_output_elements\n    \n    # Compute output tensor strides\n    S = channels * out_h * out_w\n    C_stride = out_h * out_w\n    H_stride = out_w\n    \n    # Unravel output indices\n    batch_i = tl.where(mask, output_index // S, 0)\n    channel_i = tl.where(mask, (output_index % S) // C_stride, 0)\n    pos_2d = tl.where(mask, output_index % C_stride, 0)\n    out_h_i = tl.where(mask, pos_2d // H_stride, 0)\n    out_w_i = tl.where(mask, pos_2d % H_stride, 0)\n    \n    # Compute input window\n    h_start = out_h_i * stride - padding\n    w_start = out_w_i * stride - padding\n    \n    # Input tensor strides\n    x_batch_stride = channels * height * width\n    x_channel_stride = height * width\n    x_base = x_ptr + batch_i * x_batch_stride + channel_i * x_channel_stride\n    \n    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    count = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Unroll loops for kernel processing\n    for kh in tl.static_range(0, kernel_size):\n        for kw in tl.static_range(0, kernel_size):\n            h = h_start + kh\n            w = w_start + kw\n            within_bounds = (h >= 0) & (h < height) & (w >= 0) & (w < width)\n            offset = h * width + w\n            cond = mask & within_bounds\n            value = tl.load(x_base + offset, mask=cond, other=0.0)\n            total += value\n            count += tl.where(cond, 1.0, 0.0)\n\n    # Compute average and avoid division by zero\n    output_val = tl.where(count > 0, total / count, 0.0)\n    output_offset = batch_i * S + channel_i * C_stride + out_h_i * out_w + out_w_i\n    tl.store(output_ptr + output_offset, output_val, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):\n        super(ModelNew, self).__init__()\n        if stride is None:\n            stride = kernel_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, height, width = x.shape\n        out_h = (height + 2*self.padding - self.kernel_size) // self.stride + 1\n        out_w = (width + 2*self.padding - self.kernel_size) // self.stride + 1\n        total_output_elements = batch_size * channels * out_h * out_w\n        \n        output = torch.empty((batch_size, channels, out_h, out_w), \n                             device=x.device, dtype=x.dtype)\n        \n        if total_output_elements == 0:\n            return output\n            \n        BLOCK_SIZE = 256  # Increased block size for better utilization\n        grid = (triton.cdiv(total_output_elements, BLOCK_SIZE),)\n        \n        _avg_pool2d_kernel[grid](\n            x, output,\n            batch_size, channels, height, width,\n            out_h, out_w,\n            self.stride, self.padding, self.kernel_size,\n            total_output_elements,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        return output\n\nbatch_size = 16\nchannels = 64\nheight = 256\nwidth = 256\nkernel_size = 3\n\ndef get_inputs():\n    x = torch.randn(batch_size, channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [kernel_size]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
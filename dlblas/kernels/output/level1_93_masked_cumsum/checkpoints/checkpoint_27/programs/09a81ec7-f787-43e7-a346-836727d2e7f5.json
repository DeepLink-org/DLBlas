{"id": "09a81ec7-f787-43e7-a346-836727d2e7f5", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_fn(a, b):\n    return a + b\n\n@triton.jit\ndef _masked_cumsum_kernel(\n    x_ptr,\n    mask_ptr,\n    output_ptr,\n    n_cols,\n    x_row_stride,\n    mask_row_stride,\n    output_row_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    # Compute starting pointers for current row\n    x_row_ptr = x_ptr + row_idx * x_row_stride\n    mask_row_ptr = mask_ptr + row_idx * mask_row_stride\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n\n    total_prev = 0.0\n    chunk_start = 0\n    # Process row in chunks of BLOCK_SIZE\n    while chunk_start < n_cols:\n        chunk_size = min(BLOCK_SIZE, n_cols - chunk_start)\n        chunk_offsets = tl.arange(0, BLOCK_SIZE)\n        chunk_mask = chunk_offsets < chunk_size\n\n        # Load data and mask for current chunk\n        x_vals = tl.load(x_row_ptr + chunk_start + chunk_offsets, mask=chunk_mask, other=0.0)\n        mask_vals = tl.load(mask_row_ptr + chunk_start + chunk_offsets, mask=chunk_mask, other=0)\n        mask_vals = mask_vals.to(tl.int1)\n\n        # Apply mask and compute cumulative sum\n        masked_vals = tl.where(mask_vals, x_vals, 0.0)\n        chunk_cumsum = tl.associative_scan(masked_vals, 0, add_fn)\n        total_chunk = tl.sum(masked_vals)\n\n        # Add previous total and store\n        chunk_cumsum = chunk_cumsum + total_prev\n        tl.store(output_row_ptr + chunk_start + chunk_offsets, chunk_cumsum, mask=chunk_mask)\n\n        # Update running total for next chunk\n        total_prev += total_chunk\n        chunk_start += BLOCK_SIZE\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        if self.dim != 1 or x.dim() != 2:\n            return torch.cumsum(x * mask, dim=self.dim)\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        mask = mask.contiguous()\n        output = torch.empty_like(x)\n        \n        n_rows, n_cols = x.shape\n        # Use optimal fixed block size\n        BLOCK_SIZE = 1024\n        \n        # Launch kernel\n        grid = (n_rows,)\n        _masked_cumsum_kernel[grid](\n            x, mask, output, n_cols,\n            x.stride(0), mask.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "794f24eb-dbdb-4666-8bbe-2e4d01877ccb", "generation": 3, "timestamp": 1754990751.5021915, "iteration_found": 28, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.51, "runtime_ms": 0.0461, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0235, "speed_up": 0.51, "custom_timing": 0.0461}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.538, "runtime_ms": 0.0437, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0235, "speed_up": 0.538, "custom_timing": 0.0437}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.5380\n- runtime_ms: 0.0437\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0235, 'speed_up': 0.538, 'custom_timing': 0.0437}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.3650, runtime_ms: 0.0644, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0235, 'speed_up': 0.365, 'custom_timing': 0.0644}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.5380, runtime_ms: 0.0437, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0235, 'speed_up': 0.538, 'custom_timing': 0.0437}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.5230, runtime_ms: 0.0449, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0235, 'speed_up': 0.523, 'custom_timing': 0.0449}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_fn(a, b):\n    return a + b\n\n@triton.jit\ndef _masked_cumsum_kernel(\n    x_ptr,\n    mask_ptr,\n    output_ptr,\n    n_cols,\n    x_row_stride,\n    mask_row_stride,\n    output_row_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    # Compute starting pointers for current row\n    x_row_ptr = x_ptr + row_idx * x_row_stride\n    mask_row_ptr = mask_ptr + row_idx * mask_row_stride\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n\n    total_prev = 0.0\n    chunk_start = 0\n    # Process row in chunks of BLOCK_SIZE\n    while chunk_start < n_cols:\n        chunk_size = min(BLOCK_SIZE, n_cols - chunk_start)\n        chunk_offsets = tl.arange(0, BLOCK_SIZE)\n        chunk_mask = chunk_offsets < chunk_size\n\n        # Load data and mask for current chunk\n        x_vals = tl.load(x_row_ptr + chunk_start + chunk_offsets, mask=chunk_mask, other=0.0)\n        mask_vals = tl.load(mask_row_ptr + chunk_start + chunk_offsets, mask=chunk_mask, other=0)\n        mask_vals = mask_vals.to(tl.int1)\n\n        # Apply mask and compute cumulative sum\n        masked_vals = tl.where(mask_vals, x_vals, 0.0)\n        chunk_cumsum = tl.associative_scan(masked_vals, 0, add_fn)\n        total_chunk = tl.sum(masked_vals)\n\n        # Add previous total and store\n        chunk_cumsum = chunk_cumsum + total_prev\n        tl.store(output_row_ptr + chunk_start + chunk_offsets, chunk_cumsum, mask=chunk_mask)\n\n        # Update running total for next chunk\n        total_prev += total_chunk\n        chunk_start += BLOCK_SIZE\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        if self.dim != 1 or x.dim() != 2:\n            return torch.cumsum(x * mask, dim=self.dim)\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        mask = mask.contiguous()\n        output = torch.empty_like(x)\n        \n        n_rows, n_cols = x.shape\n        # Use optimal fixed block size\n        BLOCK_SIZE = 1024\n        \n        # Launch kernel\n        grid = (n_rows,)\n        _masked_cumsum_kernel[grid](\n            x, mask, output, n_cols,\n            x.stride(0), mask.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.5230), Performs well on runtime_ms (0.0449), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0235, 'speed_up': 0.523, 'custom_timing': 0.0449})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_fn(a, b):\n    return a + b\n\n@triton.jit\ndef _masked_cumsum_kernel(\n    x_ptr,\n    mask_ptr,\n    output_ptr,\n    n_cols,\n    x_row_stride,\n    mask_row_stride,\n    output_row_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    # Compute starting pointers for current row\n    x_row_ptr = x_ptr + row_idx * x_row_stride\n    mask_row_ptr = mask_ptr + row_idx * mask_row_stride\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n\n    # Load data and mask\n    x_vals = tl.load(x_row_ptr + col_offsets, mask=mask, other=0.0)\n    mask_vals = tl.load(mask_row_ptr + col_offsets, mask=mask, other=0)\n    mask_vals = mask_vals.to(tl.int1)\n\n    # Apply mask and compute cumulative sum\n    masked_vals = tl.where(mask_vals, x_vals, 0.0)\n    cumsum = tl.associative_scan(masked_vals, 0, add_fn)\n    \n    # Store result\n    tl.store(output_row_ptr + col_offsets, cumsum, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        if self.dim != 1 or x.dim() != 2:\n            return torch.cumsum(x * mask, dim=self.dim)\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        mask = mask.contiguous()\n        output = torch.empty_like(x)\n        \n        n_rows, n_cols = x.shape\n        BLOCK_SIZE = triton.next_power_of_2(n_cols)\n        \n        # Launch kernel\n        grid = (n_rows,)\n        _masked_cumsum_kernel[grid](\n            x, mask, output, n_cols,\n            x.stride(0), mask.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.5380), Performs well on runtime_ms (0.0437), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0235, 'speed_up': 0.538, 'custom_timing': 0.0437})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_combine(a, b):\n    return a + b\n\n@triton.jit\ndef _masked_cumsum_kernel(\n    output_ptr,\n    x_ptr,\n    mask_ptr,\n    n_cols,\n    stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    start = row_idx * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    \n    # Load data in blocks\n    x = tl.load(x_ptr + start + col_offsets, mask=mask, other=0.0)\n    m = tl.load(mask_ptr + start + col_offsets, mask=mask, other=0)\n    mask_bool = m.to(tl.int1, bitcast=True)\n    \n    # Apply mask and compute cumulative sum\n    vals = tl.where(mask_bool, x, 0.0)\n    cumsum = tl.associative_scan(vals, 0, add_combine)\n    \n    # Store result\n    tl.store(output_ptr + start + col_offsets, cumsum, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        original_shape = x.shape\n        dim = self.dim if self.dim >= 0 else self.dim + x.dim()\n        \n        n_rows = 1\n        for i, s in enumerate(original_shape):\n            if i != dim:\n                n_rows *= s\n        n_cols = original_shape[dim]\n        BLOCK_SIZE = triton.next_power_of_2(n_cols)\n        \n        x_flat = x.moveaxis(dim, -1).contiguous().view(n_rows, n_cols)\n        mask_flat = mask.moveaxis(dim, -1).contiguous().view(n_rows, n_cols)\n        output_flat = torch.empty_like(x_flat)\n        \n        grid = (n_rows,)\n        _masked_cumsum_kernel[grid](\n            output_flat, x_flat, mask_flat, n_cols, \n            x_flat.stride(0), BLOCK_SIZE\n        )\n        \n        output = output_flat.view(original_shape[:dim] + (n_cols,) + original_shape[dim+1:])\n        return output.moveaxis(-1, dim)\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.3650), Performs well on runtime_ms (0.0644), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0235, 'speed_up': 0.365, 'custom_timing': 0.0644})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a masked cumulative sum, only summing elements that satisfy a condition.\n\n    Parameters:\n        dim (int): The dimension along which to perform the masked cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n            mask (torch.Tensor): Boolean mask of the same shape as x.\n\n        Returns:\n            torch.Tensor: Cumulative sum of elements where mask is True.\n        \"\"\"\n        if self.dim != 1:\n            return torch.cumsum(x * mask, dim=self.dim)\n            \n        x = x.contiguous()\n        mask = mask.contiguous()\n        output = torch.empty_like(x)\n        batch_size, seq_len = x.shape[0], x.shape[1]\n        \n        grid = (batch_size,)\n        _masked_cumsum_kernel[grid](\n            x, mask, output,\n            x.stride(0), x.stride(1),\n            seq_len\n        )\n        return output\n\n@triton.jit\ndef _masked_cumsum_kernel(\n    x_ptr,\n    mask_ptr,\n    output_ptr,\n    stride_batch,\n    stride_row,\n    seq_len,\n):\n    row_idx = tl.program_id(0)\n    base = row_idx * stride_batch\n    total = 0.0\n    VECTOR_SIZE = 4\n    for j in range(0, seq_len, VECTOR_SIZE):\n        # Create a vector of offsets\n        offsets = base + (j + tl.arange(0, VECTOR_SIZE)) * stride_row\n        # Create a mask for the vector: which elements are within the row\n        mask_val = (j + tl.arange(0, VECTOR_SIZE)) < seq_len\n        # Load x and mask with the mask_val\n        x_vals = tl.load(x_ptr + offsets, mask=mask_val, other=0.0)\n        mask_vals = tl.load(mask_ptr + offsets, mask=mask_val, other=0.0).to(tl.float32)\n        data = x_vals * mask_vals\n\n        # Process each element in the vector\n        for k in range(VECTOR_SIZE):\n            if mask_val[k]:\n                total += data[k]\n                tl.store(output_ptr + base + (j+k)*stride_row, total)\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()  # Random boolean mask\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_combine(a, b):\n    return a + b\n\n@triton.jit\ndef masked_cumsum_kernel(\n    x_ptr, \n    mask_ptr, \n    output_ptr,\n    x_row_stride, \n    mask_row_stride, \n    out_row_stride,\n    row_length, \n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    row_start = pid0 * x_row_stride\n    mask_start = pid0 * mask_row_stride\n    out_start = pid0 * out_row_stride\n    \n    offsets = tl.arange(0, BLOCK_SIZE)\n    valid_mask = offsets < row_length\n    \n    # Load data with boundary checks\n    x = tl.load(x_ptr + row_start + offsets, mask=valid_mask, other=0.0)\n    mask_val = tl.load(mask_ptr + mask_start + offsets, mask=valid_mask, other=False)\n    mask_val = mask_val.to(tl.int1)\n    \n    # Apply mask and compute cumulative sum\n    x_masked = tl.where(mask_val, x, 0.0)\n    cumsum = tl.associative_scan(x_masked, 0, add_combine)\n    \n    # Store results\n    tl.store(output_ptr + out_start + offsets, cumsum, mask=valid_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        x = x.contiguous()\n        mask = mask.contiguous()\n        output = torch.empty_like(x)\n        \n        row_length = x.size(self.dim)\n        BLOCK_SIZE = triton.next_power_of_2(row_length)\n        grid = (x.shape[0],)\n        \n        masked_cumsum_kernel[grid](\n            x, mask, output,\n            x.stride(0), mask.stride(0), output.stride(0),\n            row_length,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        return output\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_combine(a, b):\n    return a + b\n\n@triton.jit\ndef masked_cumsum_kernel(\n    x_ptr, \n    mask_ptr, \n    output_ptr,\n    x_row_stride, \n    mask_row_stride, \n    out_row_stride,\n    row_length, \n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    row_start = pid0 * x_row_stride\n    mask_start = pid0 * mask_row_stride\n    out_start = pid0 * out_row_stride\n    \n    offsets = tl.arange(0, BLOCK_SIZE)\n    valid_mask = offsets < row_length\n    \n    # Load data with boundary checks\n    x = tl.load(x_ptr + row_start + offsets, mask=valid_mask, other=0.0)\n    mask_val = tl.load(mask_ptr + mask_start + offsets, mask=valid_mask, other=False)\n    mask_val = mask_val.to(tl.int1)\n    \n    # Apply mask and compute cumulative sum\n    x_masked = tl.where(mask_val, x, 0.0)\n    cumsum = tl.associative_scan(x_masked, 0, add_combine)\n    \n    # Store results\n    tl.store(output_ptr + out_start + offsets, cumsum, mask=valid_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        x = x.contiguous()\n        mask = mask.contiguous()\n        output = torch.empty_like(x)\n        \n        row_length = x.size(self.dim)\n        BLOCK_SIZE = triton.next_power_of_2(row_length)\n        grid = (x.shape[0],)\n        \n        masked_cumsum_kernel[grid](\n            x, mask, output,\n            x.stride(0), mask.stride(0), output.stride(0),\n            row_length,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        return output\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_fn(a, b):\n    return a + b\n\n@triton.jit\ndef _masked_cumsum_kernel(\n    x_ptr,\n    mask_ptr,\n    output_ptr,\n    n_cols,\n    x_row_stride,\n    mask_row_stride,\n    output_row_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    # Compute starting pointers for current row\n    x_row_ptr = x_ptr + row_idx * x_row_stride\n    mask_row_ptr = mask_ptr + row_idx * mask_row_stride\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n\n    total_prev = 0.0\n    chunk_start = 0\n    # Process row in chunks of BLOCK_SIZE\n    while chunk_start < n_cols:\n        chunk_size = min(BLOCK_SIZE, n_cols - chunk_start)\n        chunk_offsets = tl.arange(0, BLOCK_SIZE)\n        chunk_mask = chunk_offsets < chunk_size\n\n        # Load data and mask for current chunk\n        x_vals = tl.load(x_row_ptr + chunk_start + chunk_offsets, mask=chunk_mask, other=0.0)\n        mask_vals = tl.load(mask_row_ptr + chunk_start + chunk_offsets, mask=chunk_mask, other=0)\n        mask_vals = mask_vals.to(tl.int1)\n\n        # Apply mask and compute cumulative sum\n        masked_vals = tl.where(mask_vals, x_vals, 0.0)\n        chunk_cumsum = tl.associative_scan(masked_vals, 0, add_fn)\n        total_chunk = tl.sum(masked_vals)\n\n        # Add previous total and store\n        chunk_cumsum = chunk_cumsum + total_prev\n        tl.store(output_row_ptr + chunk_start + chunk_offsets, chunk_cumsum, mask=chunk_mask)\n\n        # Update running total for next chunk\n        total_prev += total_chunk\n        chunk_start += BLOCK_SIZE\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        if self.dim != 1 or x.dim() != 2:\n            return torch.cumsum(x * mask, dim=self.dim)\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        mask = mask.contiguous()\n        output = torch.empty_like(x)\n        \n        n_rows, n_cols = x.shape\n        # Use optimal fixed block size\n        BLOCK_SIZE = 1024\n        \n        # Launch kernel\n        grid = (n_rows,)\n        _masked_cumsum_kernel[grid](\n            x, mask, output, n_cols,\n            x.stride(0), mask.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef masked_cumsum_kernel(\n    x_ptr, \n    mask_ptr, \n    output_ptr,\n    x_row_stride, \n    mask_row_stride, \n    out_row_stride,\n    row_length, \n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    row_x = x_ptr + pid0 * x_row_stride\n    row_mask = mask_ptr + pid0 * mask_row_stride\n    row_out = output_ptr + pid0 * out_row_stride\n\n    offsets = tl.arange(0, BLOCK_SIZE)\n    valid_mask = offsets < row_length\n    x = tl.load(row_x + offsets, mask=valid_mask, other=0.0)\n    mask_val = tl.load(row_mask + offsets, mask=valid_mask, other=0).to(tl.int1)\n    \n    x_masked = tl.where(mask_val, x, 0.0)\n    cumsum = tl.associative_scan(x_masked, 0, tl.associative.ADD)\n    tl.store(row_out + offsets, cumsum, mask=valid_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        x = x.contiguous()\n        mask = mask.contiguous()\n        output = torch.empty_like(x)\n        \n        row_length = x.size(self.dim)\n        BLOCK_SIZE = triton.next_power_of_2(row_length)\n        grid = (x.shape[0],)\n        \n        masked_cumsum_kernel[grid](\n            x, mask, output,\n            x.stride(0), mask.stride(0), output.stride(0),\n            row_length,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        return output\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=['n_cols']\n)\n@triton.jit\ndef _masked_cumsum_kernel(\n    output_ptr,\n    x_ptr,\n    mask_ptr,\n    n_cols,\n    stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    start = row_idx * stride\n    offsets = start + tl.arange(0, BLOCK_SIZE)\n    mask_boundary = tl.arange(0, BLOCK_SIZE) < n_cols\n    x = tl.load(x_ptr + offsets, mask=mask_boundary, other=0.0, cache_modifier=\".cg\")\n    m = tl.load(mask_ptr + offsets, mask=mask_boundary, other=False, cache_modifier=\".cg\").to(tl.float32)\n    vals = x * m\n    cumsum = tl.associative_scan(vals, 0, \"add\")\n    tl.store(output_ptr + offsets, cumsum, mask=mask_boundary, cache_modifier=\".cg\")\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        original_shape = x.shape\n        dim = self.dim if self.dim >= 0 else self.dim + x.dim()\n        \n        n_rows = 1\n        for i, s in enumerate(original_shape):\n            if i != dim:\n                n_rows *= s\n        n_cols = original_shape[dim]\n        \n        if n_cols == 0:\n            return torch.zeros_like(x)\n        \n        BLOCK_SIZE = triton.next_power_of_2(n_cols)\n        x_flat = x.moveaxis(dim, -1).contiguous().view(n_rows, n_cols)\n        mask_flat = mask.moveaxis(dim, -1).contiguous().view(n_rows, n_cols)\n        \n        x_flat_padded = torch.zeros((n_rows, BLOCK_SIZE), device=x.device, dtype=x.dtype)\n        mask_flat_padded = torch.zeros((n_rows, BLOCK_SIZE), device=mask.device, dtype=torch.bool)\n        x_flat_padded[:, :n_cols] = x_flat\n        mask_flat_padded[:, :n_cols] = mask_flat\n        \n        output_flat_padded = torch.empty((n_rows, BLOCK_SIZE), device=x.device, dtype=x.dtype)\n        \n        grid = (n_rows,)\n        _masked_cumsum_kernel[grid](\n            output_flat_padded, x_flat_padded, mask_flat_padded, n_cols, \n            x_flat_padded.stride(0), BLOCK_SIZE\n        )\n        \n        output_flat = output_flat_padded[:, :n_cols]\n        output = output_flat.view(original_shape[:dim] + (n_cols,) + original_shape[dim+1:])\n        return output.moveaxis(-1, dim)\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_combine(a, b):\n    return a + b\n\n@triton.jit\ndef _masked_cumsum_kernel(\n    output_ptr,\n    x_ptr,\n    mask_ptr,\n    n_cols,\n    stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    start = row_idx * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    \n    # Load data in blocks\n    x = tl.load(x_ptr + start + col_offsets, mask=mask, other=0.0)\n    m = tl.load(mask_ptr + start + col_offsets, mask=mask, other=0)\n    mask_bool = m.to(tl.int1, bitcast=True)\n    \n    # Apply mask and compute cumulative sum\n    vals = tl.where(mask_bool, x, 0.0)\n    cumsum = tl.associative_scan(vals, 0, add_combine)\n    \n    # Store result\n    tl.store(output_ptr + start + col_offsets, cumsum, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        original_shape = x.shape\n        dim = self.dim if self.dim >= 0 else self.dim + x.dim()\n        \n        n_rows = 1\n        for i, s in enumerate(original_shape):\n            if i != dim:\n                n_rows *= s\n        n_cols = original_shape[dim]\n        BLOCK_SIZE = triton.next_power_of_2(n_cols)\n        \n        x_flat = x.moveaxis(dim, -1).contiguous().view(n_rows, n_cols)\n        mask_flat = mask.moveaxis(dim, -1).contiguous().view(n_rows, n_cols)\n        output_flat = torch.empty_like(x_flat)\n        \n        grid = (n_rows,)\n        _masked_cumsum_kernel[grid](\n            output_flat, x_flat, mask_flat, n_cols, \n            x_flat.stride(0), BLOCK_SIZE\n        )\n        \n        output = output_flat.view(original_shape[:dim] + (n_cols,) + original_shape[dim+1:])\n        return output.moveaxis(-1, dim)\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_fn(a, b):\n    return a + b\n\n@triton.jit\ndef _masked_cumsum_kernel(\n    x_ptr,\n    mask_ptr,\n    output_ptr,\n    n_cols,\n    x_row_stride,\n    mask_row_stride,\n    output_row_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    # Compute starting pointers for current row\n    x_row_ptr = x_ptr + row_idx * x_row_stride\n    mask_row_ptr = mask_ptr + row_idx * mask_row_stride\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n\n    # Load data and mask\n    x_vals = tl.load(x_row_ptr + col_offsets, mask=mask, other=0.0)\n    mask_vals = tl.load(mask_row_ptr + col_offsets, mask=mask, other=0)\n    mask_vals = mask_vals.to(tl.int1)\n\n    # Apply mask and compute cumulative sum\n    masked_vals = tl.where(mask_vals, x_vals, 0.0)\n    cumsum = tl.associative_scan(masked_vals, 0, add_fn)\n    \n    # Store result\n    tl.store(output_row_ptr + col_offsets, cumsum, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        if self.dim != 1 or x.dim() != 2:\n            return torch.cumsum(x * mask, dim=self.dim)\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        mask = mask.contiguous()\n        output = torch.empty_like(x)\n        \n        n_rows, n_cols = x.shape\n        BLOCK_SIZE = triton.next_power_of_2(n_cols)\n        \n        # Launch kernel\n        grid = (n_rows,)\n        _masked_cumsum_kernel[grid](\n            x, mask, output, n_cols,\n            x.stride(0), mask.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a Triton kernel for a masked cumulative sum along a specified dimension (here, dim=1) for a 2D tensor.\n The current implementation uses a single kernel per row and processes the entire row in one block.\n The block size is set to the next power of two of the row length (n_cols).\n\n However, the current performance (speed_up=0.538) is worse than the reference PyTorch implementation.\n\n Analysis of the current kernel:\n   - It loads the entire row (n_cols elements) in one go. But note: n_cols=4000, which is not a power of two? \n     Actually, 4000 is less than 4096 (2^12). The kernel uses BLOCK_SIZE = triton.next_power_of_2(n_cols)=4096.\n   - This means each thread block processes 4096 elements, but only 4000 are valid. The rest are masked out.\n\n Why is it slow?\n   - The associative_scan operation in Triton for a large vector (4096) might be expensive because the scan has logarithmic depth (12 steps) but with a large vector.\n   - Also, the entire row is processed in one block, which might not be optimal for memory access and parallelism.\n\n We note that the top-performing programs in the history (like Program 1) use a chunked approach (with fixed BLOCK_SIZE=1024) and then combine the chunks.\n This allows for better parallelism and memory locality.\n\n Proposed change:\n   We will change the kernel to process the row in chunks (like in Program 1). The idea:\n     - Each row is processed in chunks of fixed size (e.g., 1024 or 2048, which is a power of two and fits within the block).\n     - We do an associative scan within the chunk and then combine the chunks by keeping a running total (prefix sum of the chunks).\n\n Steps in the kernel:\n   - We have a row and we break it into chunks of BLOCK_SIZE (which is now a fixed constant, say 1024).\n   - For each chunk:\n        * Load the chunk of data and mask.\n        * Apply mask: set non-masked elements to 0.\n        * Compute the cumulative sum within the chunk (using associative_scan).\n        * Also compute the total sum of the chunk (for the next chunk).\n        * Add the running total (from the previous chunks) to the current chunk's cumulative sum.\n        * Store the chunk's cumulative sum.\n        * Update the running total by adding the chunk's total.\n\n Why might this be faster?\n   - Smaller chunks: 1024 elements per chunk might be more efficient for the associative_scan (only 10 steps).\n   - The chunked approach might better utilize the cache and memory hierarchy.\n\n Implementation:\n\n   We change the kernel to:\n\n   @triton.jit\n   def _masked_cumsum_kernel(...):\n        row_idx = tl.program_id(0)\n        # pointers for the row\n        x_row_ptr = ... \n        mask_row_ptr = ...\n        output_row_ptr = ...\n\n        total_prev = 0.0   # cumulative sum from previous chunks\n        chunk_start = 0\n        while chunk_start < n_cols:\n            chunk_size = min(BLOCK_SIZE, n_cols - chunk_start)\n            chunk_offsets = tl.arange(0, BLOCK_SIZE)\n            chunk_mask = chunk_offsets < chunk_size\n\n            # Load the chunk\n            x_vals = tl.load(x_row_ptr + chunk_start + chunk_offsets, mask=chunk_mask, other=0.0)\n            mask_vals = tl.load(mask_row_ptr + chunk_start + chunk_offsets, mask=chunk_mask, other=0)\n            mask_vals = mask_vals.to(tl.int1)\n\n            # Apply mask\n            masked_vals = tl.where(mask_vals, x_vals, 0.0)\n\n            # Compute the cumulative sum within the chunk and the total of the chunk\n            chunk_cumsum = tl.associative_scan(masked_vals, 0, add_fn)\n            chunk_total = tl.sum(masked_vals)   # total of this chunk\n\n            # Add the running total from previous chunks to the current chunk's cumulative sum\n            chunk_cumsum = chunk_cumsum + total_prev\n\n            # Store the chunk\n            tl.store(output_row_ptr + chunk_start + chunk_offsets, chunk_cumsum, mask=chunk_mask)\n\n            # Update the running total for the next chunk\n            total_prev += chunk_total\n            chunk_start += BLOCK_SIZE\n\n   We set BLOCK_SIZE to a fixed value (like 1024) in the ModelNew.forward.\n\n Why fixed? Because we can tune it. But for simplicity, we choose 1024 (which is a power of two and divides 4096).\n\n Note: The grid configuration remains the same (one block per row).\n\n Let's change the kernel accordingly and set BLOCK_SIZE to 1024 in the forward.\n\n Also note: We must keep the function signature (parameter names) the same.\n\n However, note that the current kernel signature has a parameter named `BLOCK_SIZE` which we are now using for the chunk size.\n\n We are not changing the signature, so it's safe.\n\n Let's write the new kernel and update the ModelNew.forward to use a fixed BLOCK_SIZE (1024) instead of the next_power_of_two.\n\n But note: the current kernel also uses BLOCK_SIZE for the entire row? We are changing that.\n\n We remove the computation of BLOCK_SIZE as the next power of two and instead use a fixed block size (1024) for the chunk.\n\n We must also update the kernel to use the chunked approach.\n\n Important: We are not changing the function signature, so the call remains the same.\n\n However, note the kernel now has an extra loop and uses a fixed BLOCK_SIZE (for the chunk) that we pass.\n\n We change the forward to set BLOCK_SIZE=1024.\n\n Let's write the code accordingly.\n\n Also note: The grid configuration is still (n_rows,).\n\n We'll try with BLOCK_SIZE=1024.\n\n But note: what if the row length is not a multiple of 1024? The kernel handles that with the while loop and chunk_size.\n\n We are confident.\n\n Let's write the new code.\n\n IMPORTANT: We must not change the non-Triton fallback (the if condition for dim != 1) and the tensor preparation.\n\n We only change the kernel and the way we call it (with a fixed BLOCK_SIZE).\n\n We are going to replace the entire kernel and the BLOCK_SIZE setting in the forward.\n\n We note that the previous top-performing program (Program 1) used a fixed BLOCK_SIZE=1024 and had a similar kernel.\n\n Let's do that.\n\n We'll also remove the BLOCK_SIZE computation (triton.next_power_of_2) and set it to 1024.\n\n Code:\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_fn(a, b):\n    return a + b\n\n@triton.jit\ndef _masked_cumsum_kernel(\n    x_ptr,\n    mask_ptr,\n    output_ptr,\n    n_cols,\n    x_row_stride,\n    mask_row_stride,\n    output_row_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    # Compute starting pointers for current row\n    x_row_ptr = x_ptr + row_idx * x_row_stride\n    mask_row_ptr = mask_ptr + row_idx * mask_row_stride\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n\n    total_prev = 0.0\n    chunk_start = 0\n    # Process row in chunks of BLOCK_SIZE\n    while chunk_start < n_cols:\n        chunk_size = min(BLOCK_SIZE, n_cols - chunk_start)\n        chunk_offsets = tl.arange(0, BLOCK_SIZE)\n        chunk_mask = chunk_offsets < chunk_size\n\n        # Load data and mask for current chunk\n        x_vals = tl.load(x_row_ptr + chunk_start + chunk_offsets, mask=chunk_mask, other=0.0)\n        mask_vals = tl.load(mask_row_ptr + chunk_start + chunk_offsets, mask=chunk_mask, other=0)\n        mask_vals = mask_vals.to(tl.int1)\n\n        # Apply mask and compute cumulative sum\n        masked_vals = tl.where(mask_vals, x_vals, 0.0)\n        chunk_cumsum = tl.associative_scan(masked_vals, 0, add_fn)\n        total_chunk = tl.sum(masked_vals)\n\n        # Add previous total and store\n        chunk_cumsum = chunk_cumsum + total_prev\n        tl.store(output_row_ptr + chunk_start + chunk_offsets, chunk_cumsum, mask=chunk_mask)\n\n        # Update running total for next chunk\n        total_prev += total_chunk\n        chunk_start += BLOCK_SIZE\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        if self.dim != 1 or x.dim() != 2:\n            return torch.cumsum(x * mask, dim=self.dim)\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        mask = mask.contiguous()\n        output = torch.empty_like(x)\n        \n        n_rows, n_cols = x.shape\n        # Use optimal fixed block size\n        BLOCK_SIZE = 1024\n        \n        # Launch kernel\n        grid = (n_rows,)\n        _masked_cumsum_kernel[grid](\n            x, mask, output, n_cols,\n            x.stride(0), mask.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "18475ef5-452d-403c-801a-edb93c4a547f", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a masked cumulative sum, only summing elements that satisfy a condition.\n\n    Parameters:\n        dim (int): The dimension along which to perform the masked cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n            mask (torch.Tensor): Boolean mask of the same shape as x.\n\n        Returns:\n            torch.Tensor: Cumulative sum of elements where mask is True.\n        \"\"\"\n        if self.dim != 1:\n            return torch.cumsum(x * mask, dim=self.dim)\n            \n        x = x.contiguous()\n        mask = mask.contiguous()\n        output = torch.empty_like(x)\n        batch_size, seq_len = x.shape[0], x.shape[1]\n        \n        grid = (batch_size,)\n        _masked_cumsum_kernel[grid](\n            x, mask, output,\n            x.stride(0), x.stride(1),\n            seq_len\n        )\n        return output\n\n@triton.jit\ndef _masked_cumsum_kernel(\n    x_ptr,\n    mask_ptr,\n    output_ptr,\n    stride_batch,\n    stride_row,\n    seq_len,\n):\n    row_idx = tl.program_id(0)\n    base = row_idx * stride_batch\n    total = 0.0\n    for j in range(0, seq_len):\n        offset = base + j * stride_row\n        x_val = tl.load(x_ptr + offset)\n        mask_val = tl.load(mask_ptr + offset).to(tl.float32)\n        data = x_val * mask_val\n        total += data\n        tl.store(output_ptr + offset, total)\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()  # Random boolean mask\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "44ca9f19-e0d1-4988-8ed7-2e0602156f9e", "generation": 1, "timestamp": 1754986154.4177232, "iteration_found": 2, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.103, "runtime_ms": 0.228, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0235, "speed_up": 0.103, "custom_timing": 0.228}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0334, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0235, "speed_up": 0.704, "custom_timing": 0.0334}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0334\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0235, 'speed_up': 0.704, 'custom_timing': 0.0334}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0334, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0235, 'speed_up': 0.704, 'custom_timing': 0.0334}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a masked cumulative sum, only summing elements that satisfy a condition.\n\n    Parameters:\n        dim (int): The dimension along which to perform the masked cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n            mask (torch.Tensor): Boolean mask of the same shape as x.\n\n        Returns:\n            torch.Tensor: Cumulative sum of elements where mask is True.\n        \"\"\"\n        return torch.cumsum(x * mask, dim=self.dim)\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()  # Random boolean mask\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0334), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0235, 'speed_up': 0.704, 'custom_timing': 0.0334})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a masked cumulative sum, only summing elements that satisfy a condition.\n\n    Parameters:\n        dim (int): The dimension along which to perform the masked cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n            mask (torch.Tensor): Boolean mask of the same shape as x.\n\n        Returns:\n            torch.Tensor: Cumulative sum of elements where mask is True.\n        \"\"\"\n        return torch.cumsum(x * mask, dim=self.dim)\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()  # Random boolean mask\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does a masked cumulative sum along a dimension.\n The current implementation uses `torch.cumsum(x * mask, dim=dim)`, which is straightforward but may not be optimal for large tensors.\n\n However, note that the problem states:\n   \"Each file contains PyTorch reference (reference_code)\"\n   and then we are to generate Triton implementations (custom_code).\n\n The current code block is the reference PyTorch code. We are now to replace it with a Triton implementation.\n\n Steps:\n 1. We are allowed to change only the Triton kernel source within the EVOLVE-BLOCK.\n 2. We must keep the same interface: class ModelNew, with __init__ and forward, and the same get_inputs and get_init_inputs.\n\n Approach:\n We will write a Triton kernel for the masked cumulative sum. The kernel will:\n   - Be launched with one program per row (if dim=1) or per batch (if dim=0) but note the input is 2D: [batch_size, seq_len]\n   - The problem specifies: input_shape = (4000,), so the tensor is 2D: [128, 4000]\n   - The dim is 1, meaning we do cumulative sum along the 4000 dimension.\n\n We can design:\n   - Each program handles one row (i.e., one batch element).\n   - We will have a 1D grid of size batch_size.\n\n However, note: the problem says \"masked cumulative sum\", meaning we only add elements where the mask is True.\n\n We cannot use a simple cumulative sum because the mask might have gaps.\n\n We can do:\n   output[i, j] = x[i, j] if mask[i, j] is True and then add the previous output? But note: the cumulative sum should include all previous elements that are masked.\n\n We can do a sequential scan per row. Since the row is 4000 elements, we can do it in a block.\n\n Steps for the kernel:\n   - Load the current element and the mask for the current position.\n   - We traverse the row and maintain a running sum.\n\n However, note: Triton does not support dynamic loops very well? We can use a static loop.\n\n Since 4000 is fixed, we can set a BLOCK_SIZE = 4000? But note: the input_shape is (4000,) but it might be any length? Actually, the problem says input_shape = (4000,). So we can set BLOCK_SIZE = 4000.\n\n Alternatively, we can make it configurable.\n\n We are going to write:\n\n   @triton.jit\n   def _masked_cumsum_kernel(x_ptr, mask_ptr, output_ptr, stride_x_batch, stride_x_row, ...)\n\n   We are going to have:\n     pid = tl.program_id(0)  # which row (batch index)\n\n   We will loop over the columns (with step BLOCK_SIZE, but since we have one row per block, we can set BLOCK_SIZE to the row length? Actually, we are processing one row at a time, so we can set the block to have 1 program and then within the block, we have 4000 elements? But 4000 might be too many for a single block (max block size is 1024 for Triton). So we must break the row into multiple blocks.\n\n However, note: the cumulative sum is sequential. We cannot break it arbitrarily.\n\n Therefore, we need to do a segmented scan. We can do:\n\n   Step 1: Each block of, say, 1024 elements does a cumulative sum for its segment, and also records the total sum of the segment.\n   Step 2: Then we do a second kernel to combine the segments.\n\n But note: the problem has a mask. The mask might be sparse. Also, the cumulative sum must be done sequentially.\n\n Alternatively, we can do a single thread per row? But that would be inefficient because we have 128 rows and 4000 elements per row -> 128 threads? But we can have 128 blocks, each block having one thread? That would be inefficient because we are not utilizing the GPU fully.\n\n We can use multiple threads per row? But the cumulative sum is sequential. We can use a parallel scan algorithm.\n\n We are going to implement a parallel scan (Blelloch) for the masked cumulative sum.\n\n However, note: the mask condition: we only want to add when the mask is True.\n\n We can define:\n   data = x * mask (element-wise). Then the cumulative sum is the inclusive scan of the data.\n\n But note: the problem says \"masked cumulative sum\", meaning we do the cumulative sum only on the positions where the mask is True? Actually, the original code does:\n   output = cumsum(x * mask, dim=1)\n\n So we can compute: y = x * mask, then do an inclusive scan (sum) along the row.\n\n Steps for parallel scan:\n\n   We can use a block of 1024 threads to scan a row of 4000 elements? We can break the row into multiple segments and then combine.\n\n   We'll set BLOCK_SIZE = 1024. Then we need to break the row of 4000 into 4 segments (ceil(4000/1024)=4). We can have:\n\n     num_blocks_per_row = ceil(seq_len / BLOCK_SIZE)\n\n   Then we would have grid = (batch_size, num_blocks_per_row) ?\n\n   But then we need to do a two-level scan: first within the segment, then a second kernel to add the segment's prefix.\n\n   However, note: we are only allowed to change the EVOLVE-BLOCK. We cannot add helper kernels.\n\n   Alternatively, we can do:\n\n     We have one program per row. Each row is handled by one block. We set the block size to be the next power of two that is >= 4000? But 4096 is 4*1024, which is too big (max block size is 1024). So we cannot.\n\n   Therefore, we must break the row into multiple blocks and use two kernels. But we cannot add another kernel? We are only allowed to change the current block.\n\n   The problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" meaning we can write multiple kernels as long as they are in the same block? Actually, the example had only one kernel.\n\n   Alternatively, we can use a single kernel that does the entire row by having one block per row and using sequential processing? That would be:\n\n      for j in range(0, seq_len):\n          ....\n\n   But that would be sequential and not utilize the GPU parallelism.\n\n   We are in a dilemma.\n\n   However, note the original PyTorch implementation is already using a highly optimized CUDA kernel for cumsum. We might not be able to beat it with a naive Triton kernel.\n\n   But let's try:\n\n   We can write a kernel that does:\n\n      pid = tl.program_id(0)   # row index\n      base = pid * stride_row   # for x and mask\n\n      # We are going to have a 1D block of size BLOCK_SIZE. We set BLOCK_SIZE to 1024? Then we have 4 blocks per row? But then we have to do 4 blocks per row -> grid = (batch_size * num_blocks, 1)\n\n      Actually, we can set the grid to (batch_size * ceil(seq_len/BLOCK_SIZE), 1) and then within each block, we do a segment of the row.\n\n      Then we do:\n\n        offsets = tl.arange(0, BLOCK_SIZE) + block_start\n        mask_offsets = base + offsets\n        x_vals = tl.load(x_ptr + mask_offsets, mask=offsets < seq_len, other=0)\n        mask_vals = tl.load(mask_ptr + mask_offsets, mask=offsets < seq_len, other=0)\n\n        # Then we do: data = x_vals * mask_vals\n\n        # Now we do a block-level inclusive scan (sum) for the segment.\n\n        # Then we store the segment's cumulative sum.\n\n        # But we also need the total of the previous segment to add to the current segment? How do we get that?\n\n        We can do:\n\n          We store the segment's cumulative sum in a temporary array. Then we also store the last element of the segment (the total of the segment) in a separate array.\n\n          Then we launch a second kernel that adds the total of the previous segments to the current segment? But we cannot launch a second kernel in the same function.\n\n   We are stuck.\n\n   Alternatively, we can use atomic operations? But that would be slow.\n\n   Given the complexity and the fact that the row is only 4000, we can do:\n\n      Use one block per row, and within the block, use a single thread to do the entire row? That would be 128 blocks, each with 1 thread. Then we do:\n\n        if tl.program_id(1) == 0:   # but we only have one dimension: program_id(0) for row, and then we have one thread per row? Then we set block size to 1? That would be inefficient.\n\n   But note: the GPU has many cores. We can have 128 blocks and 1 thread per block -> 128 threads, which is very low.\n\n   We can use a block of 1 thread per row? Then we do:\n\n        pid = tl.program_id(0)   # row index\n        for j in range(0, seq_len):\n            offset = pid * stride_row + j\n            x_val = tl.load(x_ptr + offset)\n            mask_val = tl.load(mask_ptr + offset)\n            data = x_val * mask_val\n            # then we do a cumulative sum: we need to maintain a running total per row\n            total = 0\n            # but we cannot do a loop that accumulates because we cannot break the loop? We can do:\n\n            # Actually, we can do:\n            #   total = tl.atomic_add(accum_ptr, data)   # but then we want the cumulative sum per element? We want to store the cumulative sum at each position.\n\n            # We can do:\n            #   total = total + data   -> but how to share total between iterations? We can use a register.\n\n            # We can simulate:\n            #   total = 0\n            #   for j in range(0, seq_len):\n            #       ... compute data ...\n            #       total += data\n            #       tl.store(output_ptr+offset, total)\n\n        But note: we are in a single thread. We can do a for loop.\n\n   This is acceptable if the row is not too long. 4000 is acceptable.\n\n   Steps:\n\n     We set grid = (batch_size,)\n     We set block size = 1? Actually, Triton doesn't have a block size of 1? We can set it to 1.\n\n   How to write:\n\n     @triton.jit\n     def _masked_cumsum_kernel(x_ptr, mask_ptr, output_ptr, stride_batch, stride_row, seq_len, BLOCK_SIZE: tl.constexpr):\n         pid = tl.program_id(0)   # row index (batch index)\n\n         # We are going to iterate over the entire row.\n         # We precompute the base pointer for this row.\n         base = pid * stride_batch\n\n         total = 0.0\n         for j in range(0, seq_len):\n             offset = base + j * stride_row   # note: if the tensor is contiguous, stride_row=1? But we don't know.\n             x_val = tl.load(x_ptr + offset)\n             mask_val = tl.load(mask_ptr + offset)\n             data = x_val * mask_val\n             total += data\n             tl.store(output_ptr + offset, total)\n\n   But note: the tensor might not be contiguous? The problem says the input is generated by `torch.randn(batch_size, *input_shape)` and then `x = x.contiguous()`? We are not told.\n\n   We can require the input to be contiguous? Or we can handle strides.\n\n   However, the original PyTorch function does: `x * mask` which might be contiguous? We can assume the input is contiguous in the row dimension? Or we can use the strides.\n\n   The kernel above uses stride_batch and stride_row. We can get these from the tensor.\n\n   But note: the tensor might have more than 2 dimensions? The input_shape is (4000,), so it's 2D.\n\n   We can set:\n        stride_batch = stride(0)\n        stride_row = stride(1)\n\n   This kernel is simple but inefficient because it uses only one thread per row.\n\n   We can use multiple threads per row? We can break the row into chunks and then do a sequential scan per chunk? But then we have to combine the chunks. And that requires two passes.\n\n   Since we are limited by the EVOLVE-BLOCK and cannot add a second kernel, we must do one kernel.\n\n   We can do a two-pass within one kernel? But it's complex.\n\n   Given the time, we start with the simple kernel and see if it provides speedup? Actually, the original PyTorch cumsum is highly optimized. We might not beat it with a sequential kernel.\n\n   But note: the original PyTorch cumsum does a parallel scan. So we are going to lose.\n\n   However, we are on a specific GPU (H100) and Triton might have optimizations? Or maybe we can use a parallel scan with one block per row? But 4000 is more than the block size (1024). So we cannot.\n\n   We must break the row into multiple blocks? Then we have to do two passes? But we cannot have two kernels.\n\n   Alternatively, we can use the entire block to do a parallel scan for one row? But 4000 is too big. We can set BLOCK_SIZE=1024 and then break the row into 4 blocks? Then we have 4 blocks per row. Then we can do:\n\n        Pass 1: Each block computes the cumulative sum for its segment and also the segment total.\n        Then we store the segment totals in an array.\n\n        Then we need to compute the prefix of the segment totals? How? We can do:\n\n          We have an array of segment totals of length (batch_size * num_blocks). Then we do a separate kernel to compute the prefix sum of that array? But we cannot.\n\n   We are stuck.\n\n   Given the constraints, we try the sequential per-row kernel and see if it works? But note the problem says: \"Target \u22655% speedup\". We are unlikely to get that with a sequential kernel.\n\n   But note: the original PyTorch cumsum might be efficient for contiguous memory? And we are doing a sequential read, which is efficient? Also, we are only doing 128 rows * 4000 = 512000 elements, which is small.\n\n   We try:\n\n        We write a Triton kernel that does one thread per row and loops over the row.\n\n   Steps:\n\n        We set:\n            grid = (batch_size,)\n            block size = 1   (but Triton uses block size in terms of number of threads? Actually, in Triton, the block size is the number of threads per block. We can set BLOCK_SIZE=1? Then we have 128 blocks, each with 1 thread.\n\n        This is inefficient because the GPU has many cores and we are using only 128.\n\n   We can try to use more threads? For example, we can set the block size to 128? Then we have 1 block? But then we have to do 128 rows in parallel? But each row is independent.\n\n        We set grid = (ceil(batch_size / BLOCK_SIZE), ) and BLOCK_SIZE=128.\n\n        Then each thread in the block handles one row? But then we have 128 threads per block and we have 128 rows -> 1 block.\n\n        Then each thread does:\n\n            row_idx = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)   -> but wait, we are in a 1D grid.\n\n        Actually, we can do:\n\n            pid = tl.program_id(0)\n            row_idx = pid\n\n            But note: we have grid = (ceil(128/128)=1, ) so pid=0 for the entire block? Then we have 128 threads in the block, each thread handling one row.\n\n        Then:\n\n            row_id = tl.arange(0, BLOCK_SIZE)   # [0, 1, ..., 127]\n            # But note: we have exactly 128 rows? Then we can do.\n\n            # We have to iterate over the columns? We can do a loop over j from 0 to seq_len-1.\n\n        However, we cannot have each thread doing a loop of 4000 iterations? That would be 128*4000=512000 iterations in total? But they are done in parallel? Actually, the threads run in parallel, but each thread does 4000 iterations sequentially.\n\n        This is acceptable? The total time would be 4000 steps * (time per step). The GPU can run 128 threads in parallel, so we are using 128 cores.\n\n        How to write:\n\n            @triton.jit\n            def _masked_cumsum_kernel(x_ptr, mask_ptr, output_ptr, stride_batch, stride_row, seq_len, batch_size, BLOCK_SIZE: tl.constexpr):\n                pid = tl.program_id(0)\n                row_id = tl.arange(0, BLOCK_SIZE)\n                # Check if row_id is within batch_size\n                mask_row = row_id < batch_size\n\n                # We'll use a loop over j in [0, seq_len-1]\n                for j in range(0, seq_len):\n                    # Compute the offset for this row and column\n                    # offset = row_id * stride_batch + j * stride_row\n                    offsets = row_id * stride_batch + j * stride_row\n                    # Load x and mask for these offsets\n                    x_val = tl.load(x_ptr + offsets, mask=mask_row)\n                    mask_val = tl.load(mask_ptr + offsets, mask=mask_row)\n                    data = x_val * mask_val\n\n                    # Now, we want to do a cumulative sum per row. We need to maintain a state per row.\n                    # We can use a register? But note: we are in a loop over j, and we want to accumulate per row.\n\n                    # How to accumulate per row? We can use a block of shared memory? But Triton doesn't have shared memory? Actually, it does: tl.static_shared.\n\n                    # Alternatively, we can use a local variable? But each thread has its own row.\n\n                    # We can use a local array? But we are in a loop: we want to update the total for the row.\n\n                    # We can do: total = ... but we cannot have an array of BLOCK_SIZE in registers? We can have one total per thread.\n\n                    # How? We are in a block of BLOCK_SIZE threads. Each thread is responsible for one row. So thread i has its own total.\n\n                    # We can do:\n                    #   total = 0   # at the beginning of the row? But we are at j=0.\n\n                    # We cannot initialize total outside the loop? Because we want to accumulate over j.\n\n                    # We can do: initialize an array of zeros for the block?\n\n                    # Actually, we can do:\n\n                    #   total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)   # but we are going to update it per j.\n\n                    # But note: we are in a loop and we want to update total for each row independently.\n\n                    # We can do:\n\n                    #   total = tl.load(shared_mem + row_id)   -> but we don't need shared mem because each thread has its own row.\n\n                    # Actually, we can use a local variable per thread? But we are in a loop that is inside the kernel. Each thread has its own state.\n\n                    # We can do:\n\n                    #   total = 0.0   # but we want to do this at the beginning? But we are in the loop over j. We want to accumulate over j.\n\n                    # How to initialize total? We can do:\n\n                    #   if j == 0:\n                    #       total = 0.0\n\n                    #   total = total + data\n\n                    # But how to keep total between iterations? We can use a local variable that persists in the loop.\n\n                    # In Triton, we can do:\n\n                    #   total = 0.0\n                    #   for j in range(0, seq_len):\n                    #       ... compute data ...\n                    #       total += data\n                    #       tl.store(output_ptr+offsets, total)\n\n                    # But note: we are in a SIMT model: each thread is independent. So we can do that.\n\n                But wait: we are in a block of threads, and we are iterating j from 0 to 3999. Each thread i has its own row and its own total.\n\n                So we can do:\n\n                    total = 0.0\n                    for j in range(0, seq_len):\n                        offset = row_id[i] * stride_batch + j * stride_row   -> but we are in a vectorized way? Actually, we are not: we are using row_id as an array? But no: row_id is a vector of BLOCK_SIZE.\n\n                This is complex because we are loading for multiple rows at the same j? And then we are updating multiple rows at the same j.\n\n                How about we change the loop order? We want to do:\n\n                    for j in range(0, seq_len):\n                        for each row in the block:\n                            offset = row * stride_batch + j * stride_row\n                            load x and mask\n                            total_row = total_row + x * mask\n                            store total_row\n\n                But we want to vectorize the load and store. We can do:\n\n                    for j in range(0, seq_len):\n                        offsets = row_id * stride_batch + j * stride_row\n                        x_vals = tl.load(x_ptr + offsets, mask=mask_row)\n                        mask_vals = tl.load(mask_ptr + offsets, mask=mask_row)\n                        data = x_vals * mask_vals\n\n                        # Now update the total for each row: we have a vector of data and a vector of totals.\n                        # We want to do: total = total + data\n\n                        # How to maintain the total vector? We can keep it in registers? We can have a local variable that is a vector of BLOCK_SIZE.\n\n                        # We can initialize at j=0: total = data, but for j>0: total = total + data.\n\n                        # But we need to carry the total from the previous j.\n\n                    We can do:\n\n                        if j == 0:\n                            total = data\n                        else:\n                            total = total + data\n\n                        tl.store(output_ptr+offsets, total, mask=mask_row)\n\n                But note: we are in a loop. We can do:\n\n                    total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n                    for j in range(0, seq_len):\n                        offsets = row_id * stride_batch + j * stride_row\n                        x_vals = tl.load(x_ptr + offsets, mask=mask_row)\n                        mask_vals = tl.load(mask_ptr + offsets, mask=mask_row)\n                        data = x_vals * mask_vals\n                        total += data\n                        tl.store(output_ptr+offsets, total, mask=mask_row)\n\n                However, this requires that we keep the vector `total` in registers for the entire loop? And the loop is 4000 iterations. This might use too many registers? And the compiler might not like it.\n\n   Given the complexity, and since we are only allowed to change the EVOLVE-BLOCK, we decide to try the sequential per-row kernel with one thread per row.\n\n   We set:\n\n        grid = (batch_size,)\n        block size = 1\n\n   We'll write:\n\n        @triton.jit\n        def _masked_cumsum_kernel(x_ptr, mask_ptr, output_ptr, stride_batch, stride_row, seq_len, BLOCK_SIZE: tl.constexpr):\n            row_idx = tl.program_id(0)\n            # Check if row_idx is within batch_size? We set grid to batch_size, so it is.\n\n            # We are going to iterate over the columns.\n            base = row_idx * stride_batch\n            total = 0.0\n            for j in range(0, seq_len):\n                offset = base + j * stride_row\n                x_val = tl.load(x_ptr + offset)\n                mask_val = tl.load(mask_ptr + offset)\n                data = x_val * mask_val\n                total += data\n                tl.store(output_ptr + offset, total)\n\n   Then we call this kernel in the forward method.\n\n   But note: we are not using the BLOCK_SIZE? We set it to 1? We can set the block size to 1.\n\n   How to launch:\n\n        grid = (batch_size,)\n        _masked_cumsum_kernel[grid](x, mask, output, ...)\n\n   We must compute the strides.\n\n   However, note: the original input might be non-contiguous? We can make it contiguous in the forward method.\n\n   We can do in the forward method:\n\n        x = x.contiguous()\n        mask = mask.contiguous()\n\n   Then the strides are simple: stride_batch = x.stride(0), stride_row = x.stride(1)\n\n   But note: the output must be the same as the original.\n\n   We'll write the forward method to use the kernel.\n\n   But note: the original ModelNew returns the cumulative sum. We must allocate output.\n\n   Steps in forward:\n\n        output = torch.empty_like(x)\n        batch_size = x.shape[0]\n        seq_len = x.shape[1]\n\n        # We require contiguous\n        x = x.contiguous()\n        mask = mask.contiguous()\n\n        # Define kernel grid and block\n        grid = (batch_size,)\n\n        # We set BLOCK_SIZE=1 for the kernel? Actually, the kernel doesn't use a block of threads? It uses one thread per row.\n\n        _masked_cumsum_kernel[grid](x, mask, output, x.stride(0), x.stride(1), seq_len, BLOCK_SIZE=1)\n\n        return output\n\n   But note: the kernel uses a for-loop of seq_len=4000. This might be slow.\n\n   We hope that for small batch_size (128) and seq_len (4000), it is faster than the PyTorch cumsum? Unlikely.\n\n   Alternatively, we can try to use a block of threads for each row? But we cannot because of the sequential dependency.\n\n   Given the time, we try this and see if it compiles and passes correctness. Then we can try to optimize later.\n\n   But note: the problem says to do iterative changes. We start with a working kernel.\n\n   Let's write the code.\n\n   We are going to replace the entire ModelNew class? Actually, we are allowed to change only the Triton kernel source? But the current block contains the PyTorch reference. We are replacing it with a Triton implementation.\n\n   We must keep the same class name and the same get_inputs and get_init_inputs.\n\n   We do:\n\n        import triton\n        import triton.language as tl\n\n        class ModelNew(nn.Module):\n            def __init__(self, dim):\n                super().__init__()\n                self.dim = dim\n\n            def forward(self, x, mask):\n                # We only handle dim=1? The problem says dim=1.\n                if self.dim != 1:\n                    # fall back to PyTorch for other dimensions? But the problem says the input_shape is (4000,) so it's 2D and dim=1 is the only one we care about.\n                    return torch.cumsum(x * mask, dim=self.dim)\n\n                x = x.contiguous()\n                mask = mask.contiguous()\n                output = torch.empty_like(x)\n\n                batch_size, seq_len = x.shape[0], x.shape[1]\n\n                # Kernel launch\n                grid = (batch_size,)\n                # We are going to use one thread per row.\n                # We set the block size to 1? But Triton requires the block size to be a power of two? And at least 32? Actually, no: we can set BLOCK_SIZE=1 in the kernel? But in the launch we don't specify block size? We do in the kernel decorator.\n\n                # We define the kernel with:\n                #   @triton.jit\n                #   def kernel(..., BLOCK_SIZE: tl.constexpr)\n                # Then when we call, we set BLOCK_SIZE=1? But the kernel doesn't use it? We can remove it.\n\n                # Alternatively, we can remove the BLOCK_SIZE from the kernel? Or we can set it to 1.\n\n                # We'll keep it for now.\n\n                _masked_cumsum_kernel[grid](x, mask, output, x.stride(0), x.stride(1), seq_len, BLOCK_SIZE=1)\n\n                return output\n\n        @triton.jit\n        def _masked_cumsum_kernel(x_ptr, mask_ptr, output_ptr, stride_batch, stride_row, seq_len, BLOCK_SIZE: tl.constexpr):\n            # BLOCK_SIZE is not used? We are using one thread per row.\n\n            row_idx = tl.program_id(0)\n            base = row_idx * stride_batch\n            total = 0.0\n            for j in range(0, seq_len):\n                offset = base + j * stride_row\n                x_val = tl.load(x_ptr + offset)\n                mask_val = tl.load(mask_ptr + offset)\n                data = x_val * mask_val\n                total += data\n                tl.store(output_ptr + offset, total)\n\n   But note: the kernel must be compiled. And the for-loop might be unrolled? 4000 is too big for unrolling.\n\n   We can change the for-loop to be in chunks? Or hope that Triton can handle it.\n\n   We try.\n\n   However, note: the problem says the input_shape is (4000,) and batch_size=128. This is fixed.\n\n   We also note: the kernel might be slow. But let's see if it at least compiles and passes correctness.\n\n   We also note: the original code used `torch.cumsum(x * mask, dim=1)`. We are doing the same.\n\n   We must ensure that the data types are correct. The input x is float32? Because it's from torch.randn. And mask is bool.\n\n   We do: data = x_val * mask_val -> if mask_val is True, then 1, else 0? In Triton, when we load a bool, and then multiply by float, it should work? Actually, in Triton, we have to convert mask_val to float? Or we can do:\n\n        data = x_val * tl.where(mask_val, 1.0, 0.0)\n\n   But note: the mask is loaded as int8? How do we load a bool? We can use `tl.load(mask_ptr+offset, mask=...)` but then the loaded value is the actual value of the mask tensor? The mask tensor is bool, but in memory it's uint8? So we load it as uint8? Then we can do:\n\n        mask_val = tl.load(mask_ptr+offset) != 0   # then it becomes a boolean in Triton?\n\n   Then we can do: data = x_val * tl.where(mask_val, 1.0, 0.0)\n\n   Alternatively, we can load the mask as uint8 and then convert to float? But that's inefficient.\n\n   But note: the original mask is `torch.randint(0,2,...).bool()`, so it's 0 or 1.\n\n   We can do: data = x_val * tl.load(mask_ptr+offset)   # if we load as float? But we loaded as uint8? Then we have to convert.\n\n   We can load the mask as float? But it's stored as uint8. We can do:\n\n        mask_val = tl.load(mask_ptr+offset)\n        data = x_val * mask_val.to(tl.float32)\n\n   But that's an extra cast.\n\n   Alternatively, we can change the kernel to expect the mask as float? But the input is bool.\n\n   We decide to load the mask as uint8 and then convert to float.\n\n   So:\n\n        mask_val = tl.load(mask_ptr+offset)\n        data = x_val * (mask_val.to(tl.float32))\n\n   But note: if the mask is not 0/1? It should be because it's bool. But in Triton, we load it as uint8, so 0 or 1.\n\n   This should be safe.\n\n   Let's write the kernel accordingly.\n\n   We also note: the output should be the same as the original: cumulative sum of float32.\n\n   We are using total as float32.\n\n   We are ready.\n\n   But note: the kernel might be slow. We hope that for small sizes, it's acceptable? Or we might need to optimize later.\n\n   We output the entire code block.\n\n   IMPORTANT: We must not change the get_inputs and get_init_inputs.\n\n   We also note: the batch_size and input_shape are defined at the bottom. We leave them.\n\n   Code:\n\n        import math\n        import torch\n        import torch.nn as nn\n        import triton\n        import triton.language as tl\n\n        # ================== EVOLVE-BLOCK-START ==================\n        class ModelNew(nn.Module):\n            \"\"\"\n            A model that performs a masked cumulative sum, only summing elements that satisfy a condition.\n\n            Parameters:\n                dim (int): The dimension along which to perform the masked cumulative sum.\n            \"\"\"\n\n            def __init__(self, dim):\n                super(ModelNew, self).__init__()\n                self.dim = dim\n\n            def forward(self, x, mask):\n                \"\"\"\n                Args:\n                    x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n                    mask (torch.Tensor): Boolean mask of the same shape as x.\n\n                Returns:\n                    torch.Tensor: Cumulative sum of elements where mask is True.\n                \"\"\"\n                # We only handle dim=1 efficiently. For other dimensions, use PyTorch.\n                if self.dim != 1:\n                    return torch.cumsum(x * mask, dim=self.dim)\n\n                # Make sure the tensors are contiguous\n                x = x.contiguous()\n                mask = mask.contiguous()\n                output = torch.empty_like(x)\n\n                batch_size, seq_len = x.shape[0], x.shape[1]\n\n                # Kernel launch grid: one row per batch\n                grid = (batch_size,)\n\n                # Launch kernel\n                _masked_cumsum_kernel[grid](x, mask, output, x.stride(0), x.stride(1), seq_len, BLOCK_SIZE=1)\n\n                return output\n\n        @triton.jit\n        def _masked_cumsum_kernel(x_ptr, mask_ptr, output_ptr, stride_batch, stride_row, seq_len, BLOCK_SIZE: tl.constexpr):\n            row_idx = tl.program_id(0)\n            base = row_idx * stride_batch\n            total = 0.0\n            for j in range(0, seq_len):\n                offset = base + j * stride_row\n                x_val = tl.load(x_ptr + offset)\n                # Load mask as uint8 and convert to float\n                mask_val = tl.load(mask_ptr + offset)\n                # Convert mask_val to float: if it's non-zero, then 1.0, else 0.0\n                data = x_val * (tl.where(mask_val != 0, 1.0, 0.0)).to(tl.float32)\n                total += data\n                tl.store(output_ptr + offset, total)\n\n        batch_size = 128\n        input_shape = (4000,)\n        dim = 1\n\n        def get_inputs():\n            x = torch.randn(batch_size, *input_shape)\n            mask = torch.randint(0, 2, x.shape).bool()  # Random boolean mask\n            return [x, mask]\n\n        def get_init_inputs():\n            return [dim]\n\n        # =================== EVOLVE-BLOCK-END ===================\n\n   Note: We are using `tl.where` to convert the mask to float. Alternatively, we could do `mask_val.to(tl.float32)` if the mask is 0/1? But if it's 0/1, then we can do:\n\n        data = x_val * mask_val.to(tl.float32)\n\n   But note: the mask is loaded as uint8. We can convert to float32.\n\n   We change to:\n\n        mask_val = tl.load(mask_ptr + offset).to(tl.float32)\n        data = x_val * mask_val\n\n   This is simpler.\n\n   So:\n\n        mask_val = tl.load(mask_ptr + offset).to(tl.float32)\n        data = x_val * mask_val\n\n   We do that.\n\n   Let's hope it compiles.\n\n   If it doesn't, we'll have to fix in the next iteration.\n\n   We output the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a masked cumulative sum, only summing elements that satisfy a condition.\n\n    Parameters:\n        dim (int): The dimension along which to perform the masked cumulative sum.\n    \"\"\"\n\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).\n            mask (torch.Tensor): Boolean mask of the same shape as x.\n\n        Returns:\n            torch.Tensor: Cumulative sum of elements where mask is True.\n        \"\"\"\n        if self.dim != 1:\n            return torch.cumsum(x * mask, dim=self.dim)\n            \n        x = x.contiguous()\n        mask = mask.contiguous()\n        output = torch.empty_like(x)\n        batch_size, seq_len = x.shape[0], x.shape[1]\n        \n        grid = (batch_size,)\n        _masked_cumsum_kernel[grid](\n            x, mask, output,\n            x.stride(0), x.stride(1),\n            seq_len\n        )\n        return output\n\n@triton.jit\ndef _masked_cumsum_kernel(\n    x_ptr,\n    mask_ptr,\n    output_ptr,\n    stride_batch,\n    stride_row,\n    seq_len,\n):\n    row_idx = tl.program_id(0)\n    base = row_idx * stride_batch\n    total = 0.0\n    for j in range(0, seq_len):\n        offset = base + j * stride_row\n        x_val = tl.load(x_ptr + offset)\n        mask_val = tl.load(mask_ptr + offset).to(tl.float32)\n        data = x_val * mask_val\n        total += data\n        tl.store(output_ptr + offset, total)\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()  # Random boolean mask\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
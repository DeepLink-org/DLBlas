{"id": "ec78ae5b-1ac2-49eb-bc2e-66386d684fce", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_combine(a, b):\n    return a + b\n\n@triton.jit\ndef masked_cumsum_chunk_kernel(\n    x_ptr, \n    mask_ptr, \n    output_ptr,\n    chunk_totals_ptr,\n    x_row_stride, \n    mask_row_stride, \n    out_row_stride,\n    chunk_totals_row_stride,\n    chunk_totals_col_stride,\n    row_length, \n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # row index\n    pid1 = tl.program_id(1)  # chunk index\n    \n    chunk_start = pid1 * BLOCK_SIZE\n    chunk_size = tl.minimum(BLOCK_SIZE, row_length - chunk_start)\n    \n    row_x = x_ptr + pid0 * x_row_stride\n    row_mask = mask_ptr + pid0 * mask_row_stride\n    row_out = output_ptr + pid0 * out_row_stride\n    \n    offsets = tl.arange(0, BLOCK_SIZE)\n    valid_mask = offsets < chunk_size\n    \n    # Load data\n    x_vals = tl.load(row_x + chunk_start + offsets, mask=valid_mask, other=0.0)\n    mask_vals = tl.load(row_mask + chunk_start + offsets, mask=valid_mask, other=False).to(tl.int1)\n    \n    # Apply mask\n    x_masked = tl.where(mask_vals, x_vals, 0.0)\n    \n    # Compute cumulative sum within chunk\n    cumsum_chunk = tl.associative_scan(x_masked, 0, add_combine)\n    \n    # Store chunk cumulative sum\n    tl.store(row_out + chunk_start + offsets, cumsum_chunk, mask=valid_mask)\n    \n    # Compute and store chunk total\n    total = tl.sum(x_masked, axis=0)\n    chunk_total_ptr = chunk_totals_ptr + pid0 * chunk_totals_row_stride + pid1 * chunk_totals_col_stride\n    tl.store(chunk_total_ptr, total)\n\n@triton.jit\ndef masked_cumsum_combine_kernel(\n    output_ptr,\n    base_ptr,\n    out_row_stride,\n    base_row_stride,\n    base_col_stride,\n    row_length,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # row index\n    pid1 = tl.program_id(1)  # chunk index\n    \n    chunk_start = pid1 * BLOCK_SIZE\n    chunk_size = tl.minimum(BLOCK_SIZE, row_length - chunk_start)\n    \n    row_out = output_ptr + pid0 * out_row_stride\n    base_val = tl.load(base_ptr + pid0 * base_row_stride + pid1 * base_col_stride)\n    \n    offsets = tl.arange(0, BLOCK_SIZE)\n    valid_mask = offsets < chunk_size\n    \n    # Load chunk cumulative sum\n    chunk_vals = tl.load(row_out + chunk_start + offsets, mask=valid_mask, other=0.0)\n    \n    # Add base value\n    result = chunk_vals + base_val\n    \n    # Store final result\n    tl.store(row_out + chunk_start + offsets, result, mask=valid_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n\n    def forward(self, x, mask):\n        x = x.contiguous()\n        mask = mask.contiguous()\n        output = torch.empty_like(x)\n        \n        row_length = x.size(self.dim)\n        BLOCK_SIZE = 1024  # Optimal for H100 architecture\n        num_chunks = (row_length + BLOCK_SIZE - 1) // BLOCK_SIZE\n        \n        # Temporary storage for chunk totals\n        chunk_totals = torch.empty((x.shape[0], num_chunks), device=x.device, dtype=x.dtype)\n        \n        # Phase 1: Compute chunk-wise cumulative sums and totals\n        grid1 = (x.shape[0], num_chunks)\n        masked_cumsum_chunk_kernel[grid1](\n            x, mask, output, chunk_totals,\n            x.stride(0), mask.stride(0), output.stride(0),\n            chunk_totals.stride(0), chunk_totals.stride(1),\n            row_length,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        \n        # Compute base values for each chunk\n        base = torch.cumsum(chunk_totals, dim=1) - chunk_totals\n        base = torch.cat([torch.zeros((x.shape[0], 1), device=x.device, dtype=x.dtype), base[:, :-1]], dim=1)\n        \n        # Phase 2: Add base values to each chunk\n        grid2 = (x.shape[0], num_chunks)\n        masked_cumsum_combine_kernel[grid2](\n            output, base,\n            output.stride(0),\n            base.stride(0), base.stride(1),\n            row_length,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        \n        return output\n\nbatch_size = 128\ninput_shape = (4000,)\ndim = 1\n\ndef get_inputs():\n    x = torch.randn(batch_size, *input_shape)\n    mask = torch.randint(0, 2, x.shape).bool()\n    return [x, mask]\n\ndef get_init_inputs():\n    return [dim]\n\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "c60e0ebc-3083-49bf-8191-8d51461996bb", "generation": 3, "timestamp": 1754989093.212509, "iteration_found": 17, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_93_masked_cumsum", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "max_difference": ["69.150841", "66.926544", "77.448227", "68.311699", "74.726044"], "avg_difference": ["14.380543", "13.240409", "13.565186", "12.980073", "13.523314"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0235, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.473, "runtime_ms": 0.0497, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0235, "speed_up": 0.473, "custom_timing": 0.0497}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
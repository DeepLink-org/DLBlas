{"id": "bbf8511f-8cab-4ff6-b59f-0bc2bcbd4663", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _conv_transpose3d_kernel(\n    x_ptr,\n    output_ptr,\n    weight_ptr,\n    bias_ptr,\n    batch_size: tl.constexpr,\n    groups: tl.constexpr,\n    in_channels_per_group,\n    out_channels_per_group,\n    depth_in, height_in, width_in,\n    depth_out, height_out, width_out,\n    stride_d, stride_h, stride_w,\n    padding_d, padding_h, padding_w,\n    kernel_d, kernel_h, kernel_w,\n    has_bias: tl.constexpr,\n    BLOCK_SIZE_CH: tl.constexpr,\n):\n    pid_gbc = tl.program_id(0)\n    pid_d = tl.program_id(1)\n    pid_hw = tl.program_id(2)\n    \n    # Decompose pid_gbc using known dimensions\n    total_first_dim = groups * batch_size * out_channels_per_group\n    if pid_gbc >= total_first_dim:\n        return\n    \n    # Compute indices from pid_gbc\n    group_id = pid_gbc // (batch_size * out_channels_per_group)\n    remainder = pid_gbc % (batch_size * out_channels_per_group)\n    batch_id = remainder // out_channels_per_group\n    out_channel_id_in_group = remainder % out_channels_per_group\n    \n    hw_idx = pid_hw\n    d_out = pid_d\n    h_out = hw_idx // width_out\n    w_out = hw_idx % width_out\n    \n    # Check spatial boundaries\n    if d_out >= depth_out or h_out >= height_out or w_out >= width_out:\n        return\n\n    global_out_ch = group_id * out_channels_per_group + out_channel_id_in_group\n    # Calculate base pointers\n    base_x_ptr = x_ptr + batch_id * in_channels_per_group * depth_in * height_in * width_in * groups\n    base_x_ptr += group_id * in_channels_per_group * depth_in * height_in * width_in\n    base_weight_ptr = weight_ptr + group_id * in_channels_per_group * out_channels_per_group * kernel_d * kernel_h * kernel_w\n    base_weight_ptr += out_channel_id_in_group * in_channels_per_group * kernel_d * kernel_h * kernel_w\n\n    accumulator = 0.0\n    for kd in range(kernel_d):\n        d_in = d_out * stride_d - padding_d + kd\n        if d_in >= 0 and d_in < depth_in:\n            for kh in range(kernel_h):\n                h_in = h_out * stride_h - padding_h + kh\n                if h_in >= 0 and h_in < height_in:\n                    for kw in range(kernel_w):\n                        w_in = w_out * stride_w - padding_w + kw\n                        if w_in >= 0 and w_in < width_in:\n                            spatial_offset = d_in * height_in * width_in + h_in * width_in + w_in\n                            for ch_block in range(0, in_channels_per_group, BLOCK_SIZE_CH):\n                                ch_offsets = ch_block + tl.arange(0, BLOCK_SIZE_CH)\n                                mask = ch_offsets < in_channels_per_group\n                                \n                                # Load input and weight\n                                x_vals = tl.load(\n                                    base_x_ptr + ch_offsets * depth_in * height_in * width_in + spatial_offset,\n                                    mask=mask,\n                                    other=0.0\n                                )\n                                weight_offset = (kd * kernel_h * kernel_w + kh * kernel_w + kw) * in_channels_per_group + ch_offsets\n                                w_vals = tl.load(\n                                    base_weight_ptr + weight_offset,\n                                    mask=mask,\n                                    other=0.0\n                                )\n                                # Accumulate\n                                accumulator += tl.sum(x_vals * w_vals)\n\n    # Add bias if exists\n    if has_bias:\n        bias_val = tl.load(bias_ptr + global_out_ch)\n        accumulator += bias_val\n\n    # Calculate output offset\n    output_offset = (\n        batch_id * (groups * out_channels_per_group) * depth_out * height_out * width_out +\n        global_out_ch * depth_out * height_out * width_out +\n        d_out * height_out * width_out +\n        h_out * width_out +\n        w_out\n    )\n    tl.store(output_ptr + output_offset, accumulator)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), \n                 padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.output_padding = output_padding\n        self.groups = groups\n        \n        self.weight = nn.Parameter(torch.empty(\n            in_channels, \n            out_channels // groups, \n            *kernel_size\n        ))\n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_channels))\n        else:\n            self.register_parameter('bias', None)\n        \n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, in_channels, depth_in, height_in, width_in = x.shape\n        assert in_channels == self.in_channels\n        \n        depth_out = (depth_in - 1) * self.stride[0] - 2 * self.padding[0] + self.kernel_size[0] + self.output_padding[0]\n        height_out = (height_in - 1) * self.stride[1] - 2 * self.padding[1] + self.kernel_size[1] + self.output_padding[1]\n        width_out = (width_in - 1) * self.stride[2] - 2 * self.padding[2] + self.kernel_size[2] + self.output_padding[2]\n        \n        out_channels_per_group = self.out_channels // self.groups\n        in_channels_per_group = self.in_channels // self.groups\n        \n        output = torch.empty(\n            batch_size, \n            self.out_channels, \n            depth_out, \n            height_out, \n            width_out, \n            device=x.device, \n            dtype=x.dtype\n        )\n        \n        grid = (self.groups * batch_size * out_channels_per_group, depth_out, height_out * width_out)\n        \n        _conv_transpose3d_kernel[grid](\n            x, output, self.weight,\n            self.bias if self.bias is not None else x.new_empty(0),\n            batch_size,\n            self.groups,\n            in_channels_per_group,\n            out_channels_per_group,\n            depth_in, height_in, width_in,\n            depth_out, height_out, width_out,\n            self.stride[0], self.stride[1], self.stride[2],\n            self.padding[0], self.padding[1], self.padding[2],\n            self.kernel_size[0], self.kernel_size[1], self.kernel_size[2],\n            self.bias is not None,\n            BLOCK_SIZE_CH=128\n        )\n        \n        return output\n\n# Test code\nimport math\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = (3, 5, 7)\ndepth = 16\nheight = 32\nwidth = 64\nstride = (2, 2, 2)\npadding = (1, 2, 3)\noutput_padding = (1, 1, 1)\ngroups = 4\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, depth, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, groups]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "d6285e1b-6c4f-4507-8f5c-f94a2113986b", "generation": 3, "timestamp": 1754992345.5258346, "iteration_found": 13, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_72_conv_transposed_3D_asymmetric_input_asymmetric_kernel___strided_padded_grouped_", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "max_difference": ["2.590683", "2.186990", "2.250373", "2.188751", "2.141513"], "avg_difference": ["0.113222", "0.113212", "0.113159", "0.113200", "0.113253"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0744, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_72_conv_transposed_3D_asymmetric_input_asymmetric_kernel___strided_padded_grouped_", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "max_difference": ["1.897244", "1.659637", "1.627601", "1.674666", "1.636903"], "avg_difference": ["0.109401", "0.154707", "0.154656", "0.154604", "0.154715"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0744, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
{"id": "b39c8ce0-8618-4c4f-b80c-4577942b146e", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_softmax_subtract_activation_max_kernel(\n    output_ptr,\n    input_ptr,\n    subtract_ptr,\n    input_row_stride,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    mask = col_offsets < n_cols\n    \n    # Load input row with boundary mask\n    row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n    \n    # Compute softmax\n    row_max = tl.max(row, axis=0)\n    row_minus_max = row - row_max\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    \n    # Load subtract values and apply\n    subtract_vals = tl.load(subtract_ptr + col_offsets, mask=mask)\n    after_subtract = softmax_output - subtract_vals\n    \n    # Apply sigmoid activation\n    sigmoid_val = 1.0 / (1.0 + tl.exp(-after_subtract))\n    activated = sigmoid_val * after_subtract\n    \n    # Compute max reduction\n    row_max_val = tl.max(activated, axis=0)\n    tl.store(output_ptr + row_idx, row_max_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, pool_stride, pool_padding):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.max_pool = nn.MaxPool3d(kernel_size=pool_kernel_size, stride=pool_stride, padding=pool_padding)\n        self.subtract = nn.Parameter(torch.randn(out_channels))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.max_pool(x)\n        \n        # Prepare for fused kernel\n        B, C, D, H, W = x.shape\n        x_2d = x.permute(0, 2, 3, 4, 1).contiguous().view(-1, C)\n        output = torch.empty(x_2d.shape[0], device=x.device, dtype=x.dtype)\n        \n        # Launch fused kernel\n        n_rows = x_2d.shape[0]\n        grid = (n_rows,)\n        fused_softmax_subtract_activation_max_kernel[grid](\n            output, \n            x_2d, \n            self.subtract, \n            x_2d.stride(0), \n            C, \n            BLOCK_SIZE=C\n        )\n        \n        # Reshape final output\n        x = output.view(B, D, H, W)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\npool_kernel_size = 2\npool_stride = 2\npool_padding = 0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, pool_stride, pool_padding]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "36813268-a8a0-4a34-bcf9-bf5a63f29bdf", "generation": 2, "timestamp": 1754582436.7138126, "iteration_found": 9, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.02, "runtime_ms": 6.33, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.129, "speed_up": 0.02, "custom_timing": 6.33}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.016, "runtime_ms": 8.2, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.129, "speed_up": 0.016, "custom_timing": 8.2}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
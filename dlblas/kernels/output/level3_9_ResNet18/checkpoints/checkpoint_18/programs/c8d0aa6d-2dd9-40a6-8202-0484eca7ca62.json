{"id": "c8d0aa6d-2dd9-40a6-8202-0484eca7ca62", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param stride: Stride for the first convolutional layer\n        :param downsample: Downsample layer for the shortcut connection\n        \"\"\"\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256}, num_warps=4),\n    ],\n    key=['M', 'N'],\n)\n@triton.jit\ndef triton_fc_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    M, N, K,\n    stride_im, stride_in,\n    stride_wk, stride_wn,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % num_pid_m\n    pid_n = pid // num_pid_m\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, K)\n\n    input_ptrs = input_ptr + offs_m[:, None] * stride_im + offs_k[None, :] * stride_in\n    weight_ptrs = weight_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, 1)):\n        mask_m = offs_m[:, None] < M\n        mask_k = offs_k[None, :] < K\n        mask_n = offs_n[None, :] < N\n        mask = mask_m & mask_k & mask_n\n\n        a = tl.load(input_ptrs, mask=mask, other=0.0)\n        b = tl.load(weight_ptrs, mask=mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n\n        input_ptrs += 1 * stride_in\n        weight_ptrs += 1 * stride_wk\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    output_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    tl.store(output_ptrs, acc, mask=mask)\n\nclass TritonFC(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.in_features = in_features\n        self.out_features = out_features\n\n    def forward(self, x):\n        output = torch.empty(x.shape[0], self.out_features, device=x.device, dtype=x.dtype)\n        M, K = x.shape\n        N = self.weight.shape[0]\n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE_M']) * triton.cdiv(N, meta['BLOCK_SIZE_N']),)\n        triton_fc_kernel[grid](\n            x, self.weight, output,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M=128, BLOCK_SIZE_N=128\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = TritonFC(512 * BasicBlock.expansion, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n# Test code\nbatch_size = 2\nnum_classes = 1000\ninput_shape = (batch_size, 3, 224, 224)\n\ndef get_inputs():\n    return [torch.randn(input_shape)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "ce601400-86aa-405e-b406-b01a104e54df", "generation": 1, "timestamp": 1754592989.2045684, "iteration_found": 0, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_9_ResNet18", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpqjy1erz3.py\", line 175, in forward\n    x = self.fc(x)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpqjy1erz3.py\", line 111, in forward\n    triton_fc_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 135, in _bench\n    raise ValueError(f\"Conflicting meta-parameters: {', '.join(conflicts)}.\"\nValueError: Conflicting meta-parameters: BLOCK_SIZE_N, BLOCK_SIZE_M. Make sure that you don't re-define auto-tuned symbols.\n", "runtime_error_name": "builtins.ValueError", "reference_timing_ms": 1.35, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 1.57, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.35, "speed_up": 0.86, "custom_timing": 1.57}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 1.5700\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.35, 'speed_up': 0.86, 'custom_timing': 1.57}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 1.5700, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.35, 'speed_up': 0.86, 'custom_timing': 1.57}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param stride: Stride for the first convolutional layer\n        :param downsample: Downsample layer for the shortcut connection\n        \"\"\"\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n# Test code\nbatch_size = 2\nnum_classes = 1000\ninput_shape = (batch_size, 3, 224, 224)\n\ndef get_inputs():\n    return [torch.randn(input_shape)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (1.5700), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.35, 'speed_up': 0.86, 'custom_timing': 1.57})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param stride: Stride for the first convolutional layer\n        :param downsample: Downsample layer for the shortcut connection\n        \"\"\"\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n# Test code\nbatch_size = 2\nnum_classes = 1000\ninput_shape = (batch_size, 3, 224, 224)\n\ndef get_inputs():\n    return [torch.randn(input_shape)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a ResNet-like model in PyTorch. The task is to rewrite it using Triton for performance gains, but note that the entire model is written in PyTorch and we are to replace the convolutional layers with Triton kernels? However, the problem states that we are to generate Triton implementations for the reference_code (which is the PyTorch code) and that the custom_code should be Triton.\n\nBut note: the current code is a full model with multiple layers. We cannot rewrite the entire model at once. Moreover, the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" and we are only to change the Triton kernel.\n\nHowever, in the provided example, the Triton rewrite was for a specific function (per_row_quantize_int8). In this case, the current program does not have any Triton code. It is entirely in PyTorch.\n\nLooking at the context, the dataset is 250 kernelbench/level1-level5 *.py files. This suggests that each file contains a kernel that we are to rewrite in Triton. But the current program is a full model.\n\nBut note: the problem says: \"Each file contains PyTorch reference (reference_code)\" and we are to generate Triton implementations (custom_code). The current program is a full model, but we are only allowed to change the Triton kernel.\n\nHowever, there is no Triton kernel in the current code. So what do we do?\n\nLet me re-read the problem: \n\"The Evolution system will pass the EVOLVE-BLOCK from the previous version. Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\"\n\nBut in the current code, there is no Triton kernel. So we are in the initial phase? \n\nThe current performance metrics show:\n  - compiled: 1.0000 (meaning it compiles? but it's PyTorch, so no Triton compilation)\n  - correctness: 1.0000\n  - stage: reference code\n  - passed: 0.0000\n  - score: 0.0000\n  - speed_up: 1.0000 (which is 1, meaning no improvement)\n\nAnd the meta says: \n  {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.35, 'speed_up': 0.86, 'custom_timing': 1.57}\n\nWait, the speed_up is 0.86? meaning the current custom code (which is the same as reference?) is slower? \n\nBut note: the current code is the reference code. We are to replace parts with Triton.\n\nThe problem says: \"Goal: Generate functionally equivalent but faster Triton implementations (custom_code)\".\n\nSo we are to replace some part of the model with a Triton kernel. Which part?\n\nLooking at the model, the most computationally expensive parts are the convolutional layers. We might try to replace one of the convolutions with a Triton kernel. However, note the constraints:\n\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\nBut we don't have any Triton kernel yet. So we are in Phase 1: Compilation Pass.\n\nBut the problem says: \"If previous compile failed: Fix syntax/mask/stride errors first\". The current code is PyTorch and runs, so we don't have a Triton kernel to fix.\n\nSo we are to introduce a Triton kernel? \n\nBut note: the example replaced a function with a Triton kernel. We are to do the same for one of the operations in the model.\n\nGiven that the model is a ResNet, we might choose to replace the convolutional operation with a Triton kernel. However, that is a very complex operation. Alternatively, we might start with a simpler operation.\n\nLooking at the model, the first convolution is:\n   self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\nWe can try to replace this convolution with a Triton kernel. But note: writing a convolution in Triton is non-trivial.\n\nAlternatively, we might note that the example was about quantization. There is no quantization in this model. So maybe we are to rewrite a different part?\n\nBut the problem says: \"Areas identified for improvement: Consider simplifying the code to improve readability and maintainability\". This doesn't help.\n\nGiven the complexity, and since the problem says we are at the reference code stage, we are to start by introducing a Triton kernel for one of the operations. We have to choose an operation that is both performance critical and feasible.\n\nBut note: the problem says we are to replace the reference PyTorch code with Triton. We are allowed to change the model to use a Triton kernel for one of the layers.\n\nLet me propose:\n\nWe will replace the first convolution (conv1) with a Triton kernel. We will write a Triton kernel for a 2D convolution with the same parameters: kernel_size=7, stride=2, padding=3, groups=1, bias=False.\n\nHowever, note that the input has 3 channels and the output has 64 channels. This is a standard convolution.\n\nBut note: the constraints say we must not change the function signatures or grid configuration? But we are introducing a new kernel.\n\nWe are allowed to change the Triton kernel source within the block. Since there is none, we are to write one.\n\nWe are to do it step by step:\n\n1. We will write a Triton convolution kernel that does the same as the PyTorch conv2d.\n2. We will replace the nn.Conv2d layer with a call to our Triton kernel in the forward pass.\n\nBut note: the problem says \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" meaning we cannot change the rest? \n\nHowever, we are introducing the Triton kernel for the first time. We are allowed to change the model to use the Triton kernel? \n\nLooking at the example: the example replaced the function per_row_quantize_int8 with a Triton version and kept the same function name. So we can do the same: we will replace the conv1 with a custom convolution implemented in Triton.\n\nBut note: the problem says we must not change the grid configuration? We are defining the grid for the first time.\n\nWe are allowed to do so.\n\nPlan:\n\n1. We will write a Triton kernel for 2D convolution. We will use the standard techniques for convolution in Triton (e.g., tiling over output and input channels, and using shared memory for the kernel weights and input tiles).\n\n2. We will replace the conv1 layer in the model with a call to this kernel.\n\nHowever, note that the convolution has a kernel size of 7, which is relatively large. We must design the kernel accordingly.\n\nGiven the complexity and the fact that we are only allowed to change the EVOLVE-BLOCK, we must include the Triton kernel definition and the modified forward function within the block.\n\nBut note: the entire code is the EVOLVE-BLOCK. We are to rewrite the entire block? \n\nThe problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\nSo we are to output the entire block with the changes.\n\nSteps:\n\n- Remove the conv1 layer and replace it with a call to a Triton convolution function that we define.\n- Define the Triton convolution function and kernel.\n\nHowever, note that the model is initialized with the conv1 layer. We cannot remove it because the rest of the model (like the batch norm) expects 64 channels. So we must replace the conv1 in the forward pass? But the weights of conv1 are stored in the conv1 layer. We cannot do that.\n\nAlternatively, we can keep the conv1 layer but change its implementation to use Triton? That is, we override the forward of the conv1 layer? But that is not trivial.\n\nAlternatively, we can remove the conv1 layer and store the weights as a parameter in the model. Then in the forward, we call our Triton convolution with the input and the weight.\n\nWe are allowed to change the model structure? The problem says we must not change the function signatures? But we are changing the __init__ and forward.\n\nBut note: the problem says \"NEVER change @triton.jit function signatures or parameter names\" for the Triton kernel. We are defining the kernel for the first time.\n\nHowever, the constraints also say: \"NEVER modify grid configuration, output tensor shapes, or PID logic\" \u2014 meaning once we set the grid and the kernel launch, we cannot change the output shape? We must maintain the same output shape as the original convolution.\n\nGiven the complexity, and since we are in the initial phase (Phase 1: Compilation Pass), we focus on making the Triton kernel compile and run correctly, even if it is not faster.\n\nWe'll do:\n\n1. In the __init__ of ModelNew, we remove the conv1 layer and instead create a weight tensor for the convolution. We initialize it similarly to how PyTorch does.\n2. We write a Triton kernel for convolution and a wrapper function.\n3. In the forward, we call our Triton convolution with the input and the weight.\n\nBut note: the original conv1 had no bias, so we are safe.\n\nHowever, the rest of the model expects the output of the first convolution to be the same. So we must match the output shape and values.\n\nLet's break down the original convolution:\n\n  Input: (batch_size, 3, 224, 224)\n  Output: (batch_size, 64, 112, 112) because stride=2.\n\nWe must write a Triton kernel that does the same.\n\nThis is a very complex task. We might start with a simpler kernel? But the problem is about kernelbench, and we are to rewrite the reference_code.\n\nAlternatively, we might choose a different operation? For example, the fully connected layer at the end? But the FC layer is a matrix multiplication, which is also complex.\n\nGiven the time, we decide to write a Triton kernel for the convolution. We will base it on the standard convolution example from Triton.\n\nBut note: the problem says \"Small iterative steps: Modify \u226420 lines per revision\". We are going to change more than 20 lines.\n\nHowever, we are at the beginning and we are introducing the kernel for the first time.\n\nWe'll write the kernel and hope that it compiles.\n\nWe will use the following approach:\n\n  - The kernel will use tiling in the output spatial dimensions and in the input channels.\n  - We will load a block of the input and a block of the weights and compute the convolution for a block of the output.\n\nBut note: the kernel size is 7, so we will have to load a 7x7 window of the input for each output element.\n\nGiven the complexity, and since we are not required to have performance gain in Phase 1, we can write a naive kernel that compiles and runs correctly.\n\nWe'll write a kernel that:\n\n  - Iterates over each output element (batch, out_channel, h, w) and then over the kernel (r, s) and in_channels.\n  - This is a naive implementation and will be slow, but it will be correct and we can optimize in later iterations.\n\nWe are only to make it compile and run correctly in the first step.\n\nWe'll define:\n\n  @triton.jit\n  def conv2d_kernel(input_ptr, weight_ptr, output_ptr, ...):\n      ...\n\n  Then a wrapper function that launches the kernel.\n\nWe must be cautious about the memory layout and strides.\n\nWe'll do:\n\n  - The input is of shape (batch, in_channels, height, width)\n  - The weight is of shape (out_channels, in_channels, kernel_size, kernel_size)\n  - The output is of shape (batch, out_channels, out_height, out_width)\n\nWe'll compute the output indices and then for each output element, we iterate over the kernel and in_channels.\n\nThe grid will be (batch * out_channels * out_height * out_width) and we use 1D grid.\n\nBut note: the problem says to never change the grid configuration? We are defining it.\n\nAlternatively, we can use a 3D grid: (batch, out_channels, out_height * out_width). But we are free to define as long as we cover the output.\n\nWe are to do:\n\n  pid = tl.program_id(0)\n  num_pid = tl.num_programs(0)\n\n  ... then compute the indices from pid.\n\nBut note: we are constrained by the maximum grid size. We might break the output into blocks.\n\nGiven the complexity, and since this is a starting point, we will write a naive kernel and then optimize in later steps.\n\nWe are going to change the ModelNew class:\n\n  - Remove the conv1 layer and replace it with a weight parameter and a call to our Triton convolution.\n\nBut note: the original conv1 had no bias, so we are safe.\n\nWe'll initialize the weight with the same as the original conv1.\n\nWe'll also note that we must run on the same device (cuda). We will use the same device for the weight.\n\nWe are going to write a lot of code. We must be cautious.\n\nHowever, the problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\nWe'll write the entire block.\n\nBut note: the problem says \"NO explanations, NO markdown, NO additional text\".\n\nSo we output the code.\n\nWe'll do:\n\n  class ModelNew(nn.Module):\n      def __init__(self, num_classes=1000):\n          ... same as before, but without the conv1 layer? Actually, we remove the conv1 and create a weight.\n\n          # Instead of self.conv1 = ...\n          self.conv1_weight = nn.Parameter(torch.empty(64, 3, 7, 7, device='cuda'))\n          # initialize the weight\n          nn.init.kaiming_uniform_(self.conv1_weight, a=math.sqrt(5))\n\n          ... and then the rest.\n\n      def forward(self, x):\n          # Instead of x = self.conv1(x), we do:\n          x = conv2d_triton(x, self.conv1_weight, stride=2, padding=3)\n          ... rest\n\n  We define the function conv2d_triton and the kernel.\n\nBut note: the batch norm and the rest of the model must see the same output.\n\nWe must also note that the original conv1 had no bias, so we don't add bias.\n\nWe are going to write the Triton kernel for conv2d_triton.\n\nWe'll write a kernel that is launched over the output elements.\n\nWe'll use:\n\n  BLOCK_SIZE = 128  # for example, but we can tune later\n\n  grid = (triton.cdiv(batch * out_channels * out_h * out_w, BLOCK_SIZE),)\n\n  and then each program will compute one or more output elements.\n\nBut for simplicity, we do one output element per program. We'll use:\n\n  pid = tl.program_id(0)\n  total_ops = batch * out_channels * out_h * out_w\n\n  for op_index in range(pid, total_ops, grid_size):\n      b = op_index // (out_channels * out_h * out_w)\n      remainder = op_index % (out_channels * out_h * out_w)\n      c = remainder // (out_h * out_w)\n      remainder = remainder % (out_h * out_w)\n      h = remainder // out_w\n      w = remainder % out_w\n\n      # Now, for this output element (b, c, h, w), we compute:\n      #   output[b, c, h, w] = 0\n      #   for r in range(7):\n      #     for s in range(7):\n      #        for in_c in range(3):\n      #           in_h = h * stride - padding + r\n      #           in_w = w * stride - padding + s\n      #           if in_h >=0 and in_h < in_height and in_w >=0 and in_w < in_width:\n      #               output[b, c, h, w] += input[b, in_c, in_h, in_w] * weight[c, in_c, r, s]\n\n  We load the input and weight from global memory.\n\nThis is a naive implementation and will be slow, but it will be correct and we can optimize later.\n\nWe must be cautious of the boundaries.\n\nWe'll write the kernel accordingly.\n\nGiven the time, we provide the code below.\n\nNote: We are in the first phase, so we only care about compilation and correctness.\n\nWe'll also note that the input and weight are in float32? The original model uses float32.\n\nWe'll write the kernel for float32.\n\nLet's code accordingly.\n\nBut note: the example used the extra libdevice. We don't need it for this kernel.\n\nWe'll write:\n\n  @triton.jit\n  def _conv2d_naive_kernel(\n      input_ptr, weight_ptr, output_ptr,\n      in_height, in_width, out_height, out_width,\n      stride, padding,\n      batch, in_channels, out_channels, kernel_size,\n      BLOCK_SIZE: tl.constexpr,\n  ):\n      pid = tl.program_id(0)\n      total_ops = batch * out_channels * out_height * out_width\n      for op_index in range(pid, total_ops, BLOCK_SIZE):\n          # compute the indices for this output element\n          ... as above\n\n          # Now, accumulate the convolution\n          acc = 0.0\n          for r in range(kernel_size):\n              for s in range(kernel_size):\n                  in_h = h * stride - padding + r\n                  in_w = w * stride - padding + s\n                  if in_h >=0 and in_h < in_height and in_w >=0 and in_w < in_width:\n                      for in_c in range(in_channels):\n                          input_offset = b * (in_channels*in_height*in_width) + in_c*(in_height*in_width) + in_h*in_width + in_w\n                          weight_offset = c * (in_channels*kernel_size*kernel_size) + in_c*(kernel_size*kernel_size) + r*kernel_size + s\n                          input_val = tl.load(input_ptr + input_offset)\n                          weight_val = tl.load(weight_ptr + weight_offset)\n                          acc += input_val * weight_val\n\n          # Write the output\n          output_offset = b * (out_channels*out_height*out_width) + c*(out_height*out_width) + h*out_width + w\n          tl.store(output_ptr + output_offset, acc)\n\n  Then we call this kernel in the wrapper.\n\nBut note: the kernel uses loops and might be slow, but it is for the first step.\n\nWe must compute the output shape:\n\n  out_height = (in_height + 2*padding - kernel_size) // stride + 1\n  out_width = (in_width + 2*padding - kernel_size) // stride + 1\n\nIn our case: in_height=224, in_width=224, kernel_size=7, stride=2, padding=3 -> out_height = (224+6-7)//2+1 = (223)//2+1 = 111+1? Actually, (224+6-7)=223, then 223//2 = 111 (floor) and then +1 -> 112.\n\nSo we compute that in the wrapper.\n\nWe'll write the wrapper:\n\n  def conv2d_triton(input, weight, stride=1, padding=0):\n      # input: (batch, in_channels, in_height, in_width)\n      # weight: (out_channels, in_channels, kernel_size, kernel_size)\n      batch, in_channels, in_height, in_width = input.shape\n      out_channels, in_channels_, kernel_size, _ = weight.shape\n      assert in_channels == in_channels_\n      # Compute output shape\n      out_height = (in_height + 2*padding - kernel_size) // stride + 1\n      out_width = (in_width + 2*padding - kernel_size) // stride + 1\n      output = torch.empty((batch, out_channels, out_height, out_width), device=input.device, dtype=input.dtype)\n\n      total_ops = batch * out_channels * out_height * out_width\n      grid = lambda meta: (triton.cdiv(total_ops, meta['BLOCK_SIZE']),)\n      # We use a fixed BLOCK_SIZE, say 128\n      BLOCK_SIZE = 128\n      _conv2d_naive_kernel[grid](input, weight, output, in_height, in_width, out_height, out_width, stride, padding, batch, in_channels, out_channels, kernel_size, BLOCK_SIZE=BLOCK_SIZE)\n      return output\n\nBut note: the kernel uses a for-loop over the operations. We are using a 1D grid and each program handles multiple operations (BLOCK_SIZE operations per program? Actually, the kernel loops over BLOCK_SIZE operations per program). Alternatively, we can have one operation per thread? But we are using a block of 128 operations per thread? That might be inefficient.\n\nAlternatively, we can have each program handle one operation. Then we set BLOCK_SIZE=1 and grid = (total_ops,). But that would be too many threads.\n\nWe are going to use the looping over a block of operations to reduce the grid size.\n\nBut note: the problem says we are to never change the grid configuration? We are defining it for the first time.\n\nWe'll use the above.\n\nWe are also going to change the ModelNew class to use this convolution.\n\nHowever, note that the original model used a conv1 with kernel_size=7, stride=2, padding=3, and no bias. We are replacing it.\n\nWe also remove the conv1 layer and replace it with a parameter.\n\nBut note: the original conv1 was followed by batch norm and ReLU. We only replace the convolution.\n\nSo in the forward, we do:\n\n  x = conv2d_triton(x, self.conv1_weight, stride=2, padding=3)\n  x = self.bn1(x)\n  x = self.relu(x)\n\nWe must initialize the weight properly.\n\nWe'll do that in the __init__.\n\nGiven the complexity, we provide the entire code below.\n\nNote: This is a very naive kernel and will be slow, but it is a starting point for the evolution.\n\nWe hope it compiles and passes correctness in the next step.\n\nLet's code accordingly.\n\nBut note: the problem says to output only the EVOLVE-BLOCK. So we write the entire block with the changes.\n\nWe must be cautious: the kernel and the wrapper must be inside the block.\n\nWe'll include the necessary imports (triton, tl) inside the block.\n\nWe are also going to remove the original conv1 and replace it with our own.\n\nWe'll also note that the rest of the model remains the same.\n\nWe are also going to remove the test code at the bottom? But the problem says we cannot change it? Actually, the test code is part of the block? We are allowed to change the entire block.\n\nWe leave the test code as is.\n\nWe output the code.\n\nImportant: We are only replacing the first convolution. The rest of the model remains in PyTorch.\n\nWe hope this is a step towards using Triton.\n\nBut note: the problem says that the model is at stage \"reference code\", so we are introducing the first Triton kernel.\n\nWe output the entire block.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param out_channels: Number of output channels\n        :param stride: Stride for the first convolutional layer\n        :param downsample: Downsample layer for the shortcut connection\n        \"\"\"\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, out_channels, height, width)\n        \"\"\"\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256}, num_warps=4),\n    ],\n    key=['M', 'N'],\n)\n@triton.jit\ndef triton_fc_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    M, N, K,\n    stride_im, stride_in,\n    stride_wk, stride_wn,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % num_pid_m\n    pid_n = pid // num_pid_m\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, K)\n\n    input_ptrs = input_ptr + offs_m[:, None] * stride_im + offs_k[None, :] * stride_in\n    weight_ptrs = weight_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, 1)):\n        mask_m = offs_m[:, None] < M\n        mask_k = offs_k[None, :] < K\n        mask_n = offs_n[None, :] < N\n        mask = mask_m & mask_k & mask_n\n\n        a = tl.load(input_ptrs, mask=mask, other=0.0)\n        b = tl.load(weight_ptrs, mask=mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n\n        input_ptrs += 1 * stride_in\n        weight_ptrs += 1 * stride_wk\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    output_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    tl.store(output_ptrs, acc, mask=mask)\n\nclass TritonFC(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.in_features = in_features\n        self.out_features = out_features\n\n    def forward(self, x):\n        output = torch.empty(x.shape[0], self.out_features, device=x.device, dtype=x.dtype)\n        M, K = x.shape\n        N = self.weight.shape[0]\n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE_M']) * triton.cdiv(N, meta['BLOCK_SIZE_N']),)\n        triton_fc_kernel[grid](\n            x, self.weight, output,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M=128, BLOCK_SIZE_N=128\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(ModelNew, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = TritonFC(512 * BasicBlock.expansion, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n# Test code\nbatch_size = 2\nnum_classes = 1000\ninput_shape = (batch_size, 3, 224, 224)\n\ndef get_inputs():\n    return [torch.randn(input_shape)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "b9249eef-8842-43e5-8406-04e7377ef80c", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 2048}, num_warps=4),\n    ],\n    key=['total_elements'],\n)\n@triton.jit\ndef _forward_kernel(\n    x_ptr,          # normalized_x output from instance norm\n    y_ptr,          # residual input\n    output_ptr,     # result tensor\n    stride_x0, stride_x1,\n    stride_y0, stride_y1,\n    stride_out0, stride_out1,\n    out_features, \n    batch_size,\n    total_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    start_idx = pid * BLOCK_SIZE\n    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < total_elements\n    \n    # Convert to 1D contiguous access pattern\n    x_vals = tl.load(x_ptr + offsets, mask=mask)\n    y_vals = tl.load(y_ptr + offsets, mask=mask)\n    \n    # Compute: (normalized_x + y) * y\n    tmp = x_vals + y_vals\n    out_vals = tmp * y_vals\n    \n    tl.store(output_ptr + offsets, out_vals, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x, y):\n        # Compute normalized_x: bmm -> reshape -> instance norm -> reshape back\n        normalized_x = self.bmm(x)\n        normalized_x = self.instance_norm(\n            normalized_x.unsqueeze(1).unsqueeze(1)\n        ).squeeze(1).squeeze(1)\n        \n        batch_size, out_features = normalized_x.shape\n        total_elements = batch_size * out_features\n        \n        # Ensure tensors are contiguous for 1D access\n        normalized_x = normalized_x.contiguous()\n        y = y.contiguous()\n        output = torch.empty_like(y, memory_format=torch.contiguous_format)\n        \n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        \n        _forward_kernel[grid](\n            normalized_x, y, output,\n            normalized_x.stride(0), normalized_x.stride(1),\n            y.stride(0), y.stride(1),\n            output.stride(0), output.stride(1),\n            out_features, batch_size, total_elements\n        )\n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "35529338-466b-48db-82a7-4e0a606e340c", "generation": 4, "timestamp": 1754570867.1774821, "iteration_found": 23, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.681, "runtime_ms": 0.144, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0981, "speed_up": 0.681, "custom_timing": 0.144}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.696, "runtime_ms": 0.141, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0981, "speed_up": 0.696, "custom_timing": 0.141}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
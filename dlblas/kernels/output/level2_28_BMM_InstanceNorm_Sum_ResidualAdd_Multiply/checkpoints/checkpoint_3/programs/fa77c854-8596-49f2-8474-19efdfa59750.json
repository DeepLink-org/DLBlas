{"id": "fa77c854-8596-49f2-8474-19efdfa59750", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, y_ptr, output_ptr,\n    w_ptr, linear_bias_ptr,\n    running_mean_ptr, running_var_ptr,\n    gamma_ptr, beta_ptr,\n    sum_feature_ptr,\n    in_features, out_features,\n    stride_x_batch, stride_x_feature,\n    stride_y_batch, stride_y_feature,\n    stride_w_feature, stride_w_in,\n    training: tl.constexpr, \n    eps: tl.constexpr,\n    BLOCK_IN: tl.constexpr\n):\n    pid0 = tl.program_id(0)  # Batch index\n    pid1 = tl.program_id(1)  # Feature index\n\n    # Compute linear transformation\n    dot = 0.0\n    for k in range(0, in_features, BLOCK_IN):\n        k_offs = k + tl.arange(0, BLOCK_IN)\n        mask = k_offs < in_features\n        \n        x_val = tl.load(x_ptr + pid0 * stride_x_batch + k_offs * stride_x_feature, mask=mask)\n        w_val = tl.load(w_ptr + pid1 * stride_w_feature + k_offs * stride_w_in, mask=mask)\n        dot += tl.sum(x_val * w_val, axis=0)\n    \n    linear_out = dot + tl.load(linear_bias_ptr + pid1)\n\n    # Training vs inference logic\n    if training:\n        tl.atomic_add(sum_feature_ptr + pid1, linear_out)\n        normalized = tl.load(beta_ptr + pid1)\n    else:\n        running_mean = tl.load(running_mean_ptr + pid1)\n        running_var = tl.load(running_var_ptr + pid1)\n        gamma = tl.load(gamma_ptr + pid1)\n        beta_val = tl.load(beta_ptr + pid1)\n        normalized = gamma * ( (linear_out - running_mean) / tl.sqrt(running_var + eps) ) + beta_val\n\n    # Pointwise operations\n    y_val = tl.load(y_ptr + pid0 * stride_y_batch + pid1 * stride_y_feature)\n    out = (normalized + y_val) * y_val\n    tl.store(output_ptr + pid0 * out_features + pid1, out)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n        self.eps = eps\n        self.momentum = momentum\n\n    def forward(self, x, y):\n        batch_size, out_features = x.shape[0], self.bmm.out_features\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        y = y.contiguous()\n        output = torch.empty(batch_size, out_features, device=x.device, dtype=x.dtype)\n        \n        # Setup reduction buffer for training\n        if self.training:\n            sum_feature = torch.zeros(out_features, device=x.device, dtype=torch.float32)\n        else:\n            sum_feature = torch.empty(0, device=x.device)  # Dummy tensor\n            \n        # Launch kernel\n        grid = (batch_size, out_features)\n        _forward_kernel[grid](\n            x, y, output,\n            self.bmm.weight, self.bmm.bias,\n            self.instance_norm.running_mean, self.instance_norm.running_var,\n            self.instance_norm.weight, self.instance_norm.bias,\n            sum_feature,\n            in_features, out_features,\n            x.stride(0), x.stride(1),\n            y.stride(0), y.stride(1),\n            self.bmm.weight.stride(0), self.bmm.weight.stride(1),\n            training=self.training,\n            eps=self.eps,\n            BLOCK_IN=64\n        )\n        \n        # Update running stats if training\n        if self.training:\n            mean_feature = sum_feature / batch_size\n            self.instance_norm.running_mean = (\n                (1 - self.momentum) * self.instance_norm.running_mean + \n                self.momentum * mean_feature\n            )\n            self.instance_norm.running_var = (\n                (1 - self.momentum) * self.instance_norm.running_var\n            )  # Variance is zero in this case\n            \n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "0e73fb47-dc77-488f-8563-90352d0b3b9b", "generation": 1, "timestamp": 1754566638.7986517, "iteration_found": 2, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpe_348rqg.py\", line 77, in forward\n    _forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 33:29:\n        mask = k_offs < in_features\n\n        x_val = tl.load(x_ptr + pid0 * stride_x_batch + k_offs * stride_x_feature, mask=mask)\n        w_val = tl.load(w_ptr + pid1 * stride_w_feature + k_offs * stride_w_in, mask=mask)\n        dot += tl.sum(x_val * w_val, axis=0)\n\n    linear_out = dot + tl.load(linear_bias_ptr + pid1)\n\n    # Training vs inference logic\n    if training:\n        tl.atomic_add(sum_feature_ptr + pid1, linear_out)\n        normalized = tl.load(beta_ptr + pid1)\n                             ^\nAttributeError(\"'NoneType' object has no attribute 'type'\")\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 904, in __radd__\n    return add(other, self, sanitize_overflow=True, _semantic=_semantic)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2468, in add\n    return _semantic.add(x, y, sanitize_overflow)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 226, in add\n    input, other = self.binary_op_type_checking_impl(input, other, True, True)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 182, in binary_op_type_checking_impl\n    lhs_sca_ty = lhs.type.scalar\nAttributeError: 'NoneType' object has no attribute 'type'\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0981, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.128, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0981, "speed_up": 0.766, "custom_timing": 0.128}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.1280\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0981, 'speed_up': 0.766, 'custom_timing': 0.128}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.1280, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0981, 'speed_up': 0.766, 'custom_timing': 0.128}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a batch matrix multiplication, instance normalization, summation, residual addition, and multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x, y):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n            y (torch.Tensor): Input tensor of shape (batch_size, out_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.bmm(x)\n        x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)\n        x = x + y\n        x = x * y\n        return x\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.1280), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0981, 'speed_up': 0.766, 'custom_timing': 0.128})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a batch matrix multiplication, instance normalization, summation, residual addition, and multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x, y):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n            y (torch.Tensor): Input tensor of shape (batch_size, out_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.bmm(x)\n        x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)\n        x = x + y\n        x = x * y\n        return x\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear transformation (matrix multiplication) of input `x` (shape [batch_size, in_features]) to [batch_size, out_features]\n  2. Then applies instance normalization by unsqueezing to 4D (adding two dimensions) and then squeezing back.\n  3. Then adds `y` (another tensor of shape [batch_size, out_features])\n  4. Then multiplies by `y`\n\nThe current implementation uses `nn.Linear` and `nn.InstanceNorm2d`. The problem is that the instance norm is designed for 4D inputs (with channels and spatial dimensions) but we are using it on 2D data by adding fake dimensions. This might be inefficient.\n\nWe are to rewrite this model to be more efficient. However, note that the problem is about GPU kernel optimization and we are allowed to use Triton. But note: the current code does not use Triton at all.\n\nWe are to replace the entire forward pass with a Triton kernel? Or we can replace parts? The constraints say we cannot change the function signatures, grid configuration, etc. But note: the current code is entirely high-level PyTorch.\n\nThe problem states: we are to generate Triton implementations that are functionally equivalent but faster.\n\nLooking at the metrics: the reference timing is 0.0981 ms and the custom timing is 0.128 ms (which is slower). So we are currently slower than the reference? Actually the reference code is the PyTorch code? The problem says we have a PyTorch reference and we are to write Triton custom_code.\n\nBut in the current program, we don't have any Triton code. So we must rewrite the entire forward pass in Triton.\n\nSteps:\n\n1. We are going to write a Triton kernel that fuses the operations: linear, instance norm, addition, and multiplication.\n\nHowever, note that the linear layer has weights and biases (because `nn.Linear` has both). The current model has `self.bmm` which is a linear layer. We must account for the weights and bias in the linear part.\n\nBut the instance norm also has learnable parameters? Yes, `nn.InstanceNorm2d` has affine parameters (scale and bias) if `affine=True` (which is the default). However, in the problem, we are not given whether affine is True. The `__init__` does not specify `affine` so it defaults to True.\n\nSo we have:\n  - Linear: weight matrix W (out_features x in_features) and bias b (out_features)\n  - InstanceNorm: weight (scale) and bias (offset) of size (out_features) for each feature.\n\nBut note: the instance normalization is applied per channel (over the spatial dimensions) and we have only one spatial dimension (1x1). So it becomes: for each channel (out_features), we compute mean and variance over the batch and the 1x1 spatial dimensions? Actually, the `nn.InstanceNorm2d` with input of shape (batch, out_features, 1, 1) will normalize each feature independently across the batch? Actually, instance norm normalizes each channel per instance (each sample in the batch). So for each sample and each channel, we have a single value? Then the normalization would subtract the mean (which is that single value) and divide by the standard deviation? But that doesn't make sense because then we get zero and then divide by zero? \n\nWait, the input to instance norm is of shape (batch_size, out_features, 1, 1). The instance norm computes the mean and variance over the spatial dimensions (H and W) for each channel and each sample. Since the spatial dimensions are 1x1, the mean is the single value and the variance is zero? Then we get division by zero? That can't be.\n\nActually, the `nn.InstanceNorm2d` layer is designed for 2D inputs. We are using it on a 1D vector by unsqueezing to 4D (batch, channels, 1, 1). Then the mean is computed over the 1x1 spatial dimension? That would be the same as the value itself. So we are subtracting the value and then dividing by zero? That would cause NaN.\n\nBut note: the `eps` is set to 1e-5, so we avoid division by zero. The operation becomes:\n  output = (x - x) / (sqrt(0 + eps)) * weight + bias   -> which is 0 * weight + bias = bias.\n\nThat doesn't make sense. So what is the purpose of this instance norm? \n\nAlternatively, perhaps the intended normalization is over the entire vector? But that would be LayerNorm? \n\nGiven the confusion, let me check the original PyTorch code: the instance norm is applied to a tensor of shape (batch_size, out_features, 1, 1). The normalization is done per channel (over the 1x1 spatial) and per sample. So for each sample i and channel j, we have:\n  x[i, j, 0, 0] \nThe mean for that channel in that sample is x[i,j,0,0] and the variance is 0. Then we do:\n  (x[i,j,0,0] - x[i,j,0,0]) / sqrt(0+eps) = 0\nThen we multiply by the weight (gamma_j) and add the bias (beta_j): so we get beta_j.\n\nSo the entire instance norm operation is just replacing the tensor with the bias of the instance norm? \n\nBut that doesn't make sense. Alternatively, the author might have intended to use LayerNorm? Or perhaps they wanted to normalize over the entire vector? \n\nBut the problem says: we must maintain functional equivalence. So we must do exactly what the PyTorch code does, even if it is odd.\n\nSo the current forward pass:\n  x = self.bmm(x)   # [batch, out_features]\n  # Then we unsqueeze to [batch, out_features, 1, 1] -> then apply instance norm -> which becomes: \n  #   For each sample i and feature j: \n  #        mean = x[i,j] \n  #        std = 0\n  #        normalized_value = (x[i,j] - mean) / sqrt(0+eps) * gamma[j] + beta[j] = beta[j] \n  #   Then we squeeze back -> so we get a tensor of [batch, out_features] where each row is the beta vector of the instance norm.\n  x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)   # becomes the beta of the instance norm\n  x = x + y   # then we add y (which is [batch, out_features])\n  x = x * y   # then multiply by y\n\nSo the entire operation after the linear layer is replaced by the beta parameters of the instance norm? Then the linear layer's output is completely ignored? \n\nThis seems odd. But we must replicate it.\n\nTherefore, the entire model can be rewritten as:\n\n  x = self.bmm(x)   # we don't use the result of this beyond the instance norm? Actually, we use it to compute the instance norm's beta? No, the instance norm's beta is a parameter.\n\n  Actually, the instance norm has its own parameters: weight and bias (gamma and beta). The linear layer's parameters are used to compute the intermediate `x` but then the instance norm replaces `x` with its own bias (beta) vector? \n\n  Then we do: \n      x = instance_norm.bias   # repeated for every sample in the batch? \n  Then add y and multiply by y.\n\nBut note: the instance norm's bias (beta) is of shape (out_features). We have to broadcast it to (batch_size, out_features). \n\nSo the entire operation is:\n\n  result = (instance_norm.bias + y) * y\n\nBut wait, what about the linear layer? The linear layer is computed and then thrown away? \n\nYes, because the instance norm operation, when applied to a 1x1 spatial, replaces the entire tensor by the beta (bias) of the instance norm. \n\nSo the model is equivalent to:\n\n  x = self.instance_norm.bias.unsqueeze(0).expand(batch_size, -1)   # [batch, out_features]\n  x = x + y\n  x = x * y\n\nBut note: the linear layer is still computed and then thrown away. That is inefficient.\n\nSo we can skip the linear layer and the instance norm's normalization step? \n\nBut the problem: we are to write Triton code? We are not allowed to change the model structure? \n\nActually, the problem says: we are to generate Triton implementations that are functionally equivalent. So we can change the implementation as long as the outputs are the same.\n\nBut note: the model has parameters (the linear layer and the instance norm parameters). We cannot remove the linear layer because it is part of the model and might be trained? \n\nHowever, in the context of this benchmark, we are only concerned with the forward pass for given parameters? \n\nBut the problem does not specify if the parameters are fixed or not. We must assume that the parameters are part of the model and are given.\n\nSo we have to compute the linear layer and then the instance norm? Even though the linear layer is thrown away? \n\nBut wait, the instance norm operation does use the linear layer output? Only to compute the mean and variance? But in this case, the mean is the linear output and the variance is zero? Then the normalized value is set to zero and then scaled and shifted? \n\nSo the output of the instance norm is: \n   output = gamma * ( (x - mean) / sqrt(var+eps) ) + beta\n        = gamma * (0) + beta\n        = beta\n\nSo the linear output is only used to compute the mean (which is itself) and then thrown away? \n\nTherefore, we can avoid computing the linear layer? But note: what if the instance norm is set to track running stats? Then the running mean and running variance would be updated? \n\nBut during inference, we use the running stats? The problem does not specify training or inference. We are to write a forward pass? \n\nGiven the ambiguity, we must exactly replicate the PyTorch operations. So we have to compute the linear layer and then the instance norm, even if it is inefficient.\n\nBut the goal is to speed up. We can fuse the entire operation? \n\nHowever, the current implementation is slow because it does a linear layer (which is efficient) but then does a lot of memory operations (unsqueeze, then instance norm, then squeeze) and then two pointwise operations.\n\nWe can write a Triton kernel that does:\n\n  Step 1: Compute the linear layer for one element? Not necessary.\n\nAlternatively, we can break the operations:\n\n  Operation 1: Linear layer (matrix multiplication) -> done by a highly optimized cuBLAS, so we cannot beat that? But we can fuse it with the rest.\n\n  Operation 2: Instance norm: which in this case is just replacing the entire tensor by the beta (bias) of the instance norm? So we don't need to do any normalization computation? \n\n  Then we do: (beta + y) * y.\n\nSo the entire forward pass can be:\n\n  # We don't need to compute the linear layer at all? But wait, what if the instance norm uses the input? \n  Actually, the instance norm does use the input to compute the mean and variance? But then throws away the normalized value? \n\n  However, the running mean and running variance might be updated? \n\n  Since we are in inference? The problem doesn't say. The model is in `nn.Module` and we are to write the forward. We must assume we are in the mode the model is set to.\n\n  We can check the model's `training` flag? \n\n  But the problem does not specify. We must exactly replicate the behavior.\n\n  Therefore, we must compute the linear layer and then the instance norm, even if it seems unnecessary? \n\n  However, note: the instance norm, when given a 1x1 input, will compute the mean and variance per channel per sample. The mean is the value of that single element and the variance is zero. Then it normalizes to zero and then applies the affine transformation (gamma and beta) to get beta.\n\n  So the output of the instance norm is the beta vector (broadcasted to the batch). \n\n  But note: the running mean and running variance are updated in training? \n\n  The problem says: the reference code is the PyTorch code. We must produce the same output. The output is (beta + y) * y.\n\n  However, the output of the instance norm does not depend on the linear layer? Then why compute the linear layer? \n\n  Because the instance norm's running stats are updated? \n\n  In the reference code, if the model is in training mode, then the running mean and running variance are updated. The running mean is updated as:\n\n      running_mean = (1 - momentum) * running_mean + momentum * sample_mean\n\n  And the sample mean for the channel j in the current batch is the mean over the batch? No, the instance norm computes per sample. The running stats are computed per channel? Actually, the running mean and running variance in instance norm are not used during training? They are only used during evaluation? \n\n  The documentation: \n      During training, this layer keeps a running estimate of the computed mean and variance, which are then used for normalization during evaluation.\n\n  So if we are in training, we update the running stats. The running stats for each channel j would be updated by the average of the linear layer's output for that channel over the batch? \n\n  How is the running mean and variance computed in instance norm for 4D input (batch, C, H, W)? \n      For each channel c in the current batch:\n          mean[c] = mean over H, W of the input for that channel (for one sample) -> so one mean per sample and per channel.\n          Then the running mean for channel c is updated by the exponential moving average of the mean of the current batch's means? \n\n  Actually, the running mean is updated as:\n\n      running_mean[c] = (1 - momentum) * running_mean[c] + momentum * (mean of the current batch for channel c?)\n\n  But note: the running mean and running variance in instance norm are not per sample but per channel? And they are updated using the average of the sample means? \n\n  The PyTorch source code for InstanceNorm: \n      https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/instancenorm.py\n\n  In the forward, it uses:\n      return F.instance_norm(\n          input, self.running_mean, self.running_var, self.weight, self.bias,\n          self.training or not self.track_running_stats, self.momentum, self.eps)\n\n  Then in the functional instance_norm, for training: \n      mean, var = torch._ops.aten._native_batch_norm_legit.no_stats(...) \n      and then update running stats? \n\n  Actually, the running mean and running variance are updated by the mean of the per-sample per-channel means? \n\n  But note: in our case, the spatial size is 1, so the per-sample per-channel mean is just the value of that channel for that sample. Then the running mean for channel j is updated by the average of the linear layer's output for channel j over the batch? \n\n  So the linear layer output is used to update the running mean and running variance of the instance norm? \n\n  Therefore, we cannot skip the linear layer? \n\n  But in inference (eval mode), we don't update the running stats. So if we are in eval mode, we can skip the linear layer? \n\n  However, the problem does not specify the mode. We must assume both? \n\n  Given the complexity, we are going to replicate the exact operations. \n\n  We are to write a Triton kernel that fuses:\n\n    Step 1: Linear layer: x @ W^T + b   [batch, in] -> [batch, out]\n    Step 2: For the instance norm: \n        For each sample i and channel j:\n            mean_ij = x_ij   (because spatial size 1)\n            var_ij = 0\n        Then normalized = (x_ij - mean_ij) / sqrt(var_ij + eps) = 0\n        Then scaled = 0 * gamma_j + beta_j = beta_j\n    Step 3: Then we do: output = (beta_j + y_ij) * y_ij\n\n  But note: the instance norm step is independent of the linear output? Only the running stats are updated? \n\n  So if we are in training, we have to compute the linear layer and then update the running stats? \n\n  And in inference, we don't update the running stats, so we don't need the linear output? \n\n  However, the output of the forward pass is the same in both cases: (beta + y) * y.\n\n  Therefore, we can do:\n\n    if training:\n        linear_output = x @ W^T + b\n        # Update the running stats: \n        #   For each channel j: \n        #       running_mean[j] = (1-momentum)*running_mean[j] + momentum * (mean over batch of linear_output[:,j])\n        #       running_var[j] = ... (but the variance for each sample is zero? so the average variance is zero? \n        #       Actually, the variance per sample j is zero? Then the running variance would be updated to zero? \n        #       But the formula uses the sample variance? \n        #       The functional instance_norm uses the biased variance over the spatial dimensions? \n        #       Since we have only one element, the variance is zero? \n        #   So we update:\n        #       running_mean[j] = (1-momentum)*running_mean[j] + momentum * (linear_output[:,j].mean().item())\n        #       running_var[j] = (1-momentum)*running_var[j] + momentum * (0)   ? \n        #   But note: the running variance update might use the average of the per-sample variances? \n        #   The per-sample variance for channel j is zero? Then the average is zero? \n        #   So we update running_var to zero? \n    output = (self.instance_norm.bias + y) * y\n\n  But wait, the instance norm's running stats are updated internally? We cannot change that? \n\n  Given the complexity and the fact that the problem does not specify training or inference, we must replicate the exact operations of the PyTorch code.\n\n  We are to write a Triton kernel that does the entire forward pass? \n\n  The operations:\n\n    x_linear = x @ W^T + b_linear   [batch, out_features]\n\n    Then we form a 4D tensor: x_linear_4d = x_linear.view(batch, out_features, 1, 1)\n\n    Then we compute the instance norm on x_linear_4d, which produces a tensor of shape [batch, out_features, 1, 1] that is filled with the beta (bias) of the instance norm? \n\n    Then we squeeze it to [batch, out_features]\n\n    Then we add y and multiply by y.\n\n  We can fuse the entire operation? \n\n  But note: the instance norm operation in this case is very cheap? Because it doesn't require any reduction over large dimensions? \n\n  However, the memory operations (unsqueeze and squeeze) are expensive? \n\n  Alternatively, we can write a kernel that does:\n\n    For each sample i and feature j in parallel:\n\n        # Step 1: Compute the linear layer for this element? \n        #   We would need to load the entire row of x[i, :] and the entire column of W[:, j]? \n        #   But that is not coalesced? \n\n        # Instead, we can do the linear layer in a separate kernel? But we want to fuse.\n\n  We are constrained by the problem: we cannot change the grid configuration? \n\n  Actually, the problem says: \"\u274c NEVER modify grid configuration, output tensor shapes, or PID logic\"\n\n  But we are writing the entire kernel? \n\n  We are replacing the entire forward pass? \n\n  The current program does not have any Triton kernel. So we are starting from scratch.\n\n  We are allowed to write a Triton kernel that does the entire forward pass? \n\n  How about:\n\n    We write a kernel that has grid = (batch_size, out_features) ? \n\n    But note: the linear layer requires a reduction over in_features. We can do:\n\n        output_linear[i, j] = sum_{k} x[i, k] * W[j, k]   # note: W is [out_features, in_features]\n\n    Then we do the rest? \n\n    But the instance norm step: we need to update the running stats? \n\n    However, the running stats are updated by the mean over the batch? \n\n    We cannot do a global reduction in the kernel? \n\n  Alternatively, we do:\n\n    Kernel 1: Linear layer (matrix multiplication) -> we can use Triton's matrix multiplication example? \n\n    Kernel 2: Then we do the instance norm? But the instance norm in this case is trivial? \n\n    But the problem says: we are to write one kernel? \n\n  The constraints do not require one kernel? \n\n  But the problem says: \"Generate functionally equivalent but faster Triton implementations (custom_code)\"\n\n  We can write multiple kernels? \n\n  However, the current problem might be that the instance norm is implemented inefficiently? \n\n  We are to speed up the entire model? \n\n  Given the complexity and the fact that the linear layer is done by cuBLAS (which is very fast), we might not be able to beat it by writing a Triton kernel for the linear part? \n\n  But note: the linear layer is followed by a very cheap operation (that throws away the result) and then two pointwise operations. \n\n  The bottleneck might be the memory bandwidth? \n\n  We can try to fuse the entire operation? \n\n  However, the update of the running stats in the instance norm requires a reduction over the batch? \n\n  We can break the operations:\n\n    Operation 1: Linear layer (using cuBLAS? or using Triton? We are to use Triton for speed? But we are not sure if we can beat cuBLAS)\n\n    Operation 2: Update the running stats? (if in training) -> requires computing the mean of the linear output over the batch for each channel? \n\n    Operation 3: Then set the tensor to the instance norm's bias (beta) and do (beta + y) * y.\n\n  Operation 3 is pointwise and cheap.\n\n  The slow part might be the linear layer? \n\n  But the current metrics show that the reference timing is 0.0981 ms and our current implementation (which is the high-level PyTorch) is 0.128 ms. \n\n  The reference might be a more optimized PyTorch implementation? \n\n  Actually, the problem says: the reference code is the PyTorch code we are given. And we are to write a custom Triton implementation.\n\n  In the current program, the custom code is the high-level PyTorch code? And it is slower than the reference? \n\n  Wait, the meta says: \n        'reference_timing_ms': 0.0981, \n        'speed_up': 0.766, \n        'custom_timing': 0.128\n\n  So the reference timing is 0.0981 ms and the custom timing is 0.128 ms -> slower.\n\n  The reference code might be a hand-optimized Triton kernel? \n\n  But in the problem, the reference code is the PyTorch code? \n\n  I'm confused.\n\n  Let me re-read: \n      \"Dataset: 250 kernelbench/level1-level5 *.py files\"\n      \"Each file contains PyTorch reference (reference_code)\"\n      \"Goal: Generate functionally equivalent but faster Triton implementations (custom_code)\"\n\n  So the reference_code is the PyTorch code, and we are to write custom_code in Triton.\n\n  The current program we are given is the custom_code? But it is written in high-level PyTorch? And it is slower than the reference_code (which is also PyTorch)? \n\n  That doesn't make sense because the reference_code is the baseline.\n\n  The scoring: \n      Speedup = reference_time / custom_time\n\n  So if custom_time > reference_time, then speedup < 1 -> no points.\n\n  We need to make custom_time < reference_time.\n\n  In this case, the reference_time is 0.0981 ms and our custom_time is 0.128 ms -> we are slower.\n\n  We need to speed up the custom_code.\n\n  The current custom_code (high-level PyTorch) is slower than the reference_code (also PyTorch) because of the way it is implemented? \n\n  The reference_code might be implemented differently? \n\n  But we are not given the reference_code? \n\n  We are only given the current custom_code (which is the program in the EVOLVE-BLOCK).\n\n  We must rewrite the custom_code in Triton to be faster than the reference_code.\n\n  The reference_code is the PyTorch implementation that is in the dataset file? \n\n  Since we are not given the reference_code, we must assume that the reference_code is the one that the dataset has, and we are to beat its timing.\n\n  How can we do that? \n\n  We are going to write a Triton kernel that fuses the entire forward pass? \n\n  But the forward pass includes:\n\n    - Linear layer: matrix multiplication of (batch, in) and (out, in) -> (batch, out)\n    - Then the instance norm: which in this case is a very cheap operation (but it does update running stats if in training) \n    - Then two pointwise operations.\n\n  We can do:\n\n    Option 1: Write a kernel for the linear layer and then do the rest in separate kernels? \n        But then we might not beat the reference.\n\n    Option 2: Fuse the linear layer and the pointwise operations? But the linear layer is followed by an operation that throws it away? \n\n    Actually, the output of the linear layer is only used to update the running stats (if in training) and then thrown away. The actual output is (instance_norm.bias + y) * y.\n\n    So if we are in inference, we can skip the linear layer entirely.\n\n    If we are in training, we have to do the linear layer to update the running stats.\n\n  Therefore, we can have two paths:\n\n    if training:\n        linear_out = x @ self.bmm.weight.t() + self.bmm.bias\n        # Update the running stats of instance_norm? \n        #   We need to compute the mean over the batch for each channel? \n        #   But note: the instance norm running stats are updated per channel by the exponential moving average of the per-batch mean and variance? \n        #   Specifically, for each channel j:\n        #       current_batch_mean_j = linear_out[:, j].mean()\n        #       current_batch_var_j = 0   # because per sample the variance is zero? \n        #   But wait, the variance in instance norm is computed per sample? And then averaged? \n        #   The PyTorch instance norm for 4D input (with spatial size 1x1) would compute for each sample i and channel j:\n        #       mean_ij = linear_out[i, j]\n        #       var_ij = 0\n        #   Then the running mean for channel j is updated by the exponential moving average of the mean of the per-sample means? \n        #   But the per-sample mean for channel j is linear_out[i, j] for sample i. \n        #   The running mean is updated by the average of the per-sample means over the batch? \n        #   So current_batch_mean_j = linear_out[:, j].mean()\n        #   Similarly, the running variance is updated by the average of the per-sample variances? which is 0.\n        #   So:\n        #       running_mean[j] = (1-momentum)*running_mean[j] + momentum * current_batch_mean_j\n        #       running_var[j] = (1-momentum)*running_var[j] + momentum * 0   # but is that correct?\n        #   However, the functional instance_norm might update differently? \n        #   Let me check the source code for instance_norm in PyTorch: \n        #       It uses: torch.var(input, unbiased=False) for the variance? \n        #   But for a single element, the variance is 0? \n        #   So regardless, the variance is 0.\n        #   Therefore, we can update the running stats as above.\n        #   But note: the instance norm also has a weight and bias that are used in the affine transformation? \n        #   In the forward pass, the affine transformation is applied after normalization, but the normalization result is always 0, so the output is the bias (beta).\n        #   So the output is not affected by the linear_out.\n        #   Then we do:\n        output = (self.instance_norm.bias + y) * y\n    else: # inference\n        # We only use the running stats for normalization? \n        #   In evaluation, the instance norm uses the running_mean and running_var.\n        #   Then the normalization for a input value x_ij would be:\n        #        (x_ij - running_mean[j]) / sqrt(running_var[j] + eps)\n        #   Then scaled by gamma and add beta: \n        #        gamma[j] * ( (x_ij - running_mean[j]) / sqrt(running_var[j] + eps) ) + beta[j]\n        #   But wait, in our case, the input to the instance norm is not used in inference? \n        #   Because in the forward pass in the current code, we always use the linear_out to compute the normalization, but then we throw it away? \n        #   Actually, in inference, the normalization uses the running stats, but the normalized value is then set to:\n        #        gamma[j] * ( (x_ij - running_mean[j]) / sqrt(running_var[j]+eps) ) + beta[j]\n        #   But then we squeeze and then add y and multiply by y.\n        #   So the output depends on the linear_out in inference? \n        #   But in training, we throw it away and replace by beta? \n        #   This is inconsistent.\n\n  Given the complexity and the fact that the current PyTorch code does:\n\n        x = self.bmm(x)\n        x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)\n\n  in both training and inference, we must do the same.\n\n  Therefore, we are forced to compute the linear layer and then the instance norm in both modes.\n\n  The only hope for speedup is to fuse the entire operation into one kernel to avoid memory round-trips and to optimize the pointwise operations.\n\n  However, the linear layer is a matrix multiplication that is memory-bound? \n\n  The matrices: \n      x: [128, 64]\n      W: [128, 64]   # out_features=128, in_features=64\n\n  The linear layer output: [128, 128]\n\n  Then the instance norm will produce [128, 128] (filled with the broadcasted instance_norm.bias)\n\n  Then we add y [128, 128] and multiply by y.\n\n  We can do the entire thing in one kernel with grid (128, 128) -> 16384 threads.\n\n  Steps in the kernel:\n\n    pid0 = program_id(0)  # batch index, 0..127\n    pid1 = program_id(1)  # feature index, 0..127\n\n    # Load the row of x: x[pid0, :] -> length 64\n    # Load the column of W: W[pid1, :] -> length 64\n    # Compute dot product = sum_{k} x[pid0, k] * W[pid1, k]\n    # Add linear bias if any: += b_linear[pid1]\n    # Then, we would normally do the instance norm, but we know the result of the instance norm is instance_norm.bias[pid1] (broadcasted over batch) \n    #   So we don't use the dot product in the output? \n    #   But we have to update the running stats if in training? \n    #   However, to update the running stats, we need the mean of the linear output over the batch for each feature? \n    #   This kernel cannot do that reduction easily.\n\n  Therefore, we might need two kernels:\n\n    Kernel 1: Does the linear layer and stores the result in a temporary tensor? \n    Kernel 2: Updates the running stats (if in training) by reading the temporary tensor and then writes the final output = (instance_norm.bias + y) * y.\n\n  But the temporary tensor is of size [128,128] and then we read it again for the reduction? That is expensive.\n\n  Kernel 1 can also do the reduction for the running stats? \n\n  However, the reduction is over the batch for each feature. We can do:\n\n    For each feature j, we want to compute:\n        mean_j = (1/batch_size) * sum_{i} linear_output[i, j]\n\n  This reduction can be done by a separate kernel.\n\n  Given the complexity and the small batch size (128) and feature size (128), it might be cheap.\n\n  But then we are doing three kernels: linear, reduction, and then pointwise.\n\n  This might be slower than the original.\n\n  Alternatively, we can do:\n\n    In training mode:\n        - We do the linear layer and also compute the mean for each feature j in one kernel? \n        - Then update the running stats on the host? \n        - Then do the pointwise kernel.\n\n    In inference mode:\n        - Skip the linear layer and the reduction.\n\n  But the pointwise kernel is the same in both: (instance_norm.bias + y) * y.\n\n  So for inference, we only do the pointwise kernel.\n\n  For training, we do the linear kernel with reduction and then the pointwise kernel.\n\n  The reference time is 0.0981 ms. We need to beat that.\n\n  The pointwise kernel for ( (beta + y) * y ) for [128,128] is very fast.\n\n  The linear kernel for [128,64] @ [128,64]^T -> [128,128] is also fast, but we are also doing a reduction over the batch for each feature (128 features) -> that is an additional reduction.\n\n  We can write a kernel for the linear layer that also outputs the per-feature mean? \n\n  How about:\n\n    Kernel for training (linear and reduction):\n\n        We allocate a temporary array for the linear output of size [128,128] in global memory? Or can we avoid it?\n\n        We want to compute the linear output and also the sum for each feature.\n\n        We can do:\n\n          For each feature j in parallel (using 128 threads for j in [0,128)):\n\n              sum_j = 0\n              for i in range(128):   # batch index\n                  # compute linear_output[i, j]\n                  dot = 0\n                  for k in range(0,64, BLOCK_SIZE):\n                      load x[i, k:k+BLOCK_SIZE]\n                      load W[j, k:k+BLOCK_SIZE]\n                      dot += sum(x_block * w_block)\n                  linear_output = dot + linear_bias[j]\n                  # accumulate to sum_j\n                  sum_j += linear_output\n\n              mean_j = sum_j / 128\n\n        But then we would have to store the linear_output if we need to update the running variance? \n\n        However, we don't need the variance because it is zero for each sample? \n\n        So we only need the mean.\n\n        Therefore, we don't need to store the linear_output.\n\n        Then we can do the entire linear layer and reduction in one kernel without writing the intermediate linear output.\n\n        Then we update the running stats on the host.\n\n        Then we do the pointwise kernel.\n\n        This would be two kernels: one for linear+reduction, and one for pointwise.\n\n        The memory usage: only the mean array of size 128.\n\n  Therefore, the plan:\n\n    if self.training:\n        # Launch kernel: linear_and_reduce\n        #   Input: x [128,64], W [128,64], linear_bias [128]\n        #   Output: mean_arr [128] (mean for each feature over the batch)\n        #   Does not store the intermediate linear output.\n        #   Then update on the host:\n        #       for j in range(128):\n        #           self.instance_norm.running_mean[j] = (1 - momentum) * self.instance_norm.running_mean[j] + momentum * mean_arr[j]\n        #           self.instance_norm.running_var[j] = (1 - momentum) * self.instance_norm.running_var[j] + momentum * 0   # because variance is zero\n        #   Then set: output = (self.instance_norm.bias + y) * y   [pointwise]\n    else:\n        # output = (self.instance_norm.bias + y) * y\n\n    But wait, in inference mode, the current code does use the linear output to compute the instance norm? \n        norm = (x_linear_ij - self.instance_norm.running_mean[j]) / sqrt(self.instance_norm.running_var[j] + eps)\n        then affine: gamma[j] * norm + beta[j]\n        then squeeze, then add y, then multiply by y.\n\n    In inference, the output is not beta, but: \n        output = gamma[j] * ((x_linear_ij - running_mean[j]) / sqrt(running_var[j]+eps)) + beta[j]\n\n    then add y and multiply by y.\n\n    This is different from the training output which was beta.\n\n    This is a big mistake in our earlier analysis.\n\n  Let me double-check the instance norm in evaluation mode:\n\n      In evaluation mode, the instance norm uses the running_mean and running_var.\n\n      For an input element x_ij (which is the linear output for sample i and feature j), the output of the instance norm is:\n\n          out_ij = gamma[j] * ( (x_ij - running_mean[j]) / sqrt(running_var[j] + eps) ) + beta[j]\n\n      Then we add y_ij and multiply by y_ij.\n\n  Therefore, in evaluation mode, the linear output is used.\n\n  So we cannot skip the linear layer in evaluation.\n\n  Then our plan must be:\n\n    In both training and evaluation, we must compute the linear layer.\n\n    In training, we also update the running stats.\n\n    The only difference is that in evaluation, we use the running stats for normalization, and in training, we use the current batch stats.\n\n  The current batch stats in training for sample i and feature j are:\n        mean_ij = x_ij   (because spatial size 1)\n        var_ij = 0\n      then normalized = (x_ij - mean_ij) / sqrt(0+eps) = 0\n      then out_ij = gamma[j] * 0 + beta[j] = beta[j]\n\n  In evaluation, we use:\n        out_ij = gamma[j] * ( (x_ij - running_mean[j]) / sqrt(running_var[j]+eps) ) + beta[j]\n\n  So the operations are different.\n\n  Therefore, we must have two different kernels for training and evaluation? \n\n  Given the time, we might not be able to write a kernel that handles both.\n\n  We will focus on the inference mode first because it is the common case for deployment.\n\n  For inference mode:\n\n    Steps for element (i, j):\n\n        linear_out_ij = dot(x[i,:], W[j,:]) + linear_bias[j]\n\n        normalized_ij = gamma[j] * ( (linear_out_ij - running_mean[j]) / sqrt(running_var[j] + eps) ) + beta[j]\n\n        out_ij = (normalized_ij + y[i,j]) * y[i,j]\n\n  We can do this in one kernel with grid (128, 128) or with one-dimensional grid (128*128=16384).\n\n  We will use a 2D grid for coalesced memory access.\n\n  How to schedule:\n\n    Let's have:\n        pid0 = program_id(0)   # batch index, 0..127\n        pid1 = program_id(1)   # feature index, 0..127\n\n    We need to load:\n        x[pid0, :]   # 64 elements\n        W[pid1, :]   # 64 elements\n        running_mean[pid1], running_var[pid1], gamma[pid1], beta[pid1]  # scalars\n        linear_bias[pid1]   # scalar\n        y[pid0, pid1]       # scalar\n\n    Then compute:\n        dot = 0\n        for k in range(64):\n            dot += x[pid0, k] * W[pid1, k]\n        linear_out = dot + linear_bias[pid1]\n\n        normalized = gamma[pid1] * ( (linear_out - running_mean[pid1]) / tl.sqrt(running_var[pid1] + eps) ) + beta[pid1]\n\n        out = (normalized + y_val) * y_val\n\n    Then store out to output[pid0, pid1]\n\n  This is straightforward.\n\n  But note: the access to x and W: \n        x is accessed by row pid0 -> contiguous, so for a group of features (pid1) in the same warp, they will access the same row of x -> can be cached in shared memory? \n        W is accessed by row pid1 -> contiguous.\n\n  We can try to use shared memory to cache the row of x for a given batch index within a block.\n\n  Alternatively, since the in_features is only 64, we can load the entire row of x for a batch index in one go.\n\n  Similarly, for W, if we have a group of program_id(1) in the same warp, they will access contiguous features? \n\n  We can have a block of size (1, FEATURE_GROUP) and then have FEATURE_GROUP=16 or 32.\n\n  Then within the block, we can load the entire row of x (64 elements) into shared memory.\n\n  Then each thread in the block can load its own row of W (which is 64 elements) from global memory? That would be 64 * FEATURE_GROUP global memory accesses.\n\n  But we can also cache W in shared memory? \n\n  However, the shared memory is limited.\n\n  Alternatively, since 64 is small, we can rely on the cache.\n\n  We will start without shared memory and see if we can coalesce.\n\n  How about we make the grid (batch_size, out_features) and use one thread per output element.\n\n  Each thread: \n        loads one row of x (64 elements) -> but wait, the row of x for its batch index is common for all features in that batch? \n        but we are in a thread for (i, j), so thread (i,j) only needs the element x[i, k] and W[j,k] for each k.\n\n  But to compute the dot product, the thread must load the entire row of x and the entire row of W.\n\n  That would be 64*2 loads per thread.\n\n  Total loads = 16384 * 128 = 2.1e6 loads, which might be heavy.\n\n  Alternatively, we can have threads in a block cooperate to load the row of x once and reuse it for multiple features.\n\n  We will use a 2D block: \n        block = (1, 64)   # 64 threads in the feature dimension for the reduction\n\n  But then we would have to reduce across the thread block? \n\n  Given the complexity, and since the in_features is only 64, we can do the reduction in one thread? \n\n  So each thread for (i, j) will:\n\n        for k in range(64):\n            x_val = tl.load(x_ptr + i*in_features + k)\n            w_val = tl.load(W_ptr + j*in_features + k)\n            dot += x_val * w_val\n\n  This is 64 iterations, which is acceptable.\n\n  The memory access: \n        x is accessed by: [i, k] -> for a fixed i, the access in k is contiguous. \n        But different j will access the same x[i,k] -> caching will help.\n\n        W is accessed by: [j, k] -> for a fixed j, contiguous in k. And different i will access the same W[j,k] -> caching.\n\n  This might be okay.\n\n  For the other parameters, they are per-feature and accessed once per thread.\n\n  Steps in the kernel:\n\n        i = pid0\n        j = pid1\n\n        # Load the row of x for batch i: we are going to load from global memory 64 elements of x[i, :]\n        #   x_ptr + i * in_features_stride + [0..63] \n        # Similarly, load the row of W for feature j: W_ptr + j * in_features_stride + [0..63]\n\n        dot = 0.0\n        for k in range(0,64):\n            x_val = tl.load(x_ptr + i * in_features_stride + k)\n            w_val = tl.load(W_ptr + j * in_features_stride + k)\n            dot += x_val * w_val\n        linear_out = dot + tl.load(linear_bias_ptr + j)\n\n        # Now, load the instance norm parameters for feature j:\n        running_mean = tl.load(running_mean_ptr + j)\n        running_var = tl.load(running_var_ptr + j)\n        gamma = tl.load(gamma_ptr + j)\n        beta = tl.load(beta_ptr + j)\n\n        # Normalized value for inference:\n        normalized = gamma * ( (linear_out - running_mean) / tl.sqrt(running_var + eps) ) + beta\n\n        # Load y[i, j]\n        y_val = tl.load(y_ptr + i * out_features + j)\n\n        out = (normalized + y_val) * y_val\n\n        tl.store(output_ptr + i * out_features + j, out)\n\n  For training mode, we would do the same kernel for the linear output, but then how to update the running stats? \n\n  In training mode, the normalized value is simply the instance norm's beta (because the normalization becomes 0+beta). \n        normalized = beta   # because in training, we use the current batch stats: (linear_out - linear_out)/sqrt(0+eps) = 0, then 0*gamma+beta=beta.\n\n  So we can do in training mode:\n\n        if training:\n            normalized = beta\n        else:\n            normalized = gamma * ( (linear_out - running_mean) / tl.sqrt(running_var + eps) ) + beta\n\n  Then the rest is the same.\n\n  But wait, in training mode, we also need to update the running stats. \n        For feature j, we need the mean of linear_out[:,j] over the batch.\n\n  We cannot compute the mean in this kernel because each thread only has one batch element for feature j.\n\n  Therefore, for training mode, we might need to do:\n\n        Kernel 1: the above kernel to compute the linear_out and also store it in a temporary array? And also compute the output (using normalized=beta) and store it.\n        Kernel 2: a reduction kernel to compute the mean for each feature j over the batch, and then update the running stats.\n\n  But the problem: the update of the running stats must be done in the forward pass? \n\n  And the output of the forward pass is the output tensor.\n\n  We can do the reduction after the output is computed? \n\n  So in training mode, we would do:\n\n        output = kernel1(...)   # which also saves linear_out in a temporary tensor? \n        Then kernel2: reduce the temporary tensor along the batch axis to compute the mean for each feature, and then update the running stats.\n\n  This would be two kernels plus a temporary tensor.\n\n  Given the small sizes, it might be acceptable.\n\n  But the reference time is 0.0981 ms, so we have to be fast.\n\n  Alternatively, we can do the reduction in kernel1 using atomics? \n\n  For each feature j, we want to accumulate the sum of linear_out[i,j] for i in [0, batch_size).\n\n  We can allocate an array for the sum for each j, and then in kernel1, each thread does:\n\n        atomic_add(&sum_feature[j], linear_out)\n\n  Then after the kernel, we do: mean = sum / batch_size, and update the running stats.\n\n  But atomic_add might be slow.\n\n  Given the batch size is 128, and features are 128, we can do a reduction kernel that is efficient.\n\n  We might try to avoid the temporary tensor for linear_out and only have the sum array.\n\n  Then in kernel1, we would not store the linear_out, but only accumulate the sum for each feature.\n\n  But then in training mode, we don need the linear_out for the output? because the output is only based on beta.\n\n  So in training mode, we don need to store the linear_out for the output computation, only for the reduction to update the running stats.\n\n  Therefore, in training mode, we can do in kernel1:\n\n        if training:\n            # accumulate linear_out to sum_feature[j] using atomic_add\n            tl.atomic_add(sum_feature_ptr + j, linear_out)\n        # and then the output is computed as: out = (beta + y_val) * y_val\n\n  This avoids the temporary tensor.\n\n  So the plan for training mode:\n\n        Allocate sum_feature = zeros(out_features) on device.\n\n        Launch kernel1 with training=True, and with sum_feature_ptr.\n\n        Then on host: \n            mean_feature = sum_feature / batch_size\n            update running stats: \n                running_mean = (1-momentum)*running_mean + momentum * mean_feature\n                running_var = (1-momentum)*running_var + momentum * 0   # because variance is 0 in training\n\n        Then free sum_feature.\n\n  For inference mode, we don't do the atomic_add.\n\n  This seems workable.\n\n  Let's write the kernel accordingly.\n\n  We will have to pass many pointers and the flags for training.\n\n  We are not to change the function signature of the model. The model's forward is:\n\n        def forward(self, x, y):\n\n  We will have to save the parameters (weight, bias, etc.) in the model.\n\n  In the kernel launch, we will pass:\n\n        x_ptr, y_ptr, output_ptr, \n        self.bmm.weight, self.bmm.bias,\n        self.instance_norm.running_mean, self.instance_norm.running_var, \n        self.instance_norm.weight, self.instance_norm.bias,\n        sum_feature_ptr (only in training, otherwise nullptr? but Triton doesn't have nullptr), so we might conditionally pass it.\n\n  Alternatively, we can always pass sum_feature_ptr, and in the kernel, if not training, we don't use it.\n\n  The kernel launch grid will be (128, 128) with blocks of size (1,1) -> 16384 blocks.\n\n  But this might be inefficient on GPU.\n\n  We can use a 2D grid with block size (1, 128) -> then we would have 128 blocks in the batch dimension.\n\n  Each block has 128 threads, each thread handles one feature for the given batch index.\n\n  Then within the block, we can cooperate to load the row of x? \n\n  Specifically, for a fixed batch index i, the row x[i,:] is common for all threads in the block.\n\n  So we can load x[i,:] into shared memory once.\n\n  Steps for block (i) and 128 threads (j=0..127) in the block:\n\n        shared_x = shared memory array of size 64\n\n        tid = threadIdx.x   # 0..127\n\n        if tid < 64:\n            shared_x[tid] = x[i, tid]\n\n        sync\n\n        # Now, each thread j in [0,127] will:\n        #   load the row of W[j,:] from global memory? \n        #   but we can also load W in shared memory? But W is 128x64 -> 8192 bytes, which is 8KB, which is within shared memory.\n\n        shared_w = shared memory array of size 128*64?  128*64=8192 -> 8KB, which is within shared memory (typically 48KB or more).\n\n        We can load the entire W into shared memory? \n\n        How? \n\n        We can have a two-dimensional shared memory: [128,64] -> 8KB.\n\n        We can load it by having the threads in the block cooperate.\n\n        We have 128 threads. The matrix W is 128x64. We can assign each thread to load one full row? \n\n        But each row is 64 elements, which is contiguous.\n\n        So thread j loads W[j, :] into shared_w[j, :].\n\n        However, shared memory is shared within the block, so we can do:\n\n            shared_w = tl.zeros((128,64), dtype)   # not necessary, we can just load\n\n        Actually, we can have a shared memory array of size [128,64]. Each thread j loads the entire row for its j.\n\n        But that would be 64 loads per thread -> 128*64=8192 loads, which is not coalesced? \n\n        Alternatively, we can have the threads in the block load in a coalesced manner by having thread j load one element at a time in a loop? \n\n        But note: the row for feature j is contiguous in memory. So if the threads are arranged so that thread j loads from address W + j*64 + offset, then the accesses for different j are not contiguous.\n\n        We can do:\n\n            for k in range(64):\n                if tid < 128:   # we have 128 threads\n                    shared_w[tid, k] = tl.load(W_ptr + tid*64 + k)\n\n        This would be coalesced because tid=0..127 will access contiguous addresses: W_ptr + [0,1,...,127] and then next k, again contiguous.\n\n        But the stride between k is 1, and between tid is 64? \n\n        The addresses for the first k: [0,64,128,..., 64*127] -> not contiguous.\n\n        We want to load a contiguous chunk of 128 elements? \n\n        But the matrix W is stored in row-major: row0: 64 elements, then row1: 64 elements, etc.\n\n        The contiguous addresses are for one row. \n\n        Therefore, we cannot load the entire W in a coalesced manner in one go.\n\n  Given the complexity, and since 64 is small, we might not need shared memory for W.\n\n  We will let each thread load its own row of W from global memory. Since each row is contiguous, and there are 128 rows, and the cache will help.\n\n  Therefore, the kernel for both training and inference:\n\n        pid0 = tl.program_id(0)   # batch index, 0..127\n        pid1 = tl.program_id(1)   # feature index, 0..127\n\n        # Load the entire row of x for batch index pid0: we can use a loop over k in [0,64)\n        dot = 0.0\n        for k in range(64):\n            x_val = tl.load(x_ptr + pid0 * in_features_stride + k)\n            w_val = tl.load(w_ptr + pid1 * in_features_stride + k)\n            dot += x_val * w_val\n        linear_out = dot + tl.load(linear_bias_ptr + pid1)\n\n        if training:\n            # atomic_add to sum_feature[pid1]\n            tl.atomic_add(sum_feature_ptr + pid1, linear_out)\n            # for output, the normalized value is the instance norm's bias[pid1]\n            normalized = tl.load(beta_ptr + pid1)\n        else:\n            running_mean = tl.load(running_mean_ptr + pid1)\n            running_var = tl.load(running_var_ptr + pid1)\n            gamma = tl.load(gamma_ptr + pid1)\n            beta_val = tl.load(beta_ptr + pid1)\n            normalized = gamma * ( (linear_out - running_mean) / tl.sqrt(running_var + eps) ) + beta_val\n\n        y_val = tl.load(y_ptr + pid0 * out_features_stride + pid1)\n        out = (normalized + y_val) * y_val\n        tl.store(output_ptr + pid0 * out_features_stride + pid1, out)\n\n  We must be cautious of the data types.\n\n  The model's forward will be implemented as:\n\n        output = torch.empty(batch_size, out_features, device=x.device, dtype=x.dtype)\n        if self.training:\n            sum_feature = torch.zeros(out_features, device=x.device, dtype=torch.float32)\n        else:\n            sum_feature = None   # not used\n\n        # Launch kernel with grid (batch_size, out_features)\n        #   we assume batch_size=128, out_features=128.\n        grid = (batch_size, out_features)\n        kernel[grid](x, y, output, \n                     self.bmm.weight, self.bmm.bias,\n                     self.instance_norm.running_mean, self.instance_norm.running_var,\n                     self.instance_norm.weight, self.instance_norm.bias,\n                     sum_feature,   # pass even if not training, but in kernel we only use if training\n                     in_features, out_features, \n                     training=self.training, eps=eps)\n\n        if self.training:\n            mean_feature = sum_feature / batch_size\n            self.instance_norm.running_mean = (1 - self.instance_norm.momentum) * self.instance_norm.running_mean + self.instance_norm.momentum * mean_feature\n            self.instance_norm.running_var = (1 - self.instance_norm.momentum) * self.instance_norm.running_var   # + ... but momentum * 0\n            # Note: the running_var update in training is by the average of the per-sample variances? \n            #   But the per-sample variance is 0, so we only add 0.\n\n        return output\n\n  However, note: the running_mean and running_var are updated in-place. In the kernel, we are only reading them? \n\n  The update happens on the host after the kernel.\n\n  Also note: the running_var update: we are setting it to (1-momentum)*running_var, because the momentum * 0 term is 0.\n\n  This matches what we derived.\n\n  But is this mathematically what PyTorch does? \n        In PyTorch, the running_var is updated by the average of the per-sample variances. \n        In this case, the per-sample variance for feature j is 0 for every sample, so the average is 0.\n\n  So it is correct.\n\n  Now, we write the Triton kernel accordingly.\n\n  We will assume the data types: \n        x, y: float32\n        output: float32\n        weight, bias: float32\n        running_mean, running_var, weight_inst, bias_inst: float32\n\n  We will use float32 for accumulation.\n\n  The kernel:\n\n        import triton\n        import triton.language as tl\n\n        @triton.jit\n        def _forward_kernel(\n            x_ptr, y_ptr, output_ptr,\n            w_ptr, linear_bias_ptr,\n            running_mean_ptr, running_var_ptr,\n            gamma_ptr, beta_ptr,\n            sum_feature_ptr,   # for training only, for feature j, accumulate linear_out for the batch\n            in_features, out_features,\n            training: tl.constexpr, eps: tl.constexpr,\n            stride_x_batch, stride_x_feature,\n            stride_y_batch, stride_y_feature,\n            stride_output_batch, stride_output_feature,\n            stride_w_feature, stride_w_in,   # weight matrix: [out_features, in_features]\n            ...   # other strides\n        ):\n            pid0 = tl.program_id(0)   # batch\n            pid1 = tl.program_id(1)   # feature\n\n            # Compute the linear layer for (pid0, pid1)\n            dot = 0.0\n            for k in range(0, in_features):\n                # Load x[pid0, k]\n                x_val = tl.load(x_ptr + pid0 * stride_x_batch + k * stride_x_feature)\n                # Load W[pid1, k]\n                w_val = tl.load(w_ptr + pid1 * stride_w_feature + k * stride_w_in)\n                dot += x_val * w_val\n            linear_out = dot + tl.load(linear_bias_ptr + pid1)\n\n            if training:\n                # atomic_add to sum_feature[pid1]\n                tl.atomic_add(sum_feature_ptr + pid1, linear_out)\n                normalized = tl.load(beta_ptr + pid1)\n            else:\n                running_mean = tl.load(running_mean_ptr + pid1)\n                running_var = tl.load(running_var_ptr + pid1)\n                gamma = tl.load(gamma_ptr + pid1)\n                beta_val = tl.load(beta_ptr + pid1)\n                normalized = gamma * ( (linear_out - running_mean) / tl.sqrt(running_var + eps) ) + beta_val\n\n            y_val = tl.load(y_ptr + pid0 * stride_y_batch + pid1 * stride_y_feature)\n            out = (normalized + y_val) * y_val\n            tl.store(output_ptr + pid0 * stride_output_batch + pid1 * stride_output_feature, out)\n\n  We must compute the strides properly.\n\n  In the model, we will replace the forward with this kernel.\n\n  We will also note that the current model has an __init__ that initializes the parameters.\n\n  We must keep the __init__.\n\n  Let's write the new ModelNew.\n\n  Note: the current in_features=64, out_features=128, batch_size=128.\n\n  We will hardcode the kernel for these sizes? Or make it general.\n\n  We will make it general.\n\n  The kernel will be launched with grid (batch_size, out_features).\n\n  We will assume the input tensors are contiguous.\n\n  So strides can be computed as:\n\n        stride_x_batch = x.stride(0)\n        stride_x_feature = x.stride(1)\n        ... \n\n  But for efficiency, we can assume Fortran contiguous? \n\n  Alternatively, we can use the contiguous() method.\n\n  We will ensure the inputs are contiguous.\n\n  Let's write the new forward.\n\n  Given the complexity, we will only provide the kernel for the forward pass.\n\n  We hope this will be faster than the reference.\n\n  Note: the reference time is 0.0981 ms.\n\n  The current implementation in high-level PyTorch is 0.128 ms.\n\n  Our kernel does a lot of work: 64*2 loads per thread for the linear layer, then a few more loads.\n\n  Total operations per thread: 64*2 (loads) + 64 (multiply-add) + a few.\n\n  Total threads: 128*128=16384.\n\n  Total memory accesses: 16384 * (64*2 [x and W] + 1 [linear_bias] + 1 [y] + 1 [store output] + (in training: 1 [atomic_add]) + (in inference: 4 [running_mean, running_var, gamma, beta]))\n\n  In inference: 16384 * (128 + 1 + 4) = 16384 * 133 = 2.15e6 loads and 16384 stores.\n\n  This might be acceptable.\n\n  Let's hope it is faster than the reference.\n\n  We will now write the code.\n\n  Note: the original model had an __init__ and also the batch_size, in_features, out_features, and get_inputs.\n\n  We will keep the __init__ and the parameters, but replace the forward.\n\n  We will also define the kernel inside the model? Or outside? \n\n  We will define the kernel outside the model.\n\n  We will also note that the model might be used in different batch sizes? But the benchmark has batch_size=128.\n\n  We will assume batch_size is passed in the forward? Actually, the inputs are (x, y) of shapes (batch_size, in_features) and (batch_size, out_features).\n\n  We will use the batch_size = x.size(0).\n\n  The out_features = self.bmm.out_features.\n\n  We will write the kernel launch accordingly.\n\n  Given the complexity and the constraints of the problem, we provide the following solution.\n\n  Note: This is a complex implementation and might have bugs. We hope it will compile and be correct.\n\n  If it doesn't work, we might have to debug.\n\n  Let's write the code.\n</think>\nTo optimize the given model, we'll replace the high-level PyTorch operations with a fused Triton kernel. The key improvements include:\n1. Fusing linear transformation, normalization, and pointwise operations\n2. Using efficient memory access patterns\n3. Handling both training and inference modes\n4. Minimizing memory operations and kernel launches\n\nHere's the optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, y_ptr, output_ptr,\n    w_ptr, linear_bias_ptr,\n    running_mean_ptr, running_var_ptr,\n    gamma_ptr, beta_ptr,\n    sum_feature_ptr,\n    in_features, out_features,\n    stride_x_batch, stride_x_feature,\n    stride_y_batch, stride_y_feature,\n    stride_w_feature, stride_w_in,\n    training: tl.constexpr, \n    eps: tl.constexpr,\n    BLOCK_IN: tl.constexpr\n):\n    pid0 = tl.program_id(0)  # Batch index\n    pid1 = tl.program_id(1)  # Feature index\n\n    # Compute linear transformation\n    dot = 0.0\n    for k in range(0, in_features, BLOCK_IN):\n        k_offs = k + tl.arange(0, BLOCK_IN)\n        mask = k_offs < in_features\n        \n        x_val = tl.load(x_ptr + pid0 * stride_x_batch + k_offs * stride_x_feature, mask=mask)\n        w_val = tl.load(w_ptr + pid1 * stride_w_feature + k_offs * stride_w_in, mask=mask)\n        dot += tl.sum(x_val * w_val, axis=0)\n    \n    linear_out = dot + tl.load(linear_bias_ptr + pid1)\n\n    # Training vs inference logic\n    if training:\n        tl.atomic_add(sum_feature_ptr + pid1, linear_out)\n        normalized = tl.load(beta_ptr + pid1)\n    else:\n        running_mean = tl.load(running_mean_ptr + pid1)\n        running_var = tl.load(running_var_ptr + pid1)\n        gamma = tl.load(gamma_ptr + pid1)\n        beta_val = tl.load(beta_ptr + pid1)\n        normalized = gamma * ( (linear_out - running_mean) / tl.sqrt(running_var + eps) ) + beta_val\n\n    # Pointwise operations\n    y_val = tl.load(y_ptr + pid0 * stride_y_batch + pid1 * stride_y_feature)\n    out = (normalized + y_val) * y_val\n    tl.store(output_ptr + pid0 * out_features + pid1, out)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n        self.eps = eps\n        self.momentum = momentum\n\n    def forward(self, x, y):\n        batch_size, out_features = x.shape[0], self.bmm.out_features\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        y = y.contiguous()\n        output = torch.empty(batch_size, out_features, device=x.device, dtype=x.dtype)\n        \n        # Setup reduction buffer for training\n        if self.training:\n            sum_feature = torch.zeros(out_features, device=x.device, dtype=torch.float32)\n        else:\n            sum_feature = torch.empty(0, device=x.device)  # Dummy tensor\n            \n        # Launch kernel\n        grid = (batch_size, out_features)\n        _forward_kernel[grid](\n            x, y, output,\n            self.bmm.weight, self.bmm.bias,\n            self.instance_norm.running_mean, self.instance_norm.running_var,\n            self.instance_norm.weight, self.instance_norm.bias,\n            sum_feature,\n            in_features, out_features,\n            x.stride(0), x.stride(1),\n            y.stride(0), y.stride(1),\n            self.bmm.weight.stride(0), self.bmm.weight.stride(1),\n            training=self.training,\n            eps=self.eps,\n            BLOCK_IN=64\n        )\n        \n        # Update running stats if training\n        if self.training:\n            mean_feature = sum_feature / batch_size\n            self.instance_norm.running_mean = (\n                (1 - self.momentum) * self.instance_norm.running_mean + \n                self.momentum * mean_feature\n            )\n            self.instance_norm.running_var = (\n                (1 - self.momentum) * self.instance_norm.running_var\n            )  # Variance is zero in this case\n            \n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
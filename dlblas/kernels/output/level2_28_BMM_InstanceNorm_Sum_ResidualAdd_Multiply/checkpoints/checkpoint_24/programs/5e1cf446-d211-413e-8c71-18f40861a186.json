{"id": "5e1cf446-d211-413e-8c71-18f40861a186", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_residual_mult_kernel(\n    x_ptr,\n    y_ptr,\n    output_ptr,\n    out_features: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    row_start = pid * out_features\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < out_features\n\n    x_vals = tl.load(x_ptr + row_start + offsets, mask=mask, other=0.0)\n    y_vals = tl.load(y_ptr + row_start + offsets, mask=mask, other=0.0)\n\n    result = (x_vals + y_vals) * y_vals\n    tl.store(output_ptr + row_start + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n        self.out_features = out_features\n\n    def forward(self, x, y):\n        x = self.linear(x)\n        x = x.unsqueeze(1).unsqueeze(1)\n        x = self.instance_norm(x)\n        x = x.squeeze(1).squeeze(1)\n        \n        output = torch.empty_like(x)\n        batch_size = x.size(0)\n        n_cols = x.size(1)\n        \n        grid = (batch_size,)\n        BLOCK_SIZE = triton.next_power_of_2(n_cols)\n        fused_residual_mult_kernel[grid](\n            x, y, output,\n            n_cols,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "a68b9c6d-959f-496e-ad92-ec316ccbcbd6", "generation": 3, "timestamp": 1754570695.373028, "iteration_found": 22, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.846, "runtime_ms": 0.116, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0981, "speed_up": 0.846, "custom_timing": 0.116}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpumnedd24.py\", line 50, in forward\n    bias = torch.zeros_like(self.instance_norm.weight)\nTypeError: zeros_like(): argument 'input' (position 1) must be Tensor, not NoneType\n", "runtime_error_name": "builtins.TypeError", "reference_timing_ms": 0.0981, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 0.0000\n- score: 10.0000\n- stage: wrong_answer\n- verify_payload: {'id': 'level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpumnedd24.py\", line 50, in forward\\n    bias = torch.zeros_like(self.instance_norm.weight)\\nTypeError: zeros_like(): argument \\'input\\' (position 1) must be Tensor, not NoneType\\n', 'runtime_error_name': 'builtins.TypeError', 'reference_timing_ms': 0.0981, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpumnedd24.py\", line 50, in forward\\n    bias = torch.zeros_like(self.instance_norm.weight)\\nTypeError: zeros_like(): argument \\'input\\' (position 1) must be Tensor, not NoneType\\n', 'runtime_error_name': 'builtins.TypeError', 'reference_timing_ms': 0.0981, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpbiasu1e4.py\", line 40, in forward\\n    _forward_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 14:19:\\n    beta_inst_ptr,\\n    output_ptr,\\n    out_features: tl.constexpr,\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    pid = tl.program_id(0)   # Batch index\\n    row_start = pid * out_features\\n    col_offsets = tl.arange(0, BLOCK_SIZE)\\n    mask = col_offsets < out_features\\n\\n    # Load instance norm bias and input y\\n    bias = tl.load(beta_inst_ptr + col_offsets, mask=mask)\\n                   ^\\nAttributeError(\"\\'NoneType\\' object has no attribute \\'type\\'\")\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 904, in __radd__\\n    return add(other, self, sanitize_overflow=True, _semantic=_semantic)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2468, in add\\n    return _semantic.add(x, y, sanitize_overflow)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 226, in add\\n    input, other = self.binary_op_type_checking_impl(input, other, True, True)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 182, in binary_op_type_checking_impl\\n    lhs_sca_ty = lhs.type.scalar\\nAttributeError: \\'NoneType\\' object has no attribute \\'type\\'\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.0981, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\nTypeError: dot() got an unexpected keyword argument \\'trans_b\\'\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpu2x8ocwg.py\", line 79, in forward\\n    x = triton_linear(x, self.weight, self.bias)\\n  File \"/tmp/tmpu2x8ocwg.py\", line 60, in triton_linear\\n    linear_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 30:15:\\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\\n        k_remaining = K - k * BLOCK_SIZE_K\\n        k_valid = tl.minimum(BLOCK_SIZE_K, k_remaining)\\n\\n        x_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_valid)\\n        w_mask = (offs_n[:, None] < N) & (offs_k[None, :] < k_valid)\\n\\n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\\n\\n        acc += tl.dot(x, w, trans_b=True, allow_tf32=True)\\n               ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\nTypeError: dot() got an unexpected keyword argument \\'trans_b\\'\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.0981, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 10.0000)\n```python\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef linear_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    offs_m = pid0 * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid1 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + (offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk)\n    w_ptrs = w_ptr + (offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk)\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        k_valid = tl.minimum(BLOCK_SIZE_K, k_remaining)\n        \n        x_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_valid)\n        w_mask = (offs_n[:, None] < N) & (offs_k[None, :] < k_valid)\n        \n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x, w, trans_b=True, allow_tf32=True)\n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n    \n    b_ptrs = b_ptr + offs_n\n    b = tl.load(b_ptrs, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n    \n    offs_m_out = offs_m[:, None]\n    offs_n_out = offs_n[None, :]\n    out_ptrs = output_ptr + stride_om * offs_m_out + stride_on * offs_n_out\n    out_mask = (offs_m_out < M) & (offs_n_out < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\ndef triton_linear(x, weight, bias):\n    M, K = x.shape\n    N, _ = weight.shape\n    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n    \n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N'])\n    )\n    \n    linear_kernel[grid](\n        x, weight, bias, output,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32,\n        num_warps=4\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.randn(out_features))\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x, y):\n        x = triton_linear(x, self.weight, self.bias)\n        x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)\n        x = x + y\n        x = x * y\n        return x\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\nTypeError: dot() got an unexpected keyword argument \\'trans_b\\'\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpu2x8ocwg.py\", line 79, in forward\\n    x = triton_linear(x, self.weight, self.bias)\\n  File \"/tmp/tmpu2x8ocwg.py\", line 60, in triton_linear\\n    linear_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 30:15:\\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\\n        k_remaining = K - k * BLOCK_SIZE_K\\n        k_valid = tl.minimum(BLOCK_SIZE_K, k_remaining)\\n\\n        x_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_valid)\\n        w_mask = (offs_n[:, None] < N) & (offs_k[None, :] < k_valid)\\n\\n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\\n\\n        acc += tl.dot(x, w, trans_b=True, allow_tf32=True)\\n               ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\nTypeError: dot() got an unexpected keyword argument \\'trans_b\\'\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.0981, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n### Program 2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    y_ptr,\n    beta_inst_ptr,\n    output_ptr,\n    out_features: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)   # Batch index\n    row_start = pid * out_features\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < out_features\n    \n    # Load instance norm bias and input y\n    bias = tl.load(beta_inst_ptr + col_offsets, mask=mask)\n    y_vals = tl.load(y_ptr + row_start + col_offsets, mask=mask)\n    \n    # Compute: (bias + y) * y\n    result = (bias + y_vals) * y_vals\n    tl.store(output_ptr + row_start + col_offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x, y):\n        beta_inst = self.instance_norm.bias\n        output = torch.empty_like(y)\n        n_elements = y.numel()\n        \n        grid = (y.shape[0],)\n        _forward_kernel[grid](\n            y, \n            beta_inst, \n            output, \n            out_features=y.shape[1], \n            BLOCK_SIZE=triton.next_power_of_2(y.shape[1])\n        )\n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpbiasu1e4.py\", line 40, in forward\\n    _forward_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 14:19:\\n    beta_inst_ptr,\\n    output_ptr,\\n    out_features: tl.constexpr,\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    pid = tl.program_id(0)   # Batch index\\n    row_start = pid * out_features\\n    col_offsets = tl.arange(0, BLOCK_SIZE)\\n    mask = col_offsets < out_features\\n\\n    # Load instance norm bias and input y\\n    bias = tl.load(beta_inst_ptr + col_offsets, mask=mask)\\n                   ^\\nAttributeError(\"\\'NoneType\\' object has no attribute \\'type\\'\")\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 904, in __radd__\\n    return add(other, self, sanitize_overflow=True, _semantic=_semantic)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2468, in add\\n    return _semantic.add(x, y, sanitize_overflow)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 226, in add\\n    input, other = self.binary_op_type_checking_impl(input, other, True, True)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 182, in binary_op_type_checking_impl\\n    lhs_sca_ty = lhs.type.scalar\\nAttributeError: \\'NoneType\\' object has no attribute \\'type\\'\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.0981, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n### Program 3 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef elementwise_kernel(\n    y_ptr,\n    bias_ptr,\n    output_ptr,\n    out_features,\n    batch_size,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n    \n    row_start = pid * out_features\n    offsets = tl.arange(0, BLOCK_SIZE)\n    col_mask = offsets < out_features\n    \n    # Load current row of y and broadcasted bias\n    y_vals = tl.load(y_ptr + row_start + offsets, mask=col_mask)\n    bias_vals = tl.load(bias_ptr + offsets, mask=col_mask)\n    \n    # Compute (bias + y) * y\n    result = (bias_vals + y_vals) * y_vals\n    \n    # Store results\n    tl.store(output_ptr + row_start + offsets, result, mask=col_mask)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model using Triton for batched element-wise operations\n    \"\"\"\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n        self.out_features = out_features\n\n    def forward(self, x, y):\n        batch_size = y.size(0)\n        output = torch.empty_like(y)\n        \n        # Get instance norm bias\n        bias = self.instance_norm.bias\n        if bias is None:\n            bias = torch.zeros_like(self.instance_norm.weight)\n        \n        # Configure and launch Triton kernel\n        grid = (batch_size,)\n        elementwise_kernel[grid](\n            y, \n            bias, \n            output,\n            self.out_features,\n            batch_size,\n            BLOCK_SIZE=triton.next_power_of_2(self.out_features)\n        )\n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpumnedd24.py\", line 50, in forward\\n    bias = torch.zeros_like(self.instance_norm.weight)\\nTypeError: zeros_like(): argument \\'input\\' (position 1) must be Tensor, not NoneType\\n', 'runtime_error_name': 'builtins.TypeError', 'reference_timing_ms': 0.0981, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a batch matrix multiplication, instance normalization, summation, residual addition, and multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x, y):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n            y (torch.Tensor): Input tensor of shape (batch_size, out_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.bmm(x)\n        x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)\n        x = x + y\n        x = x * y\n        return x\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef elementwise_kernel(\n    y_ptr,\n    bias_ptr,\n    output_ptr,\n    out_features,\n    batch_size,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n    \n    row_start = pid * out_features\n    offsets = tl.arange(0, BLOCK_SIZE)\n    col_mask = offsets < out_features\n    \n    # Load current row of y and broadcasted bias\n    y_vals = tl.load(y_ptr + row_start + offsets, mask=col_mask)\n    bias_vals = tl.load(bias_ptr + offsets, mask=col_mask)\n    \n    # Compute (bias + y) * y\n    result = (bias_vals + y_vals) * y_vals\n    \n    # Store results\n    tl.store(output_ptr + row_start + offsets, result, mask=col_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n        self.out_features = out_features\n\n    def forward(self, x, y):\n        batch_size = y.size(0)\n        output = torch.empty_like(y)\n        \n        # Get instance norm bias\n        bias = self.instance_norm.bias\n        if bias is None:\n            bias = torch.zeros(self.out_features, device=y.device, dtype=y.dtype)\n        \n        # Configure and launch Triton kernel\n        grid = (batch_size,)\n        BLOCK_SIZE = triton.next_power_of_2(self.out_features)\n        elementwise_kernel[grid](\n            y, \n            bias, \n            output,\n            self.out_features,\n            batch_size,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    y_ptr,\n    beta_inst_ptr,\n    output_ptr,\n    out_features: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)   # Batch index\n    row_start = pid * out_features\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < out_features\n    \n    # Load instance norm bias and input y\n    bias = tl.load(beta_inst_ptr + col_offsets, mask=mask)\n    y_vals = tl.load(y_ptr + row_start + col_offsets, mask=mask)\n    \n    # Compute: (bias + y) * y\n    result = (bias + y_vals) * y_vals\n    tl.store(output_ptr + row_start + col_offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x, y):\n        beta_inst = self.instance_norm.bias\n        output = torch.empty_like(y)\n        n_elements = y.numel()\n        \n        grid = (y.shape[0],)\n        _forward_kernel[grid](\n            y, \n            beta_inst, \n            output, \n            out_features=y.shape[1], \n            BLOCK_SIZE=triton.next_power_of_2(y.shape[1])\n        )\n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef linear_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    offs_m = pid0 * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid1 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + (offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk)\n    w_ptrs = w_ptr + (offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk)\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        k_valid = tl.minimum(BLOCK_SIZE_K, k_remaining)\n        \n        x_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_valid)\n        w_mask = (offs_n[:, None] < N) & (offs_k[None, :] < k_valid)\n        \n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x, w, trans_b=True, allow_tf32=True)\n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n    \n    b_ptrs = b_ptr + offs_n\n    b = tl.load(b_ptrs, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n    \n    offs_m_out = offs_m[:, None]\n    offs_n_out = offs_n[None, :]\n    out_ptrs = output_ptr + stride_om * offs_m_out + stride_on * offs_n_out\n    out_mask = (offs_m_out < M) & (offs_n_out < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\ndef triton_linear(x, weight, bias):\n    M, K = x.shape\n    N, _ = weight.shape\n    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n    \n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N'])\n    )\n    \n    linear_kernel[grid](\n        x, weight, bias, output,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32,\n        num_warps=4\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.randn(out_features))\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x, y):\n        x = triton_linear(x, self.weight, self.bias)\n        x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)\n        x = x + y\n        x = x * y\n        return x\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: -100.0000, Type: Exploratory)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a batch matrix multiplication, instance normalization, summation, residual addition, and multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x, y):\n        # Skip unnecessary computations: linear layer and normalization are redundant\n        # since normalization of scalar values always produces zero. Directly compute\n        # the equivalent result: (0 + y) * y = y * y\n        return y * y\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 4 (Score: -100.0000, Type: Exploratory)\n```python\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n        ...\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a batch matrix multiplication, instance normalization, summation, residual addition, and multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x, y):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n            y (torch.Tensor): Input tensor of shape (batch_size, out_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.bmm(x)\n        x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)\n        x = x + y\n        x = x * y\n        return x\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nUnique approach: Excellent compiled (1.000), Excellent correctness (1.000), Alternative passed approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef elementwise_kernel(\n    y_ptr,\n    bias_ptr,\n    output_ptr,\n    out_features,\n    batch_size,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= batch_size:\n        return\n    \n    row_start = pid * out_features\n    offsets = tl.arange(0, BLOCK_SIZE)\n    col_mask = offsets < out_features\n    \n    # Load current row of y and broadcasted bias\n    y_vals = tl.load(y_ptr + row_start + offsets, mask=col_mask)\n    bias_vals = tl.load(bias_ptr + offsets, mask=col_mask)\n    \n    # Compute (bias + y) * y\n    result = (bias_vals + y_vals) * y_vals\n    \n    # Store results\n    tl.store(output_ptr + row_start + offsets, result, mask=col_mask)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model using Triton for batched element-wise operations\n    \"\"\"\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n        self.out_features = out_features\n\n    def forward(self, x, y):\n        batch_size = y.size(0)\n        output = torch.empty_like(y)\n        \n        # Get instance norm bias\n        bias = self.instance_norm.bias\n        if bias is None:\n            bias = torch.zeros_like(self.instance_norm.weight)\n        \n        # Configure and launch Triton kernel\n        grid = (batch_size,)\n        elementwise_kernel[grid](\n            y, \n            bias, \n            output,\n            self.out_features,\n            batch_size,\n            BLOCK_SIZE=triton.next_power_of_2(self.out_features)\n        )\n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program that fails with a TypeError because `self.instance_norm.weight` is None.\n The error occurs in the forward method when trying to create a zeros tensor like `self.instance_norm.weight`.\n In nn.InstanceNorm2d, the `weight` and `bias` are optional (they are learnable affine parameters). \n The current code assumes that if `bias` is None, we should use zeros like the `weight` tensor. However, the `weight` can also be None.\n\n According to the PyTorch documentation for nn.InstanceNorm2d:\n   - If `affine=False` (default), then the module does not have learnable affine parameters, meaning both `weight` and `bias` are None.\n   - If `affine=True`, then it will have both `weight` and `bias` as tensors of size `out_features`.\n\n The current program does not set `affine=True` in the instance norm, so both `weight` and `bias` are None.\n\n Therefore, when `bias` is None, we should create a zeros tensor of size `out_features` (without relying on `weight`).\n\n We are going to change:\n   if bias is None:\n        bias = torch.zeros_like(self.instance_norm.weight)\n\n to:\n   if bias is None:\n        bias = torch.zeros(self.out_features, device=y.device, dtype=y.dtype)\n\n Additionally, we note that the current program does not use the input `x` at all. \n The problem description says the model should do:\n   x = self.bmm(x)   [but we don't have a bmm layer?]\n   then instance norm, then x+y, then x*y.\n\n However, in the current ModelNew, we have:\n   - The __init__ only has instance_norm, and no linear layer (bmm?).\n   - The forward ignores `x` and only uses `y`.\n\n This is a discrepancy. Looking at the previous attempts and the reference, we see that the model should have a linear layer (bmm) and then the instance norm.\n\n But note: the problem says \"BMM_InstanceNorm_Sum_ResidualAdd_Multiply\". \n We must have a linear layer (which is a batched matrix multiplication) and then the instance norm, then add y and multiply by y.\n\n Therefore, we must include the linear layer (bmm) in the model.\n\n However, in the current program (and the one we are to rewrite) we are only allowed to change the Triton kernel part? \n But note: the entire code block is given and we are allowed to change the whole block.\n\n Let's look at the reference PyTorch model (from the problem description, which we don't see directly, but from the top programs we see):\n\n   class ModelNew(nn.Module):\n        def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n            super(ModelNew, self).__init__()\n            self.bmm = nn.Linear(in_features, out_features)   # This is the linear layer\n            self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n\n        def forward(self, x, y):\n            x = self.bmm(x)\n            x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)\n            x = x + y\n            x = x * y\n            return x\n\n But our current program does not have the linear layer. It only has the instance norm and then the elementwise operation on `y`.\n\n This is a mistake.\n\n We must restructure the model to include the linear layer and then the instance norm, and then the operations with `y`.\n\n However, note the evaluation: we are given two inputs (x and y) and the output is of shape (batch_size, out_features).\n\n The Triton kernel we have is only for the elementwise part: (bias + y) * y. But that is only the last two operations.\n\n The linear layer and instance norm are not being accelerated by Triton in the current program.\n\n We have two options:\n   Option 1: Rewrite the entire forward pass in Triton (including the linear layer and instance norm) for maximum performance.\n   Option 2: Use Triton only for the elementwise part, but fix the model to include the linear layer and instance norm as in the reference.\n\n Given the complexity and the fact that the current task is level2 (which might be intermediate), and the error we are facing is a simple one, \n we might fix the immediate error and then restructure the model to include the missing linear layer.\n\n However, note the error message from the verify_payload for the current program:\n\n   TypeError: zeros_like(): argument 'input' (position 1) must be Tensor, not NoneType\n\n So let's fix the immediate issue first and then restructure.\n\n Steps:\n\n 1. In the forward method, we are missing the linear layer and the instance norm on `x`. We are directly doing the elementwise on `y`.\n    This is incorrect. We should process `x` through a linear layer and then instance norm, then combine with `y`.\n\n 2. The current Triton kernel is for the elementwise operation. But the entire model is not implemented.\n\n 3. We note that the previous top programs (like Program 1 and Program 2) attempted to do the linear layer in Triton and then the instance norm in PyTorch, \n    and then the elementwise in Triton? But they had their own issues.\n\n 4. Alternatively, we can use the reference structure and only accelerate the elementwise part with Triton? \n    But the elementwise part is very simple and might not be worth accelerating. However, the problem requires Triton.\n\n 5. Given the complexity and the fact that we are in the wrong_answer stage, we must first fix the model to be functionally equivalent.\n\n After reading the problem again: the goal is to generate functionally equivalent but faster Triton implementations.\n\n We see that the reference does:\n\n   x = linear(x)   # (batch_size, out_features)\n   x = instance_norm(x)   # which requires 4D input: (batch, channels, H, W) -> so we unsqueeze twice and then squeeze twice\n   x = x + y\n   x = x * y\n\n The last two operations are elementwise and can be fused.\n\n However, note: the instance norm operation in the reference is done on a 4D tensor that is created by unsqueezing the 2D tensor (batch, out_features) to (batch, 1, 1, out_features). \n The instance norm then operates on the spatial dimensions (which are 1x1) and then we squeeze back.\n\n This is equivalent to doing a 1D instance norm? But PyTorch doesn't have InstanceNorm1d for the last dimension? \n Actually, InstanceNorm2d on a tensor of shape (batch, out_features, 1, 1) is the same as normalizing each feature independently.\n\n We can consider doing the normalization manually? But that might be complex.\n\n Alternatively, we can keep the instance norm as is and only accelerate the last two operations? \n But the last two operations are very simple and might not be the bottleneck.\n\n However, the problem says: \"Generate functionally equivalent but faster Triton implementations\".\n\n We are allowed to rewrite any part. But note that the linear layer is a good candidate for Triton acceleration.\n\n But the problem is level2, and the previous attempts to accelerate the linear layer had compilation errors.\n\n Considering the time, we will do:\n\n   - Fix the immediate error (bias being None) by creating a zero tensor of the correct shape and device.\n   - Restore the linear layer and instance norm as in the reference.\n\n However, note: the current program does not have a linear layer. We must add it.\n\n Let's restructure the model:\n\n   class ModelNew(nn.Module):\n        def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n            super().__init__()\n            self.linear = nn.Linear(in_features, out_features)   # we add the linear layer\n            self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n            self.out_features = out_features\n\n        def forward(self, x, y):\n            # Step 1: linear transformation\n            x = self.linear(x)   # (batch_size, out_features)\n\n            # Step 2: instance normalization\n            # We need to reshape to 4D: (batch, out_features, 1, 1)\n            x = x.unsqueeze(1).unsqueeze(1)\n            x = self.instance_norm(x)\n            x = x.squeeze(1).squeeze(1)\n\n            # Step 3: residual add and multiply: x = (x + y) * y\n            # We want to do this with a Triton kernel for the entire batch of vectors (each row in x and y)\n\n            # But note: we have two tensors: x and y, both of shape (batch, out_features)\n            # We want to compute: (x + y) * y? \n            # However, the reference does: x = x + y and then x = x * y -> which is (x+y)*y.\n\n            # We can do this with a Triton kernel that takes x and y and writes to output.\n\n            # However, the current Triton kernel only uses y and the bias (from instance_norm) which is not what we want.\n\n            # We must change the Triton kernel to take x and y and compute (x+y)*y.\n\n            # Alternatively, we can skip the Triton kernel for now and just do it in PyTorch? \n            # But the problem requires Triton.\n\n            # We are going to write a Triton kernel for: output = (x + y) * y.\n\n            # But note: the current program has an elementwise kernel that does (bias + y)*y. \n            # That is a different operation.\n\n            # Therefore, we must change the kernel.\n\n Given the above, we decide to:\n\n   - Remove the old elementwise kernel and write a new one for (x+y)*y.\n   - The new kernel will take two input tensors (x and y) and write one output tensor.\n\n However, note that the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\"\n\n But wait, the entire block is given and we are allowed to change the entire block? The instructions say: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\n So we can restructure the entire model.\n\n Let's design:\n\n   We will have a Triton kernel for the operation: output = (x + y) * y.\n\n   The kernel will be launched with grid = (batch_size,), and each block will handle one row (of length out_features).\n\n   The kernel:\n\n        @triton.jit\n        def fused_residual_mult_kernel(\n            x_ptr,\n            y_ptr,\n            output_ptr,\n            out_features,\n            BLOCK_SIZE: tl.constexpr,\n        ):\n            pid = tl.program_id(0)\n            row_start = pid * out_features\n            offsets = tl.arange(0, BLOCK_SIZE)\n            mask = offsets < out_features\n\n            x_vals = tl.load(x_ptr + row_start + offsets, mask=mask)\n            y_vals = tl.load(y_ptr + row_start + offsets, mask=mask)\n\n            res = (x_vals + y_vals) * y_vals\n            tl.store(output_ptr + row_start + offsets, res, mask=mask)\n\n   Then in the forward:\n\n        x = self.linear(x)\n        x = ... # instance norm as above\n        output = torch.empty_like(x)\n        grid = (x.shape[0],)\n        BLOCK_SIZE = triton.next_power_of_2(x.shape[1])\n        fused_residual_mult_kernel[grid](x, y, output, x.shape[1], BLOCK_SIZE=BLOCK_SIZE)\n        return output\n\n But note: the problem requires that we use the two inputs: x (the original first input) and y (the second input). \n In the above, the linear layer uses the first input `x`, and then we combine with `y`.\n\n However, the current program does not have the linear layer in __init__. We must add it.\n\n But wait: the __init__ parameters are (in_features, out_features). We can create the linear layer with these.\n\n Let's write the corrected ModelNew:\n\n   class ModelNew(nn.Module):\n        def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n            super().__init__()\n            self.linear = nn.Linear(in_features, out_features)\n            self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n            self.out_features = out_features\n\n        def forward(self, x, y):\n            x = self.linear(x)\n            # Do instance norm by reshaping to 4D and then back\n            x_4d = x.unsqueeze(1).unsqueeze(1)   # (batch, 1, 1, out_features)\n            x_4d = self.instance_norm(x_4d)\n            x = x_4d.squeeze(1).squeeze(1)   # (batch, out_features)\n\n            # Now x and y are both (batch, out_features)\n            # We want to compute: (x + y) * y\n            output = torch.empty_like(x)\n            batch_size = x.shape[0]\n            n_cols = x.shape[1]\n\n            # We can use the same kernel for each row\n            grid = (batch_size,)\n            BLOCK_SIZE = triton.next_power_of_2(n_cols)\n            fused_residual_mult_kernel[grid](\n                x, y, output, \n                n_cols, \n                BLOCK_SIZE=BLOCK_SIZE\n            )\n            return output\n\n   And the kernel:\n\n        @triton.jit\n        def fused_residual_mult_kernel(\n            x_ptr,\n            y_ptr,\n            output_ptr,\n            out_features: tl.constexpr,\n            BLOCK_SIZE: tl.constexpr,\n        ):\n            pid = tl.program_id(0)\n            row_start = pid * out_features\n            offsets = tl.arange(0, BLOCK_SIZE)\n            mask = offsets < out_features\n\n            x_vals = tl.load(x_ptr + row_start + offsets, mask=mask)\n            y_vals = tl.load(y_ptr + row_start + offsets, mask=mask)\n\n            res = (x_vals + y_vals) * y_vals\n            tl.store(output_ptr + row_start + offsets, res, mask=mask)\n\n However, note: the kernel must handle the case when out_features is not a power of two? We are using BLOCK_SIZE as the next power of two and then masking.\n\n This should be safe.\n\n But note: the original reference does:\n        x = x + y\n        x = x * y\n\n This is equivalent to (x+y)*y.\n\n But wait, the problem says \"ResidualAdd\" and then \"Multiply\". So it's two separate operations? \n However, we can fuse them.\n\n Let's check the reference PyTorch code? We don't have it, but the problem says it's in the dataset.\n\n We assume it's two separate elementwise operations. Fusing them is safe and equivalent.\n\n Now, what about the error we started with? The error was in the old code that tried to use `self.instance_norm.weight` when it was None.\n\n In the new code, we are not using the bias of the instance norm in the Triton kernel? We are using the instance norm in the standard way.\n\n The instance_norm layer in the new code is applied to the 4D tensor. It will use its internal parameters if they exist (if affine=True), or not.\n\n But note: in the __init__, we are not setting affine=True. The default is affine=False. So the instance_norm has no weight and no bias.\n\n That is acceptable because the instance_norm operation without affine is just normalization (subtract mean, divide by std).\n\n So we don't need the bias of the instance norm in the Triton kernel for the last part.\n\n Therefore, the new kernel does not use any bias from the instance norm.\n\n We are only using the output of the instance norm (which is `x` after normalization) and then `y`.\n\n This should be correct.\n\n However, note: the reference model in the problem might have affine=True? \n We are not told. But the __init__ of the problem's reference model is:\n\n   def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        ... \n\n It doesn't specify affine. So it defaults to False.\n\n But to be safe, we should mimic the reference. The reference uses:\n\n   self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n\n which by default has affine=False.\n\n Therefore, we are safe.\n\n Let's write the entire code block.\n\n But note: the problem says the model is called ModelNew and we must keep the batch_size, in_features, out_features and the get_inputs, get_init_inputs.\n\n We will do:\n\n   - Replace the old Triton kernel with the new one (fused_residual_mult_kernel).\n   - Restructure the ModelNew to include the linear layer and the instance norm, and then the new kernel.\n\n However, note that the current program has:\n\n   batch_size = 128\n   in_features = 64\n   out_features = 128\n\n   and the functions get_inputs and get_init_inputs.\n\n We will keep these.\n\n But note: the old ModelNew did not have the linear layer, so it didn't use in_features. Now we use it in the linear layer.\n\n We are going to write:\n\n   class ModelNew(nn.Module):\n        def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n            super(ModelNew, self).__init__()\n            self.linear = nn.Linear(in_features, out_features)\n            self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n            self.out_features = out_features   # we can remove this? we use out_features from the shape\n\n        def forward(self, x, y):\n            x = self.linear(x)\n            # Reshape for instance norm: add two dummy dimensions (H and W of size 1)\n            x = x.unsqueeze(1).unsqueeze(1)\n            x = self.instance_norm(x)\n            x = x.squeeze(1).squeeze(1)\n\n            # Now x and y are of shape (batch_size, out_features)\n            output = torch.empty_like(x)\n            batch_size, n_cols = x.shape\n\n            # Grid of batch_size blocks\n            grid = (batch_size,)\n            BLOCK_SIZE = triton.next_power_of_2(n_cols)\n\n            # Launch kernel\n            fused_residual_mult_kernel[grid](\n                x, y, output,\n                n_cols,\n                BLOCK_SIZE=BLOCK_SIZE\n            )\n            return output\n\n   And the kernel:\n\n        @triton.jit\n        def fused_residual_mult_kernel(\n            x_ptr,  # pointer to the x tensor (after instance norm)\n            y_ptr,  # pointer to the y tensor\n            output_ptr, \n            out_features: tl.constexpr,  # number of columns (out_features)\n            BLOCK_SIZE: tl.constexpr,    # power-of-two block size\n        ):\n            pid = tl.program_id(0)   # row index\n            row_start = pid * out_features\n            offsets = tl.arange(0, BLOCK_SIZE)\n            mask = offsets < out_features\n\n            # Load x and y for this row\n            x_vals = tl.load(x_ptr + row_start + offsets, mask=mask, other=0.0)\n            y_vals = tl.load(y_ptr + row_start + offsets, mask=mask, other=0.0)\n\n            # Compute: (x + y) * y\n            result = (x_vals + y_vals) * y_vals\n\n            # Store\n            tl.store(output_ptr + row_start + offsets, result, mask=mask)\n\n We hope this will fix the TypeError and also implement the correct model.\n\n However, note: the old program did not use `x` at all. Now we are using `x` (the first input) for the linear layer and then combine with `y`.\n\n This is the intended model.\n\n Let's output the code accordingly.\n\n IMPORTANT: We are replacing the entire block, so we must include the Triton kernel and the ModelNew.\n\n We note that the old block had a kernel called `elementwise_kernel` and a ModelNew that only had instance_norm.\n\n We are replacing both.\n\n But note: the old block also had the batch_size, in_features, out_features and get_inputs, get_init_inputs. We keep those.\n\n Let's write the new block.\n</think>\nTo address the compilation and correctness issues in the current program, we need to restructure the model to include the missing linear transformation layer and correct the Triton kernel implementation. The key changes are:\n\n1. Add the linear transformation layer (nn.Linear) to match the reference model structure\n2. Replace the Triton kernel with one that correctly implements the residual add and multiply operations\n3. Remove unused dependencies on instance normalization parameters in the kernel\n4. Ensure proper tensor reshaping for instance normalization\n5. Fix the bias initialization when instance norm parameters are None\n\nHere's the complete solution:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_residual_mult_kernel(\n    x_ptr,\n    y_ptr,\n    output_ptr,\n    out_features: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    row_start = pid * out_features\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < out_features\n\n    x_vals = tl.load(x_ptr + row_start + offsets, mask=mask, other=0.0)\n    y_vals = tl.load(y_ptr + row_start + offsets, mask=mask, other=0.0)\n\n    result = (x_vals + y_vals) * y_vals\n    tl.store(output_ptr + row_start + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n        self.out_features = out_features\n\n    def forward(self, x, y):\n        x = self.linear(x)\n        x = x.unsqueeze(1).unsqueeze(1)\n        x = self.instance_norm(x)\n        x = x.squeeze(1).squeeze(1)\n        \n        output = torch.empty_like(x)\n        batch_size = x.size(0)\n        n_cols = x.size(1)\n        \n        grid = (batch_size,)\n        BLOCK_SIZE = triton.next_power_of_2(n_cols)\n        fused_residual_mult_kernel[grid](\n            x, y, output,\n            n_cols,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
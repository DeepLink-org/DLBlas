{"id": "4c643b2e-6b7d-4158-ba96-6bdbc6b9111d", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _elementwise_kernel(\n    x_ptr,\n    y_ptr,\n    output_ptr,\n    total_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    start_idx = pid * BLOCK_SIZE\n    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < total_elements\n    \n    x_vals = tl.load(x_ptr + offsets, mask=mask)\n    y_vals = tl.load(y_ptr + offsets, mask=mask)\n    \n    result = (x_vals + y_vals) * y_vals\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum, affine=True)\n    \n    def forward(self, x, y):\n        # Compute full forward path\n        x = self.bmm(x)\n        x = x.unsqueeze(1).unsqueeze(1)  # Add dummy dimensions for instance norm\n        x = self.instance_norm(x)\n        x = x.squeeze(1).squeeze(1)  # Remove dummy dimensions\n        \n        # Prepare for element-wise kernel\n        output = torch.empty_like(y)\n        total_elements = y.numel()\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        y = y.contiguous()\n        \n        _elementwise_kernel[grid](\n            x, \n            y, \n            output,\n            total_elements,\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "10cafad6-fcba-4d6f-ba3f-4292e37d306c", "generation": 3, "timestamp": 1754569750.63279, "iteration_found": 17, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpjmw6u8y2.py\", line 36, in forward\n    x = self.instance_norm(x)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py\", line 110, in forward\n    raise ValueError(\nValueError: expected input's size at dim=1 to match num_features (128), but got: 1.\n", "runtime_error_name": "builtins.ValueError", "reference_timing_ms": 0.0981, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpm3908l3l.py\", line 47, in forward\n    out_features = bias.size(0)\nAttributeError: 'NoneType' object has no attribute 'size'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 0.0981, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 0.0000\n- score: 10.0000\n- stage: wrong_answer\n- verify_payload: {'id': 'level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpm3908l3l.py\", line 47, in forward\\n    out_features = bias.size(0)\\nAttributeError: \\'NoneType\\' object has no attribute \\'size\\'\\n', 'runtime_error_name': 'builtins.AttributeError', 'reference_timing_ms': 0.0981, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 0.0000, correctness: 0.0000, score: -100.0000, stage: no_valid_triton_code, feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpm3908l3l.py\", line 47, in forward\\n    out_features = bias.size(0)\\nAttributeError: \\'NoneType\\' object has no attribute \\'size\\'\\n', 'runtime_error_name': 'builtins.AttributeError', 'reference_timing_ms': 0.0981, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpwqx9qwsm.py\", line 40, in forward\\n    beta_inst = beta_inst.contiguous()\\nAttributeError: \\'NoneType\\' object has no attribute \\'contiguous\\'\\n', 'runtime_error_name': 'builtins.AttributeError', 'reference_timing_ms': 0.0981, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    y_ptr,\n    beta_inst_ptr,\n    output_ptr,\n    out_features: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)   # Batch index\n    row_start = pid * out_features\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < out_features\n    \n    # Load instance norm bias and input y with proper masking\n    bias = tl.load(beta_inst_ptr + col_offsets, mask=mask)\n    y_vals = tl.load(y_ptr + row_start + col_offsets, mask=mask)\n    \n    # Compute: (bias + y) * y\n    result = (bias + y_vals) * y_vals\n    tl.store(output_ptr + row_start + col_offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x, y):\n        beta_inst = self.instance_norm.bias\n        output = torch.empty_like(y)\n        \n        # Ensure tensors are contiguous\n        y = y.contiguous()\n        beta_inst = beta_inst.contiguous()\n        \n        grid = (y.shape[0],)\n        _forward_kernel[grid](\n            y, \n            beta_inst, \n            output, \n            out_features=y.shape[1], \n            BLOCK_SIZE=triton.next_power_of_2(y.shape[1])\n        )\n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpwqx9qwsm.py\", line 40, in forward\\n    beta_inst = beta_inst.contiguous()\\nAttributeError: \\'NoneType\\' object has no attribute \\'contiguous\\'\\n', 'runtime_error_name': 'builtins.AttributeError', 'reference_timing_ms': 0.0981, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n### Program 2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    bias_ptr, \n    y_ptr, \n    output_ptr,\n    stride_bias, \n    stride_y0, stride_y1,\n    stride_out0, stride_out1,\n    out_features, \n    batch_size,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    start_idx = pid * BLOCK_SIZE\n    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < batch_size * out_features\n    \n    feature_id = offsets % out_features\n    batch_id = offsets // out_features\n    \n    bias_vals = tl.load(bias_ptr + feature_id * stride_bias, mask=mask)\n    y_vals = tl.load(y_ptr + batch_id * stride_y0 + feature_id * stride_y1, mask=mask)\n    \n    tmp = bias_vals + y_vals\n    out_vals = tmp * y_vals\n    \n    tl.store(output_ptr + batch_id * stride_out0 + feature_id * stride_out1, out_vals, mask=mask)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model using Triton for degenerate instance normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x, y):\n        bias = self.instance_norm.bias\n        batch_size = y.size(0)\n        out_features = bias.size(0)\n        output = torch.empty_like(y)\n        \n        total_elements = batch_size * out_features\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        \n        _forward_kernel[grid](\n            bias, y, output,\n            bias.stride(0),\n            y.stride(0), y.stride(1),\n            output.stride(0), output.stride(1),\n            out_features, batch_size,\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpm3908l3l.py\", line 47, in forward\\n    out_features = bias.size(0)\\nAttributeError: \\'NoneType\\' object has no attribute \\'size\\'\\n', 'runtime_error_name': 'builtins.AttributeError', 'reference_timing_ms': 0.0981, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n### Program 3 (Score: -100.0000)\n```python\n@triton.jit\ndef _forward_kernel(\n    y_ptr,\n    beta_inst_ptr,\n    output_ptr,\n    out_features: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)   # Batch index\n    row_start = pid * out_features\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < out_features\n    \n    # Load instance norm bias and input y\n    bias = tl.load(beta_inst_ptr + col_offsets, mask=mask)\n    y_vals = tl.load(y_ptr + row_start + col_offsets, mask=mask)\n    \n    # Compute: (bias + y) * y\n    result = (bias + y_vals) * y_vals\n    tl.store(output_ptr + row_start + col_offsets, result, mask=mask)\n```\nKey features: Performs well on compiled (0.0000), Performs well on correctness (0.0000), Performs well on score (-100.0000), Performs well on stage (no_valid_triton_code), Performs well on feedback (You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: -100.0000)\n```python\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x, y):\n        x = self.bmm(x)\n        x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)\n        x = x + y\n        x = x * y\n        return x\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    y_ptr,\n    beta_inst_ptr,\n    output_ptr,\n    out_features: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)   # Batch index\n    row_start = pid * out_features\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < out_features\n    \n    # Load instance norm bias and input y with proper masking\n    bias = tl.load(beta_inst_ptr + col_offsets, mask=mask)\n    y_vals = tl.load(y_ptr + row_start + col_offsets, mask=mask)\n    \n    # Compute: (bias + y) * y\n    result = (bias + y_vals) * y_vals\n    tl.store(output_ptr + row_start + col_offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n    \n    def forward(self, x, y):\n        beta_inst = self.instance_norm.bias\n        output = torch.empty_like(y)\n        \n        # Ensure tensors are contiguous\n        y = y.contiguous()\n        beta_inst = beta_inst.contiguous()\n        \n        grid = (y.shape[0],)\n        _forward_kernel[grid](\n            y, \n            beta_inst, \n            output, \n            out_features=y.shape[1], \n            BLOCK_SIZE=triton.next_power_of_2(y.shape[1])\n        )\n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _forward_kernel(\n    bias_ptr, \n    y_ptr, \n    output_ptr,\n    stride_bias, \n    stride_y0, stride_y1,\n    stride_out0, stride_out1,\n    out_features, \n    batch_size,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    start_idx = pid * BLOCK_SIZE\n    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < batch_size * out_features\n    \n    feature_id = offsets % out_features\n    batch_id = offsets // out_features\n    \n    bias_vals = tl.load(bias_ptr + feature_id * stride_bias, mask=mask)\n    y_vals = tl.load(y_ptr + batch_id * stride_y0 + feature_id * stride_y1, mask=mask)\n    \n    tmp = bias_vals + y_vals\n    out_vals = tmp * y_vals\n    \n    tl.store(output_ptr + batch_id * stride_out0 + feature_id * stride_out1, out_vals, mask=mask)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model using Triton for degenerate instance normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x, y):\n        bias = self.instance_norm.bias\n        batch_size = y.size(0)\n        out_features = bias.size(0)\n        output = torch.empty_like(y)\n        \n        total_elements = batch_size * out_features\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        \n        _forward_kernel[grid](\n            bias, y, output,\n            bias.stride(0),\n            y.stride(0), y.stride(1),\n            output.stride(0), output.stride(1),\n            out_features, batch_size,\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a program that uses Triton for a kernel that performs (bias + y) * y.\n The current program fails with an AttributeError: 'NoneType' object has no attribute 'size'\n The error occurs in the forward method at: out_features = bias.size(0)\n\n The issue: self.instance_norm.bias might be None. In PyTorch, the bias of InstanceNorm2d is optional.\n\n According to the PyTorch documentation for nn.InstanceNorm2d:\n   - It has optional parameters `affine` and `track_running_stats`.\n   - If `affine` is True, then the module has learnable affine parameters (weight and bias).\n   - If `affine` is False (default), then the module does not have bias.\n\n The current code in __init__ does not set `affine=True` for the instance norm.\n\n Therefore, we must set `affine=True` to have a bias.\n\n Alternatively, we can check if the bias is None and handle it appropriately.\n\n However, note the reference code (which we don't see) must have used an instance norm with bias.\n\n But the error indicates that in our ModelNew, the bias is None.\n\n Let's look at the __init__ of ModelNew:\n\n   self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)\n\n By default, `affine` is False. So the instance norm does not have a bias.\n\n We should change the instance norm to have affine=True.\n\n Alternatively, if the reference model does not use affine, then we should not be using the bias.\n\n However, the kernel requires a bias. Therefore, we must ensure that the instance norm has a bias.\n\n Since the problem is about the model structure (not the kernel), we change the instance norm to set affine=True.\n\n But note: the reference code might not have affine. We must be cautious.\n\n However, the error in the current program is because we are trying to get the size of a None.\n\n We have two options:\n\n   1. Change the instance norm to have affine=True so that bias is not None.\n   2. In the forward method, if bias is None, then we use a zero tensor.\n\n Since the reference code might have been using an instance norm without affine, we must match.\n\n But note: the problem says we are to rewrite the Triton kernel part? Actually, the entire model is within the EVOLVE-BLOCK.\n\n We are allowed to change the model as long as the inputs and outputs are the same.\n\n However, the reference model in the dataset might have been using affine. We don't know.\n\n But the error says: the reference model must have had a bias? Or maybe not.\n\n Let me reexamine the problem: we are given a dataset of kernelbench files. The current one is level2_28_BMM_InstanceNorm_Sum_ResidualAdd_Multiply.\n\n The name suggests: BMM (batch matrix multiplication) then InstanceNorm, then Sum, ResidualAdd, Multiply.\n\n The model we are building is:\n\n   x -> linear (bmm) -> instance_norm -> then the result is added to y? and then multiplied by y? \n\n Actually, the forward method in our ModelNew is:\n\n   def forward(self, x, y):\n        ... # we are not using x at all in the forward? \n\n Wait, in the current ModelNew, we are not using the linear layer (bmm) and we are not using x.\n\n This is a problem.\n\n The model is defined as:\n\n   self.bmm = nn.Linear(in_features, out_features)\n   self.instance_norm = nn.InstanceNorm2d(...)\n\n Then in forward:\n\n   We take two inputs: x and y.\n\n   The reference model should be:\n\n        x = self.bmm(x)   # [batch, out_features]\n        x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)   # because InstanceNorm2d expects [N, C, H, W]\n        x = x + y\n        x = x * y\n        return x\n\n But in our Triton version, we are ignoring x and only using the instance norm's bias and y.\n\n This is incorrect.\n\n Therefore, we must compute the entire reference model.\n\n However, note the Triton kernel is only for the element-wise operation: (bias + y) * y? \n\n But the reference model does:\n\n   x -> linear -> instance_norm -> then add y and multiply by y.\n\n The Triton kernel we have is for the last part: (bias + y) * y, but that is not the entire model.\n\n Why are we only doing that?\n\n The problem might be that the entire model is not being run in Triton? \n\n The error message we are getting is because we are not running the linear and instance norm at all.\n\n We are trying to get the bias from the instance norm, but the instance norm hasn't been run so its parameters might not be initialized? \n\n Actually, the parameters are initialized in __init__, but the instance norm without affine has no bias.\n\n How did the reference model run? \n\n We must reconstruct the entire model.\n\n Alternatively, the problem might be that we are only supposed to optimize the element-wise operation? \n\n But the model class is part of the benchmark.\n\n We are given the reference model in the dataset, but we don't have it here.\n\n We must fix the forward method to match the reference.\n\n Steps for the forward method:\n\n   1. Compute x = self.bmm(x)\n   2. Reshape x for instance_norm: because nn.InstanceNorm2d expects 4D input, we add two dummy dimensions (H and W of 1).\n   3. Apply instance_norm and then remove the dummy dimensions.\n   4. Then we do: x = x + y, and then x = x * y.\n\n However, note that the instance norm operation is not just adding a bias. It does:\n\n        x = (x - mean) / sqrt(var + eps) * weight + bias\n\n So the entire operation is not just a bias addition.\n\n But in our current Triton kernel, we are only doing (bias + y) * y, which is not the same.\n\n Therefore, we must reconsider.\n\n Alternatively, the benchmark might be testing only the element-wise operation? \n\n But the model name includes \"BMM_InstanceNorm\", so we must run the entire model.\n\n How to fix:\n\n   We cannot ignore the linear and instance norm.\n\n   We must run the linear and instance norm on x, then add y and multiply by y.\n\n   The Triton kernel is only for the last two operations? But they are simple element-wise.\n\n   Why use Triton for element-wise? It might not be faster.\n\n   However, the benchmark might be designed so that the entire model is run, and we are allowed to use Triton for any part.\n\n   But the current error is because we are not running the linear and instance norm, so the bias is None.\n\n   We must run the linear and instance norm to have the intermediate result, and then use that intermediate result in the element-wise operation.\n\n   However, the element-wise operation is (x + y) * y, where x is the output of the instance norm.\n\n   So we don't need the bias of the instance norm separately? We need the entire output of the instance norm.\n\n   Therefore, the Triton kernel is not for the instance norm but for the element-wise part.\n\n   But the element-wise part is very simple and might be memory bound. We are not likely to get a speedup.\n\n   Alternatively, we can use Triton for the entire model? \n\n   However, the problem says: we are given a PyTorch reference and we are to generate Triton for the kernel.\n\n   We are only allowed to change the Triton kernel and the model structure as long as the inputs and outputs are the same.\n\n   Given the time, we must fix the immediate error: the bias being None.\n\n   Let me change the instance norm to have affine=True.\n\n   Then, we must run the linear and instance norm in the forward method.\n\n   We'll compute:\n\n        x = self.bmm(x)\n        x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)\n\n        # Then we want to compute: (x + y) * y\n        # But note: the kernel we have is for (bias + y) * y, which is not the same.\n\n   We don't have the bias of the instance norm as the entire result? We have the entire result in x.\n\n   So we don't need the bias separately.\n\n   Therefore, we should change the kernel to take two tensors: x (the output of instance norm) and y.\n\n   Then the kernel does: (x + y) * y.\n\n   Alternatively, we can do that in PyTorch without a kernel? \n\n   But we are required to use Triton for the kernel.\n\n   We'll write a new kernel for (x + y) * y.\n\n   However, the current kernel is designed for (bias + y) * y and the bias is a vector of length out_features.\n\n   Now, x is a 2D tensor of shape [batch, out_features]. So we have one value per element.\n\n   We can use the same kernel structure but load from the x tensor instead of the bias vector.\n\n   We change the kernel to take x_ptr and y_ptr and output_ptr.\n\n   We'll do:\n\n        pid = program_id(0) for the batch index.\n\n        for each row, we load a row of x and a row of y, then compute (x_row + y_row) * y_row.\n\n   This is a simple element-wise operation.\n\n   We can also do it without a kernel? but we are required to use Triton.\n\n   Alternatively, we can use the existing kernel structure but change the parameters.\n\n   Given the time, we will restructure the forward method and the kernel.\n\n   Steps:\n\n     1. In __init__, set affine=True for the instance norm to have bias (and weight) so that we don't get None.\n        But note: the reference might have affine=False? We don't know. However, without affine the instance norm output is centered and scaled but without learnable parameters.\n\n        The output of instance norm is then a tensor of the same shape as the input (without the dummy dimensions).\n\n     2. We compute x = linear(x) and then x = instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)\n\n     3. Then we want to compute: output = (x + y) * y\n\n     4. We can do this with a Triton kernel that takes x and y and writes output.\n\n   We'll write a kernel for step 3.\n\n   The kernel:\n\n        @triton.jit\n        def _elementwise_kernel(\n            x_ptr,\n            y_ptr,\n            output_ptr,\n            stride_x0, stride_x1,\n            stride_y0, stride_y1,\n            stride_out0, stride_out1,\n            out_features,\n            BLOCK_SIZE: tl.constexpr,\n        ):\n            pid = tl.program_id(0)   # batch index\n            offsets = pid * out_features + tl.arange(0, BLOCK_SIZE)\n            mask = tl.arange(0, BLOCK_SIZE) < out_features\n\n            x_vals = tl.load(x_ptr + offsets, mask=mask)\n            y_vals = tl.load(y_ptr + offsets, mask=mask)\n\n            result = (x_vals + y_vals) * y_vals\n            tl.store(output_ptr + offsets, result, mask=mask)\n\n   Then in forward:\n\n        x = self.bmm(x)\n        # reshape for instance norm: from [batch, out_features] to [batch, 1, 1, out_features] -> then back\n        x = x.unsqueeze(1).unsqueeze(1)\n        x = self.instance_norm(x)\n        x = x.squeeze(1).squeeze(1)\n\n        # Now x is [batch, out_features]\n        output = torch.empty_like(y)\n        grid = (y.shape[0],)\n        _elementwise_kernel[grid](\n            x, y, output,\n            x.stride(0), x.stride(1),\n            y.stride(0), y.stride(1),\n            output.stride(0), output.stride(1),\n            out_features = y.shape[1],\n            BLOCK_SIZE = triton.next_power_of_2(y.shape[1])\n        )\n\n   But note: the instance norm might not have been run with affine=True? \n\n   We set affine=True in __init__ to avoid the None bias, but now we are not using the bias separately.\n\n   However, the instance norm with affine=True has both weight and bias. We are using the entire operation.\n\n   But the reference model might have affine=False? \n\n   We don't know. But the reference model must have run without error. \n\n   The error we are getting is because we tried to access the bias of an instance norm without affine.\n\n   Therefore, we change the instance norm to have affine=True.\n\n   Alternatively, we could set affine=False and then avoid using the bias? \n\n   But then we don't have the bias at all, and the kernel that uses the bias would be broken.\n\n   However, in this new plan, we are not using the bias separately. We are using the entire output of instance norm.\n\n   So the instance norm without affine is acceptable? \n\n   But note: without affine, the instance norm still returns a tensor. So we can use that tensor.\n\n   Therefore, we can set affine=False and it would work? \n\n   Then why did we get the None? Because we were accessing self.instance_norm.bias, which is None when affine=False.\n\n   In the new plan, we are not accessing self.instance_norm.bias. We are using the output of the instance norm.\n\n   So we can set affine=False and it should be fine.\n\n   But the reference model might have affine=True? \n\n   We don't know. But the benchmark expects a model that takes x and y and returns a tensor.\n\n   We are following the structure: linear -> instance norm -> (x+y)*y.\n\n   The exact parameters of the instance norm (affine or not) might be part of the model's state.\n\n   We are initializing the model with in_features and out_features.\n\n   We are not told whether to use affine or not.\n\n   However, the original error was in the forward method when we tried to get bias.size(0). \n\n   We are removing that.\n\n   Therefore, we change the forward method to compute the entire graph and remove the direct access to the bias.\n\n   We'll set affine=False to avoid having the bias parameter at all? \n\n   But note: the reference model might have learnable parameters? \n\n   We are not told. But the model is being initialized with in_features and out_features, and then we are to run it.\n\n   We must match the reference model's behavior.\n\n   Without knowing, let's set affine=True so that we have both weight and bias, and then the model has learnable parameters.\n\n   But the benchmark might be testing the model with affine=True.\n\n   Alternatively, we can set affine=True and then in the forward, we don't use the bias separately? \n\n   We are using the entire instance norm module, so it will use the affine parameters internally.\n\n   So we change the instance norm to:\n\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum, affine=True)\n\n   Then we don't access its bias directly in the forward method.\n\n   We only use the output.\n\n   This should be safe.\n\n   Let me summarize the changes:\n\n     - Change the instance norm to use affine=True to avoid the bias being None (if we ever access it, but we won't) and to match the reference if it uses affine.\n\n     - Actually, we are not accessing the bias directly anymore. But if the reference uses affine, then we must have affine=True.\n\n     - We run the entire model: linear, then instance norm (with dummy dimensions), then element-wise with y.\n\n     - We write a Triton kernel for the element-wise part: (x+y)*y.\n\n   We hope this will fix the AttributeError.\n\n   We also hope that the reference model uses affine=True.\n\n   If not, we might get a correctness error? But the benchmark might allow both as long as the structure is the same? \n\n   We are not told. But we must avoid the None error.\n\n   Alternatively, we can make the affine setting configurable? But we are not told.\n\n   Given the time, we set affine=True.\n\n   Now, the forward method:\n\n        x = self.bmm(x)   # [batch, out_features]\n        x = x.unsqueeze(1).unsqueeze(1)   # [batch, 1, 1, out_features]\n        x = self.instance_norm(x)   # [batch, 1, 1, out_features]\n        x = x.squeeze(1).squeeze(1)   # [batch, out_features]\n\n        # Now we have x and y (both [batch, out_features])\n        # We want to compute: (x + y) * y\n\n        # We'll use a Triton kernel for this.\n\n   We design the kernel to process one row at a time? or all elements together? \n\n   We can do:\n\n        total_elements = batch_size * out_features\n\n        and then have a 1D grid.\n\n   But the previous kernel that used a 1D grid for the entire tensor is also acceptable.\n\n   We'll do:\n\n        grid = (triton.cdiv(total_elements, BLOCK_SIZE),)\n\n        and then in the kernel:\n\n            pid = tl.program_id(0)\n            off = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n            mask = off < total_elements\n\n            x_val = tl.load(x_ptr + off, mask=mask)\n            y_val = tl.load(y_ptr + off, mask=mask)\n\n            out = (x_val + y_val) * y_val\n            tl.store(output_ptr + off, out, mask=mask)\n\n   This is simpler.\n\n   But note: the tensors might not be contiguous? We should make them contiguous.\n\n   We'll require contiguous tensors.\n\n   Alternatively, we can use strides? But the element-wise kernel with 1D index should work for any contiguous or non-contiguous? \n\n   We'll assume the tensors are contiguous. We can call .contiguous() in the forward.\n\n   We'll change:\n\n        x = x.contiguous()\n        y = y.contiguous()\n\n   Then we can use the 1D kernel.\n\n   Let's write the kernel.\n\n   We'll call it `_elementwise_kernel`.\n\n   We'll set BLOCK_SIZE to a power of two.\n\n   We'll use autotuning? But the problem is level2, so we can skip autotuning for now.\n\n   We are only required to get it correct and then optimize for speed.\n\n   Since the operation is memory bound, we just need to ensure coalesced access.\n\n   The 1D access is coalesced.\n\n   Now, we must change the ModelNew class accordingly.\n\n   We'll remove the old kernel and write the new kernel.\n\n   We'll change the forward method to compute the entire graph and then call the kernel.\n\n   We hope this will fix the error.\n\n   Let's write the code.\n\n   IMPORTANT: We must not change the signature of the kernel function arbitrarily? But we are allowed to change the kernel as long as we fix the model.\n\n   The entire code block is within the EVOLVE-BLOCK, so we can change it entirely.\n\n   We'll output the entire code block.\n\n   We note: the benchmark expects the model to be called with two inputs: x and y.\n\n   And the model returns one output.\n\n   We are doing:\n\n        x = linear(x)\n        x = instance_norm (with dummy dimensions)\n        then element-wise: (x+y)*y\n\n   This matches the model name.\n\n   We hope this will be correct.\n\n   We also note: the linear layer is not run in Triton? That's okay for now.\n\n   We are focusing on the element-wise part.\n\n   We'll write the code accordingly.\n\n   We'll set affine=True for the instance norm.\n\n   We'll set the BLOCK_SIZE to 1024 or use triton.next_power_of_2.\n\n   We'll try to run.\n\n   If we get a correctness error, we might have to adjust.\n\n   But at least we avoid the AttributeError.\n\n   Let's code.\n\n   Note: the benchmark also defines batch_size, in_features, out_features, and the get_inputs and get_init_inputs.\n\n   We leave those unchanged.\n\n   We change the ModelNew class and the kernel.\n\n   We remove the old kernel and write the new one.\n\n   We hope this will pass compilation and correctness.\n\n   If it does, then we can think about performance.\n\n   But note: the current score is 10.0? That doesn't make sense because correctness is 0.\n\n   We are trying to get correctness to 1.\n\n   We'll write the code now.\n</think>\nTo address the AttributeError and improve the implementation, we need to:\n1. Properly handle instance normalization with affine parameters\n2. Correctly compute the entire forward pass including linear transformation and normalization\n3. Optimize the element-wise operations with Triton\n\nThe key changes are:\n- Enable affine parameters in instance normalization to ensure bias exists\n- Compute the full forward path (linear \u2192 instance norm \u2192 element-wise ops)\n- Create a new Triton kernel for efficient element-wise operations\n- Maintain proper tensor shapes throughout the process\n\nHere's the optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _elementwise_kernel(\n    x_ptr,\n    y_ptr,\n    output_ptr,\n    total_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    start_idx = pid * BLOCK_SIZE\n    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < total_elements\n    \n    x_vals = tl.load(x_ptr + offsets, mask=mask)\n    y_vals = tl.load(y_ptr + offsets, mask=mask)\n    \n    result = (x_vals + y_vals) * y_vals\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.bmm = nn.Linear(in_features, out_features)\n        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum, affine=True)\n    \n    def forward(self, x, y):\n        # Compute full forward path\n        x = self.bmm(x)\n        x = x.unsqueeze(1).unsqueeze(1)  # Add dummy dimensions for instance norm\n        x = self.instance_norm(x)\n        x = x.squeeze(1).squeeze(1)  # Remove dummy dimensions\n        \n        # Prepare for element-wise kernel\n        output = torch.empty_like(y)\n        total_elements = y.numel()\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        y = y.contiguous()\n        \n        _elementwise_kernel[grid](\n            x, \n            y, \n            output,\n            total_elements,\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 128\nin_features = 64\nout_features = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features), torch.randn(batch_size, out_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
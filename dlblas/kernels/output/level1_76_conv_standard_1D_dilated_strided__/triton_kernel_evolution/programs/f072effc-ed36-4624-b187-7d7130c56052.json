{"id": "f072effc-ed36-4624-b187-7d7130c56052", "code": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"P2\"],  # Changed key to P2 for better tuning\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale_ptr,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, P2)\n    row_mask = tl.arange(0, P2) < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask, other=0.0)\n\n    abs_x = tl.abs(x)\n    # Optimized max calculation by removing redundant tl.where\n    max_val = tl.max(abs_x, axis=0)\n    max_val = tl.maximum(max_val, 1e-4)\n    # Fused scaling and clipping operations\n    quantized = tl.round(tl.minimum(tl.maximum(x * (127.0 / max_val), -127.0), 127.0)).to(tl.int8)\n    \n    tl.store(output_ptr + offsets, quantized, mask=row_mask)\n    tl.store(output_scale_ptr + pid, max_val / 127.0)\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n    return output, output_scale", "language": "python", "parent_id": "04bb6c7f-62d0-4051-be15-8e9389ea5c04", "generation": 4, "timestamp": 1754650374.9092464, "iteration_found": 24, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 31.5, "speed_up": 1.115, "runtime_ms": 0.0312, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0348, "speed_up": 1.115, "custom_timing": 0.0312}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 27.099999999999994, "speed_up": 1.071, "runtime_ms": 0.0325, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0348, "speed_up": 1.071, "custom_timing": 0.0325}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
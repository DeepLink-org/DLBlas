{"id": "614ca299-8aee-4c1b-9678-93173153f0dc", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 128}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 128}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 128}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=8, num_stages=3),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef linear_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    offs_m = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid1 * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_n[None, :] * stride_bn + offs_k[:, None] * stride_bk\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(K, BLOCK_K) * BLOCK_K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = offs_k[None, :] < k_remaining\n        \n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & k_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_n[None, :] < N) & (offs_k[:, None] < k_remaining), other=0.0)\n        \n        # Convert to FP16 for Tensor Core acceleration\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        acc += tl.dot(a_f16, b_f16, out_dtype=tl.float32)\n        \n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    tl.store(c_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        bound = 1 / math.sqrt(in_features) if in_features > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        M, K = x.shape\n        N, _ = self.weight.shape\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_M']), triton.cdiv(N, META['BLOCK_N']))\n        linear_kernel[grid](\n            x, self.weight, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n        )\n        \n        x = output\n        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scale_factor\n        x = torch.max(x, dim=1).values\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "c5c70b78-3754-4b8e-8a4c-175a449f440b", "generation": 4, "timestamp": 1754588210.6368742, "iteration_found": 24, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.723, "runtime_ms": 0.115, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0832, "speed_up": 0.723, "custom_timing": 0.115}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.723, "runtime_ms": 0.115, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0832, "speed_up": 0.723, "custom_timing": 0.115}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.7230\n- runtime_ms: 0.1150\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0832, 'speed_up': 0.723, 'custom_timing': 0.115}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7230, runtime_ms: 0.1150, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0832, 'speed_up': 0.723, 'custom_timing': 0.115}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7360, runtime_ms: 0.1130, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0832, 'speed_up': 0.736, 'custom_timing': 0.113}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7230, runtime_ms: 0.1150, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0832, 'speed_up': 0.723, 'custom_timing': 0.115}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef linear_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    offs_m = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid1 * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_n[None, :] * stride_bn + offs_k[:, None] * stride_bk\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_n[None, :] < N) & (offs_k[:, None] < K - k), other=0.0)\n        # Convert to FP16 for Tensor Core acceleration\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        acc += tl.dot(a_f16, b_f16, out_dtype=tl.float32)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    tl.store(c_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        bound = 1 / math.sqrt(in_features) if in_features > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        M, K = x.shape\n        N, _ = self.weight.shape\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_M']), triton.cdiv(N, META['BLOCK_N']))\n        linear_kernel[grid](\n            x, self.weight, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n        )\n        \n        x = output\n        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scale_factor\n        x = torch.max(x, dim=1).values\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7230), Performs well on runtime_ms (0.1150), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0832, 'speed_up': 0.723, 'custom_timing': 0.115})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef linear_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    offs_m = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid1 * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_n[None, :] * stride_bn + offs_k[:, None] * stride_bk\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_n[None, :] < N) & (offs_k[:, None] < K - k), other=0.0)\n        acc += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    tl.store(c_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        # Validate divisible pooling\n        if out_features % pool_kernel_size != 0:\n            raise ValueError(\"out_features must be divisible by pool_kernel_size\")\n            \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        bound = 1 / math.sqrt(in_features) if in_features > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        M, K = x.shape\n        N, _ = self.weight.shape\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_M']), triton.cdiv(N, META['BLOCK_N']))\n        linear_kernel[grid](\n            x, self.weight, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n        )\n        \n        x = output\n        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scale_factor\n        x = torch.max(x, dim=1).values\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7360), Performs well on runtime_ms (0.1130), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0832, 'speed_up': 0.736, 'custom_timing': 0.113})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef linear_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    offs_m = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid1 * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_n[None, :] * stride_bn + offs_k[:, None] * stride_bk\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_n[None, :] < N) & (offs_k[:, None] < K - k), other=0.0)\n        acc += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    tl.store(c_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        bound = 1 / math.sqrt(in_features) if in_features > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        M, K = x.shape\n        N, _ = self.weight.shape\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_M']), triton.cdiv(N, META['BLOCK_N']))\n        linear_kernel[grid](\n            x, self.weight, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n        )\n        \n        x = output\n        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scale_factor\n        x = torch.max(x, dim=1).values\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7230), Performs well on runtime_ms (0.1150), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0832, 'speed_up': 0.723, 'custom_timing': 0.115})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    pool_size,\n    scale_factor,\n    stride_xb,\n    stride_wr,\n    stride_wc,\n    BLOCK_IN: tl.constexpr,\n    BLOCK_OUT: tl.constexpr,\n):\n    # Get program indices\n    pid_batch = tl.program_id(0)\n    pid_group = tl.program_id(1)\n    \n    # Calculate group start index\n    group_start = pid_group * pool_size\n    \n    # Create pointers for current batch\n    x_batch_ptr = x_ptr + pid_batch * stride_xb\n    output_batch_ptr = output_ptr + pid_batch\n    \n    # Initialize accumulators for pooling group\n    group_acc = tl.zeros((pool_size,), dtype=tl.float32)\n    \n    # Loop over input features in blocks\n    for block_in in range(0, in_features, BLOCK_IN):\n        in_offs = block_in + tl.arange(0, BLOCK_IN)\n        in_mask = in_offs < in_features\n        \n        # Load input block\n        x_vals = tl.load(x_batch_ptr + in_offs, mask=in_mask, other=0.0)\n        \n        # Process each feature in the pooling group\n        for idx in tl.static_range(0, pool_size):\n            feature_idx = group_start + idx\n            w_ptr = weight_ptr + feature_idx * stride_wr + in_offs * stride_wc\n            w_vals = tl.load(w_ptr, mask=in_mask, other=0.0)\n            \n            # Compute partial dot product\n            partial = tl.sum(x_vals * w_vals)\n            group_acc = tl.where(tl.arange(0, pool_size) == idx, group_acc + partial, group_acc)\n    \n    # Load biases for the group\n    bias_vals = tl.load(bias_ptr + group_start + tl.arange(0, pool_size))\n    group_acc += bias_vals\n    \n    # Compute average pooling\n    pooled = tl.sum(group_acc) / pool_size\n    \n    # Apply GELU and scaling\n    gelu = 0.5 * pooled * (1.0 + tl.erf(pooled * 0.7071067811865475))\n    scaled = gelu * scale_factor\n    \n    # Store intermediate result\n    intermediate_ptr = output_batch_ptr + pid_group\n    tl.store(intermediate_ptr, scaled)\n    \n    # Max reduction across groups\n    if pid_group == 0:\n        max_val = tl.load(intermediate_ptr)\n        for i in range(1, out_features // pool_size):\n            val = tl.load(intermediate_ptr + i)\n            max_val = tl.maximum(max_val, val)\n        tl.store(output_batch_ptr, max_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.pool_kernel_size = pool_kernel_size\n        self.scale_factor = scale_factor\n        \n        # Validate divisible pooling\n        if out_features % pool_kernel_size != 0:\n            raise ValueError(\"out_features must be divisible by pool_kernel_size\")\n        \n        # Initialize weights\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        num_groups = self.out_features // self.pool_kernel_size\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        weight = self.weight.contiguous()\n        bias = self.bias.contiguous()\n        \n        # Intermediate storage for max reduction\n        intermediate = torch.empty(batch_size, num_groups, device=x.device, dtype=x.dtype)\n        output = torch.empty(batch_size, device=x.device, dtype=x.dtype)\n        \n        # Configure grid\n        grid = (batch_size, num_groups)\n        \n        # Launch kernel\n        fused_forward_kernel[grid](\n            x, weight, bias, output,\n            self.in_features, self.out_features, self.pool_kernel_size, self.scale_factor,\n            x.stride(0), weight.stride(0), weight.stride(1),\n            BLOCK_IN=128, BLOCK_OUT=1\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    pool_size: tl.constexpr,\n    scale_factor,\n    stride_xm, stride_xn,\n    stride_wm, stride_wn,\n    stride_om,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(out_features // pool_size, BLOCK_SIZE_M)\n    pid_m = pid % num_pid_m\n    pid_b = pid // num_pid_m\n    \n    # Create pointers for the current batch\n    x_ptr += pid_b * stride_xm\n    output_ptr += pid_b * stride_om\n    \n    # Define ranges\n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    rp = tl.arange(0, pool_size)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_M, pool_size), dtype=tl.float32)\n    \n    # Matrix multiplication\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_remaining = in_features - k\n        k_mask = rk < k_remaining\n        \n        # Load input tile\n        x = tl.load(x_ptr + k + rk, mask=k_mask, other=0.0)\n        \n        # Compute row indices and masks\n        row_indices = rm[:, None] * pool_size + rp[None, :]\n        row_mask = row_indices < out_features\n        \n        # Load weight tile\n        w_ptrs = weight_ptr + row_indices[:, :, None] * stride_wm + (k + rk[None, None, :]) * stride_wn\n        w_mask = row_mask[:, :, None] & k_mask[None, None, :]\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(w * x[None, None, :], axis=2)\n    \n    # Add bias\n    bias_ptrs = bias_ptr + row_indices\n    bias = tl.load(bias_ptrs, mask=row_mask, other=0.0)\n    acc += bias\n    \n    # Average pooling\n    pooled = tl.sum(acc, axis=1) / pool_size\n    \n    # Apply GELU and scaling\n    pooled = 0.5 * pooled * (1.0 + tl.erf(pooled * 0.7071067811865475))\n    pooled = pooled * scale_factor\n    \n    # Store final result\n    out_mask = rm < (out_features // pool_size)\n    tl.store(output_ptr + rm, pooled, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.pool_kernel_size = pool_kernel_size\n        self.scale_factor = scale_factor\n        \n        # Validate divisible pooling\n        if out_features % pool_kernel_size != 0:\n            raise ValueError(\"out_features must be divisible by pool_kernel_size\")\n        \n        # Initialize weights\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, self.out_features // self.pool_kernel_size, \n                            device=x.device, dtype=x.dtype)\n        \n        # Configure kernel grid\n        grid = lambda meta: (batch_size * triton.cdiv(self.out_features // self.pool_kernel_size, meta['BLOCK_SIZE_M']),)\n        \n        # Launch kernel\n        fused_forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features, self.pool_kernel_size, self.scale_factor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0),\n            BLOCK_SIZE_M=64,\n            BLOCK_SIZE_K=128,\n        )\n        \n        # Final max reduction\n        return torch.max(output, dim=1).values\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef linear_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    offs_m = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid1 * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_n[None, :] * stride_bn + offs_k[:, None] * stride_bk\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_n[None, :] < N) & (offs_k[:, None] < K - k), other=0.0)\n        acc += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    tl.store(c_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        bound = 1 / math.sqrt(in_features) if in_features > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        M, K = x.shape\n        N, _ = self.weight.shape\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_M']), triton.cdiv(N, META['BLOCK_N']))\n        linear_kernel[grid](\n            x, self.weight, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n        )\n        \n        x = output\n        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scale_factor\n        x = torch.max(x, dim=1).values\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    pool_kernel_size,\n    scale_factor,\n    stride_x0,\n    stride_x1,\n    stride_w0,\n    stride_w1,\n    stride_out,\n    BLOCK_FEATURES: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    batch_idx = pid\n    reduced_features = out_features // pool_kernel_size\n    \n    # Initialize max value storage\n    max_val = -tl.float32(1e9)\n    \n    # Process each feature group\n    for group in range(reduced_features):\n        group_acc = tl.float32(0.0)\n        group_start = group * pool_kernel_size\n        \n        # Process each feature in the group\n        for feature_offset in range(pool_kernel_size):\n            feature_idx = group_start + feature_offset\n            feature_acc = tl.float32(0.0)\n            \n            # Process input features in blocks\n            for block_start in range(0, in_features, BLOCK_FEATURES):\n                col_offsets = block_start + tl.arange(0, BLOCK_FEATURES)\n                mask = col_offsets < in_features\n                \n                # Load input and weight data\n                x_val = tl.load(\n                    x_ptr + batch_idx * stride_x0 + col_offsets * stride_x1,\n                    mask=mask,\n                    other=tl.float32(0.0)\n                )\n                w_val = tl.load(\n                    w_ptr + feature_idx * stride_w0 + col_offsets * stride_w1,\n                    mask=mask,\n                    other=tl.float32(0.0)\n                )\n                feature_acc += tl.sum(x_val * w_val, axis=0)\n            \n            # Add bias\n            bias_val = tl.load(b_ptr + feature_idx)\n            feature_acc += bias_val\n            group_acc += feature_acc\n        \n        # Compute group average\n        group_avg = group_acc / pool_kernel_size\n        \n        # Apply GELU approximation using available math functions\n        inner = group_avg * tl.math.sqrt(2.0/tl.math.pi) * \\\n               (group_avg + 0.044715 * group_avg * group_avg * group_avg)\n        gelu_val = group_avg * 0.5 * (1.0 + (2/(1 + tl.exp(-2*inner)) - 1))\n        \n        # Apply scaling\n        scaled_val = gelu_val * scale_factor\n        \n        # Track max value\n        if scaled_val > max_val:\n            max_val = scaled_val\n    \n    # Store final max value\n    tl.store(output_ptr + batch_idx * stride_out, max_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.in_features = in_features\n        self.out_features = out_features\n        self.pool_kernel_size = pool_kernel_size\n        self.scale_factor = scale_factor\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        \n        # Validate dimensions\n        if self.out_features % self.pool_kernel_size != 0:\n            raise ValueError(\"out_features must be divisible by pool_kernel_size\")\n            \n        grid = (batch_size,)\n        _forward_kernel[grid](\n            x, \n            self.matmul.weight, \n            self.matmul.bias,\n            output,\n            self.in_features,\n            self.out_features,\n            self.pool_kernel_size,\n            self.scale_factor,\n            x.stride(0), x.stride(1),\n            self.matmul.weight.stride(0), self.matmul.weight.stride(1),\n            output.stride(0),\n            BLOCK_FEATURES=128,\n            BLOCK_SIZE=triton.next_power_of_2(self.pool_kernel_size)\n        )\n        return output\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    pool_size,\n    scale_factor,\n    stride_xm, stride_xn,\n    stride_wm, stride_wn,\n    stride_om,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(out_features // pool_size, BLOCK_SIZE_M)\n    pid_m = pid % num_pid_m\n    pid_b = pid // num_pid_m\n    \n    # Create pointers for the current batch\n    x_ptr += pid_b * stride_xm\n    output_ptr += pid_b * stride_om\n    \n    # Define ranges for operations\n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Matrix multiplication\n    acc = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_remaining = in_features - k\n        k_mask = rk < k_remaining\n        \n        # Load input tile\n        x = tl.load(x_ptr + rk, mask=k_mask, other=0.0)\n        \n        # Load weight tile\n        w_ptrs = weight_ptr + rm[:, None] * stride_wm + (k + rk[None, :]) * stride_wn\n        w_mask = (rm < out_features) & k_mask[None, :]\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(w * x[None, :], axis=1)\n    \n    # Add bias and average pooling\n    bias = tl.load(bias_ptr + rm, mask=rm < out_features, other=0.0)\n    acc += bias\n    \n    # Apply GELU and scaling\n    acc = 0.5 * acc * (1.0 + tl.erf(acc * 0.7071067811865475))  # GELU approximation\n    acc = acc * scale_factor\n    \n    # Store final result\n    tl.store(output_ptr + rm, acc, mask=rm < out_features // pool_size)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.pool_kernel_size = pool_kernel_size\n        self.scale_factor = scale_factor\n        \n        # Validate divisible pooling\n        if out_features % pool_kernel_size != 0:\n            raise ValueError(\"out_features must be divisible by pool_kernel_size\")\n        \n        # Initialize weights\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, self.out_features // self.pool_kernel_size, \n                            device=x.device, dtype=x.dtype)\n        \n        # Configure kernel grid and launch\n        grid = lambda meta: (batch_size * triton.cdiv(self.out_features // self.pool_kernel_size, meta['BLOCK_SIZE_M']),)\n        \n        fused_forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features, self.pool_kernel_size, self.scale_factor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0),\n            BLOCK_SIZE_M=64,\n            BLOCK_SIZE_K=128,\n        )\n        \n        # Final max reduction\n        return torch.max(output, dim=1).values\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    pool_size: tl.constexpr,\n    scale_factor,\n    stride_xm, stride_xn,\n    stride_wm, stride_wn,\n    stride_om,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(out_features // pool_size, BLOCK_SIZE_M)\n    pid_m = pid % num_pid_m\n    pid_b = pid // num_pid_m\n    \n    # Create pointers for the current batch\n    x_ptr += pid_b * stride_xm\n    output_ptr += pid_b * stride_om\n    \n    # Define ranges\n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    rp = tl.arange(0, pool_size)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_M, pool_size), dtype=tl.float32)\n    \n    # Matrix multiplication\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_remaining = in_features - k\n        k_mask = rk < k_remaining\n        \n        # Load input tile\n        x = tl.load(x_ptr + k + rk, mask=k_mask, other=0.0)\n        \n        # Compute row indices and masks\n        row_indices = rm[:, None] * pool_size + rp[None, :]\n        row_mask = row_indices < out_features\n        \n        # Load weight tile\n        w_ptrs = weight_ptr + row_indices[:, :, None] * stride_wm + (k + rk[None, None, :]) * stride_wn\n        w_mask = row_mask[:, :, None] & k_mask[None, None, :]\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(w * x[None, None, :], axis=2)\n    \n    # Add bias\n    bias_ptrs = bias_ptr + row_indices\n    bias = tl.load(bias_ptrs, mask=row_mask, other=0.0)\n    acc += bias\n    \n    # Average pooling\n    pooled = tl.sum(acc, axis=1) / pool_size\n    \n    # Apply GELU and scaling\n    pooled = 0.5 * pooled * (1.0 + tl.erf(pooled * 0.7071067811865475))\n    pooled = pooled * scale_factor\n    \n    # Store final result\n    out_mask = rm < (out_features // pool_size)\n    tl.store(output_ptr + rm, pooled, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.pool_kernel_size = pool_kernel_size\n        self.scale_factor = scale_factor\n        \n        # Validate divisible pooling\n        if out_features % pool_kernel_size != 0:\n            raise ValueError(\"out_features must be divisible by pool_kernel_size\")\n        \n        # Initialize weights\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, self.out_features // self.pool_kernel_size, \n                            device=x.device, dtype=x.dtype)\n        \n        # Configure kernel grid\n        grid = lambda meta: (batch_size * triton.cdiv(self.out_features // self.pool_kernel_size, meta['BLOCK_SIZE_M']),)\n        \n        # Launch kernel\n        fused_forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features, self.pool_kernel_size, self.scale_factor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0),\n            BLOCK_SIZE_M=64,\n            BLOCK_SIZE_K=128,\n        )\n        \n        # Final max reduction\n        return torch.max(output, dim=1).values\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    pool_size,\n    scale_factor,\n    stride_xb,\n    stride_wr,\n    stride_wc,\n    BLOCK_IN: tl.constexpr,\n    BLOCK_OUT: tl.constexpr,\n):\n    # Get program indices\n    pid_batch = tl.program_id(0)\n    pid_group = tl.program_id(1)\n    \n    # Calculate group start index\n    group_start = pid_group * pool_size\n    \n    # Create pointers for current batch\n    x_batch_ptr = x_ptr + pid_batch * stride_xb\n    output_batch_ptr = output_ptr + pid_batch\n    \n    # Initialize accumulators for pooling group\n    group_acc = tl.zeros((pool_size,), dtype=tl.float32)\n    \n    # Loop over input features in blocks\n    for block_in in range(0, in_features, BLOCK_IN):\n        in_offs = block_in + tl.arange(0, BLOCK_IN)\n        in_mask = in_offs < in_features\n        \n        # Load input block\n        x_vals = tl.load(x_batch_ptr + in_offs, mask=in_mask, other=0.0)\n        \n        # Process each feature in the pooling group\n        for idx in tl.static_range(0, pool_size):\n            feature_idx = group_start + idx\n            w_ptr = weight_ptr + feature_idx * stride_wr + in_offs * stride_wc\n            w_vals = tl.load(w_ptr, mask=in_mask, other=0.0)\n            \n            # Compute partial dot product\n            partial = tl.sum(x_vals * w_vals)\n            group_acc = tl.where(tl.arange(0, pool_size) == idx, group_acc + partial, group_acc)\n    \n    # Load biases for the group\n    bias_vals = tl.load(bias_ptr + group_start + tl.arange(0, pool_size))\n    group_acc += bias_vals\n    \n    # Compute average pooling\n    pooled = tl.sum(group_acc) / pool_size\n    \n    # Apply GELU and scaling\n    gelu = 0.5 * pooled * (1.0 + tl.erf(pooled * 0.7071067811865475))\n    scaled = gelu * scale_factor\n    \n    # Store intermediate result\n    intermediate_ptr = output_batch_ptr + pid_group\n    tl.store(intermediate_ptr, scaled)\n    \n    # Max reduction across groups\n    if pid_group == 0:\n        max_val = tl.load(intermediate_ptr)\n        for i in range(1, out_features // pool_size):\n            val = tl.load(intermediate_ptr + i)\n            max_val = tl.maximum(max_val, val)\n        tl.store(output_batch_ptr, max_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.pool_kernel_size = pool_kernel_size\n        self.scale_factor = scale_factor\n        \n        # Validate divisible pooling\n        if out_features % pool_kernel_size != 0:\n            raise ValueError(\"out_features must be divisible by pool_kernel_size\")\n        \n        # Initialize weights\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        num_groups = self.out_features // self.pool_kernel_size\n        \n        # Ensure contiguous tensors\n        x = x.contiguous()\n        weight = self.weight.contiguous()\n        bias = self.bias.contiguous()\n        \n        # Intermediate storage for max reduction\n        intermediate = torch.empty(batch_size, num_groups, device=x.device, dtype=x.dtype)\n        output = torch.empty(batch_size, device=x.device, dtype=x.dtype)\n        \n        # Configure grid\n        grid = (batch_size, num_groups)\n        \n        # Launch kernel\n        fused_forward_kernel[grid](\n            x, weight, bias, output,\n            self.in_features, self.out_features, self.pool_kernel_size, self.scale_factor,\n            x.stride(0), weight.stride(0), weight.stride(1),\n            BLOCK_IN=128, BLOCK_OUT=1\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef linear_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    offs_m = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid1 * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_n[None, :] * stride_bn + offs_k[:, None] * stride_bk\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_n[None, :] < N) & (offs_k[:, None] < K - k), other=0.0)\n        # Convert to FP16 for Tensor Core acceleration\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        acc += tl.dot(a_f16, b_f16, out_dtype=tl.float32)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    tl.store(c_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        bound = 1 / math.sqrt(in_features) if in_features > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        M, K = x.shape\n        N, _ = self.weight.shape\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_M']), triton.cdiv(N, META['BLOCK_N']))\n        linear_kernel[grid](\n            x, self.weight, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n        )\n        \n        x = output\n        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scale_factor\n        x = torch.max(x, dim=1).values\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program that uses Triton for a linear layer, followed by average pooling, GELU, scaling, and max reduction.\n The current performance shows a speedup of 0.723 (which is less than 1, meaning slower than the reference PyTorch).\n The goal is to improve the speedup to at least 1.05 (5% faster) and ideally 15% or more.\n\n Analysis of the current Triton kernel (linear_kernel):\n  - It uses tensor cores by converting the loaded a and b to float16 and then doing a dot product in float32.\n  - However, the current kernel might not be fully optimized for the H100/H800 architecture (SM8.9) with Tensor Cores.\n\n Steps for improvement:\n 1. We note that the current kernel uses a block size for K of 64. However, for Tensor Cores, it is recommended to use larger blocks (like 128 or 256) to get better utilization.\n 2. The autotune configurations are set for BLOCK_M and BLOCK_N, but the BLOCK_K is fixed at 64. We should allow BLOCK_K to be tuned as well, and include larger values (like 128, 256) that are multiples of 16 (for Tensor Core requirements).\n 3. We are using 8 warps (256 threads) in some configurations. The H100 has 128 cores per SM, so we might adjust the number of warps to better fit the hardware.\n 4. The current kernel does not use any shared memory for blocking. However, the Triton dot operation already uses shared memory internally. But we can try to adjust the block sizes to improve memory access patterns.\n\n However, note that the kernel is already using tensor cores. The problem might be that the block sizes are too small to hide latency.\n\n Let's look at the top-performing programs for inspiration:\n  - The top program (Program 2) had a speedup of 0.736, which is slightly better, but still less than 1. It did not use tensor cores (it used `tl.dot` without converting to fp16).\n  - The current program uses tensor cores but is slower. This might be due to the overhead of converting and the block sizes.\n\n Alternative approach:\n  We might try to use a kernel that is optimized for larger block sizes and better utilization of tensor cores.\n\n Important: We must maintain the same function signature and grid configuration.\n\n Proposed changes to the kernel:\n  - We will change the autotune configurations to include larger block sizes and variable BLOCK_K (with values 64, 128, 256) and also try different combinations of BLOCK_M and BLOCK_N that are multiples of 64 (for tensor cores).\n  - We note that the tensor core operation in Triton requires that the inner dimension (BLOCK_K) must be a multiple of 16, and BLOCK_M and BLOCK_N should be multiples of 64 for best performance on H100.\n\n According to the Triton documentation, for tensor cores:\n   - The dot product operation is efficient when the block sizes are multiples of 16 for the K dimension and 64 for M and N.\n\n Let's try to change the autotune configs to:\n   configs = [\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128}, num_warps=8),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 256}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 256}, num_warps=4),\n    ]\n\n  We also note that the current kernel does not precompute the pointers for the next iteration. We can try to preload the next block of A and B to overlap memory and compute.\n\n However, note the constraints: we cannot change the grid configuration or the PID logic.\n\n Another idea: use the `num_stages` parameter in the autotune config to control the pipelining. The H100 might benefit from more stages (like 3 or 4). But note that the autotune config in Triton also allows setting `num_stages`. We can add that.\n\n But note: the current autotune does not set `num_stages`. We can set it to 3 or 4 for the larger block sizes.\n\n Let's change the autotune configs to include `num_stages` and larger block sizes.\n\n However, note that the kernel is already compiling and correct. We are only tuning for performance.\n\n Proposed autotune configs:\n\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 128}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 128}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 128}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 256}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 256}, num_warps=4, num_stages=4),\n    ]\n\n Additionally, we note that the current kernel converts the loaded data to fp16. This might be necessary for tensor cores, but we are doing it every time. We can try to use the `tl.dot` with fp32 and see if that is faster? But the tensor cores require fp16 or bf16.\n\n However, the current kernel is already using fp16 for the tensor cores. So we keep that.\n\n Another idea: use the `allow_tf32` option? But the H100 supports tf32, but we are using fp16. We can try to use tf32 which might be faster? But note the inputs are fp32? Actually, the input tensor `x` is from the user and we don't know the dtype. The problem states that the reference is PyTorch and we are to be equivalent. The current program uses the same dtype as the input for the output of the linear layer.\n\n Let me check: the current program creates `output` with the same dtype as `x`. The input `x` in the test is `torch.randn` which is float32. So we are doing:\n   - Convert a (float32) to float16, and b (float32) to float16, then do a dot product in float32.\n\n Alternatively, we can use tf32 for the tensor cores? But Triton's dot operation does not currently support tf32. So we stick to fp16.\n\n However, note: the conversion from fp32 to fp16 might be costly. We can try to load directly as fp16 if the input is fp16? But the input is fp32. So we have to convert.\n\n We can try to change the kernel to use a tile size that is larger to amortize the conversion cost? That's why we are increasing BLOCK_K.\n\n We also note that the current kernel does not use vectorized loads. We can try to use `tl.load(..., cache_modifier='.cg')` to cache the loads in the L1 cache? But the Triton documentation says that the dot operation already uses shared memory, so we might not need to change the cache modifier.\n\n Given the above, we will change the autotune configurations to the ones above and also increase the BLOCK_K.\n\n Additionally, we note that the current kernel does not handle the case when K is not divisible by BLOCK_K. We have a mask in the load. This is correct.\n\n We'll change the autotune configs and see if we get better performance.\n\n However, note: the current program has a score of 20 (which is (0.723-1)*100 = negative, but the passed is 0). We must achieve at least 5% speedup.\n\n Another idea: fuse the entire operation (linear, avg_pool, gelu, scaling, max) into one kernel? But the current program structure is to have the Triton kernel only for the linear part.\n\n The problem says: \"Generate functionally equivalent but faster Triton implementations (custom_code)\". We are allowed to change the Triton kernel, but the rest of the code (like the nn.Module) is locked? Actually, the instructions say: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\".\n\n So we cannot change the rest of the code. We are only allowed to change the Triton kernel and the autotune configs.\n\n Therefore, we focus on improving the linear kernel.\n\n Let's rewrite the kernel with the new autotune configs and hope for a better performance.\n\n We'll also try to precompute the pointers for the next iteration to overlap memory and compute. However, note that the Triton compiler might already do this. We can try to unroll the loop by a factor of 2 and preload the next block.\n\n But note: the constraints say we cannot introduce race conditions. We are safe because each thread is independent.\n\n However, due to the complexity and to keep the change minimal, we will only adjust the autotune configs and block sizes for now.\n\n We'll change the autotune configs to include larger BLOCK_K and set num_stages.\n\n We also note that the current kernel uses a for-loop over K. We are using a step of BLOCK_K. We keep that.\n\n We are going to change the autotune configs and hope that the autotuner picks a better configuration.\n\n If we don't see improvement, we might try to change the kernel to use double buffering (prefetch next block). But for now, we do the autotune config.\n\n Let's write the new kernel with the updated autotune configs.\n\n Note: We must keep the same kernel name and parameters.\n\n We'll replace the autotune configs and also adjust the kernel to use the new block sizes. The kernel code itself remains the same because it uses BLOCK_M, BLOCK_N, BLOCK_K as tunable parameters.\n\n We also note that the current kernel uses:\n   a_ptrs += BLOCK_K * stride_ak\n   b_ptrs += BLOCK_K * stride_bk\n\n That is correct for the new BLOCK_K.\n\n We'll update the autotune configs and also add the `num_stages` parameter.\n\n Additionally, we note that the current kernel uses `tl.zeros` for the accumulator. We can try to use `tl.zeros(..., dtype=tl.float32)` which is already done.\n\n We'll try to run with the new configs.\n\n Since the current program is already working and correct, we are only changing the autotune parameters and the kernel's block sizes.\n\n We'll also note that the grid configuration is defined by:\n   grid = lambda META: (triton.cdiv(M, META['BLOCK_M']), triton.cdiv(N, META['BLOCK_N']))\n\n This is correct.\n\n We'll write the new code accordingly.\n\n Important: We must not change the function signature of the kernel.\n\n Let's generate the new code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 128}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 128}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 128}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=8, num_stages=3),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef linear_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    offs_m = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid1 * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_n[None, :] * stride_bn + offs_k[:, None] * stride_bk\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(K, BLOCK_K) * BLOCK_K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = offs_k[None, :] < k_remaining\n        \n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & k_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_n[None, :] < N) & (offs_k[:, None] < k_remaining), other=0.0)\n        \n        # Convert to FP16 for Tensor Core acceleration\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        acc += tl.dot(a_f16, b_f16, out_dtype=tl.float32)\n        \n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    tl.store(c_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        bound = 1 / math.sqrt(in_features) if in_features > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        M, K = x.shape\n        N, _ = self.weight.shape\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_M']), triton.cdiv(N, META['BLOCK_N']))\n        linear_kernel[grid](\n            x, self.weight, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n        )\n        \n        x = output\n        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scale_factor\n        x = torch.max(x, dim=1).values\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "4fe2f281-3de5-4ec9-9442-f714cdff95fe", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    pool_size: tl.constexpr,\n    scale_factor,\n    stride_xm, stride_xn,\n    stride_wm, stride_wn,\n    stride_om,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(out_features // pool_size, BLOCK_SIZE_M)\n    pid_m = pid % num_pid_m\n    pid_b = pid // num_pid_m\n    \n    # Create pointers for the current batch\n    x_ptr += pid_b * stride_xm\n    output_ptr += pid_b * stride_om\n    \n    # Define ranges\n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    rp = tl.arange(0, pool_size)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_M, pool_size), dtype=tl.float32)\n    \n    # Matrix multiplication\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_remaining = in_features - k\n        k_mask = rk < k_remaining\n        \n        # Load input tile\n        x = tl.load(x_ptr + k + rk, mask=k_mask, other=0.0)\n        \n        # Compute row indices and masks\n        row_indices = rm[:, None] * pool_size + rp[None, :]\n        row_mask = row_indices < out_features\n        \n        # Load weight tile\n        w_ptrs = weight_ptr + row_indices[:, :, None] * stride_wm + (k + rk[None, None, :]) * stride_wn\n        w_mask = row_mask[:, :, None] & k_mask[None, None, :]\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(w * x[None, None, :], axis=2)\n    \n    # Add bias\n    bias_ptrs = bias_ptr + row_indices\n    bias = tl.load(bias_ptrs, mask=row_mask, other=0.0)\n    acc += bias\n    \n    # Average pooling\n    pooled = tl.sum(acc, axis=1) / pool_size\n    \n    # Apply GELU and scaling\n    pooled = 0.5 * pooled * (1.0 + tl.erf(pooled * 0.7071067811865475))\n    pooled = pooled * scale_factor\n    \n    # Store final result\n    out_mask = rm < (out_features // pool_size)\n    tl.store(output_ptr + rm, pooled, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.pool_kernel_size = pool_kernel_size\n        self.scale_factor = scale_factor\n        \n        # Validate divisible pooling\n        if out_features % pool_kernel_size != 0:\n            raise ValueError(\"out_features must be divisible by pool_kernel_size\")\n        \n        # Initialize weights\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, self.out_features // self.pool_kernel_size, \n                            device=x.device, dtype=x.dtype)\n        \n        # Configure kernel grid\n        grid = lambda meta: (batch_size * triton.cdiv(self.out_features // self.pool_kernel_size, meta['BLOCK_SIZE_M']),)\n        \n        # Launch kernel\n        fused_forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features, self.pool_kernel_size, self.scale_factor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0),\n            BLOCK_SIZE_M=64,\n            BLOCK_SIZE_K=128,\n        )\n        \n        # Final max reduction\n        return torch.max(output, dim=1).values\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "acb0c1ea-34a5-4bf3-a2d4-76318321015e", "generation": 1, "timestamp": 1754583989.423599, "iteration_found": 4, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_98_Matmul_AvgPool_GELU_Scale_Max", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmphhta258z.py\", line 106, in forward\n    fused_forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 54:27:\n        row_mask = row_indices < out_features\n\n        # Load weight tile\n        w_ptrs = weight_ptr + row_indices[:, :, None] * stride_wm + (k + rk[None, None, :]) * stride_wn\n        w_mask = row_mask[:, :, None] & k_mask[None, None, :]\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n\n        # Compute partial dot product\n        acc += tl.sum(w * x[None, None, :], axis=2)\n\n    # Add bias\n    bias_ptrs = bias_ptr + row_indices\n                           ^\nNameError('row_indices is not defined')\n\n\nThe above exception was the direct cause of the following exception:\n\nNameError: row_indices is not defined\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0832, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.078, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0832, "speed_up": 1.067, "custom_timing": 0.078}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0780\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0832, 'speed_up': 1.067, 'custom_timing': 0.078}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0780, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0832, 'speed_up': 1.067, 'custom_timing': 0.078}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_98_Matmul_AvgPool_GELU_Scale_Max', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmp8m2dj0x2.py\", line 94, in forward\\n    fused_forward_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 561, in run\\n    bound_args, specialization, options = binder(*args, **kwargs)\\nTypeError: dynamic_func() missing 1 required positional argument: \\'BLOCK_SIZE_N\\'\\n', 'runtime_error_name': 'builtins.TypeError', 'reference_timing_ms': 0.0832, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    pool_size,\n    scale_factor,\n    stride_xm, stride_xn,\n    stride_wm, stride_wn,\n    stride_om,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(out_features // pool_size, BLOCK_SIZE_M)\n    pid_m = pid % num_pid_m\n    pid_b = pid // num_pid_m\n    \n    # Create pointers for the current batch\n    x_ptr += pid_b * stride_xm\n    output_ptr += pid_b * stride_om\n    \n    # Define ranges for operations\n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Matrix multiplication\n    acc = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_remaining = in_features - k\n        k_mask = rk < k_remaining\n        \n        # Load input tile\n        x = tl.load(x_ptr + rk, mask=k_mask, other=0.0)\n        \n        # Load weight tile\n        w_ptrs = weight_ptr + rm[:, None] * stride_wm + (k + rk[None, :]) * stride_wn\n        w_mask = (rm < out_features) & k_mask[None, :]\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(w * x[None, :], axis=1)\n    \n    # Add bias and average pooling\n    bias = tl.load(bias_ptr + rm, mask=rm < out_features, other=0.0)\n    acc += bias\n    \n    # Apply GELU and scaling\n    acc = 0.5 * acc * (1.0 + tl.erf(acc * 0.7071067811865475))  # GELU approximation\n    acc = acc * scale_factor\n    \n    # Store final result\n    tl.store(output_ptr + rm, acc, mask=rm < out_features // pool_size)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.pool_kernel_size = pool_kernel_size\n        self.scale_factor = scale_factor\n        \n        # Validate divisible pooling\n        if out_features % pool_kernel_size != 0:\n            raise ValueError(\"out_features must be divisible by pool_kernel_size\")\n        \n        # Initialize weights\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, self.out_features // self.pool_kernel_size, \n                            device=x.device, dtype=x.dtype)\n        \n        # Configure kernel grid and launch\n        grid = lambda meta: (batch_size * triton.cdiv(self.out_features // self.pool_kernel_size, meta['BLOCK_SIZE_M']),)\n        \n        fused_forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features, self.pool_kernel_size, self.scale_factor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0),\n            BLOCK_SIZE_M=64,\n            BLOCK_SIZE_K=128,\n        )\n        \n        # Final max reduction\n        return torch.max(output, dim=1).values\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_98_Matmul_AvgPool_GELU_Scale_Max', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmp8m2dj0x2.py\", line 94, in forward\\n    fused_forward_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 561, in run\\n    bound_args, specialization, options = binder(*args, **kwargs)\\nTypeError: dynamic_func() missing 1 required positional argument: \\'BLOCK_SIZE_N\\'\\n', 'runtime_error_name': 'builtins.TypeError', 'reference_timing_ms': 0.0832, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n### Program 2 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model implementing the pattern \"Matmul_AvgPool_GELU_Scale_Max\".\n    \"\"\"\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scale_factor\n        x = torch.max(x, dim=1).values\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0780), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0832, 'speed_up': 1.067, 'custom_timing': 0.078})\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    pool_size,\n    scale_factor,\n    stride_xm, stride_xn,\n    stride_wm, stride_wn,\n    stride_om,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(out_features // pool_size, BLOCK_SIZE_M)\n    pid_m = pid % num_pid_m\n    pid_b = pid // num_pid_m\n    \n    # Create pointers for the current batch\n    x_ptr += pid_b * stride_xm\n    output_ptr += pid_b * stride_om\n    \n    # Define ranges for operations\n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Matrix multiplication\n    acc = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_remaining = in_features - k\n        k_mask = rk < k_remaining\n        \n        # Load input tile\n        x = tl.load(x_ptr + rk, mask=k_mask, other=0.0)\n        \n        # Load weight tile\n        w_ptrs = weight_ptr + rm[:, None] * stride_wm + (k + rk[None, :]) * stride_wn\n        w_mask = (rm < out_features) & k_mask[None, :]\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(w * x[None, :], axis=1)\n    \n    # Add bias and average pooling\n    bias = tl.load(bias_ptr + rm, mask=rm < out_features, other=0.0)\n    acc += bias\n    \n    # Apply GELU and scaling\n    acc = 0.5 * acc * (1.0 + tl.erf(acc * 0.7071067811865475))  # GELU approximation\n    acc = acc * scale_factor\n    \n    # Store final result\n    tl.store(output_ptr + rm, acc, mask=rm < out_features // pool_size)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.pool_kernel_size = pool_kernel_size\n        self.scale_factor = scale_factor\n        \n        # Validate divisible pooling\n        if out_features % pool_kernel_size != 0:\n            raise ValueError(\"out_features must be divisible by pool_kernel_size\")\n        \n        # Initialize weights\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, self.out_features // self.pool_kernel_size, \n                            device=x.device, dtype=x.dtype)\n        \n        # Configure kernel grid and launch\n        grid = lambda meta: (batch_size * triton.cdiv(self.out_features // self.pool_kernel_size, meta['BLOCK_SIZE_M']),)\n        \n        fused_forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features, self.pool_kernel_size, self.scale_factor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0),\n            BLOCK_SIZE_M=64,\n            BLOCK_SIZE_K=128,\n        )\n        \n        # Final max reduction\n        return torch.max(output, dim=1).values\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model implementing the pattern \"Matmul_AvgPool_GELU_Scale_Max\".\n    \"\"\"\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scale_factor\n        x = torch.max(x, dim=1).values\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a reference PyTorch implementation that does:\n  1. Linear (matmul) layer\n  2. AvgPool1d (with kernel_size=pool_kernel_size) on the feature dimension (after adding a dummy dimension)\n  3. GELU activation\n  4. Scaling by a factor\n  5. Max reduction along the last dimension (which after pooling has size `out_features // pool_kernel_size`)\n\nThe goal is to write a Triton kernel that fuses these operations.\n\nHowever, note that the current Triton implementation in the top-performing program (Program 1) has a correctness issue and a compilation error (missing BLOCK_SIZE_N).\n\nLet's analyze the operations:\n\n1. The input `x` is of shape (batch_size, in_features).\n2. We do a matrix multiplication with weight of shape (out_features, in_features) and add bias, resulting in (batch_size, out_features).\n3. Then we do average pooling with kernel size `pool_kernel_size` and stride `pool_kernel_size` (non-overlapping). This requires that we reshape the output to (batch_size, 1, out_features) and then apply AvgPool1d, which gives (batch_size, 1, out_features//pool_kernel_size). Then we squeeze to (batch_size, out_features//pool_kernel_size).\n4. Then apply GELU and scale.\n5. Then take the max over the last dimension (so we get (batch_size,)).\n\nBut note: the reference code does:\n   x = self.matmul(x)  # (batch_size, out_features)\n   x = self.avg_pool(x.unsqueeze(1)).squeeze(1)  # (batch_size, out_features//pool_kernel_size)\n   x = torch.nn.functional.gelu(x)\n   x = x * self.scale_factor\n   x = torch.max(x, dim=1).values  # (batch_size,)\n\nHowever, the Triton kernel in Program 1 only does the first three operations (matmul, add bias, GELU, scaling) and then stores the result of the pooling? Actually, it does not do the pooling by averaging. It does:\n\n  - The matrix multiplication and bias addition.\n  - Then it applies GELU and scaling.\n  - Then it stores the result for each row in a reduced dimension (out_features // pool_size) without actually averaging.\n\nAlso, the kernel does not do the max reduction at the end? The kernel writes to `output` of shape (batch_size, out_features//pool_kernel_size) and then the host does a max reduction.\n\nBut note: the kernel does not do the average pooling! It only does the matmul, bias, GELU, and scaling, and then writes to a tensor that is already of the pooled size? How?\n\nActually, the kernel does the matmul and then immediately stores the result in a tensor of size (batch_size, out_features//pool_kernel_size). This is incorrect because the matmul output has `out_features` elements, and then we average pool to get `out_features//pool_kernel_size`.\n\nSo we must do the average pooling in the kernel.\n\nPlan:\n\n1. We will have each program ID (pid) correspond to a batch and a block of output features after pooling. Since after pooling we have `out_features // pool_kernel_size` features, we can split that dimension into blocks of size `BLOCK_SIZE_M`.\n\n2. For each output feature after pooling (say at index `m`), we need to average over `pool_kernel_size` elements of the linear output. But note: the linear output has to be computed for all the `pool_kernel_size` elements that are averaged.\n\n3. Therefore, for each output pooling block, we need to compute the linear layer for `pool_kernel_size` consecutive rows of the weight matrix.\n\n4. We can compute a block of the linear output for multiple rows at once? We can do a block matrix multiplication: for a block of input features (K dimension) and a block of output features (M dimension) that spans `pool_kernel_size` rows? Actually, we can compute multiple rows of the linear output in one block.\n\n5. Alternatively, we can compute one row of the pooled output by summing over `pool_kernel_size` rows of the linear output and then dividing by `pool_kernel_size`.\n\nWe can break the kernel into:\n\n  pid_b: batch index\n  pid_m: index of the pooled feature (each pooled feature corresponds to `pool_kernel_size` original features)\n\nFor each (pid_b, pid_m), we compute:\n  acc = 0\n  for k in range(0, in_features, BLOCK_SIZE_K):\n      load a block of input (x[pid_b, k:k+BLOCK_SIZE_K])\n      for p in range(pool_kernel_size):\n          row_index = pid_m * pool_kernel_size + p\n          load a block of weights (weight[row_index, k:k+BLOCK_SIZE_K])\n          do dot product and accumulate to a temporary variable for that row.\n  Then we have the linear output for `pool_kernel_size` rows? Then we average them? But note: we can accumulate the average by summing the rows and then divide by pool_kernel_size.\n\nBut note: we also have the bias. The bias for each row is different. So we would add the bias for each row and then average? Actually, the average of (linear(x) + bias) is the same as the average of linear(x) plus the average of the bias for that pool.\n\nSo we can do:\n\n  linear_output_pool = 0\n  for p in range(pool_kernel_size):\n      row = pid_m * pool_kernel_size + p\n      linear_output_row = dot(x, weight[row, :]) + bias[row]\n      linear_output_pool += linear_output_row\n  linear_output_pool /= pool_kernel_size\n\nThen apply GELU and scaling.\n\nBut note: we are doing the entire linear row in one thread? That might be inefficient because the row might be long.\n\nAlternatively, we can do:\n\n  We break the input feature dimension into blocks of size BLOCK_SIZE_K. Then for each block, we load a segment of the input and the corresponding segment of the weight rows for all p in the pool. Then we do:\n\n      for p in range(pool_kernel_size):\n          temp[p] += dot(block_x, block_weights[p])\n\n  Then after the K loop, we sum over p to get the total for each row? Actually, we want to average over the rows. But note: we are computing multiple rows at the same time? We can use a 2D accumulator: [pool_kernel_size] and then reduce over the pool.\n\nHowever, note that the number of rows in the pool is small (pool_kernel_size=4). We can unroll this loop.\n\nBut the problem: we are limited by the number of registers? We might have to store `pool_kernel_size` accumulators per thread.\n\nAlternatively, we can do the entire pool in one thread? Since pool_kernel_size is small, we can compute the linear layer for multiple rows (the entire pool) in one thread.\n\nProposal:\n\n  We assign each program to a block of pooled outputs? Actually, we are going to have one thread block per (batch, pooled feature block). But we can have a block of pooled features? Actually, we can have:\n\n      pid = pid_b * (num_blocks_pooled) + pid_m\n\n  where num_blocks_pooled = ceil(out_features/(pool_kernel_size * BLOCK_SIZE_M))\n\n  Then each block handles a group of `BLOCK_SIZE_M` pooled features. And each pooled feature requires `pool_kernel_size` rows.\n\n  So we will have:\n\n      rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n      rp = tl.arange(0, pool_kernel_size)\n\n      Then the rows we need are: [rm * pool_kernel_size + rp] (with broadcasting)\n\n  But note: we cannot have two dimensions in the same loop? We can use nested loops? Actually, we can use the outer product of rm and rp? Then we would have a 2D array of rows: (BLOCK_SIZE_M, pool_kernel_size). Then we do the matmul for all these rows.\n\n  However, note that the input vector is the same for all these rows.\n\n  So we can do:\n\n      for k in range(0, in_features, BLOCK_SIZE_K):\n          load input[k:k+BLOCK_SIZE_K] into a vector xk.\n          for each m in BLOCK_SIZE_M and each p in pool_kernel_size:\n              load weights[rm[m] * pool_kernel_size + rp[p], k:k+BLOCK_SIZE_K] into a vector w.\n              acc[m, p] += dot(xk, w)\n\n  Then after the K loop, we add bias and then average over p for each m.\n\n  But note: the bias is per row. So we have:\n\n      for each m and p:\n          row_index = rm[m] * pool_kernel_size + rp[p]\n          acc[m, p] += bias[row_index]\n\n      Then we do: pooled[m] = (sum_{p} acc[m, p]) / pool_kernel_size\n\n  Then apply GELU and scaling.\n\n  Then store pooled[m] for each m.\n\n  Then we do the max reduction? Actually, the max reduction is done in the host code? But note the host code does:\n\n      return torch.max(output, dim=1).values\n\n  and the output of the kernel is of shape (batch_size, out_features//pool_kernel_size). Then the host does the max over the last dimension.\n\n  So we don't do the max in the kernel.\n\nBut note: the Triton kernel must write the intermediate pooled and activated result (after scaling) to the output tensor, and then the host does the max.\n\nHowever, we can also fuse the max reduction? But the max reduction is over the entire feature dimension. So we would have to do it across multiple blocks? That would require a separate reduction kernel? Since the problem says to not change the grid configuration arbitrarily, we leave the max to the host.\n\nBut note: the reference code does the max on the host. So we must output a tensor of shape (batch_size, out_features//pool_kernel_size) and then do the max on the host.\n\nSo the kernel output is (batch_size, out_features//pool_kernel_size).\n\nNow, let's design the kernel:\n\n  Inputs:\n      x_ptr: input tensor of shape (batch, in_features)\n      weight_ptr: tensor of shape (out_features, in_features)\n      bias_ptr: tensor of shape (out_features)\n      output_ptr: output tensor of shape (batch, out_features//pool_kernel_size)\n\n  We are given:\n      in_features, out_features, pool_kernel_size, scale_factor\n\n  And strides.\n\n  Kernel:\n\n      pid = tl.program_id(0)\n      num_pid_m = tl.cdiv(out_features // pool_kernel_size, BLOCK_SIZE_M)\n      pid_m = pid % num_pid_m\n      pid_b = pid // num_pid_m\n\n      # Pointers for the current batch\n      x_ptr_b = x_ptr + pid_b * stride_xm   # stride_xm: stride for batch in x, which is in_features? Actually, x is (batch, in_features) so stride_xm = in_features? But we have row-major: so stride(0)=in_features, stride(1)=1? But in Triton we have to consider the layout.\n\n      # We'll assume row-major contiguous. So:\n      #   x_ptr: [batch, in_features] -> stride_xm = in_features, stride_xn = 1\n      #   weight_ptr: [out_features, in_features] -> stride_wm = in_features? Actually, if row-major then stride(0)=in_features, stride(1)=1? But note: the weight is typically stored as (out_features, in_features) and row-major, so stride(0)=in_features, stride(1)=1.\n\n      # Define the range for pooled features in this block:\n      rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n      # We need to check if rm is within the pooled features: which is [0, out_features//pool_kernel_size)\n\n      # We also have the pool index: rp = tl.arange(0, pool_kernel_size)\n\n      # We'll create a 2D array of row indices in the weight matrix: \n      #   row_indices = rm[:, None] * pool_kernel_size + rp[None, :]   -> shape (BLOCK_SIZE_M, pool_kernel_size)\n      # But note: we must check bounds for the weight rows: row_indices must be < out_features.\n\n      # Initialize accumulators for the linear outputs for each row in the pool and for each m.\n      # We'll have a 2D accumulator: (BLOCK_SIZE_M, pool_kernel_size) -> float32\n      acc = tl.zeros((BLOCK_SIZE_M, pool_kernel_size), dtype=tl.float32)\n\n      # Loop over K (in_features) in blocks of BLOCK_SIZE_K\n      for k in range(0, in_features, BLOCK_SIZE_K):\n          # Load a block of input\n          k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n          mask_k = k_offsets < in_features\n          x = tl.load(x_ptr_b + k_offsets, mask=mask_k, other=0.0)\n\n          # For each m and p, load a block of weights\n          # We need to form a pointer to the weight block for the rows in [rm * pool_kernel_size + rp] and columns in [k, k+BLOCK_SIZE_K]\n          # The weight pointer for a row r and column c is: weight_ptr + r * stride_wm + c * stride_wn.\n          # We have a 2D array of rows: (BLOCK_SIZE_M, pool_kernel_size) -> row_indices = rm[:, None] * pool_kernel_size + rp[None, :]\n          # We also have a 1D array of column offsets: k_offsets (size BLOCK_SIZE_K)\n\n          # We want to load a block of weights of shape (BLOCK_SIZE_M, pool_kernel_size, BLOCK_SIZE_K)\n          # How to do that? We can do nested loops? Or use tl.dot? Actually, we can use broadcasting.\n\n          # Let's create a pointer array for the weights:\n          w_ptrs = weight_ptr + (rm[:, None] * pool_kernel_size + rp[None, :])[:, :, None] * stride_wm + k_offsets[None, None, :] * stride_wn\n          # w_ptrs shape: (BLOCK_SIZE_M, pool_kernel_size, BLOCK_SIZE_K)\n\n          # Now, we need to mask: \n          #   - the rows: each row index must be < out_features -> mask_rows = (rm[:, None] * pool_kernel_size + rp[None, :]) < out_features\n          #   - the columns: mask_k for the columns\n          mask_rows = (rm[:, None] * pool_kernel_size + rp[None, :]) < out_features\n          mask = mask_rows[:, :, None] & mask_k[None, None, :]\n\n          w = tl.load(w_ptrs, mask=mask, other=0.0)\n\n          # Now, we want to compute: for each m and p, the dot product of w[m, p, :] and x.\n          # We can do: \n          #   acc[m, p] += tl.sum(w[m, p, :] * x)   # but note: x is 1D and we are multiplying along the last dimension.\n\n          # We can do:\n          #   acc += tl.sum(w * x[None, None, :], axis=2)\n          # But Triton doesn't support reduction along an arbitrary axis? We can do:\n          #   dot = tl.sum(w * x, axis=2)   # but note: x is of shape (BLOCK_SIZE_K,) and w is (BLOCK_SIZE_M, pool_kernel_size, BLOCK_SIZE_K) -> then w * x[None,None,:] is (BLOCK_SIZE_M, pool_kernel_size, BLOCK_SIZE_K) and then sum along axis=2.\n\n          # Alternatively, we can use tl.dot but we don't have a built-in for 3D. Instead, we can reshape.\n\n          # Let's reshape w to (BLOCK_SIZE_M * pool_kernel_size, BLOCK_SIZE_K) and then do a matrix-vector product? But note: we have one vector x. Then we would get a vector of length (BLOCK_SIZE_M * pool_kernel_size). Then we can reshape back.\n\n          # But note: the inner dimension is small (BLOCK_SIZE_K) so we can do a loop over the p dimension? Actually, the pool_kernel_size is small (4). We can unroll the p loop.\n\n          # Instead, we can do:\n          #   for p in range(pool_kernel_size):\n          #       w_p = w[:, p, :]   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n          #       acc_p = tl.sum(w_p * x[None, :], axis=1)   # (BLOCK_SIZE_M,)\n          #       acc[:, p] += acc_p\n\n          # But we cannot do a for loop over p because p is a compile-time constant? Actually, we can if pool_kernel_size is known at compile time? We can make it a constant.\n\n          # Alternatively, we can use tl.reshape and tl.dot:\n\n          #   w_flat = tl.reshape(w, (BLOCK_SIZE_M * pool_kernel_size, BLOCK_SIZE_K))   # (M*P, K)\n          #   x_col = tl.reshape(x, (BLOCK_SIZE_K, 1))   # (K, 1)\n          #   dot_flat = tl.dot(w_flat, x_col)   # (M*P, 1)\n          #   dot = tl.reshape(dot_flat, (BLOCK_SIZE_M, pool_kernel_size))\n          #   acc += dot\n\n          # This might be efficient if the dimensions are multiples of 16? But note: BLOCK_SIZE_M and BLOCK_SIZE_K are tunable.\n\n          # Since pool_kernel_size is small (4) and BLOCK_SIZE_M is at least 64, we can do the reshape and dot. But note: the total number of rows in w_flat is BLOCK_SIZE_M * pool_kernel_size, which is 256 for BLOCK_SIZE_M=64 and pool_kernel_size=4 -> 256 rows. And we have BLOCK_SIZE_K=128 columns. Then the dot product would be 256x128 * 128x1 -> 256x1. That is acceptable.\n\n          # Let's do that.\n\n          w_flat = tl.reshape(w, (BLOCK_SIZE_M * pool_kernel_size, BLOCK_SIZE_K))\n          dot_flat = tl.dot(w_flat, x)   # (M*P,)\n          dot = tl.reshape(dot_flat, (BLOCK_SIZE_M, pool_kernel_size))\n          acc += dot\n\n      # After the K loop, we have the linear outputs (without bias) for each row in the pool.\n\n      # Now, load the biases for the same rows: \n      bias_ptrs = bias_ptr + (rm[:, None] * pool_kernel_size + rp[None, :])\n      bias_vals = tl.load(bias_ptrs, mask=mask_rows, other=0.0)   # (BLOCK_SIZE_M, pool_kernel_size)\n\n      acc += bias_vals\n\n      # Now, average over the pool dimension (axis=1) to get the pooled output for each m.\n      pooled = tl.sum(acc, axis=1) / pool_kernel_size   # (BLOCK_SIZE_M,)\n\n      # Apply GELU: \n      pooled = 0.5 * pooled * (1.0 + tl.erf(pooled * 0.7071067811865475))\n\n      # Scale:\n      pooled = pooled * scale_factor\n\n      # Now, store the pooled output. The output tensor has shape (batch, out_features//pool_kernel_size)\n      # The index for the pooled feature is rm.\n      output_ptr_b = output_ptr + pid_b * stride_om   # stride_om: output stride for batch, which is (out_features//pool_kernel_size)\n      # And then the feature index is rm.\n\n      mask_rm = rm < (out_features // pool_kernel_size)\n      tl.store(output_ptr_b + rm, pooled, mask=mask_rm)\n\nBut note: the output tensor is of size (batch, out_features//pool_kernel_size). So the stride_om is the stride for the batch dimension? Actually, if it's contiguous, then stride(0)=out_features//pool_kernel_size, stride(1)=1? But we are storing at output_ptr_b + rm, which is the feature index. So we must have the output stored in row-major.\n\nHowever, in the kernel call we are passing the strides. We have:\n\n      x.stride(0), x.stride(1)   -> for x: (batch, in_features) -> stride(0)=in_features, stride(1)=1?\n      weight.stride(0), weight.stride(1) -> (out_features, in_features) -> stride(0)=in_features, stride(1)=1?\n      output.stride(0) -> for output: (batch, out_features//pool_kernel_size) -> stride(0)=out_features//pool_kernel_size? and then we store at output_ptr_b + rm, which is the column index? Then we need to set stride_om to the stride for the batch, which is the number of columns? Actually, the output tensor is (batch, F) and row-major, so:\n\n          output_ptr_b = output_ptr + pid_b * (out_features//pool_kernel_size)\n\n      and then we store at output_ptr_b + rm, which is the column index? But note: the stride for the feature dimension is 1.\n\n      So we don't need to multiply by a stride? Because we are storing in a contiguous way? But the kernel is given the stride for the batch dimension of the output: stride_om = output.stride(0) = out_features//pool_kernel_size.\n\n      Therefore, we did:\n\n          output_ptr_b = output_ptr + pid_b * stride_om   # which is output_ptr + pid_b * (out_features//pool_kernel_size)\n\n          then we store at output_ptr_b + rm, which is the column index.\n\n      This is correct.\n\nBut note: we must check that the rm indices are within [0, out_features//pool_kernel_size). We do that with mask_rm.\n\nNow, we must also note that the kernel parameters include BLOCK_SIZE_M, BLOCK_SIZE_K, and we also need to pass the pool_kernel_size? Actually, we are passing it as an argument.\n\nHowever, note that the kernel also has a parameter BLOCK_SIZE_N? We don't use it? We can remove it? But the previous kernel had a compilation error because of missing BLOCK_SIZE_N.\n\nWe see in the previous attempt:\n\n      fused_forward_kernel[grid](\n          ...,\n          BLOCK_SIZE_M=64,\n          BLOCK_SIZE_K=128,\n      )\n\nIt only passed two block sizes. So we must adjust the kernel signature to match.\n\nWe can remove the unused BLOCK_SIZE_N? But the kernel signature has:\n\n      BLOCK_SIZE_M: tl.constexpr,\n      BLOCK_SIZE_N: tl.constexpr,\n      BLOCK_SIZE_K: tl.constexpr,\n\nBut in our kernel, we don't use BLOCK_SIZE_N. We can remove it? But note the grid function uses BLOCK_SIZE_M. We must adjust.\n\nAlternatively, we can leave it and set it to 1? But the autotuner might not like it? We are not using it.\n\nBut note: the grid function in the model's forward call:\n\n      grid = lambda meta: (batch_size * triton.cdiv(self.out_features // self.pool_kernel_size, meta['BLOCK_SIZE_M']),)\n\nThis grid uses only BLOCK_SIZE_M.\n\nSo we can remove BLOCK_SIZE_N from the kernel? But then the call must not pass it? Actually, the call in the previous attempt did not pass it and that caused an error.\n\nSo we change the kernel signature to:\n\n      BLOCK_SIZE_M: tl.constexpr,\n      BLOCK_SIZE_K: tl.constexpr,\n\nand remove BLOCK_SIZE_N.\n\nAlso, we must note that the kernel uses the pool_kernel_size, which is passed as an argument (not a constexpr). But we know it at compile time? Actually, it is passed as a variable. However, we are going to make it a kernel argument.\n\nBut note: the kernel must be compiled for each different pool_kernel_size? That's acceptable because in the model the pool_kernel_size is fixed.\n\nAlternatively, we could make it a constexpr? But we don't know it until the model is created. So we pass it as a regular argument.\n\nNow, let's write the kernel and the model.\n\nWe also note that the kernel must have the same function name and arguments as the previous one? Because the model's forward call is fixed.\n\nBut the previous kernel had an argument `pool_size`? We are calling it `pool_kernel_size` in the model? Actually, the model passes `self.pool_kernel_size` as the argument. So we'll name the kernel argument `pool_size`.\n\nWe'll change the kernel argument to `pool_size` to match the call.\n\nAlso, note: the kernel call in the model:\n\n        fused_forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features, self.pool_kernel_size, self.scale_factor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0),\n            BLOCK_SIZE_M=64,\n            BLOCK_SIZE_K=128,\n        )\n\nWe are passing 13 arguments and then two block sizes. The kernel now has:\n\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    in_features, out_features, pool_size, scale_factor,\n    stride_xm, stride_xn, stride_wm, stride_wn, stride_om,\n    BLOCK_SIZE_M, BLOCK_SIZE_K\n\nSo 15 arguments? But we defined 15 parameters? Yes.\n\nBut note: the kernel also has the two constexprs: BLOCK_SIZE_M and BLOCK_SIZE_K.\n\nWe'll implement accordingly.\n\nHowever, note: the kernel uses the pool_size as a regular argument, and we use it to create the row indices. But we need to know it at compile time? Actually, we can use it as a regular variable. But the inner loop over the pool dimension (rp = tl.arange(0, pool_size)) must have a fixed size? We can make the pool_size a constant? Or we can use a loop that runs for pool_size times? But pool_size is not known at compile time.\n\nWe have a problem: the size of the rp array is dynamic? How can we do the 2D indexing if pool_size is not known at compile time?\n\nWe must know pool_size at kernel compilation? Then we should make it a constexpr? But the model's pool_kernel_size is passed as an argument and is fixed for the model, but can be different for different models.\n\nWe can make the kernel take pool_size as a constexpr? Then we have to recompile the kernel for each different pool_size? That is acceptable because the same model uses a fixed pool_size.\n\nBut how do we pass a constexpr from the model? We can pass it as a regular argument and then use it as a constexpr? Actually, in Triton, we can pass it as a regular argument and then use it in a constexpr context? Not exactly. We have to mark it as a constexpr.\n\nWe can change the kernel to:\n\n    @triton.jit\n    def fused_forward_kernel(\n        ...,\n        pool_size: tl.constexpr,   # now it's a compile-time constant\n        ...,\n        BLOCK_SIZE_M: tl.constexpr,\n        BLOCK_SIZE_K: tl.constexpr,\n    ):\n\nThen when we call the kernel, we pass the pool_size as a regular argument? But the kernel will treat it as a constexpr.\n\nThis is acceptable.\n\nSo we do:\n\n    pool_size: tl.constexpr,\n\nThen inside the kernel, we can define:\n\n    rp = tl.arange(0, pool_size)\n\nBut note: the maximum pool_size we might have? We know that in this problem pool_kernel_size=4, but we want the kernel to be general? It will be recompiled for each different pool_size.\n\nSo we change.\n\nNow, let's write the kernel accordingly.\n\nWe also note: the kernel must be autotuned? The previous kernel did not have autotune. We can add autotune? But the problem says to make small changes. We can start without autotune and then add it later? Or we can include a simple autotune.\n\nBut note: the current performance is the reference code (1x). We are trying to get a speedup.\n\nWe'll start with fixed block sizes and then add autotune in a later iteration.\n\nWe'll set BLOCK_SIZE_M=64, BLOCK_SIZE_K=128 as in the previous attempt.\n\nNow, the model class:\n\n  We are going to replace the forward method to call the kernel.\n\n  We must create the output tensor: (batch_size, out_features//pool_kernel_size)\n\n  And then call the kernel.\n\n  The grid: as before, (batch_size * num_blocks_pooled) where num_blocks_pooled = ceil(out_features//pool_kernel_size / BLOCK_SIZE_M)\n\n  So grid = (batch_size * triton.cdiv(out_features//pool_kernel_size, BLOCK_SIZE_M),)\n\n  We pass the arguments as above.\n\n  And then return the max over the last dimension? Actually, the kernel output is (batch, out_features//pool_kernel_size) and then we do:\n\n      return torch.max(output, dim=1).values\n\nBut note: the reference code does:\n\n      x = self.matmul(x)\n      x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n      x = torch.nn.functional.gelu(x)\n      x = x * self.scale_factor\n      x = torch.max(x, dim=1).values\n\nSo we must do the max on the host.\n\nNow, we must validate:\n\n  - The kernel must compile.\n  - The kernel must be numerically correct.\n\nBut note: the previous kernel had a correctness issue? We are completely rewriting.\n\nLet's code accordingly.\n\nWe'll write the kernel and the model.\n\nNote: We must not change the class name: ModelNew.\n\nWe must not change the get_inputs and get_init_inputs.\n\nWe are replacing the entire EVOLVE-BLOCK.\n\nLet's code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    pool_size: tl.constexpr,\n    scale_factor,\n    stride_xm, stride_xn,\n    stride_wm, stride_wn,\n    stride_om,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(out_features // pool_size, BLOCK_SIZE_M)\n    pid_m = pid % num_pid_m\n    pid_b = pid // num_pid_m\n    \n    # Create pointers for the current batch\n    x_ptr += pid_b * stride_xm\n    output_ptr += pid_b * stride_om\n    \n    # Define ranges\n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    rp = tl.arange(0, pool_size)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_M, pool_size), dtype=tl.float32)\n    \n    # Matrix multiplication\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_remaining = in_features - k\n        k_mask = rk < k_remaining\n        \n        # Load input tile\n        x = tl.load(x_ptr + k + rk, mask=k_mask, other=0.0)\n        \n        # Compute row indices and masks\n        row_indices = rm[:, None] * pool_size + rp[None, :]\n        row_mask = row_indices < out_features\n        \n        # Load weight tile\n        w_ptrs = weight_ptr + row_indices[:, :, None] * stride_wm + (k + rk[None, None, :]) * stride_wn\n        w_mask = row_mask[:, :, None] & k_mask[None, None, :]\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(w * x[None, None, :], axis=2)\n    \n    # Add bias\n    bias_ptrs = bias_ptr + row_indices\n    bias = tl.load(bias_ptrs, mask=row_mask, other=0.0)\n    acc += bias\n    \n    # Average pooling\n    pooled = tl.sum(acc, axis=1) / pool_size\n    \n    # Apply GELU and scaling\n    pooled = 0.5 * pooled * (1.0 + tl.erf(pooled * 0.7071067811865475))\n    pooled = pooled * scale_factor\n    \n    # Store final result\n    out_mask = rm < (out_features // pool_size)\n    tl.store(output_ptr + rm, pooled, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.pool_kernel_size = pool_kernel_size\n        self.scale_factor = scale_factor\n        \n        # Validate divisible pooling\n        if out_features % pool_kernel_size != 0:\n            raise ValueError(\"out_features must be divisible by pool_kernel_size\")\n        \n        # Initialize weights\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, self.out_features // self.pool_kernel_size, \n                            device=x.device, dtype=x.dtype)\n        \n        # Configure kernel grid\n        grid = lambda meta: (batch_size * triton.cdiv(self.out_features // self.pool_kernel_size, meta['BLOCK_SIZE_M']),)\n        \n        # Launch kernel\n        fused_forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features, self.pool_kernel_size, self.scale_factor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0),\n            BLOCK_SIZE_M=64,\n            BLOCK_SIZE_K=128,\n        )\n        \n        # Final max reduction\n        return torch.max(output, dim=1).values\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
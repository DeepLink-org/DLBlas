{"id": "174a7e2e-3acb-4dfc-ba80-04a48abad788", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    pool_size,\n    scale_factor,\n    BLOCK_SIZE_IN: tl.constexpr,\n    BLOCK_SIZE_OUT: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Compute linear layer output\n    acc = tl.zeros((BLOCK_SIZE_OUT,), dtype=tl.float32)\n    for block in range(0, tl.cdiv(in_features, BLOCK_SIZE_IN)):\n        off_in = block * BLOCK_SIZE_IN + tl.arange(0, BLOCK_SIZE_IN)\n        mask_in = off_in < in_features\n        \n        # Load input vector block\n        x = tl.load(x_ptr + pid * in_features + off_in, mask=mask_in, other=0.0)\n        \n        # Load weight matrix block\n        for out_idx in tl.static_range(0, BLOCK_SIZE_OUT):\n            off_w = out_idx * in_features + off_in\n            w = tl.load(w_ptr + off_w, mask=mask_in, other=0.0)\n            acc = tl.where(tl.arange(0, BLOCK_SIZE_OUT) == out_idx, \n                           acc + tl.sum(x * w), acc)\n    \n    # Add bias\n    off_b = tl.arange(0, BLOCK_SIZE_OUT)\n    mask_b = off_b < out_features\n    bias = tl.load(b_ptr + off_b, mask=mask_b, other=0.0)\n    linear_out = tl.where(mask_b, acc + bias, 0.0)\n    \n    # Average pooling\n    n_pool = out_features // pool_size\n    pooled = tl.zeros((n_pool,), dtype=tl.float32)\n    for i in range(n_pool):\n        start = i * pool_size\n        segment = linear_out[start:start+pool_size]\n        pooled = tl.where(tl.arange(0, n_pool) == i, \n                         tl.sum(segment) / pool_size, pooled)\n    \n    # GELU activation\n    gelu_out = pooled * 0.5 * (1.0 + tl.erf(pooled * 0.7071067811865475))\n    \n    # Scale and max reduction\n    scaled = gelu_out * scale_factor\n    max_val = tl.max(scaled, axis=0)\n    \n    tl.store(output_ptr + pid, max_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.pool_kernel_size = pool_kernel_size\n        self.scale_factor = scale_factor\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        output = torch.empty(x.size(0), device=x.device, dtype=torch.float32)\n        grid = (x.size(0),)\n        fused_forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features,\n            self.pool_kernel_size, self.scale_factor,\n            BLOCK_SIZE_IN=64, BLOCK_SIZE_OUT=64\n        )\n        return output\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "acb0c1ea-34a5-4bf3-a2d4-76318321015e", "generation": 1, "timestamp": 1754583144.97644, "iteration_found": 2, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_98_Matmul_AvgPool_GELU_Scale_Max", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1693, in full\n    shape = _shape_check_impl(shape)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1677, in _shape_check_impl\n    validate_block_shape(shape)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/_utils.py\", line 50, in validate_block_shape\n    raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d)}]\")\nTypeError: Shape element 0 must have type `constexpr[int]`, got `constexpr[<class 'triton.language.core.tensor'>]\n\nThe above exception was the direct cause of the following exception:\n\ntriton.compiler.errors.CompilationError: at 10:11:\ndef zeros(shape, dtype):\n    \"\"\"\n    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n\n    :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n    :type shape: tuple of ints\n    :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n    :type dtype: DType\n    \"\"\"\n    return core.full(shape, 0, dtype)\n           ^\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp5gsxop4m.py\", line 80, in forward\n    fused_forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 39:13:\n            acc = tl.where(tl.arange(0, BLOCK_SIZE_OUT) == out_idx, \n                           acc + tl.sum(x * w), acc)\n\n    # Add bias\n    off_b = tl.arange(0, BLOCK_SIZE_OUT)\n    mask_b = off_b < out_features\n    bias = tl.load(b_ptr + off_b, mask=mask_b, other=0.0)\n    linear_out = tl.where(mask_b, acc + bias, 0.0)\n\n    # Average pooling\n    n_pool = out_features // pool_size\n    pooled = tl.zeros((n_pool,), dtype=tl.float32)\n             ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1693, in full\n    shape = _shape_check_impl(shape)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1677, in _shape_check_impl\n    validate_block_shape(shape)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/_utils.py\", line 50, in validate_block_shape\n    raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d)}]\")\nTypeError: Shape element 0 must have type `constexpr[int]`, got `constexpr[<class 'triton.language.core.tensor'>]\n\nThe above exception was the direct cause of the following exception:\n\ntriton.compiler.errors.CompilationError: at 10:11:\ndef zeros(shape, dtype):\n    \"\"\"\n    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n\n    :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n    :type shape: tuple of ints\n    :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n    :type dtype: DType\n    \"\"\"\n    return core.full(shape, 0, dtype)\n           ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1693, in full\n    shape = _shape_check_impl(shape)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1677, in _shape_check_impl\n    validate_block_shape(shape)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/_utils.py\", line 50, in validate_block_shape\n    raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d)}]\")\nTypeError: Shape element 0 must have type `constexpr[int]`, got `constexpr[<class 'triton.language.core.tensor'>]\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0832, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.078, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0832, "speed_up": 1.067, "custom_timing": 0.078}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0780\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0832, 'speed_up': 1.067, 'custom_timing': 0.078}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0780, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0832, 'speed_up': 1.067, 'custom_timing': 0.078}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model implementing the pattern \"Matmul_AvgPool_GELU_Scale_Max\".\n    \"\"\"\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scale_factor\n        x = torch.max(x, dim=1).values\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0780), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0832, 'speed_up': 1.067, 'custom_timing': 0.078})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model implementing the pattern \"Matmul_AvgPool_GELU_Scale_Max\".\n    \"\"\"\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.nn.functional.gelu(x)\n        x = x * self.scale_factor\n        x = torch.max(x, dim=1).values\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear (matmul) transformation: (batch, in_features) -> (batch, out_features)\n  2. AvgPool1d: requires input of shape (batch, channels, length). So we unsqueeze to (batch, 1, out_features) and then pool with kernel_size, then squeeze back to (batch, out_features//pool_kernel_size?) \n     Note: The pooling kernel size is given as 4. Since the input to pooling is of length `out_features` (which is 256) and we are pooling with kernel_size=4, the output length will be 256//4 = 64? \n     However, note that the original code does: \n        x = self.avg_pool(x.unsqueeze(1))   # becomes (batch, 1, out_features) -> pooling over the last dim with kernel_size=4 and stride=kernel_size (by default)\n        then we squeeze(1) -> (batch, 64)   [because 256//4=64]\n  3. Then GELU activation\n  4. Then scale by a factor\n  5. Then take max along dim=1 -> output shape (batch,)\n\nBut wait: the original code returns a tensor of shape (batch,). However, the docstring says output shape is (batch_size, out_features). This is inconsistent.\n\nLooking at the original forward:\n  x = self.matmul(x)   # (batch, in_features) -> (batch, out_features) [256]\n  x = self.avg_pool(x.unsqueeze(1)).squeeze(1)   # becomes (batch, 1, 256) -> avg pool over last dim with kernel 4 -> (batch, 1, 64) -> squeeze to (batch, 64)\n  ... then GELU, scale, then max(dim=1) -> (batch,)\n\nBut the problem: the original code does a max along the last remaining feature dimension (64) and reduces to a scalar per batch -> output (batch,). However, the docstring says (batch, out_features) which would be (batch, 256). This is a discrepancy.\n\nHowever, the evolution system says that we must maintain the same inputs and outputs. So we must output a tensor of shape (batch, out_features) as per the docstring? But the code does not. \n\nBut note: the current code we are given is the reference code (PyTorch). The Triton rewrite is for a kernel that does a specific pattern? Actually, the problem says we are given a dataset of kernelbench files. We are to rewrite the Triton kernel for one of these patterns.\n\nBut wait: the example we are given is a full model. We are not given a Triton kernel. The task is to rewrite the entire model? But the evolution system is for Triton kernels.\n\nLooking at the context: the evolution system is for Triton kernels. However, the current code block is a PyTorch model. How to interpret?\n\nRe-read the problem: \n  \"Each file contains PyTorch reference (reference_code)\"\n  \"Goal: Generate functionally equivalent but faster Triton implementations (custom_code)\"\n\nSo the reference code is PyTorch, and we are to replace it with a Triton implementation? But the example provided (per_row_quantize_int8) shows a Triton kernel.\n\nHowever, the current code block we are to rewrite is a PyTorch model. This suggests that we are to rewrite the forward pass of this model using Triton kernels.\n\nBut note: the model has multiple operations (linear, avgpool, gelu, scale, max). We cannot write one Triton kernel for the entire model? Or can we? The problem says \"functional equivalence\". We can break it down into multiple kernels, but the problem says \"custom_code\" (singular) in the goal.\n\nAlternatively, we might be expected to write a single Triton kernel that fuses all these operations? That would be the optimal way to get speedup.\n\nBut the problem says: \"Modify ONLY the Triton kernel source within this block\". However, the current block does not contain a Triton kernel. It contains a PyTorch model.\n\nThis suggests that the previous evolution step (the current program) is the reference PyTorch code. Now we are to write the Triton version.\n\nTherefore, we are to replace the entire forward pass with a Triton kernel? But note the constraints:\n\n  \u274c NEVER change @triton.jit function signatures or parameter names  \n  \u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\nBut we don't have a Triton kernel yet. So we are starting from scratch.\n\nGiven the complexity, we must break down the operations:\n\n  Step 1: Linear (matmul) layer: we can write a Triton kernel for matmul? But note the input is (batch, in_features) and weight is (out_features, in_features). We can do a batched matrix multiplication? But the batch size is 128, in_features=512, out_features=256.\n\n  Step 2: AvgPool1d: we are going to take the output of the linear (shape [128,256]) and then do average pooling over the last dimension with kernel=4 and stride=4? Then output becomes [128,64].\n\n  Step 3: GELU: element-wise\n  Step 4: Scale: element-wise multiplication by a scalar\n  Step 5: Max reduction along the last dimension (which is now 64) to get [128] (but note: the docstring says output should be [batch, out_features] which is [128,256] but the code actually produces [128]). This is a problem.\n\nBut wait: the current PyTorch model's forward returns a tensor of shape (batch,) (i.e., [128]). However, the docstring says (batch_size, out_features) -> which is [128,256]. This is inconsistent.\n\nWe must resolve: what is the expected output?\n\nLooking at the provided `get_inputs` and the model initialization:\n\n  The model is initialized with:\n      in_features = 512\n      out_features = 256\n\n  The forward input: (128,512)\n\n  After linear: (128,256)\n  After avgpool: (128,64)   [because 256//4=64? Actually, by default AvgPool1d uses ceil_mode=False and if the size is divisible by kernel, then 256/4=64]\n  Then after max(dim=1): (128,)\n\nBut the docstring says output shape (batch_size, out_features) -> (128,256). This is a clear discrepancy.\n\nWe must be cautious. The problem says: \"functional equivalence\". Therefore, we must produce the same output as the current PyTorch model. The current PyTorch model returns (128,). However, the docstring is wrong.\n\nBut the problem says: \"maintain the same inputs and outputs\". So we must output (128,).\n\nTherefore, the expected output is a tensor of shape (128,).\n\nBut note: the model class is called `ModelNew` and we are to rewrite it. We are allowed to change the internal implementation.\n\nPlan:\n\n  We are going to write a Triton kernel that fuses the entire operation? That would be ideal for performance.\n\n  Steps in the kernel:\n\n    Step 1: Load the input (x) for a given batch element? We have batch size 128. We can use a program_id for the batch.\n\n    Step 2: For each batch element, we do:\n        - Linear: for i in range(out_features): \n            result[i] = 0\n            for j in range(in_features):\n                result[i] += x[j] * weight[i, j]   [plus bias? The nn.Linear has bias by default?]\n\n        But wait: the current model uses `nn.Linear` which has both weight and bias. We haven't considered the bias.\n\n    Step 3: Then we do average pooling: we take the result of the linear (a vector of length out_features) and then break it into segments of 4, average each segment? Then we get a vector of length out_features//4.\n\n    Step 4: Apply GELU to each element in the pooled vector.\n    Step 5: Scale by the scale factor.\n    Step 6: Then take the max of this vector? Then we get a single scalar per batch.\n\n  So the output per batch is a scalar.\n\n  Therefore, the kernel would:\n\n      - Have grid of (batch_size,)\n      - Each block (or program) handles one batch element.\n\n  However, note: the linear layer has a weight matrix of shape (out_features, in_features) and a bias of shape (out_features,). We must load these as constant tensors? They are parameters of the model.\n\n  How to pass the parameters? We can pass the pointers to the weight and bias to the kernel.\n\n  Steps in the kernel for one batch element:\n\n      pid = tl.program_id(0)   # which batch element\n\n      # Load the input for this batch: x[pid, :] -> shape (in_features,)\n      # We'll use a block for the input features? But we have to do a dot product for each output feature.\n\n      # We are going to loop over the output features? But 256 is too big to do in one block? We can break the output features into blocks.\n\n      Alternatively, we can do:\n\n        We break the output features into groups (say BLOCK_SIZE) and have a 2D grid: (batch_size, num_blocks) where num_blocks = ceil(out_features / BLOCK_SIZE). But then we have to combine the results for the entire output feature vector? And then do pooling? This becomes complex.\n\n  Given the complexity, we might break the operations into multiple kernels? But the problem says we are to generate a Triton implementation (custom_code) for the entire pattern? And we are allowed to write one kernel that fuses everything.\n\n  However, the problem also says: \"Modify ONLY the Triton kernel source within this block\". And we are to output the entire code block that replaces the current one.\n\n  We are going to restructure the model:\n\n      We will write a Triton kernel that does the entire forward pass for one batch element? And then we call this kernel in the model's forward.\n\n  But note: the current model has parameters (weight and bias for the linear). We must initialize them and pass them to the kernel.\n\n  Proposed plan:\n\n      We keep the nn.Module, but we remove the nn.Linear and replace it by storing the weight and bias as parameters.\n\n      We write a Triton kernel that takes:\n          x: input tensor of shape (batch, in_features)\n          weight: tensor of shape (out_features, in_features)\n          bias: tensor of shape (out_features,)\n          output: tensor of shape (batch,)   [the result]\n\n      The kernel will do:\n          for each batch index, and for each output feature (with blocking for efficiency) we compute the linear, then do the pooling, then activation, then max.\n\n  However, note that the pooling operation: we need to take the linear output (a vector of length out_features) and break it into chunks of 4. Then average each chunk? Then we get a vector of length (out_features//4). Then we do GELU and scaling and then max.\n\n  Steps for one batch element:\n\n      Step 1: Compute the linear part: we get a vector `y` of length out_features.\n\n      Step 2: Pooling: \n          Let pool_size = 4\n          Let n_pool = out_features // pool_size   # must be divisible? in the example 256//4=64 -> divisible.\n          We create an array `pooled` of length n_pool.\n          For i in range(n_pool):\n              start = i * pool_size\n              end = start + pool_size\n              pooled[i] = (y[start] + y[start+1] + y[start+2] + y[start+3]) / pool_size\n\n      Step 3: For each element in pooled: apply GELU and then multiply by scale_factor.\n\n      Step 4: Then find the maximum value in the pooled vector.\n\n  We then output that maximum value for the batch element.\n\n  How to do this in Triton:\n\n      We can have one program per batch element? Then we have grid=(batch_size,).\n\n      For each program:\n\n          We need to load the entire input row? That's 512 elements. Then we need to compute 256 dot products? That's 512*256 = 131072 operations per batch element? And then the pooling and activation. This might be too heavy for one program.\n\n      Alternatively, we can break the linear part into blocks? We can use a 2D grid: (batch_size, num_blocks) where each block computes a portion of the output features? Then we need to store the partial results and then combine? We can use shared memory? But note: the pooling is over the entire output feature vector. So we need the whole vector.\n\n      We can do:\n\n          Step 1: Compute the linear in blocks and store the entire output feature vector in global memory? Then do the pooling in a separate kernel? But we are to fuse.\n\n      Given the constraints, we might do:\n\n          We break the output features into groups of BLOCK_SIZE. Then we have:\n\n            pid_batch = tl.program_id(0)\n            pid_block = tl.program_id(1)\n\n          Then we compute a segment of the output vector (of length BLOCK_SIZE) for the linear.\n\n          Then we need to do the pooling? But the pooling requires the entire vector. So we would have to write the partial linear result to global memory and then do a second kernel? That breaks fusion.\n\n      Alternatively, we can do the entire linear and then the pooling and reduction in one program? But the output feature vector is 256, which is not too big? We can store it in registers? 256 * (size of float32) = 1024 bytes per program. But the number of registers per thread is limited. We might run out of registers.\n\n      We can use shared memory? But note: the shared memory is limited (about 100KB per SM). We are having 128 batch elements and each batch element requires 256*4=1024 bytes? Then 128*1024=128KB? That might be too much.\n\n      We are forced to do one batch element per program? Then we have 128 programs. Each program must compute 256 output features (each a dot product of 512 elements). Then the total operations per program is 512*256 = 131072 operations. That is acceptable? But we must do it efficiently.\n\n      We can break the dot product into blocks? We can use a block for the reduction in the dot product? We can have:\n\n          Let BLOCK_SIZE = 128 (for example)\n\n          We break the in_features dimension into blocks of 128. Then we do:\n\n            accumulator = 0\n            for block in range(ceil(in_features / BLOCK_SIZE)):\n                load a block of 128 input features and a block of 128 weights (for one output feature) and do a dot product? But note: we are computing one output feature at a time? That would be inefficient.\n\n      Alternatively, we can compute multiple output features at once? We can use a 2D block:\n\n          Let's say we have:\n\n            BLOCK_SIZE_IN = 128   # for input features\n            BLOCK_SIZE_OUT = 32   # for output features\n\n          Then each program would compute 32 output features for a given block of input features? Then we would need to accumulate over the input feature blocks.\n\n      We can do:\n\n          pid_batch = tl.program_id(0)\n          pid_block_in = tl.program_id(1)   # for input feature block\n          pid_block_out = tl.program_id(2)   # for output feature block? \n\n          But note: we are going to have to combine the partial results for the entire input feature dimension? We can use shared memory to accumulate the partial results for the linear output.\n\n      This becomes very complex.\n\n  Given the time, and the fact that we are only at Phase 1 (compilation) and then Phase 2 (correctness), we might start with a simple kernel that does one batch element per program and then optimize later.\n\n  However, the problem says: \"Small iterative steps: Modify \u226420 lines per revision\"\n\n  We are starting from scratch. We have to write a Triton kernel. We cannot write a very long kernel in 20 lines.\n\n  Therefore, we break the problem into steps:\n\n      Step 1: Write a Triton kernel that does only the linear layer? But then we break the fusion.\n\n      But the problem says we are to rewrite the entire pattern.\n\n  Alternatively, we can write multiple kernels? The problem does not say we can only have one kernel. It says \"custom_code\" (singular) but that might refer to the entire implementation.\n\n  We are to replace the forward function. We can do:\n\n      We write two kernels:\n\n          kernel1: linear\n          kernel2: avgpool, gelu, scale, max\n\n      But then we lose fusion.\n\n  Given the constraints of the problem and the fact that we are in the early phases, we decide to write a simple kernel for the entire pattern that is not optimized but compiles and is correct.\n\n  We will write a kernel that does:\n\n      pid = tl.program_id(0)   # batch index\n\n      We loop over the output features (0 to out_features-1) to compute the linear:\n\n          for i in range(out_features):\n              acc = 0.0\n              for j in range(in_features):\n                  x_val = tl.load(x_ptr + pid * in_features + j)\n                  w_val = tl.load(weight_ptr + i * in_features + j)\n                  acc += x_val * w_val\n              # add bias\n              b_val = tl.load(bias_ptr + i)\n              linear_out[i] = acc + b_val\n\n      Then we do the pooling:\n\n          n_pool = out_features // pool_size   # must be divisible\n          for i in range(n_pool):\n              start = i * pool_size\n              # load 4 consecutive elements\n              val0 = linear_out[start]\n              val1 = linear_out[start+1]\n              val2 = linear_out[start+2]\n              val3 = linear_out[start+3]\n              avg = (val0+val1+val2+val3) / pool_size\n              pooled[i] = avg\n\n      Then we do:\n\n          max_val = -infinity\n          for i in range(n_pool):\n              # apply GELU and scale\n              gelu_val = gelu(pooled[i])   # we need a GELU function\n              scaled_val = gelu_val * scale_factor\n              if scaled_val > max_val:\n                  max_val = scaled_val\n          tl.store(output_ptr + pid, max_val)\n\n  This is very naive and will be slow, but it should compile and be correct.\n\n  Then we can optimize in subsequent iterations.\n\n  We must define GELU. Triton has an approximation? Or we can use the erf from libdevice.\n\n  We can use: \n        gelu_val = x * 0.5 * (1.0 + tl.erf(x * 0.7071067811865475))\n\n  This is the exact formula? But we are in FP32.\n\n  Now, we design the kernel:\n\n      We are going to have:\n          x_ptr: pointer to the input tensor (shape [batch, in_features])\n          weight_ptr: pointer to weight (shape [out_features, in_features])\n          bias_ptr: pointer to bias (shape [out_features])\n          output_ptr: pointer to output (shape [batch])\n          in_features, out_features, pool_size, scale_factor: constants.\n\n      We assume that out_features is divisible by pool_size.\n\n  We must allocate shared memory? We need to store the linear_out vector? It is of length out_features (256). We can use shared memory? But 256 * 4 bytes = 1024 bytes per program. The shared memory is per block? And we are having one program per batch element? And we are using one block per program? Then we can use shared memory? But note: the shared memory per block is 48KB? So 1024 bytes is acceptable.\n\n  Steps:\n\n      pid = tl.program_id(0)\n\n      # Allocate shared memory for linear_out and pooled? We only need linear_out (size out_features) and then we can do the pooling without storing the entire pooled vector? We can do the max reduction on the fly.\n\n      We can avoid storing the entire pooled vector? We can do:\n\n          max_val = -inf\n          for i in range(0, out_features, pool_size):\n              # compute the average of the next pool_size elements in linear_out\n              total = 0.0\n              for j in range(pool_size):\n                  idx = i + j\n                  total += linear_out[idx]\n              avg = total / pool_size\n              gelu_val = ... \n              scaled_val = gelu_val * scale_factor\n              if scaled_val > max_val:\n                  max_val = scaled_val\n\n      Then we store max_val.\n\n  But note: we need the entire linear_out vector. How to compute it without storing it? We can compute the average for one segment at a time? But we still need the entire linear_out vector because the segments are non-overlapping and contiguous.\n\n      However, we can compute the linear_out vector on the fly and then immediately break it into segments? We can compute one segment at a time and then discard the linear_out values? But note: the linear_out vector is used in multiple segments? No, each value is used in exactly one segment.\n\n      Therefore, we can avoid storing the entire linear_out vector? We can compute the linear_out for one segment at a time? But then we would have to recompute the linear_out for the entire vector for every segment? That would be inefficient.\n\n  So we store the linear_out in shared memory? Or we can use registers? We can have an array of registers? But 256 registers is too many.\n\n  Therefore, we use shared memory.\n\n  Steps:\n\n      pid = tl.program_id(0)\n\n      # Define shared memory for linear_out of size out_features\n      linear_out = tl.zeros((out_features,), dtype=tl.float32)   # but this is in registers? We want shared memory.\n\n      How to declare shared memory in Triton? We can use:\n\n          shmem = tl.zeros((out_features,), dtype=tl.float32)   # but this is in shared memory? Not exactly.\n\n      Actually, we can declare static shared memory? But the out_features is a constant? In this case, out_features=256, which is known at compile time.\n\n      We can do:\n\n          linear_out = tl.zeros((256,), dtype=tl.float32)   # but this will use registers? We don't want that.\n\n      Instead, we can use:\n\n          linear_out = tl.full((256,), 0, dtype=tl.float32)   # but still registers.\n\n      We must use the `static` keyword for shared memory? In Triton, we can declare:\n\n          shmem = tl.zeros((256,), dtype=tl.float32)   # and hope it goes to shared memory? But the compiler might not.\n\n      Alternatively, we can use:\n\n          off = tl.arange(0, 256)\n          linear_out = tl.zeros((256,), dtype=tl.float32)   # this is a tensor in registers? Not shared.\n\n      We need to use `tl.make_block_ptr`? Not necessarily.\n\n      Given the complexity, and since we are in Phase 1, we will do a naive version without shared memory and hope that the compiler optimizes? But we are limited by the number of registers.\n\n      Instead, we can break the output features into blocks? But then we lose the ability to do the entire vector.\n\n  Given the complexity and the fact that we are only at Phase 1, we decide to write a kernel that does not try to store the entire linear_out vector, but rather computes each segment of the pooling by recomputing the necessary part of the linear_out.\n\n  This is inefficient but will work for correctness.\n\n  Steps for one batch element and one segment:\n\n      Let segment_index in [0, n_pool-1]\n\n      We want to compute the average of the linear_out over the indices [segment_index*pool_size, (segment_index+1)*pool_size-1]\n\n      How to compute one element of linear_out for a given output feature index i?\n\n          linear_out[i] = bias[i] + sum_{j=0}^{in_features-1} x[pid, j] * weight[i, j]\n\n      Then for a segment, we need to compute:\n\n          segment_avg = 0\n          for i in segment (which is a contiguous block of pool_size output features):\n              linear_out[i] = ...   [as above]\n              segment_avg += linear_out[i]\n          segment_avg /= pool_size\n\n      Then we apply GELU and scale and then compare to the current max.\n\n  But note: we are doing this for each segment? Then we would be recomputing the linear_out for every segment? And the entire linear_out is computed n_pool times? That is 64 times? And each time we do in_features * pool_size operations? So total operations per batch element: in_features * pool_size * n_pool = in_features * out_features? Because n_pool * pool_size = out_features.\n\n  That is the same as doing the entire linear first? But we are doing it in segments and then throwing away the intermediate results? And then we are not saving anything.\n\n  Therefore, we might as well compute the entire linear_out first and then do the pooling.\n\n  Given the register pressure, we decide to do it in two nested loops? But we hope that the out_features (256) is small enough to fit in registers? We try to use shared memory.\n\n  How to use shared memory in Triton? We can declare a static array in shared memory if the size is known at compile time.\n\n  We know out_features=256. So we can do:\n\n          shmem = tl.zeros((256,), dtype=tl.float32)   # and hope it is in shared memory? But the Triton compiler might not put it there.\n\n  Actually, Triton does not have explicit shared memory control. It uses automatic allocation. We can try to use a local array and hope it spills to shared memory.\n\n  We will write the kernel assuming we can have an array of 256 floats.\n\n  Steps in the kernel:\n\n      pid = tl.program_id(0)\n\n      # Allocate an array for linear_out of size 256\n      linear_out = tl.zeros((256,), dtype=tl.float32)\n\n      # Compute linear_out for each output feature i in [0,255]:\n      for i in range(256):\n          acc = 0.0\n          for j in range(512):\n              x_val = tl.load(x_ptr + pid * 512 + j)\n              w_val = tl.load(weight_ptr + i * 512 + j)\n              acc += x_val * w_val\n          b_val = tl.load(bias_ptr + i)\n          linear_out[i] = acc + b_val\n\n      # Then do the pooling, activation, scaling, and max reduction\n      max_val = -float('inf')\n      pool_size = 4\n      n_pool = 256 // pool_size   # 64\n      for i in range(n_pool):\n          start = i * pool_size\n          total = 0.0\n          for j in range(pool_size):\n              total += linear_out[start+j]\n          avg = total / pool_size\n          # GELU: \n          gelu_val = avg * 0.5 * (1.0 + tl.erf(avg * 0.7071067811865475))\n          scaled_val = gelu_val * scale_factor   # scale_factor is passed as a constant\n          if scaled_val > max_val:\n              max_val = scaled_val\n\n      tl.store(output_ptr + pid, max_val)\n\n  This kernel will be very slow, but it should be correct.\n\n  We must note: the Triton kernel does not support nested loops with dynamic bounds? It does, but it may unroll. We hope that the in_features (512) and out_features (256) are constants so that the loops are unrolled? But that would create very long code.\n\n  Alternatively, we can use tl.arange and tl.sum? For the linear part, we can do:\n\n      j = tl.arange(0, 512)\n      for i in range(256):\n          x_vals = tl.load(x_ptr + pid * 512 + j)\n          w_vals = tl.load(weight_ptr + i * 512 + j)\n          dot = tl.sum(x_vals * w_vals, axis=0)\n          linear_out[i] = dot + tl.load(bias_ptr+i)\n\n  But note: the tl.load for x_vals and w_vals would be contiguous? We can do:\n\n      x_base = x_ptr + pid * 512\n      w_base = weight_ptr + i * 512\n      x_vals = tl.load(x_base + j, mask=j<512)   # mask not needed if exactly 512\n      w_vals = tl.load(w_base + j, mask=j<512)\n\n  Then dot = tl.sum(x_vals * w_vals)\n\n  This is better.\n\n  Similarly for the pooling part:\n\n      for i in range(64):\n          base = i * 4\n          indices = base + tl.arange(0,4)\n          vals = tl.load(linear_out + indices)   # but linear_out is in registers? We cannot load from a register array.\n\n  We cannot do that. The linear_out is a local array, not in global memory.\n\n  Therefore, we must keep the inner loops for the linear computation and the pooling as nested loops.\n\n  Given the complexity, and since we are in Phase 1, we will write the kernel with nested loops and hope it compiles.\n\n  We will then optimize in later steps.\n\n  Now, we must write the full code for the model:\n\n      We will remove the nn.Linear and make weight and bias as parameters.\n\n      We will write a Triton kernel for the forward.\n\n  The new model:\n\n      class ModelNew(nn.Module):\n          def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n              super().__init__()\n              self.in_features = in_features\n              self.out_features = out_features\n              self.pool_kernel_size = pool_kernel_size\n              self.scale_factor = scale_factor\n              # Initialize weight and bias\n              self.weight = nn.Parameter(torch.empty(out_features, in_features))\n              self.bias = nn.Parameter(torch.empty(out_features))\n              # reset parameters\n              nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n              fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n              bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n              nn.init.uniform_(self.bias, -bound, bound)\n\n          def forward(self, x):\n              # We will call the Triton kernel here.\n              output = torch.empty(x.size(0), device=x.device, dtype=torch.float32)\n              # Check that out_features is divisible by pool_kernel_size\n              assert self.out_features % self.pool_kernel_size == 0\n              # Call the kernel\n              batch_size = x.size(0)\n              grid = (batch_size,)\n              _forward_kernel[grid](x, self.weight, self.bias, output, self.scale_factor, self.out_features, self.in_features, self.pool_kernel_size)\n              return output\n\n      We must define the kernel.\n\n  But note: the kernel uses constants 512 and 256? We should use the passed in_features, out_features, and pool_kernel_size.\n\n  However, Triton does not support dynamic sizes in the loops? We can use tl.arange(0, in_features) and then use a loop? But the in_features might be 512, which is fixed in this example, but we want it to be general.\n\n  We can pass in_features, out_features, pool_kernel_size as kernel arguments.\n\n  But note: the pooling requires that out_features is divisible by pool_kernel_size. We check in the forward.\n\n  Now, the kernel:\n\n      @triton.jit\n      def _forward_kernel(\n          x_ptr,\n          weight_ptr,\n          bias_ptr,\n          output_ptr,\n          scale_factor,\n          out_features,\n          in_features,\n          pool_size,\n          BLOCK_SIZE_IN: tl.constexpr,\n          BLOCK_SIZE_OUT: tl.constexpr,\n      ):\n          pid = tl.program_id(0)\n\n          # We are not using blocking for now, so we ignore BLOCK_SIZE_*.\n\n          # Compute linear_out for each output feature\n          linear_out = tl.zeros((out_features,), dtype=tl.float32)\n          for i in range(out_features):\n              acc = 0.0\n              for j in range(in_features):\n                  x_val = tl.load(x_ptr + pid * in_features + j)\n                  w_val = tl.load(weight_ptr + i * in_features + j)\n                  acc += x_val * w_val\n              b_val = tl.load(bias_ptr + i)\n              linear_out[i] = acc + b_val\n\n          # Pooling and activation and max\n          n_pool = out_features // pool_size\n          max_val = -float('inf')\n          for i in range(n_pool):\n              start = i * pool_size\n              total = 0.0\n              for j in range(pool_size):\n                  total += linear_out[start+j]\n              avg = total / pool_size\n              # GELU: \n              gelu_val = avg * 0.5 * (1.0 + tl.erf(avg * 0.7071067811865475))\n              scaled_val = gelu_val * scale_factor\n              if scaled_val > max_val:\n                  max_val = scaled_val\n\n          tl.store(output_ptr + pid, max_val)\n\n  But note: the Triton kernel does not support dynamic loops? It does, but it requires that the bounds are passed as constants? Not necessarily, but the performance may suffer.\n\n  However, we are in Phase 1. We want it to compile.\n\n  We must note: the Triton kernel does not allow nested loops with non-constant bounds? Actually, it does, but we must be cautious.\n\n  We will try.\n\n  Also, note: the kernel uses a large array `linear_out` of size `out_features`. This may cause register spill.\n\n  Given the current parameters (out_features=256), it might work.\n\n  Now, we write the full code.\n\n  We will also include the autotune? Not in the first phase.\n\n  We must note: the kernel uses tl.erf, which is available in Triton.\n\n  Let's write the code.\n\n  Important: we must not change the get_inputs and get_init_inputs.\n\n  We will replace the entire EVOLVE-BLOCK.\n\n  Note: the original model had batch_size=128, in_features=512, out_features=256, pool_kernel_size=4, scale_factor=2.0.\n\n  We assume these.\n\n  We will write the kernel inside the class? Or outside? Outside.\n\n  We put the kernel outside the class.\n\n  The new code block:\n\n      import torch\n      import torch.nn as nn\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def _forward_kernel(\n          x_ptr,\n          weight_ptr,\n          bias_ptr,\n          output_ptr,\n          scale_factor,\n          out_features,\n          in_features,\n          pool_size,\n          BLOCK_SIZE_IN: tl.constexpr,\n          BLOCK_SIZE_OUT: tl.constexpr,\n      ):\n          pid = tl.program_id(0)\n\n          # We are not using blocking for now, so we ignore BLOCK_SIZE_*.\n\n          # Compute linear_out for each output feature\n          linear_out = tl.zeros((out_features,), dtype=tl.float32)\n          for i in range(out_features):\n              acc = 0.0\n              for j in range(in_features):\n                  x_val = tl.load(x_ptr + pid * in_features + j)\n                  w_val = tl.load(weight_ptr + i * in_features + j)\n                  acc += x_val * w_val\n              b_val = tl.load(bias_ptr + i)\n              linear_out[i] = acc + b_val\n\n          # Pooling and activation and max\n          n_pool = out_features // pool_size\n          max_val = -float('inf')\n          for i in range(n_pool):\n              start = i * pool_size\n              total = 0.0\n              for j in range(pool_size):\n                  total += linear_out[start+j]\n              avg = total / pool_size\n              # GELU: \n              gelu_val = avg * 0.5 * (1.0 + tl.erf(avg * 0.7071067811865475))\n              scaled_val = gelu_val * scale_factor\n              if scaled_val > max_val:\n                  max_val = scaled_val\n\n          tl.store(output_ptr + pid, max_val)\n\n      class ModelNew(nn.Module):\n          def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n              super(ModelNew, self).__init__()\n              self.in_features = in_features\n              self.out_features = out_features\n              self.pool_kernel_size = pool_kernel_size\n              self.scale_factor = scale_factor\n              # Initialize weight and bias\n              self.weight = nn.Parameter(torch.empty(out_features, in_features))\n              self.bias = nn.Parameter(torch.empty(out_features))\n              # reset parameters\n              nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n              fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n              bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n              nn.init.uniform_(self.bias, -bound, bound)\n\n          def forward(self, x):\n              output = torch.empty(x.size(0), device=x.device, dtype=torch.float32)\n              # Check that out_features is divisible by pool_kernel_size\n              assert self.out_features % self.pool_kernel_size == 0\n              batch_size = x.size(0)\n              grid = (batch_size,)\n              # We pass the pointers and constants\n              _forward_kernel[grid](\n                  x, \n                  self.weight, \n                  self.bias, \n                  output, \n                  self.scale_factor, \n                  self.out_features, \n                  self.in_features, \n                  self.pool_kernel_size,\n                  BLOCK_SIZE_IN=triton.next_power_of_2(self.in_features),\n                  BLOCK_SIZE_OUT=triton.next_power_of_2(self.out_features),\n              )\n              return output\n\n      batch_size = 128\n      in_features = 512\n      out_features = 256\n      pool_kernel_size = 4\n      scale_factor = 2.0\n\n      def get_inputs():\n          return [torch.randn(batch_size, in_features)]\n\n      def get_init_inputs():\n          return [in_features, out_features, pool_kernel_size, scale_factor]\n\n  Note: we added `math` import? We use math.sqrt.\n\n  We must import math.\n\n  However, note: the original code did not have math imported. We will add it.\n\n  This is a very long kernel and may not be efficient, but it should work for correctness.\n\n  We are in Phase 1: compilation.\n\n  We hope it compiles.\n\n  If not, we will fix in the next iteration.\n\n  We are modifying more than 20 lines? Yes, but it's the first version.\n\n  Given the complexity, we output this code.\n\n  Note: the kernel uses two additional arguments BLOCK_SIZE_IN and BLOCK_SIZE_OUT, but we don't use them. We pass them as constants to avoid compilation errors.\n\n  We use triton.next_power_of_2 to get a constant.\n\n  Let's hope.\n\n  We output the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef fused_forward_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    pool_size,\n    scale_factor,\n    BLOCK_SIZE_IN: tl.constexpr,\n    BLOCK_SIZE_OUT: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Compute linear layer output\n    acc = tl.zeros((BLOCK_SIZE_OUT,), dtype=tl.float32)\n    for block in range(0, tl.cdiv(in_features, BLOCK_SIZE_IN)):\n        off_in = block * BLOCK_SIZE_IN + tl.arange(0, BLOCK_SIZE_IN)\n        mask_in = off_in < in_features\n        \n        # Load input vector block\n        x = tl.load(x_ptr + pid * in_features + off_in, mask=mask_in, other=0.0)\n        \n        # Load weight matrix block\n        for out_idx in tl.static_range(0, BLOCK_SIZE_OUT):\n            off_w = out_idx * in_features + off_in\n            w = tl.load(w_ptr + off_w, mask=mask_in, other=0.0)\n            acc = tl.where(tl.arange(0, BLOCK_SIZE_OUT) == out_idx, \n                           acc + tl.sum(x * w), acc)\n    \n    # Add bias\n    off_b = tl.arange(0, BLOCK_SIZE_OUT)\n    mask_b = off_b < out_features\n    bias = tl.load(b_ptr + off_b, mask=mask_b, other=0.0)\n    linear_out = tl.where(mask_b, acc + bias, 0.0)\n    \n    # Average pooling\n    n_pool = out_features // pool_size\n    pooled = tl.zeros((n_pool,), dtype=tl.float32)\n    for i in range(n_pool):\n        start = i * pool_size\n        segment = linear_out[start:start+pool_size]\n        pooled = tl.where(tl.arange(0, n_pool) == i, \n                         tl.sum(segment) / pool_size, pooled)\n    \n    # GELU activation\n    gelu_out = pooled * 0.5 * (1.0 + tl.erf(pooled * 0.7071067811865475))\n    \n    # Scale and max reduction\n    scaled = gelu_out * scale_factor\n    max_val = tl.max(scaled, axis=0)\n    \n    tl.store(output_ptr + pid, max_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.pool_kernel_size = pool_kernel_size\n        self.scale_factor = scale_factor\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        output = torch.empty(x.size(0), device=x.device, dtype=torch.float32)\n        grid = (x.size(0),)\n        fused_forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features,\n            self.pool_kernel_size, self.scale_factor,\n            BLOCK_SIZE_IN=64, BLOCK_SIZE_OUT=64\n        )\n        return output\n\nbatch_size = 128\nin_features = 512\nout_features = 256\npool_kernel_size = 4\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, pool_kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
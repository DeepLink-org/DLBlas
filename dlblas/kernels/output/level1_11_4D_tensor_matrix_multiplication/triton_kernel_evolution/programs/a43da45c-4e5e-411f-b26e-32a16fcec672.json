{"id": "a43da45c-4e5e-411f-b26e-32a16fcec672", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n# Triton kernel for optimized 4D tensor-matrix multiplication\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 128}, num_warps=4, num_stages=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _tensor_matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = A_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = B_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        \n        a = tl.load(a_ptrs, mask=k_mask[None, :] & (offs_m[:, None] < M), other=0.0)\n        b = tl.load(b_ptrs, mask=k_mask[:, None] & (offs_n[None, :] < N), other=0.0)\n        \n        accumulator += tl.dot(a, b, allow_tf32=False)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    c_ptrs = C_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    tl.store(c_ptrs, accumulator, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Performs 4D tensor-matrix multiplication: \n        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n\n    Args:\n        A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n        B (torch.Tensor): Input matrix of shape (l, k)\n\n    Returns:\n        torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, A, B):\n        b, i, j, l = A.shape\n        k = B.shape[1]\n        \n        # Flatten batch and spatial dimensions\n        A_flat = A.reshape(-1, l).contiguous()\n        C_flat = torch.empty(b*i*j, k, device=A.device, dtype=A.dtype)\n        \n        M, K = A_flat.shape\n        N = B.shape[1]\n        \n        # Compute optimal grid size\n        grid = lambda META: (\n            triton.cdiv(M, META['BLOCK_M']),\n            triton.cdiv(N, META['BLOCK_N']),\n        )\n        \n        # Launch kernel\n        _tensor_matmul_kernel[grid](\n            A_flat, B, C_flat,\n            M, N, K,\n            A_flat.stride(0), A_flat.stride(1),\n            B.stride(0), B.stride(1),\n            C_flat.stride(0), C_flat.stride(1),\n        )\n        \n        # Reshape back to original dimensions\n        return C_flat.reshape(b, i, j, k)\n\n# Test code\nb = 16\ni = 256\nj = 512\nl = 256\nk = 768\n\ndef get_inputs():\n    A = torch.randn(b, i, j, l)\n    B = torch.randn(l, k)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "a1aad93d-cfe2-413c-98b2-8238ef3bd48e", "generation": 1, "timestamp": 1754634577.3067613, "iteration_found": 2, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.004, "runtime_ms": 20.8, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.075, "speed_up": 0.004, "custom_timing": 20.8}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 22.5, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.075, "speed_up": 0.003, "custom_timing": 22.5}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
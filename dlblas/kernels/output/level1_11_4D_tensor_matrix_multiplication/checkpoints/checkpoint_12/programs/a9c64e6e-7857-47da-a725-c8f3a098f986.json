{"id": "a9c64e6e-7857-47da-a725-c8f3a098f986", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=5),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=5),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=5),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=8, num_stages=3),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef tensor_matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_k[None, :] < K - k) & (offs_m[:, None] < M), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_n[None, :] < N), other=0.0)\n        \n        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        \n    def forward(self, A, B):\n        b, i, j, l = A.shape\n        k = B.shape[1]\n        M = b * i * j\n        N = k\n        K = l\n        \n        A_flat = A.view(M, l).contiguous()\n        output_flat = torch.empty((M, k), device=A.device, dtype=A.dtype)\n        \n        if A.dtype == torch.float16:\n            A_flat = A_flat.to(torch.float32)\n            B_float = B.to(torch.float32)\n            output_float = torch.empty((M, k), device=A.device, dtype=torch.float32)\n        else:\n            B_float = B\n            output_float = output_flat\n        \n        grid = lambda opt: (triton.cdiv(M, opt.d['BLOCK_SIZE_M']) * triton.cdiv(k, opt.d['BLOCK_SIZE_N']),)\n        \n        tensor_matmul_kernel[grid](\n            A_flat, B_float, output_float,\n            M, N, K,\n            A_flat.stride(0), A_flat.stride(1),\n            B_float.stride(0), B_float.stride(1),\n            output_float.stride(0), output_float.stride(1),\n        )\n        \n        if A.dtype == torch.float16:\n            output_flat = output_float.to(A.dtype)\n        \n        return output_flat.view(b, i, j, k)\n\n# Test code\nb = 16\ni = 256\nj = 512\nl = 256\nk = 768\n\ndef get_inputs():\n    A = torch.randn(b, i, j, l)\n    B = torch.randn(l, k)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "a1aad93d-cfe2-413c-98b2-8238ef3bd48e", "generation": 1, "timestamp": 1754634671.2877307, "iteration_found": 1, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_11_4D_tensor_matrix_multiplication", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpo3oqoo5v.py\", line 85, in forward\n    tensor_matmul_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 160, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 146, in kernel_call\n    self.fn.run(\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 610, in run\n    grid = grid(bound_args)\n  File \"/tmp/tmpo3oqoo5v.py\", line 83, in <lambda>\n    grid = lambda opt: (triton.cdiv(M, opt.d['BLOCK_SIZE_M']) * triton.cdiv(k, opt.d['BLOCK_SIZE_N']),)\nAttributeError: 'dict' object has no attribute 'd'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 0.075, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 22.5, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.075, "speed_up": 0.003, "custom_timing": 22.5}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 22.5000\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.075, 'speed_up': 0.003, 'custom_timing': 22.5}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 22.5000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.075, 'speed_up': 0.003, 'custom_timing': 22.5}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Performs 4D tensor-matrix multiplication: \n        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n\n    Args:\n        A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n        B (torch.Tensor): Input matrix of shape (l, k)\n\n    Returns:\n        torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, A, B):\n        \"\"\"\n        Performs the 4D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n            B (torch.Tensor): Input matrix of shape (l, k)\n\n        Returns:\n            torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n        \"\"\"\n        return torch.einsum(\"bijl,lk->bijk\", A, B)\n\n# Test code\nb = 16\ni = 256\nj = 512\nl = 256\nk = 768\n\ndef get_inputs():\n    A = torch.randn(b, i, j, l)\n    B = torch.randn(l, k)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (22.5000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.075, 'speed_up': 0.003, 'custom_timing': 22.5})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Performs 4D tensor-matrix multiplication: \n        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n\n    Args:\n        A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n        B (torch.Tensor): Input matrix of shape (l, k)\n\n    Returns:\n        torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n\n    def forward(self, A, B):\n        \"\"\"\n        Performs the 4D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n            B (torch.Tensor): Input matrix of shape (l, k)\n\n        Returns:\n            torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n        \"\"\"\n        return torch.einsum(\"bijl,lk->bijk\", A, B)\n\n# Test code\nb = 16\ni = 256\nj = 512\nl = 256\nk = 768\n\ndef get_inputs():\n    A = torch.randn(b, i, j, l)\n    B = torch.randn(l, k)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch reference implementation that uses `torch.einsum` to perform a 4D tensor-matrix multiplication:\n    C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n\nThe current implementation is the reference code (using einsum) and we are to replace it with a Triton kernel for better performance.\n\nConstraints:\n- The input A is of shape (b, i, j, l) and B is of shape (l, k).\n- The output should be of shape (b, i, j, k).\n\nWe note that the operation is essentially a batched matrix multiplication over the first three dimensions of A (with the last dimension being the contraction dimension) and the matrix B.\n\nHowever, note that the batch has three dimensions: b, i, j. We can view this as a batch of size (b * i * j) of vectors (length l) multiplied by the matrix B (l x k) to produce a vector of length k for each element in the batch.\n\nTherefore, we can flatten the first three dimensions of A to form a 2D tensor of shape (b*i*j, l) and then perform a matrix multiplication with B (l, k) to get a result of shape (b*i*j, k). Then we reshape it to (b, i, j, k).\n\nBut note: the reference implementation uses `torch.einsum` which is likely efficient, but we are to write a Triton kernel to accelerate it.\n\nPlan for Triton kernel:\n\nWe are going to write a kernel that processes multiple rows of the flattened A (i.e., multiple (l,) vectors) and multiplies by B. However, note that B is shared by all rows. We can load B into shared memory? But B might be too big.\n\nAlternatively, we can use a block tiling approach:\n\nEach program will compute a block of the output. We can assign each block to a group of rows (in the flattened A) and a contiguous set of columns in B.\n\nHowever, note that the output is 4D. We can use a grid of (b, i, j) and then each block computes a contiguous segment of the k-dimension? But the k-dimension might be large.\n\nAlternatively, we can use a 2D grid:\n    pid_b = program_id(0)   # for the batch dimension? but we have b, i, j -> we can combine b*i*j as one dimension and then k as the other.\n\nBut note: the output is 4D: (b, i, j, k). We can think of the output as a 2D tensor: (b*i*j, k). Then we can use a 2D grid of blocks, each block computing a tile of the output.\n\nLet:\n    - BLOCK_SIZE_K: the block size for the k dimension (columns of B and output)\n    - BLOCK_SIZE_L: the block size for the contraction dimension (l)\n\nBut note: we are reading A in rows of length l, and we are multiplying by a matrix B of shape (l, k). For a single row of A, we compute a row of the output of length k. We can compute a row in blocks of k (and similarly for the contraction in blocks of l).\n\nWe can design the kernel as follows:\n\nEach program is responsible for a set of rows (in the flattened A) and a set of columns (in k). But note that we have to reduce over l. We can do:\n\n    for l_offset in range(0, L, BLOCK_SIZE_L):\n        load a block of A: [BLOCK_SIZE_M, BLOCK_SIZE_L] (where BLOCK_SIZE_M is the number of rows we are processing at once, but note we are only processing one row per program? Actually, we can process multiple rows and multiple columns at once).\n\nWe'll use a 2D block arrangement:\n\n    pid = [pid_m, pid_k]  where:\n        pid_m: index over the flattened batch dimension (b*i*j) in blocks of BLOCK_SIZE_M\n        pid_k: index over the k dimension in blocks of BLOCK_SIZE_K\n\nBut note: the total number of rows in the flattened A is (b*i*j). We can let:\n    pid_m = program_id(0)   # over the rows of the flattened A\n    pid_k = program_id(1)   # over the columns of B (and output)\n\nThen each block computes a tile of the output of size [BLOCK_SIZE_M, BLOCK_SIZE_K].\n\nSteps:\n\n1. Compute the row indices for the flattened A that this block handles: [start_m : start_m + BLOCK_SIZE_M]. We have to be careful with boundaries.\n\n2. For the k dimension: we compute the column indices: [start_k : start_k + BLOCK_SIZE_K].\n\n3. Initialize a block accumulator of zeros of size [BLOCK_SIZE_M, BLOCK_SIZE_K].\n\n4. Loop over l in blocks of BLOCK_SIZE_L:\n\n    a. Load a block of A from memory: [BLOCK_SIZE_M, BLOCK_SIZE_L]. The A matrix is of shape (b*i*j, l). We need to compute the pointer for A for the current block of rows and the current l block.\n\n    b. Load a block of B from memory: [BLOCK_SIZE_L, BLOCK_SIZE_K]. The B matrix is of shape (l, k). We load the block at (l_offset, start_k).\n\n    c. Multiply the two blocks and accumulate.\n\n5. Store the accumulator to the output.\n\nBut note: the matrix A is actually 4D. How do we compute the pointer? We have to flatten the first three dimensions. We can do:\n\n    A_flat = A.view(-1, A.shape[-1])   # shape [b*i*j, l]\n\nSimilarly, the output will be of shape [b*i*j, k] and then we reshape to (b, i, j, k).\n\nHowever, note that we are not allowed to change the output tensor shapes. We must produce the exact same shape. So we can do the reshape in the kernel? Actually, we can write the kernel to work on the flattened view and then reshape the output.\n\nBut the kernel is inside the forward function. We can do:\n\n    def forward(self, A, B):\n        b, i, j, l = A.shape\n        k = B.shape[1]\n        # Flatten the first three dimensions of A\n        A_flat = A.view(-1, l)   # [b*i*j, l]\n        output_flat = torch.empty(b*i*j, k, device=A.device, dtype=A.dtype)\n        # Call the triton kernel to compute output_flat = A_flat @ B\n        # Then reshape output_flat to (b, i, j, k)\n\nSo the kernel will be written for 2D matrices.\n\nWe must design the kernel to handle boundaries because the dimensions might not be multiples of the block sizes.\n\nWe'll use:\n\n    BLOCK_SIZE_M: the number of rows per block (for the flattened A)\n    BLOCK_SIZE_N: the number of columns per block (for the k dimension) -> note: we use N for the columns of B, but in the kernel we call it BLOCK_SIZE_K? Actually, let's use:\n        BLOCK_SIZE_L for the contraction dimension (l)\n        BLOCK_SIZE_K for the k dimension (columns of B and output)\n\n    We don't need to split the M dimension (the flattened batch) because we are using a grid over M and K.\n\nBut note: the M dimension (b*i*j) might be very large. We are using a grid of (ceil(b*i*j / BLOCK_SIZE_M), ceil(k / BLOCK_SIZE_K)).\n\nWe'll write the kernel accordingly.\n\nSteps in the kernel:\n\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n\n    # Create ranges for the M and K dimensions for the block.\n    m_offs = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    k_offs = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n\n    # We will accumulate in a block of size [BLOCK_SIZE_M, BLOCK_SIZE_K]\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n\n    # Loop over L in blocks of BLOCK_SIZE_L\n    for l_off in range(0, L, BLOCK_SIZE_L):\n        l_offs = l_off + tl.arange(0, BLOCK_SIZE_L)\n\n        # Load a block of A: [BLOCK_SIZE_M, BLOCK_SIZE_L]\n        a_ptrs = A_flat + m_offs[:, None] * stride_am + l_offs[None, :] * stride_al\n        a_mask = (m_offs[:, None] < M) & (l_offs[None, :] < L)\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n\n        # Load a block of B: [BLOCK_SIZE_L, BLOCK_SIZE_K]\n        b_ptrs = B + l_offs[:, None] * stride_bl + k_offs[None, :] * stride_bk\n        b_mask = (l_offs[:, None] < L) & (k_offs[None, :] < K)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n\n        # Accumulate\n        acc += tl.dot(a, b)\n\n    # Write back the output block\n    c_ptrs = output_flat + m_offs[:, None] * stride_cm + k_offs[None, :] * stride_ck\n    c_mask = (m_offs[:, None] < M) & (k_offs[None, :] < K)\n    tl.store(c_ptrs, acc, mask=c_mask)\n\nWe must note the strides for A_flat, B, and output_flat.\n\nHowever, note: A_flat is contiguous? We created it by view, so it should be contiguous. Similarly, we create output_flat as empty and then we are writing in row-major order, so it will be contiguous. But B might be non-contiguous? We are given B of shape (l, k). We assume it is row-major (so contiguous in k, then next row). But if it is transposed? The problem says B is (l, k). So we can assume row-major.\n\nBut note: the stride for A_flat: \n    stride_am = the stride for the row (which is l, the number of columns) \n    stride_al = 1\n\nSimilarly for B:\n    stride_bl = k   (because the next row in B is offset by k elements)\n    stride_bk = 1\n\nBut wait: in row-major, the element at (i, j) is at offset i * stride_row + j * stride_col. For a 2D tensor of shape [l, k] (row-major), the stride for the row is k, and the stride for the column is 1.\n\nSimilarly, for output_flat (which is [M, K]): \n    stride_cm = K\n    stride_ck = 1\n\nWe'll set:\n    M = b*i*j\n    K = k\n    L = l\n\nWe must be cautious about the data type: the reference uses the same dtype as A. We assume A and B are float32? The problem does not specify, but we can make the kernel generic? Actually, the Triton dot operation requires specific data types. We'll assume the kernel is for float32.\n\nBut note: the H100 has tensor cores for FP16, BF16, TF32, and FP32. We can use FP32 accumulation? Or we can use TF32? We can leave the precision as is.\n\nAlternatively, we can let the kernel handle different data types? But the problem does not specify. We'll assume float32.\n\nWe are going to write the kernel and then call it from the forward function.\n\nWe also note that we must tune the block sizes. We can use autotune.\n\nLet's write the kernel.\n\nImportant: We must avoid out-of-bound accesses by using masks.\n\nWe'll define the kernel with the following parameters:\n\n    @triton.autotune(...)  # we'll provide a few configurations\n    @triton.jit\n    def _kernel(\n        A_ptr, \n        B_ptr,\n        C_ptr,\n        M, K, L,   # dimensions: M = b*i*j, K = k, L = l\n        stride_am, stride_al, \n        stride_bl, stride_bk,\n        stride_cm, stride_ck,\n        BLOCK_SIZE_M: tl.constexpr, \n        BLOCK_SIZE_K: tl.constexpr,\n        BLOCK_SIZE_L: tl.constexpr,\n    ):\n        ...\n\nWe'll set the grid as:\n\n    grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(K, BLOCK_SIZE_K))\n\nWe must choose reasonable block sizes. For example:\n    BLOCK_SIZE_M = 32\n    BLOCK_SIZE_K = 64\n    BLOCK_SIZE_L = 64\n\nBut note: the dot product in Triton requires that the inner dimension (BLOCK_SIZE_L) is a multiple of 16 for some data types? We'll use 64 for now.\n\nWe'll provide several configurations for autotune.\n\nHowever, note that the kernel might be memory bound? Because we are reading A (size M*L) and B (size L*K) and writing C (size M*K). The arithmetic intensity is about 2*M*L*K / (M*L + L*K + M*K) which is about 2*min(M, K, L) if the dimensions are similar? \n\nBut we are not sure. We'll try to use tile sizes that are known to work well on H100.\n\nWe note that the H100 has 128KB shared memory per SM. The block we are loading for A is [BLOCK_SIZE_M, BLOCK_SIZE_L] and for B is [BLOCK_SIZE_L, BLOCK_SIZE_K]. The total shared memory usage per block is:\n\n    BLOCK_SIZE_M * BLOCK_SIZE_L * sizeof(float) + \n    BLOCK_SIZE_L * BLOCK_SIZE_K * sizeof(float)\n\nFor example, with BLOCK_SIZE_M=32, BLOCK_SIZE_L=64, BLOCK_SIZE_K=64:\n\n    32*64*4 + 64*64*4 = 8192 + 16384 = 24576 (24KB) which is acceptable.\n\nWe can try larger blocks? But note that the number of blocks in flight might be limited by the shared memory.\n\nWe'll let the autotuner decide.\n\nWe'll write the kernel and then integrate it into the ModelNew class.\n\nWe are going to replace the forward function with a Triton kernel call.\n\nSteps in the forward function:\n\n    b, i, j, l = A.shape\n    k = B.shape[1]\n    M = b * i * j\n    # Flatten A to [M, l]\n    A_flat = A.view(M, l)\n    # Create output for the flattened version\n    output_flat = torch.empty(M, k, device=A.device, dtype=A.dtype)\n    # Check the dimensions and data types\n    # Call the kernel with the grid and block sizes\n\nBut note: the kernel uses tl.dot which requires that the data types of A and B are the same? And the output is float32? We must ensure the data type of the kernel matches the input.\n\nWe can get the data type from A. But the kernel currently is written for float32. We must handle the case if A is float16? We can write two kernels? Or we can use the same kernel and convert? Alternatively, we can use the input data type.\n\nActually, Triton's dot operation supports mixed precision? We can specify the input and output types. We'll design the kernel to use the same data type as A and B.\n\nBut note: the problem does not specify the data type. We'll assume the kernel is called with float32. However, the reference code uses whatever dtype A and B have.\n\nWe can do:\n\n    if A.dtype == torch.float16:\n        use fp16 kernel?\n    else:\n        use fp32 kernel?\n\nBut to keep it simple, we'll write the kernel for float32 and then if the input is float16, we can convert? Or we can write a generic kernel.\n\nAlternatively, we can use the `tl.dot` with the ability to specify the input and output types? Actually, the dot product in Triton is type-driven: it will use the types of the operands.\n\nSo if we load A and B as float16, then the dot product will be in float16 and then we accumulate in float32? We can do:\n\n    a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n    b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n\n    acc += tl.dot(a, b, out_dtype=tl.float32)\n\nThis way, we can use any input data type and accumulate in float32 to avoid precision loss.\n\nWe'll change the accumulator to float32 and then convert to the output data type? But the output might be float16. We can do:\n\n    tl.store(c_ptrs, acc.to(tl.float16), mask=c_mask)   # if the output is float16?\n\nBut we don't know the output type. We can pass the output type? Alternatively, we can let the kernel output float32 and then convert in the forward function? That would be inefficient.\n\nAlternatively, we can do:\n\n    acc = tl.dot(a, b, out_dtype=acc_dtype)   # but we want to accumulate in float32 even for float16 inputs.\n\nSo we'll do:\n\n    a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n    b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n    # Convert a and b to float32 for computation? Or let dot product do the accumulation in float32?\n    # We can do:\n    #   product = tl.dot(a, b)   # but then the product might be in float16? \n    # Instead, we can do:\n    acc += tl.dot(a.to(tl.float32), b.to(tl.float32))\n\nBut that would be inefficient. Alternatively, we can use the `out_dtype` of the dot product.\n\nLet me check: the dot product in Triton can take an `out_dtype` parameter. We can set it to float32 regardless of the input types.\n\nSo:\n\n    acc += tl.dot(a, b, out_dtype=tl.float32)\n\nThen we can store the accumulator as the output type? But the output tensor might be float16. We have to convert.\n\nWe can do:\n\n    c = acc.to(tl.float16)   # if the output is float16? But we don't know.\n\nWe can pass the data type of the output? Or we can have the kernel output float32 and then the output_flat is float32? But the reference code uses the same dtype as A. So we can set output_flat to be the same dtype as A.\n\nIn the forward function:\n\n    output_flat = torch.empty(M, k, device=A.device, dtype=A.dtype)\n\nThen in the kernel, we can convert the accumulator to the data type of A? But how? We can pass the data type as a kernel argument? We can use:\n\n    ACC_TYPE = tl.float32\n    # ... then in the end\n    if ACC_TYPE != output_type:\n        acc = acc.to(output_type)\n\nBut we don't know the output type in the kernel. Alternatively, we can store the accumulator in the output type? We can do:\n\n    tl.store(c_ptrs, acc.to(C_ptr.dtype.element_ty), mask=c_mask)\n\nBut note: the kernel language does not have access to the dtype of the pointer. We must pass the output data type as an argument? Or we can have the kernel only work in float32 and then the forward function will convert? That would be inefficient for float16.\n\nAlternatively, we can write the kernel to be type-agnostic: we do the accumulation in float32 and then convert to the output data type. We can pass the output data type as a kernel argument? But Triton does not allow dynamic types. We can have multiple kernels? \n\nGiven the complexity, and since the problem does not specify the data type, we'll assume the input is float32. We can extend later if needed.\n\nBut the example in the problem (the quantize example) used float32. So we'll do:\n\n    We assume the input is float32. We'll write the kernel for float32.\n\n    We'll load A and B as float32, do the dot product in float32, and store as float32.\n\nThen in the forward function, if the input is not float32, we can convert? But that would be inefficient. Alternatively, we can write the kernel to handle both? \n\nWe can use:\n\n    if A.dtype == torch.float16:\n        # call a kernel that uses float16\n    else:\n        # call the kernel that uses float32\n\nBut to keep the code simple for now, we'll write one kernel for float32 and hope the input is float32.\n\nAlternatively, we can use the same kernel for multiple data types by using the element type of the pointers? Actually, we can use:\n\n    a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n    b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n    acc += tl.dot(a, b, out_dtype=tl.float32)\n\n    # Then convert to the output data type? But we don't know the output data type.\n\nWe can store the accumulator as float32? Then the output_flat would be float32. But the reference uses the same dtype as A. So if A is float16, then we must output float16? \n\nWe can do:\n\n    output_flat = torch.empty(M, k, device=A.device, dtype=torch.float32)\n    then after the kernel, convert to A.dtype? But that would be an extra copy.\n\nAlternatively, we can do:\n\n    output_flat = torch.empty(M, k, device=A.device, dtype=A.dtype)\n\nand then in the kernel, we convert the accumulator to the data type of the output. But we don't know the data type in the kernel. We must pass it as a constant? We can pass the output data type as a string? and then use:\n\n    if output_type == 'float16':\n        acc = acc.to(tl.float16)\n    elif output_type == 'float32':\n        acc = acc.to(tl.float32)\n\nBut that would be messy.\n\nGiven the complexity and the fact that the problem does not specify, we'll assume the input is float32. We can note that the reference timing is 0.075 ms, which is very fast, so the dimensions must be small? But the example dimensions are large: b=16, i=256, j=512, l=256, k=768 -> M = 16*256*512 = 2,097,152, which is huge.\n\nWe must write an efficient kernel.\n\nWe'll write the kernel for float32 and then in the forward function, we convert the inputs to float32 if they are not? But that would change the reference. We must not change the data type.\n\nTherefore, we must support the same data type as the input.\n\nWe can do:\n\n    We will use the same data type for accumulation? But that might be float16 and we lose precision.\n\n    Instead, we accumulate in float32 and then convert to the output data type.\n\n    In the kernel, we can do:\n\n        acc = ...   # in float32\n        # Convert to the output data type if it is float16, else keep as float32?\n        # But we don't know the output data type.\n\n    Alternatively, we can pass the output data type as a kernel argument and then use conditional conversion? But Triton does not support conditionals on data types? \n\nGiven the constraints, we decide to do:\n\n    In the forward function:\n        output_flat = torch.empty(M, k, device=A.device, dtype=torch.float32)\n        then after the kernel, we convert output_flat to A.dtype and then reshape.\n\n    But wait: the reference uses A.dtype. So if A is float16, we do the entire computation in float32 and then convert to float16? That would be acceptable and common practice.\n\n    Steps:\n\n        A_flat = A.view(M, l).to(torch.float32)   # if A is not float32? But we don't want to change the input. We can do:\n        # Instead, we do the kernel in float32 regardless.\n\n        We convert A and B to float32, compute, then convert the output to the original dtype.\n\n    But that would be:\n\n        A_flat = A.view(M, l).float()\n        B_float = B.float()\n        output_flat = ... (in float32)\n        output = output_flat.to(A.dtype).view(b, i, j, k)\n\n    However, this adds two extra conversions. We can avoid if the input is already float32.\n\n    We can do:\n\n        if A.dtype == torch.float32 and B.dtype == torch.float32:\n            # use the kernel directly\n        else:\n            A_flat = A.view(M, l).float()\n            B_float = B.float()\n            output_flat = ... (in float32)\n            output = output_flat.to(A.dtype).view(b, i, j, k)\n\n    To avoid the overhead when already in float32, we can have two kernels? That complicates.\n\n    Alternatively, we can always convert to float32 and then back? The problem says we must be functionally equivalent. This is acceptable.\n\n    We'll do:\n\n        A_flat = A.view(M, l)\n        B_float = B\n        # Convert to float32\n        if A_flat.dtype != torch.float32:\n            A_flat = A_flat.float()\n        if B_float.dtype != torch.float32:\n            B_float = B_float.float()\n        output_flat = torch.empty(M, k, device=A.device, dtype=torch.float32)\n        # call kernel with A_flat and B_float\n        output = output_flat.to(A.dtype).view(b, i, j, k)\n\n    This way, the kernel is always in float32.\n\n    This adds two conversions (if the input is not float32) and one conversion at the end. But it's acceptable.\n\n    But note: the reference timing is 0.075 ms and our kernel must be faster. We hope the kernel is fast enough to compensate.\n\nWe'll proceed with this plan.\n\nNow, we write the kernel for float32.\n\nWe'll also note that we must tune the block sizes. We'll provide a few configurations.\n\nWe'll write the autotune configs for the kernel.\n\nWe'll try to cover:\n\n    BLOCK_SIZE_M: 32, 64, 128\n    BLOCK_SIZE_K: 32, 64, 128\n    BLOCK_SIZE_L: 32, 64, 128\n\nBut note the shared memory constraint.\n\nWe'll generate a set of configs that fit in shared memory (<= 128KB).\n\nWe'll write:\n\n    configs = []\n    for block_m in [32, 64, 128]:\n        for block_k in [32, 64, 128]:\n            for block_l in [32, 64, 128]:\n                # Check shared memory usage: block_m * block_l * 4 + block_l * block_k * 4 <= 128 * 1024\n                smem = block_m * block_l * 4 + block_l * block_k * 4\n                if smem <= 128 * 1024:\n                    configs.append(triton.Config({'BLOCK_SIZE_M': block_m, 'BLOCK_SIZE_K': block_k, 'BLOCK_SIZE_L': block_l}, num_warps=4, num_stages=3))\n\n    and also try different num_warps and num_stages.\n\nBut to keep the number of configs reasonable, we'll fix num_warps and num_stages for now and only tune the block sizes.\n\nAlternatively, we can use the autotuner with a key that includes M, K, L.\n\nWe'll write:\n\n    @triton.autotune(\n        configs=[\n            triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_L': 64}, num_warps=4, num_stages=3),\n            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_L': 64}, num_warps=4, num_stages=3),\n            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_L': 64}, num_warps=4, num_stages=3),\n            triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_L': 64}, num_warps=4, num_stages=3),\n            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_L': 64}, num_warps=4, num_stages=3),\n            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_L': 64}, num_warps=4, num_stages=3),\n            triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_L': 128}, num_warps=4, num_stages=3),\n            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_L': 128}, num_warps=4, num_stages=3),\n            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_L': 128}, num_warps=4, num_stages=3),\n        ],\n        key=['M', 'K', 'L'],\n    )\n\nBut note: the problem says to use the H100 which has SM8.9. We'll use 4 warps (128 threads) per block? Or 8 warps? We can try 4 and 8.\n\nWe'll add some with 8 warps.\n\nBut we are limited by the autotune configs. We'll provide a few.\n\nNow, we write the code.\n\nNote: We must not change the function signature of the forward function. We must keep the same inputs and outputs.\n\nWe'll put everything inside the ModelNew class.\n\nBut note: the test code at the bottom must remain. We are only allowed to change the EVOLVE-BLOCK.\n\nWe'll write the entire block.\n\nLet's code accordingly.\n</think>\nTo optimize the 4D tensor-matrix multiplication using Triton, we'll implement a kernel that efficiently computes the operation by leveraging GPU memory coalescing and block tiling. The key optimizations include:\n\n1. Flattening the batch dimensions for efficient parallelization\n2. Using block tiling to optimize memory access patterns\n3. Accumulating intermediate results in shared memory\n4. Employing autotuning to find optimal block sizes\n\nHere's the optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=5),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=5),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=5),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=8, num_stages=3),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef tensor_matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_k[None, :] < K - k) & (offs_m[:, None] < M), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_n[None, :] < N), other=0.0)\n        \n        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        \n    def forward(self, A, B):\n        b, i, j, l = A.shape\n        k = B.shape[1]\n        M = b * i * j\n        N = k\n        K = l\n        \n        A_flat = A.view(M, l).contiguous()\n        output_flat = torch.empty((M, k), device=A.device, dtype=A.dtype)\n        \n        if A.dtype == torch.float16:\n            A_flat = A_flat.to(torch.float32)\n            B_float = B.to(torch.float32)\n            output_float = torch.empty((M, k), device=A.device, dtype=torch.float32)\n        else:\n            B_float = B\n            output_float = output_flat\n        \n        grid = lambda opt: (triton.cdiv(M, opt.d['BLOCK_SIZE_M']) * triton.cdiv(k, opt.d['BLOCK_SIZE_N']),)\n        \n        tensor_matmul_kernel[grid](\n            A_flat, B_float, output_float,\n            M, N, K,\n            A_flat.stride(0), A_flat.stride(1),\n            B_float.stride(0), B_float.stride(1),\n            output_float.stride(0), output_float.stride(1),\n        )\n        \n        if A.dtype == torch.float16:\n            output_flat = output_float.to(A.dtype)\n        \n        return output_flat.view(b, i, j, k)\n\n# Test code\nb = 16\ni = 256\nj = 512\nl = 256\nk = 768\n\ndef get_inputs():\n    A = torch.randn(b, i, j, l)\n    B = torch.randn(l, k)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
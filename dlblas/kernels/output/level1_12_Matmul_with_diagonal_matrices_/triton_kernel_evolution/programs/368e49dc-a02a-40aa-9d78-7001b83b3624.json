{"id": "368e49dc-a02a-40aa-9d78-7001b83b3624", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A, B):\n        N, M = B.shape\n        output = torch.empty_like(B)\n        \n        A = A.contiguous()\n        B = B.contiguous()\n        output = output.contiguous()\n        \n        stride_B_row = B.stride(0)\n        stride_B_col = B.stride(1)\n        stride_out_row = output.stride(0)\n        stride_out_col = output.stride(1)\n        \n        grid = lambda meta: (\n            triton.cdiv(N, meta['BLOCK_SIZE_M']),\n            triton.cdiv(M, meta['BLOCK_SIZE_N']),\n        )\n        self.diag_mul_kernel[grid](\n            A, B, output,\n            M, N,\n            stride_B_row, stride_B_col,\n            stride_out_row, stride_out_col\n        )\n        \n        return output\n\n    @staticmethod\n    @triton.autotune(\n        configs=[\n            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 256}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 512}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 1024}, num_warps=8),\n        ],\n        key=['M', 'N'],\n    )\n    @triton.jit\n    def diag_mul_kernel(\n        A_ptr,\n        B_ptr,\n        output_ptr,\n        M,\n        N,\n        stride_B_row, stride_B_col,\n        stride_out_row, stride_out_col,\n        BLOCK_SIZE_M: tl.constexpr,\n        BLOCK_SIZE_N: tl.constexpr,\n    ):\n        pid_m = tl.program_id(0)\n        pid_n = tl.program_id(1)\n        \n        row_idx = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        col_idx = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        \n        row_mask = row_idx < N\n        col_mask = col_idx < M\n        mask = row_mask[:, None] & col_mask[None, :]\n        \n        a_vals = tl.load(A_ptr + row_idx, mask=row_mask, other=0.0)\n        \n        b_ptrs = B_ptr + row_idx[:, None] * stride_B_row + col_idx[None, :] * stride_B_col\n        b_vals = tl.load(b_ptrs, mask=mask, other=0.0)\n        \n        output_vals = a_vals[:, None] * b_vals\n        \n        out_ptrs = output_ptr + row_idx[:, None] * stride_out_row + col_idx[None, :] * stride_out_col\n        tl.store(out_ptrs, output_vals, mask=mask)\n\nM = 4096\nN = 4096\n\ndef get_inputs():\n    A = torch.randn(N, device='cuda')\n    B = torch.randn(N, M, device='cuda')\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "8d9e74eb-3e4e-4e4c-89a4-effc54342d51", "generation": 3, "timestamp": 1754640771.7286365, "iteration_found": 29, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.491, "runtime_ms": 0.0956, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0469, "speed_up": 0.491, "custom_timing": 0.0956}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.503, "runtime_ms": 0.0933, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0469, "speed_up": 0.503, "custom_timing": 0.0933}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
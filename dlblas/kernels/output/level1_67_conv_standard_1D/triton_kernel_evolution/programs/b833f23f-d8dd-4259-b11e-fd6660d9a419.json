{"id": "b833f23f-d8dd-4259-b11e-fd6660d9a419", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_OC': 32, 'BLOCK_LEN': 32, 'BLOCK_IC': 32}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_OC': 64, 'BLOCK_LEN': 32, 'BLOCK_IC': 32}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_OC': 64, 'BLOCK_LEN': 64, 'BLOCK_IC': 32}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_OC': 64, 'BLOCK_LEN': 64, 'BLOCK_IC': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_OC': 128, 'BLOCK_LEN': 64, 'BLOCK_IC': 32}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_OC': 128, 'BLOCK_LEN': 64, 'BLOCK_IC': 64}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_OC': 128, 'BLOCK_LEN': 128, 'BLOCK_IC': 32}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_OC': 128, 'BLOCK_LEN': 128, 'BLOCK_IC': 64}, num_warps=8, num_stages=4),\n    ],\n    key=['in_channels', 'out_channels', 'kernel_size', 'output_length']\n)\n@triton.jit\ndef conv1d_kernel(\n    x_ptr, \n    weight_ptr, \n    output_ptr,\n    batch_size, \n    in_channels, \n    length, \n    out_channels, \n    kernel_size,\n    stride,\n    padding,\n    dilation,\n    groups,\n    stride_x_batch, stride_x_in, stride_x_len,\n    stride_weight_oc, stride_weight_ic, stride_weight_k,\n    stride_output_batch, stride_output_oc, stride_output_len,\n    output_length,\n    BLOCK_OC: tl.constexpr,\n    BLOCK_LEN: tl.constexpr,\n    BLOCK_IC: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_oc = tl.program_id(1)\n    pid_ol = tl.program_id(2)\n    \n    batch_idx = pid_batch\n    oc_offsets = pid_oc * BLOCK_OC + tl.arange(0, BLOCK_OC)\n    ol_offsets = pid_ol * BLOCK_LEN + tl.arange(0, BLOCK_LEN)\n    \n    if batch_idx >= batch_size:\n        return\n        \n    mask_oc = oc_offsets < out_channels\n    mask_ol = ol_offsets < output_length\n    \n    acc = tl.zeros((BLOCK_OC, BLOCK_LEN), dtype=tl.float32)\n    \n    for k in range(kernel_size):\n        input_pos = ol_offsets * stride - padding + k * dilation\n        mask_pos = (input_pos >= 0) & (input_pos < length)\n        \n        for ic in range(0, in_channels, BLOCK_IC):\n            ic_offsets = ic + tl.arange(0, BLOCK_IC)\n            mask_ic = ic_offsets < in_channels\n            \n            input_block = tl.load(\n                x_ptr + batch_idx * stride_x_batch + ic_offsets[:, None] * stride_x_in + input_pos[None, :] * stride_x_len,\n                mask=mask_ic[:, None] & mask_pos[None, :],\n                other=0.0\n            )\n            \n            weight_block = tl.load(\n                weight_ptr + oc_offsets[:, None] * stride_weight_oc + ic_offsets[None, :] * stride_weight_ic + k * stride_weight_k,\n                mask=mask_oc[:, None] & mask_ic[None, :],\n                other=0.0\n            )\n            \n            acc += tl.dot(weight_block, input_block, allow_tf32=True)\n    \n    output_ptrs = output_ptr + batch_idx * stride_output_batch + oc_offsets[:, None] * stride_output_oc + ol_offsets[None, :] * stride_output_len\n    tl.store(output_ptrs, acc, mask=mask_oc[:, None] & mask_ol[None, :])\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        \n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels // groups, kernel_size))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        \n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_channels))\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n        else:\n            self.register_parameter('bias', None)\n            \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.groups != 1:\n            return nn.functional.conv1d(\n                x, self.weight, self.bias, self.stride, \n                self.padding, self.dilation, self.groups\n            )\n        \n        batch_size, _, length = x.shape\n        output_length = (length + 2 * self.padding - self.dilation * (self.kernel_size - 1) - 1) // self.stride + 1\n        output = torch.empty(batch_size, self.out_channels, output_length, device=x.device, dtype=x.dtype)\n        \n        stride_x_batch, stride_x_in, stride_x_len = x.stride()\n        stride_weight_oc, stride_weight_ic, stride_weight_k = self.weight.stride()\n        stride_output_batch, stride_output_oc, stride_output_len = output.stride()\n        \n        grid = lambda opt: (\n            batch_size,\n            triton.cdiv(self.out_channels, opt['BLOCK_OC']),\n            triton.cdiv(output_length, opt['BLOCK_LEN'])\n        )\n        \n        conv1d_kernel[grid](\n            x, self.weight, output,\n            batch_size, self.in_channels, length, self.out_channels, self.kernel_size,\n            self.stride, self.padding, self.dilation, self.groups,\n            stride_x_batch, stride_x_in, stride_x_len,\n            stride_weight_oc, stride_weight_ic, stride_weight_k,\n            stride_output_batch, stride_output_oc, stride_output_len,\n            output_length\n        )\n        \n        if self.bias is not None:\n            output += self.bias[:, None]\n            \n        return output\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "3d305790-6618-4280-b403-bdf3a1a2ddd4", "generation": 5, "timestamp": 1754988510.8419535, "iteration_found": 23, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.445, "runtime_ms": 0.0829, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0369, "speed_up": 0.445, "custom_timing": 0.0829}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.547, "runtime_ms": 0.0674, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0369, "speed_up": 0.547, "custom_timing": 0.0674}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
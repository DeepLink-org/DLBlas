{"id": "cf6bf10a-2315-4c34-9afd-aca90baf9157", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % num_pid_m\n    pid_n = pid // num_pid_m\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\n@triton.jit\ndef group_norm_kernel(\n    x_ptr, y_ptr, weight_ptr, bias_ptr,\n    M, C, G,\n    stride_xm, stride_xc,\n    stride_ym, stride_yc,\n    BLOCK_SIZE: tl.constexpr,\n    eps: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    gid = pid // G\n    group_id = pid % G\n    group_size = C // G\n    start = group_id * group_size\n    offsets = gid * stride_xm + (start + tl.arange(0, BLOCK_SIZE)) * stride_xc\n    \n    x_ptrs = x_ptr + offsets\n    mask = tl.arange(0, BLOCK_SIZE) < group_size\n    x = tl.load(x_ptrs, mask=mask, other=0.0)\n    \n    mean = tl.sum(x, axis=0) / group_size\n    x_centered = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / group_size + eps\n    rstd = tl.libdevice.rsqrt(var)\n    \n    weight = tl.load(weight_ptr + start + tl.arange(0, BLOCK_SIZE), mask=mask, other=1.0)\n    bias = tl.load(bias_ptr + start + tl.arange(0, BLOCK_SIZE), mask=mask, other=0.0)\n    y = (x_centered * rstd) * weight + bias\n    \n    tl.store(x_ptrs, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups, bias_shape):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.num_groups = num_groups\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias_linear = nn.Parameter(torch.randn(out_features))\n        self.weight_gn = nn.Parameter(torch.ones(out_features))\n        self.bias_gn = nn.Parameter(torch.zeros(out_features))\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        \n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        y = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE_M']) * triton.cdiv(N, meta['BLOCK_SIZE_N']),)\n        gemm_kernel[grid](\n            x, self.weight, y, self.bias_linear,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            y.stride(0), y.stride(1),\n            BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n        )\n        \n        C = self.out_features\n        G = self.num_groups\n        group_size = C // G\n        grid_gn = (M * G,)\n        group_norm_kernel[grid_gn](\n            y, y, self.weight_gn, self.bias_gn,\n            M, C, G,\n            y.stride(0), y.stride(1),\n            y.stride(0), y.stride(1),\n            BLOCK_SIZE=group_size,\n            eps=1e-5\n        )\n        \n        y = torch.min(y, dim=1, keepdim=True)[0]\n        y = y + self.bias\n        return y\n\nbatch_size = 128\nin_features = 512\nout_features = 256\nnum_groups = 8\nbias_shape = (1, out_features, 1, 1)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "21c4a02c-0ed0-450a-abeb-96a4a3767c45", "generation": 1, "timestamp": 1754577309.2994156, "iteration_found": 1, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_75_Gemm_GroupNorm_Min_BiasAdd", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp5lrm0c3m.py\", line 109, in forward\n    group_norm_kernel[grid_gn](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 151, in visit_Attribute\n    lhs = self.visit(node.value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'libdevice'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 0.0921, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0774, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0921, "speed_up": 1.19, "custom_timing": 0.0774}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0774\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0921, 'speed_up': 1.19, 'custom_timing': 0.0774}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0774, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0921, 'speed_up': 1.19, 'custom_timing': 0.0774}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, Group Normalization, Minimum operation, and Bias addition.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, bias_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = self.group_norm(x)\n        x = torch.min(x, dim=1, keepdim=True)[0] \n        x = x + self.bias\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 256\nnum_groups = 8\nbias_shape = (1, out_features, 1, 1)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0774), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0921, 'speed_up': 1.19, 'custom_timing': 0.0774})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, Group Normalization, Minimum operation, and Bias addition.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, bias_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = self.group_norm(x)\n        x = torch.min(x, dim=1, keepdim=True)[0] \n        x = x + self.bias\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 256\nnum_groups = 8\nbias_shape = (1, out_features, 1, 1)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. GEMM (via nn.Linear)\n  2. Group Normalization\n  3. Min reduction along dimension 1 (with keepdim=True)\n  4. Add a bias\n\nThe current implementation is using high-level PyTorch operations.\n\nWe are to rewrite the model using Triton to replace the operations that are bottlenecks.\n\nHowever, note that the current model is at the \"reference code\" stage and we are getting a speedup of 1.19 (so 19% faster than the reference?).\nBut the task says: \"Rewrite the program to improve its performance on the specified metrics.\"\n\nLooking at the current implementation, the operations are:\n  - nn.Linear: This is a matrix multiplication. We can write a Triton kernel for GEMM.\n  - nn.GroupNorm: We can write a Triton kernel for Group Normalization.\n  - torch.min: We can write a reduction kernel in Triton.\n  - Addition of bias: This is element-wise and cheap.\n\nBut note: the entire model is small and the tensors are not extremely large (batch_size=128, in_features=512, out_features=256). \nHowever, we are to optimize for the NVIDIA H100/H800 architecture.\n\nGiven the constraints, we should focus on the most expensive operations: GEMM and GroupNorm.\n\nHowever, note that the current model is already implemented in PyTorch and we are getting a speedup of 1.19? \nWait, the meta says: \n   'reference_timing_ms': 0.0921, \n   'speed_up': 1.19, \n   'custom_timing': 0.0774\n\nThis means that the current custom implementation (which is the same as the reference?) is running in 0.0774 ms and the reference in 0.0921 ms? \nBut the current program is the reference? \n\nActually, the task says: \"stage: reference code\", so we are to replace the reference code with a custom Triton implementation.\n\nWe are to generate a Triton implementation for the entire model? But note that the model is composed of multiple operations.\n\nHowever, the instructions say: \"Generate functionally equivalent but faster Triton implementations (custom_code)\".\n\nWe are allowed to break the model into multiple kernels? But note the constraints:\n\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\nBut in this case, we are replacing the entire model.\n\nWe are to write a single Triton kernel for the entire model? That might be difficult because the operations are sequential.\n\nAlternatively, we can write separate kernels for each operation and then chain them. But note the evaluation pipeline: \n  We are to replace the model with a custom implementation that uses Triton.\n\nLooking at the example provided (the per_row_quantize_int8), they replaced the entire function with a Triton kernel.\n\nBut note: the current model is a nn.Module. We are to replace the forward function with a custom implementation that uses Triton.\n\nWe are going to write a single Triton kernel that does the entire forward pass? That might be too complex.\n\nAlternatively, we can break the model into separate Triton kernels for each operation. However, the constraints say we cannot change the function signatures? \n\nWait, the constraints are for the Triton kernel. We are allowed to have multiple kernels.\n\nBut note the output format: we must return the complete updated code containing the EVOLVE-BLOCK comment marker.\n\nWe are going to rewrite the forward function to use Triton kernels for the key operations.\n\nPlan:\n\n1. Replace the GEMM (nn.Linear) with a Triton GEMM kernel.\n2. Replace the GroupNorm with a Triton GroupNorm kernel.\n3. Replace the min reduction with a Triton reduction kernel.\n4. The bias addition we can leave as is (or include in the last kernel).\n\nBut note: the model currently expects the input to be 2D (batch_size, in_features). The GroupNorm expects the input to be 4D? Actually, GroupNorm in PyTorch for 2D input? \n\nLooking at the nn.GroupNorm documentation: \n  The input must have shape (N, C, ...) and the number of channels C must be divisible by num_groups.\n\nIn our model, after the GEMM we have (batch_size, out_features). Then we do GroupNorm on that? \nBut note: the GroupNorm is defined as `nn.GroupNorm(num_groups, out_features)`. This expects the input to be (N, C) and then it will internally reshape to (N, num_groups, C // num_groups) and normalize over the last dimension and the groups.\n\nBut then we do a min reduction over dim=1 (which is the channel dimension) and keepdim, so we get (batch_size, 1). Then we add a bias of shape (1, out_features, 1, 1) -> but wait, the bias_shape is (1, out_features, 1, 1). \n\nHowever, the output of the min reduction is (batch_size, 1) and the bias is 4D? This will use broadcasting.\n\nThis model seems to be designed for 4D input? But the input is 2D. \n\nLet me clarify the dimensions:\n\n- Input: (128, 512)\n- After GEMM: (128, 256)  [because nn.Linear(512,256)]\n- GroupNorm: expects (N, C) and will treat it as (N, C, 1) for 1D? Actually, GroupNorm works for 2D input. It will normalize over the entire 2D tensor? No, it normalizes per group over the channel dimension. The groups are formed along the channel dimension.\n\nSo the GroupNorm will split the 256 channels into `num_groups=8` groups (each group has 32 channels) and then normalize each group independently. The output is (128, 256).\n\nThen we do min over dim=1 -> (128,1). Then we add a bias of (1,256,1,1). How does that broadcast? \n  (128,1) + (1,256,1,1) -> we need to see how they are expanded.\n\nBut note: the output of min is (128,1) and the bias is (1,256,1,1). We can expand (128,1) to (128,256,1,1) by unsqueezing? Actually, the operation x + self.bias would require:\n\n  x: (128,1) -> view as (128,1,1,1) and then expanded to (128,256,1,1) by repeating in the second dimension? \n  But the bias is (1,256,1,1) so it would be expanded to (128,256,1,1).\n\nBut the current code: \n   x = torch.min(x, dim=1, keepdim=True)[0]   # output shape (128,1)\n   x = x + self.bias   # (128,1) + (1,256,1,1) -> this will broadcast to (128,256,1,1) ? \n\nBut then the output of the model would be (128,256,1,1). However, the original input was 2D and the output is 4D? \n\nThis seems inconsistent. \n\nAlternatively, the model might be designed for 4D input? But the input is 2D. \n\nLooking at the get_inputs: it returns a tensor of shape (128,512). \n\nSo the model must return a 4D tensor? That would be (128,256,1,1). \n\nBut the original model code is provided and we are to replicate it exactly.\n\nWe have two options:\n\nOption 1: Rewrite the entire forward pass in one Triton kernel? That would be complex.\n\nOption 2: Write separate Triton kernels for each operation and then chain them.\n\nGiven the complexity, we will start by replacing the GEMM and GroupNorm with Triton kernels, and then do the min reduction and bias addition in PyTorch.\n\nBut note: the min reduction is along the entire channel dimension and then we add a bias that is 4D. We can do the min reduction in Triton and then the bias addition in PyTorch.\n\nHowever, the goal is to get as much speedup as possible.\n\nWe are going to write:\n\n1. A Triton GEMM kernel for the linear layer.\n2. A Triton kernel for GroupNorm for 2D input? Actually, we can write a kernel that does GroupNorm for 2D input by treating it as 1D spatial dimension? \n\nBut note: GroupNorm in PyTorch for 2D input (N, C) is equivalent to normalizing over the channel dimension in groups? Actually, it normalizes each group independently over the entire set of features? \n\nThe GroupNorm operation for 2D input (N, C) is:\n\n  Input: x of shape (N, C)\n  We split the C dimension into num_groups groups, each group has C // num_groups channels.\n  Then we compute mean and variance for each group over the entire set of features? Actually, no: the normalization is done per group over the channels in the group and the spatial dimensions? \n\nBut in 2D input, there is no spatial dimension? Then the GroupNorm for a 2D input is:\n\n  Reshape x to (N, num_groups, C // num_groups)\n  Then compute mean and variance over the last dimension (the group channels) for each group and each sample.\n\nSo it's a per-sample per-group normalization.\n\nWe can write a Triton kernel for that.\n\nAlternatively, we can use the existing Triton kernel for GroupNorm? But note that Triton provides a tutorial for LayerNorm but not GroupNorm. \n\nWe can write a GroupNorm kernel by:\n\n  - Each program handles one group in one sample? \n  - We need to compute mean and variance for a group in a sample.\n\nThe number of elements in a group for one sample: C // num_groups.\n\nFor our example: C=256, num_groups=8, so group_size=32.\n\nWe can have a kernel that launches with grid (N, num_groups). Each block handles one group in one sample.\n\nSteps in the kernel:\n  - Load the entire group (32 elements) and compute mean and variance.\n  - Then normalize the group.\n\nBut note: we are constrained by the block size? We can use a 1D block of size 32.\n\nHowever, 32 is small and we can use a single warp.\n\nBut we have to be careful: the group size might not be a power of two? In our case it is 32.\n\nAlternatively, we can write a reduction kernel for mean and variance.\n\nBut note: the input to GroupNorm is the output of the GEMM, which is (128,256). We have 128 samples and 8 groups per sample -> 1024 groups.\n\nWe can write a kernel that does:\n\n  pid = program_id(0)   # which group (from 0 to 128*8-1)\n  sample_id = pid // num_groups\n  group_id = pid % num_groups\n\n  Then we load the 32 elements in the group, compute mean and variance, then normalize.\n\nWe also have learnable parameters? But note: the current GroupNorm in the model has learnable weight and bias? \n\nWait, the model definition:\n\n   self.group_norm = nn.GroupNorm(num_groups, out_features)\n\nThis creates learnable affine parameters (weight and bias) of size (out_features). \n\nIn our Triton kernel, we have to apply the affine transformation.\n\nSo we need to pass the weight and bias to the kernel.\n\nBut note: the current model does not show the affine parameters? Actually, the nn.GroupNorm by default has affine=True. So we have:\n\n   weight: (out_features)  -> 256\n   bias: (out_features) -> 256\n\nBut in the normalization, we apply:\n\n   y = (x - mean) / sqrt(var+eps) * weight[channel] + bias[channel]\n\nBut note: the weight and bias are per-channel.\n\nSo in our kernel for one group, we have 32 channels. We have to multiply by the weight[channel] and add bias[channel] for each channel in the group.\n\nWe can do that.\n\nBut note: the current model does not store the weight and bias? It uses the default initialization? We are going to copy the parameters from the PyTorch module.\n\nWe are to rewrite the entire model as a single nn.Module that uses Triton kernels for the operations.\n\nWe are going to:\n\n  - Remove the nn.Linear and replace it with a custom function that calls a Triton GEMM kernel.\n  - Remove the nn.GroupNorm and replace it with a custom function that calls a Triton GroupNorm kernel.\n  - Then do the min reduction and bias addition in PyTorch? Or we can also write a Triton kernel for the min reduction.\n\nGiven the time, we will focus on the GEMM and GroupNorm first because they are the most expensive.\n\nBut note: the min reduction is over 256 elements per sample and then we do an addition. This is cheap compared to GEMM and GroupNorm.\n\nSo we will do:\n\n  Step 1: Write a Triton GEMM kernel for the linear transformation.\n  Step 2: Write a Triton GroupNorm kernel for 2D input.\n\nHowever, note that the input to the model is 2D and the output of the GEMM is 2D. The GroupNorm expects 2D. The min reduction then reduces to 1D per sample (with keepdim so 2D) and then we add a 4D bias.\n\nWe are going to change the model:\n\n  We will keep the bias as a parameter, but we will change its shape? Actually, the bias is defined with shape (1, out_features, 1, 1). But the output of the min reduction is (128,1). \n\n  We can change the bias to be 2D? But that would change the model. We must keep the same functionality.\n\n  Alternatively, we can reshape the output of the min reduction to (128,1,1,1) and then add the bias, and then we can squeeze the last two dimensions? \n\n  But note: the output of the model is then (128,256,1,1). The original model returns that? \n\n  We must return the same shape.\n\n  So we will do:\n\n      x = self.gemm(x)   # (128,512) -> (128,256)\n      x = self.group_norm(x)   # (128,256)\n      x = torch.min(x, dim=1, keepdim=True)[0]   # (128,1)\n      x = x.unsqueeze(-1).unsqueeze(-1)   # (128,1,1,1) \n      x = x + self.bias   # (128,1,1,1) + (1,256,1,1) -> (128,256,1,1)\n\n  But the original code did:\n\n        x = torch.min(x, dim=1, keepdim=True)[0] \n        x = x + self.bias\n\n  And the broadcast would have produced (128,256,1,1) because self.bias is (1,256,1,1) and x is (128,1) which is viewed as (128,1,1,1) and then expanded to (128,256,1,1) by broadcasting the second dimension.\n\n  So we can do the same.\n\nBut to avoid extra operations, we can do the min reduction and then add the bias without reshaping? PyTorch broadcasting will handle it.\n\nActually, the current code works without reshaping. So we leave it.\n\nNow, we design the kernels.\n\nKernel 1: GEMM\n\n  We have: \n      input: x (128, 512)\n      weight: W (256, 512)   [because nn.Linear: in_features=512, out_features=256, and it uses weight matrix of shape (out_features, in_features)]\n      bias: none? The nn.Linear has bias by default? But the model does not specify. We see:\n\n          self.gemm = nn.Linear(in_features, out_features)\n\n      The default is bias=True. So we have a bias vector of shape (256).\n\n  We need to compute: Y = x @ W.t() + bias   [if bias is present]\n\n  We can write a Triton GEMM kernel that supports bias.\n\n  We can follow the tutorial: https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html\n\n  But note: the input matrix x is (128,512) and W is (256,512). So we are computing:\n\n        Y = x @ W.t()   [without transposing W, we can use W in row-major and then do x @ W.t()]\n\n  Alternatively, we can store W in column-major? But the nn.Linear uses row-major for weight.\n\n  We will write a kernel that assumes:\n\n        a: (M, K)   -> x: (128,512)\n        b: (K, N)   -> W.t() is (512,256) so we can pass W as (512,256) by transposing it in advance? \n\n  But note: we are going to pre-transpose the weight matrix for efficiency.\n\n  Steps:\n\n      In the module, we will store the weight for the linear layer as a parameter and we will transpose it once.\n\n      Then the kernel will do:\n\n          Y = x @ b   # where b is the weight matrix transposed (so of shape (512,256))\n\n      Then we add the bias.\n\n  We can write a GEMM kernel that does:\n\n          for each row i in [0, M-1]:\n          for each column j in [0, N-1]:\n              y[i, j] = sum_k (x[i,k] * b[k,j]) + bias[j]\n\n  We can use a block approach.\n\n  We will tune the block sizes.\n\nKernel 2: GroupNorm\n\n  We have input of shape (M, C) = (128, 256)\n  We have weight and bias of shape (C) = (256)\n\n  Steps:\n\n      Split C into G groups (G=8). Each group has CG = C // G = 32.\n\n      For each sample i and group g, we have a block of 32 elements.\n\n      We compute:\n          mean = average of the 32 elements in the block\n          variance = average of squared differences from the mean\n\n      Then we normalize the block: \n          out = (x - mean) / sqrt(variance + eps) * weight[channel] + bias[channel]\n\n  We can write a kernel that launches M * G blocks. Each block handles 32 elements.\n\n  We can use one warp per block.\n\n  We will have to load the weight and bias for the 32 channels in the group.\n\nGiven the complexity of writing both kernels, we start with the GEMM.\n\nBut note: the min reduction is cheap? It's 256 elements per sample. We can do it in PyTorch.\n\nWe will do:\n\n  - Replace the GEMM with a Triton GEMM kernel (with bias).\n  - Replace the GroupNorm with a Triton GroupNorm kernel.\n\nWe leave the rest in PyTorch.\n\nWe are going to write two kernels:\n\n  triton_gemm\n  triton_group_norm\n\nWe will remove the nn.Linear and nn.GroupNorm and replace them with custom functions.\n\nLet's define the model:\n\n  We will store:\n      self.weight: the weight for the linear layer (of shape (out_features, in_features)) -> but we will store the transposed version for the kernel? \n          Actually, we can store the weight in the original form and then in the forward we transpose it? \n          But the GEMM kernel expects the second matrix to be in column-major? Actually, we can design the kernel to take the weight in row-major without transposing? \n\n      We can store the weight as (in_features, out_features) and then use it as the b matrix in the GEMM (so no transpose needed). \n\n      But the nn.Linear originally stores weight as (out_features, in_features). We can transpose it once in __init__ and store as (in_features, out_features).\n\n  Similarly, for GroupNorm, we store the weight and bias as provided.\n\nWe are going to write:\n\n  class ModelNew(nn.Module):\n      def __init__(self, in_features, out_features, num_groups, bias_shape):\n          super().__init__()\n          # Instead of nn.Linear, we store weight and bias for the linear layer\n          self.in_features = in_features\n          self.out_features = out_features\n          self.weight = nn.Parameter(torch.randn(out_features, in_features))   # but we will transpose it for the kernel\n          self.bias_linear = nn.Parameter(torch.randn(out_features))   # bias for linear\n\n          self.num_groups = num_groups\n          # For GroupNorm, we need weight and bias of size (out_features)\n          self.weight_gn = nn.Parameter(torch.ones(out_features))\n          self.bias_gn = nn.Parameter(torch.zeros(out_features))\n\n          # The last bias (the one in the original model)\n          self.bias = nn.Parameter(torch.randn(bias_shape))\n\n      def forward(self, x):\n          # Step 1: GEMM\n          # We are going to use a Triton kernel for y = x @ self.weight.t() + self.bias_linear\n          # But note: we stored weight as (out_features, in_features) -> we need to transpose to (in_features, out_features) for the kernel? \n          # Actually, we can store the weight in the kernel in a transposed way? \n          # We can precompute the transpose and store it? Or we can change the kernel to accept the weight in row-major and then do the transpose implicitly? \n\n          # Alternatively, we can store the weight as (in_features, out_features) and then the kernel does: x @ weight + bias\n          # But then we have to initialize it differently.\n\n          # We will store the weight for the linear layer as (in_features, out_features) to avoid transposing in the kernel.\n          # But the original nn.Linear initializes weight as (out_features, in_features). We can transpose it at initialization.\n\n          # However, we are using randn. We can do:\n          #   self.weight = nn.Parameter(torch.randn(in_features, out_features))\n\n          # But note: the original model uses nn.Linear which initializes weight with (out_features, in_features). \n          # We are going to change the shape? We must replicate the same numerical behavior.\n\n          # Actually, the model is being created with the same parameters? We are to replicate the function, not the initialization.\n\n          # We can change the initialization in __init__ to match the shape we need.\n\n          # Let me change: in __init__ we do:\n          #   self.weight = nn.Parameter(torch.randn(in_features, out_features))\n\n          # Then in the GEMM kernel we do: y = x @ weight + bias_linear\n\n          # This is the same as the original: x @ (self.weight_original.t()) + bias_linear, where self.weight_original was (out_features, in_features)\n\n          # So if we let W = self.weight (in_features, out_features), then it is the transpose of the original weight.\n\n          # We can do that.\n\n          # So we don't need to change the initialization? Because we are using randn and it's the same distribution.\n\n          # Now, call the GEMM kernel.\n          y = triton_gemm(x, self.weight, self.bias_linear)\n\n          # Step 2: GroupNorm\n          z = triton_group_norm(y, self.num_groups, self.weight_gn, self.bias_gn)\n\n          # Step 3: min reduction and add bias\n          z = torch.min(z, dim=1, keepdim=True)[0]\n          z = z + self.bias\n\n          return z\n\nBut note: the GroupNorm in the original model used the nn.GroupNorm which has affine parameters. We are using the same affine parameters.\n\nWe have to write the two kernels.\n\nWe are going to write:\n\n  def triton_gemm(a, b, bias):\n      # a: (M, K)\n      # b: (K, N)   [the weight matrix stored as (in_features, out_features)]\n      # bias: (N,)\n      # Output: (M, N)\n\n  We can write a kernel that does the matrix multiplication.\n\n  We will follow the tutorial and tune the block sizes.\n\nSimilarly, for group_norm.\n\nGiven the complexity, we might not be able to write both kernels at once. But we are to provide the code.\n\nWe will write a basic GEMM kernel and a basic GroupNorm kernel.\n\nWe note that the GEMM kernel must be efficient. We will use tiling and shared memory? Actually, Triton handles shared memory automatically with the block pointers.\n\nWe will start with the GEMM kernel.\n\nWe design the GEMM kernel:\n\n  We assume:\n      M = a.shape[0] = 128\n      K = a.shape[1] = 512\n      N = b.shape[1] = 256\n\n  We will use a 2D grid: \n      grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n\n  Each program will compute a block of (BLOCK_SIZE_M, BLOCK_SIZE_N) of the output.\n\n  We will split the K dimension into blocks of size BLOCK_SIZE_K.\n\n  We will load a block of a of size (BLOCK_SIZE_M, BLOCK_SIZE_K) and a block of b of size (BLOCK_SIZE_K, BLOCK_SIZE_N) and compute the product.\n\n  Then we add the bias.\n\nWe will write the kernel accordingly.\n\nSimilarly, for GroupNorm, we design:\n\n  Input: x (M, C) = (128, 256)\n  We have num_groups = 8, so group_size = 32.\n\n  We will have a 1D grid of size M * num_groups (128*8=1024).\n\n  Each block handles one group of one sample.\n\n  We will use a 1D block of size group_size (32) and compute the mean and variance for the group.\n\n  Then we normalize and apply the affine transformation.\n\nWe will write the kernel accordingly.\n\nHowever, note: the time is limited. We are to provide the code.\n\nWe are going to write the kernels inside the class? Or outside? We can write them as top-level functions.\n\nBut the constraints: we must not change the function signatures of the kernels? But we are defining them.\n\nWe are going to write:\n\n  @triton.jit\n  def gemm_kernel( ... )\n\n  and\n\n  @triton.jit\n  def group_norm_kernel( ... )\n\nThen we write the wrapper functions.\n\nBut note: the entire code must be within the EVOLVE-BLOCK.\n\nWe are going to write a long code.\n\nWe must be cautious of the line limit? We are allowed to output up to 20 lines of change? But the previous code was 30 lines. We are replacing it with a longer code.\n\nWe are allowed to do up to 20 lines per revision? But the example they gave was long. We are to replace the entire block.\n\nWe will output the entire block.\n\nGiven the complexity, we might not have time to debug. We are to provide a working version that compiles and is numerically correct.\n\nWe will write the GEMM kernel first and then the GroupNorm kernel.\n\nBut note: the min reduction is done in PyTorch, so we leave it.\n\nWe are going to write the code accordingly.\n\nWe will now write the code.\n</think>\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % num_pid_m\n    pid_n = pid // num_pid_m\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\n@triton.jit\ndef group_norm_kernel(\n    x_ptr, y_ptr, weight_ptr, bias_ptr,\n    M, C, G,\n    stride_xm, stride_xc,\n    stride_ym, stride_yc,\n    BLOCK_SIZE: tl.constexpr,\n    eps: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    gid = pid // G\n    group_id = pid % G\n    group_size = C // G\n    start = group_id * group_size\n    offsets = gid * stride_xm + (start + tl.arange(0, BLOCK_SIZE)) * stride_xc\n    \n    x_ptrs = x_ptr + offsets\n    mask = tl.arange(0, BLOCK_SIZE) < group_size\n    x = tl.load(x_ptrs, mask=mask, other=0.0)\n    \n    mean = tl.sum(x, axis=0) / group_size\n    x_centered = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(x_centered * x_centered, axis=0) / group_size + eps\n    rstd = tl.libdevice.rsqrt(var)\n    \n    weight = tl.load(weight_ptr + start + tl.arange(0, BLOCK_SIZE), mask=mask, other=1.0)\n    bias = tl.load(bias_ptr + start + tl.arange(0, BLOCK_SIZE), mask=mask, other=0.0)\n    y = (x_centered * rstd) * weight + bias\n    \n    tl.store(x_ptrs, y, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups, bias_shape):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.num_groups = num_groups\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias_linear = nn.Parameter(torch.randn(out_features))\n        self.weight_gn = nn.Parameter(torch.ones(out_features))\n        self.bias_gn = nn.Parameter(torch.zeros(out_features))\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        \n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        y = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE_M']) * triton.cdiv(N, meta['BLOCK_SIZE_N']),)\n        gemm_kernel[grid](\n            x, self.weight, y, self.bias_linear,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            y.stride(0), y.stride(1),\n            BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n        )\n        \n        C = self.out_features\n        G = self.num_groups\n        group_size = C // G\n        grid_gn = (M * G,)\n        group_norm_kernel[grid_gn](\n            y, y, self.weight_gn, self.bias_gn,\n            M, C, G,\n            y.stride(0), y.stride(1),\n            y.stride(0), y.stride(1),\n            BLOCK_SIZE=group_size,\n            eps=1e-5\n        )\n        \n        y = torch.min(y, dim=1, keepdim=True)[0]\n        y = y + self.bias\n        return y\n\nbatch_size = 128\nin_features = 512\nout_features = 256\nnum_groups = 8\nbias_shape = (1, out_features, 1, 1)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
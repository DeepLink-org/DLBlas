{"id": "1ef6f5c7-04b3-4800-8e36-3797f7d56769", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit(num_warps=4)\ndef min_reduction_kernel(\n    input_ptr,\n    output_ptr,\n    n_cols,\n    stride_row,\n    stride_col,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    tid = tl.local_program_id(0)\n    block_size = tl.num_threads(0)\n    shmem = tl.zeros((block_size,), dtype=tl.float32) + float('inf')\n    \n    elements_per_thread = tl.cdiv(n_cols, block_size)\n    base = tid * elements_per_thread\n    for i in range(elements_per_thread):\n        col = base + i\n        if col < n_cols:\n            ptr = input_ptr + row_idx * stride_row + col * stride_col\n            val = tl.load(ptr)\n            shmem[tid] = tl.minimum(shmem[tid], val)\n\n    offset = block_size // 2\n    while offset > 0:\n        tl.debug_barrier()\n        if tid < offset:\n            other = shmem[tid + offset]\n            shmem[tid] = tl.minimum(shmem[tid], other)\n        offset //= 2\n\n    if tid == 0:\n        tl.store(output_ptr + row_idx, shmem[0])\n\ndef triton_min(x, dim, keepdim=False):\n    if x.dim() != 2 or dim != 1 or not keepdim:\n        return torch.min(x, dim=dim, keepdim=keepdim)\n    output = torch.empty(x.size(0), 1, device=x.device, dtype=x.dtype)\n    n_cols = x.size(1)\n    grid = (x.size(0),)\n    min_reduction_kernel[grid](\n        x, output, n_cols, x.stride(0), x.stride(1), BLOCK_SIZE=triton.next_power_of_2(n_cols)\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups, bias_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = self.group_norm(x)\n        x = triton_min(x, dim=1, keepdim=True)\n        x = x + self.bias\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 256\nnum_groups = 8\nbias_shape = (1, out_features, 1, 1)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, bias_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "5a58b08e-ee22-4052-8c43-f583a86d0939", "generation": 4, "timestamp": 1754583101.9894435, "iteration_found": 21, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "", "exec_log": "[Child] \u7ed1\u5b9a GPU 4, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 7, in <module>\nTypeError: jit() got an unexpected keyword argument 'num_warps'\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.787, "runtime_ms": 0.117, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0921, "speed_up": 0.787, "custom_timing": 0.117}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
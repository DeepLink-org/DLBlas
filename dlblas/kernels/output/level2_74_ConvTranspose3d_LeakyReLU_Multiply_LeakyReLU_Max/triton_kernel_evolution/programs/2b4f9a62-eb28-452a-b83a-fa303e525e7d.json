{"id": "2b4f9a62-eb28-452a-b83a-fa303e525e7d", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_pointwise_pool_kernel(\n    input_ptr,\n    output_ptr,\n    multiplier_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_depth_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    output_depth_stride,\n    output_height_stride,\n    output_width_stride,\n    num_batch,\n    num_channels,\n    input_depth,\n    input_height,\n    input_width,\n    BLOCK_D: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n    BLOCK_W: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Coalesce output dimensions\n    output_size = num_batch * num_channels * (input_depth//2) * (input_height//2) * (input_width//2)\n    if pid >= output_size:\n        return\n\n    # Reconstruct 5D output indices\n    b_idx = pid // (num_channels * (input_depth//2) * (input_height//2) * (input_width//2))\n    remainder = pid % (num_channels * (input_depth//2) * (input_height//2) * (input_width//2))\n    c_idx = remainder // ((input_depth//2) * (input_height//2) * (input_width//2))\n    remainder = remainder % ((input_depth//2) * (input_height//2) * (input_width//2))\n    d_idx = remainder // ((input_height//2) * (input_width//2))\n    remainder = remainder % ((input_height//2) * (input_width//2))\n    h_idx = remainder // (input_width//2)\n    w_idx = remainder % (input_width//2)\n\n    # Input window for pooling (2x2x2)\n    d_start = d_idx * 2\n    h_start = h_idx * 2\n    w_start = w_idx * 2\n    \n    # Precompute base pointer for batch and channel\n    base_ptr = input_ptr + b_idx * input_batch_stride + c_idx * input_channel_stride\n    \n    # Load multiplier once per thread\n    multiplier = tl.load(multiplier_ptr + c_idx)\n    \n    max_val = -3.4028235e+38  # Min float32 value\n    for dd in range(2):\n        d_in = d_start + dd\n        for dh in range(2):\n            h_in = h_start + dh\n            \n            # Process two consecutive width elements\n            w_in0 = w_start\n            w_in1 = w_start + 1\n            \n            # Combined boundary checks\n            within_bounds0 = (d_in < input_depth) & (h_in < input_height) & (w_in0 < input_width)\n            within_bounds1 = (d_in < input_depth) & (h_in < input_height) & (w_in1 < input_width)\n            \n            # Compute offsets\n            base_offset = d_in * input_depth_stride + h_in * input_height_stride\n            offset0 = base_offset + w_in0 * input_width_stride\n            offset1 = base_offset + w_in1 * input_width_stride\n            \n            # Load two width elements\n            val0 = tl.load(base_ptr + offset0, mask=within_bounds0, other=0)\n            val1 = tl.load(base_ptr + offset1, mask=within_bounds1, other=0)\n            \n            # Apply fused pointwise operations to both elements\n            val0 = tl.where(val0 >= 0, val0, val0 * 0.2)\n            val1 = tl.where(val1 >= 0, val1, val1 * 0.2)\n            \n            val0 = val0 * multiplier\n            val1 = val1 * multiplier\n            \n            val0 = tl.where(val0 >= 0, val0, val0 * 0.2)\n            val1 = tl.where(val1 >= 0, val1, val1 * 0.2)\n            \n            # Update max pooling value\n            max_val = tl.maximum(max_val, val0)\n            max_val = tl.maximum(max_val, val1)\n    \n    # Calculate output pointer offset\n    output_offset = (\n        b_idx * output_batch_stride +\n        c_idx * output_channel_stride +\n        d_idx * output_depth_stride +\n        h_idx * output_height_stride +\n        w_idx * output_width_stride\n    )\n    tl.store(output_ptr + output_offset, max_val)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier_shape):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.multiplier = nn.Parameter(torch.randn(multiplier_shape))\n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n        self.max_pool = nn.MaxPool3d(kernel_size=2)\n        \n    def forward(self, x):\n        x = self.conv_transpose(x)\n        \n        # Capture output shape for kernel configuration\n        B, C, D, H, W = x.shape\n        output = torch.empty(B, C, D//2, H//2, W//2, device=x.device, dtype=x.dtype)\n        \n        # Calculate strides for tensor access\n        def get_strides(tensor):\n            return (\n                tensor.stride(0),\n                tensor.stride(1),\n                tensor.stride(2),\n                tensor.stride(3),\n                tensor.stride(4)\n            )\n        \n        input_strides = get_strides(x)\n        output_strides = get_strides(output)\n        \n        # Configure kernel launch parameters\n        grid_size = B * C * (D//2) * (H//2) * (W//2)\n        \n        # Launch fused kernel with fixed grid tuple\n        fused_pointwise_pool_kernel[(grid_size,)](\n            x,\n            output,\n            self.multiplier,\n            input_strides[0],\n            input_strides[1],\n            input_strides[2],\n            input_strides[3],\n            input_strides[4],\n            output_strides[0],\n            output_strides[1],\n            output_strides[2],\n            output_strides[3],\n            output_strides[4],\n            B,\n            C,\n            D,\n            H,\n            W,\n            BLOCK_D=2,\n            BLOCK_H=2,\n            BLOCK_W=2\n        )\n        \n        return output\n\nbatch_size = 16\nin_channels = 16\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nmultiplier_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "f10d526b-baec-46ba-a699-26b5b6ccee6f", "generation": 3, "timestamp": 1754582007.9784281, "iteration_found": 16, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.006, "runtime_ms": 19.8, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.114, "speed_up": 0.006, "custom_timing": 19.8}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.006, "runtime_ms": 19.0, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.114, "speed_up": 0.006, "custom_timing": 19.0}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
{"id": "6e366efb-fabb-4349-a8d3-40368827ffc0", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef min_reduction_kernel(\n    input_ptr,\n    output_ptr,\n    reduction_dim_size,\n    other_dims_product,\n    stride_reduction,\n    stride_other,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    start_idx = pid * BLOCK_SIZE\n    offsets_other = start_idx + tl.arange(0, BLOCK_SIZE)\n    mask = offsets_other < other_dims_product\n    \n    min_val = tl.full((BLOCK_SIZE,), float('inf'), dtype=tl.float32)\n    for k in range(0, reduction_dim_size):\n        offsets = offsets_other * stride_other + k * stride_reduction\n        current = tl.load(input_ptr + offsets, mask=mask, other=float('inf'))\n        min_val = tl.minimum(min_val, current)\n    \n    tl.store(output_ptr + offsets_other, min_val, mask=mask)\n\n@triton.jit\ndef softmax_kernel(\n    input_ptr,\n    output_ptr,\n    n_channels,\n    spatial_size,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    start_idx = tl.arange(0, BLOCK_SIZE)\n    offsets = pid * n_channels + start_idx\n    mask = start_idx < n_channels\n    \n    row = tl.load(input_ptr + offsets, mask=mask, other=-float('inf'))\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_out = numerator / denominator\n    \n    tl.store(output_ptr + offsets, softmax_out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dim):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.dim = dim\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        x = self.conv(x)\n        orig_shape = x.shape\n        \n        if self.dim < 0:\n            dim = self.dim + x.dim()\n        else:\n            dim = self.dim\n        \n        reduction_dim_size = x.size(dim)\n        other_dims = [size for i, size in enumerate(x.shape) if i != dim]\n        other_dims_product = torch.prod(torch.tensor(other_dims)).item()\n        \n        # Reorganize tensor: move reduction dim to front and flatten other dims\n        x_contig = x.contiguous()\n        x_contig = x_contig.movedim(dim, 0)\n        x_contig_2d = x_contig.flatten(1)\n        \n        min_output_1d = torch.empty(other_dims_product, device=x.device, dtype=x.dtype)\n        stride_reduction = x_contig_2d.stride(0)\n        stride_other = 1\n        \n        grid = (triton.cdiv(other_dims_product, 256),)\n        min_reduction_kernel[grid](\n            x_contig_2d,\n            min_output_1d,\n            reduction_dim_size,\n            other_dims_product,\n            stride_reduction,\n            stride_other,\n            BLOCK_SIZE=256,\n        )\n        \n        min_output = min_output_1d.reshape(other_dims)\n        \n        # Prepare for softmax: move channel dim to last and flatten spatial dims\n        channel_dim = min_output.shape.index(self.out_channels)\n        perm = list(range(len(min_output.shape)))\n        perm.remove(channel_dim)\n        perm.append(channel_dim)\n        softmax_input = min_output.permute(*perm)\n        non_channel_dims = softmax_input.shape[:-1]\n        softmax_input = softmax_input.contiguous().view(-1, self.out_channels)\n        \n        softmax_output = torch.empty_like(softmax_input)\n        \n        grid_softmax = (softmax_input.shape[0],)\n        BLOCK_SIZE = triton.next_power_of_2(self.out_channels)\n        softmax_kernel[grid_softmax](\n            softmax_input,\n            softmax_output,\n            self.out_channels,\n            softmax_input.shape[0],\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n        \n        # Reshape back to original dimensions\n        softmax_output = softmax_output.view(*non_channel_dims, self.out_channels)\n        perm_back = [perm.index(i) for i in range(len(perm))]\n        softmax_output = softmax_output.permute(*perm_back)\n        \n        return softmax_output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nD, H, W = 16, 32, 32\nkernel_size = 3\ndim = 2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, D, H, W)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, dim]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "e26d4968-cc02-48dd-95c3-6071082037c5", "generation": 3, "timestamp": 1754568714.719205, "iteration_found": 12, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.083, "runtime_ms": 0.726, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0599, "speed_up": 0.083, "custom_timing": 0.726}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_24_Conv3d_Min_Softmax", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "max_difference": ["0.199738", "0.194956", "0.155975", "0.151355", "0.166642"], "avg_difference": ["0.062175", "0.062177", "0.020986", "0.020947", "0.020941"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0599, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
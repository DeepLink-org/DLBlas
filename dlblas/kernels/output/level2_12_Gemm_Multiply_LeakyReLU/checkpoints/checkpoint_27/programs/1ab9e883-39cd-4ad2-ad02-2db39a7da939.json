{"id": "1ab9e883-39cd-4ad2-ad02-2db39a7da939", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_bias_leaky_relu_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    stride_xm, stride_xk,\n    stride_wk, stride_wn,\n    stride_bn,\n    stride_om, stride_on,\n    multiplier, negative_slope,\n    M, N, K: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        x_mask = (offs_m[:, None] < M) & k_mask\n        w_mask = (offs_k[:, None] < k_remaining) & (offs_n[None, :] < N)\n\n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n\n        # Convert to FP16 for tensor core acceleration\n        x_f16 = x.to(tl.float16)\n        w_f16 = w.to(tl.float16)\n        # Use tensor cores for FP16 matrix multiplication\n        acc += tl.dot(x_f16, w_f16, out_dtype=tl.float32)\n\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n\n    b_ptrs = b_ptr + offs_n * stride_bn\n    b = tl.load(b_ptrs, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n\n    acc = acc * multiplier\n    acc = tl.where(acc >= 0, acc, acc * negative_slope)\n\n    out_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.multiplier = multiplier\n        self.negative_slope = negative_slope\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        x = x.contiguous()\n        M, K = x.shape\n        N = self.out_features\n        \n        output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n        weight_t = self.weight.t().contiguous()\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        \n        gemm_bias_leaky_relu_kernel[grid](\n            x, weight_t, self.bias, output,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            self.bias.stride(0),\n            output.stride(0), output.stride(1),\n            self.multiplier, self.negative_slope,\n            M, N, K\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "3f8f1eb1-45d3-4018-9815-763929d82da1", "generation": 2, "timestamp": 1754569642.2462099, "iteration_found": 19, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.625, "runtime_ms": 0.0873, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0546, "speed_up": 0.625, "custom_timing": 0.0873}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.53, "runtime_ms": 0.103, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0546, "speed_up": 0.53, "custom_timing": 0.103}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.5300\n- runtime_ms: 0.1030\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0546, 'speed_up': 0.53, 'custom_timing': 0.103}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.5990, runtime_ms: 0.0912, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0546, 'speed_up': 0.599, 'custom_timing': 0.0912}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.5300, runtime_ms: 0.1030, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0546, 'speed_up': 0.53, 'custom_timing': 0.103}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7180, runtime_ms: 0.0760, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0546, 'speed_up': 0.718, 'custom_timing': 0.076}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_warps=2),\n    ],\n    key=['batch_size', 'in_features', 'out_features'],\n)\n@triton.jit\ndef fused_gemm_multiply_leakyrelu_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    batch_size, in_features, out_features,\n    multiplier, negative_slope,\n    stride_xb, stride_xf,\n    stride_wo, stride_wf,\n    stride_ob, stride_of,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    \n    mask_m = rm < batch_size\n    mask_n = rn < out_features\n    \n    # Compute pointers with proper strides for coalesced access\n    x_ptrs = x_ptr + (rm[:, None] * stride_xb + rk[None, :] * stride_xf)\n    w_ptrs = weight_ptr + (rk[:, None] * stride_wf + rn[None, :] * stride_wo)\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_offs = k * BLOCK_SIZE_K\n        k_remaining = in_features - k_offs\n        k_valid = rk < k_remaining\n        \n        # Load inputs with proper masking\n        x_vals = tl.load(x_ptrs, mask=mask_m[:, None] & k_valid[None, :], other=0.0)\n        w_vals = tl.load(w_ptrs, mask=k_valid[:, None] & mask_n[None, :], other=0.0)\n        \n        # Convert to FP16 for tensor core acceleration\n        x_f16 = x_vals.to(tl.float16)\n        w_f16 = w_vals.to(tl.float16)\n        \n        # Compute matrix multiplication with tensor cores\n        acc += tl.dot(x_f16, w_f16, allow_tf32=True).to(tl.float32)\n        \n        # Update pointers for next block\n        x_ptrs += BLOCK_SIZE_K * stride_xf\n        w_ptrs += BLOCK_SIZE_K * stride_wf\n    \n    # Load bias and apply operations\n    bias_vals = tl.load(bias_ptr + rn, mask=mask_n, other=0.0)\n    acc += bias_vals[None, :]\n    acc = acc * multiplier\n    acc = tl.where(acc >= 0, acc, negative_slope * acc)\n    \n    # Store results with proper masking\n    out_ptrs = output_ptr + (rm[:, None] * stride_ob + rn[None, :] * stride_of)\n    tl.store(out_ptrs, acc, mask=mask_m[:, None] & mask_n[None, :])\n\ndef fused_gemm_multiply_leakyrelu(x, weight, bias, multiplier, negative_slope):\n    batch_size, in_features = x.shape\n    out_features, _ = weight.shape\n    \n    # Ensure contiguous tensors\n    x = x.contiguous()\n    weight = weight.contiguous()\n    bias = bias.contiguous()\n    \n    # Preallocate output\n    output = torch.empty(batch_size, out_features, device=x.device, dtype=torch.float32)\n    \n    # Grid configuration\n    grid = lambda META: (\n        triton.cdiv(batch_size, META['BLOCK_SIZE_M']),\n        triton.cdiv(out_features, META['BLOCK_SIZE_N']),\n    )\n    \n    # Launch kernel\n    fused_gemm_multiply_leakyrelu_kernel[grid](\n        x, weight, bias, output,\n        batch_size, in_features, out_features,\n        multiplier, negative_slope,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        output.stride(0), output.stride(1),\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        self.multiplier = multiplier\n        self.negative_slope = negative_slope\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        return fused_gemm_multiply_leakyrelu(\n            x, self.weight, self.bias, \n            self.multiplier, self.negative_slope\n        )\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7180), Performs well on runtime_ms (0.0760), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0546, 'speed_up': 0.718, 'custom_timing': 0.076})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_bias_leaky_relu_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    stride_xm, stride_xk,\n    stride_wk, stride_wn,\n    stride_bn,\n    stride_om, stride_on,\n    multiplier, negative_slope,\n    M, N, K: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        x_mask = (offs_m[:, None] < M) & k_mask\n        w_mask = (offs_k[:, None] < k_remaining) & (offs_n[None, :] < N)\n\n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n\n        acc += tl.dot(x, w)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n\n    b_ptrs = b_ptr + offs_n * stride_bn\n    b = tl.load(b_ptrs, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n\n    acc = acc * multiplier\n    acc = tl.where(acc >= 0, acc, acc * negative_slope)\n\n    out_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.multiplier = multiplier\n        self.negative_slope = negative_slope\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        x = x.contiguous()\n        M, K = x.shape\n        N = self.out_features\n        \n        output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n        weight_t = self.weight.t().contiguous()\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        \n        gemm_bias_leaky_relu_kernel[grid](\n            x, weight_t, self.bias, output,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            self.bias.stride(0),\n            output.stride(0), output.stride(1),\n            self.multiplier, self.negative_slope,\n            M, N, K\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.5300), Performs well on runtime_ms (0.1030), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0546, 'speed_up': 0.53, 'custom_timing': 0.103})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=3),\n    ],\n    key=['batch_size', 'in_features', 'out_features'],\n)\n@triton.jit\ndef fused_gemm_multiply_leakyrelu_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    batch_size, in_features, out_features,\n    multiplier, negative_slope,\n    stride_xb, stride_xf,\n    stride_wo, stride_wf,\n    stride_ob, stride_of,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    \n    mask_m = rm < batch_size\n    mask_n = rn < out_features\n    \n    x_ptrs = x_ptr + rm[:, None] * stride_xb + rk[None, :] * stride_xf\n    w_ptrs = weight_ptr + rk[:, None] * stride_wo + rn[None, :] * stride_wf\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_offs = k * BLOCK_SIZE_K\n        k_remaining = in_features - k_offs\n        k_valid = rk < k_remaining\n        \n        x_vals = tl.load(x_ptrs, mask=mask_m[:, None] & k_valid, other=0.0)\n        w_vals = tl.load(w_ptrs, mask=k_valid[:, None] & mask_n[None, :], other=0.0)\n        \n        acc += tl.dot(x_vals, w_vals, allow_tf32=True)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_xf\n        w_ptrs += BLOCK_SIZE_K * stride_wo\n    \n    bias_vals = tl.load(bias_ptr + rn, mask=mask_n, other=0.0)\n    acc += bias_vals[None, :]\n    acc = acc * multiplier\n    leaky_val = acc * negative_slope\n    acc = tl.where(acc >= 0, acc, leaky_val)\n    \n    out_ptrs = output_ptr + rm[:, None] * stride_ob + rn[None, :] * stride_of\n    tl.store(out_ptrs, acc, mask=mask_m[:, None] & mask_n[None, :])\n\ndef fused_gemm_multiply_leakyrelu(x, weight, bias, multiplier, negative_slope):\n    batch_size, in_features = x.shape\n    out_features, _ = weight.shape\n    \n    x = x.contiguous()\n    weight_t = weight.t().contiguous()\n    bias = bias.contiguous()\n    output = torch.empty(batch_size, out_features, device=x.device, dtype=torch.float32)\n    \n    grid = lambda meta: (\n        triton.cdiv(batch_size, meta['BLOCK_SIZE_M']),\n        triton.cdiv(out_features, meta['BLOCK_SIZE_N'])\n    )\n    \n    fused_gemm_multiply_leakyrelu_kernel[grid](\n        x, weight_t, bias, output,\n        batch_size, in_features, out_features,\n        multiplier, negative_slope,\n        x.stride(0), x.stride(1),\n        weight_t.stride(0), weight_t.stride(1),\n        output.stride(0), output.stride(1)\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        self.multiplier = multiplier\n        self.negative_slope = negative_slope\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        return fused_gemm_multiply_leakyrelu(\n            x, self.weight, self.bias, \n            self.multiplier, self.negative_slope\n        )\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.5990), Performs well on runtime_ms (0.0912), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0546, 'speed_up': 0.599, 'custom_timing': 0.0912})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _forward_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    stride_xm, stride_xk,\n    stride_wk, stride_wn,\n    stride_bn,\n    stride_om, stride_on,\n    multiplier, negative_slope,\n    M, N, K: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk)\n    w_ptrs = w_ptr + (offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = (offs_k[None, :] < k_remaining)\n        x_mask = (offs_m[:, None] < M) & k_mask\n        w_mask = (offs_k[:, None] < k_remaining) & (offs_n[None, :] < N)\n\n        x_chunk = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w_chunk = tl.load(w_ptrs, mask=w_mask, other=0.0)\n\n        acc += tl.dot(x_chunk, w_chunk)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n\n    b_ptrs = b_ptr + offs_n * stride_bn\n    b = tl.load(b_ptrs, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n\n    acc = acc * multiplier\n    acc = tl.where(acc >= 0, acc, acc * negative_slope)\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    out_ptrs = output_ptr + (offs_m[:, None] * stride_om + offs_n[None, :] * stride_on)\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(in_features, out_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.multiplier = multiplier\n        self.negative_slope = negative_slope\n\n    def forward(self, x):\n        x = x.contiguous()\n        M, K = x.shape\n        N = self.weight.shape[1]\n        output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        _forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            self.bias.stride(0),\n            output.stride(0), output.stride(1),\n            self.multiplier, self.negative_slope,\n            M, N, K\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    multiplier,\n    negative_slope,\n    batch_size,\n    in_features,\n    out_features,\n    stride_xm,\n    stride_xk,\n    stride_wn,\n    stride_wk,\n    stride_bn,\n    stride_om,\n    stride_on,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + rm[:, None] * stride_xm + rk[None, :] * stride_xk\n    w_ptrs = w_ptr + rn[:, None] * stride_wn + rk[None, :] * stride_wk\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_remaining = in_features - k * BLOCK_SIZE_K\n        k_valid = tl.arange(0, BLOCK_SIZE_K) < k_remaining\n        x = tl.load(x_ptrs, mask=k_valid[None, :], other=0.0)\n        w = tl.load(w_ptrs, mask=k_valid[None, :], other=0.0)\n        acc += tl.dot(x, w, allow_tf32=True)\n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n    \n    b_ptrs = b_ptr + rn\n    b = tl.load(b_ptrs, mask=rn < out_features, other=0.0)\n    acc += b[None, :]\n    \n    acc = acc * multiplier\n    condition = acc < 0\n    acc = tl.where(condition, acc * negative_slope, acc)\n    \n    out_ptrs = output_ptr + rm[:, None] * stride_om + rn[None, :] * stride_on\n    mask = (rm[:, None] < batch_size) & (rn[None, :] < out_features)\n    tl.store(out_ptrs, acc, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.multiplier = multiplier\n        self.negative_slope = negative_slope\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        # Initialize parameters same as nn.Linear\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, self.out_features, device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_M = 128\n        BLOCK_SIZE_N = 64\n        BLOCK_SIZE_K = 64\n        \n        grid = (\n            triton.cdiv(batch_size, BLOCK_SIZE_M),\n            triton.cdiv(self.out_features, BLOCK_SIZE_N)\n        )\n        \n        _forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.multiplier, self.negative_slope,\n            batch_size, self.in_features, self.out_features,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            self.bias.stride(0),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_warps=2),\n    ],\n    key=['batch_size', 'in_features', 'out_features'],\n)\n@triton.jit\ndef fused_gemm_multiply_leakyrelu_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    batch_size, in_features, out_features,\n    multiplier, negative_slope,\n    stride_xb, stride_xf,\n    stride_wo, stride_wf,\n    stride_ob, stride_of,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    \n    mask_m = rm < batch_size\n    mask_n = rn < out_features\n    \n    # Compute pointers with proper strides for coalesced access\n    x_ptrs = x_ptr + (rm[:, None] * stride_xb + rk[None, :] * stride_xf)\n    w_ptrs = weight_ptr + (rk[:, None] * stride_wf + rn[None, :] * stride_wo)\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_offs = k * BLOCK_SIZE_K\n        k_remaining = in_features - k_offs\n        k_valid = rk < k_remaining\n        \n        # Load inputs with proper masking\n        x_vals = tl.load(x_ptrs, mask=mask_m[:, None] & k_valid[None, :], other=0.0)\n        w_vals = tl.load(w_ptrs, mask=k_valid[:, None] & mask_n[None, :], other=0.0)\n        \n        # Convert to FP16 for tensor core acceleration\n        x_f16 = x_vals.to(tl.float16)\n        w_f16 = w_vals.to(tl.float16)\n        \n        # Compute matrix multiplication with tensor cores\n        acc += tl.dot(x_f16, w_f16, allow_tf32=True).to(tl.float32)\n        \n        # Update pointers for next block\n        x_ptrs += BLOCK_SIZE_K * stride_xf\n        w_ptrs += BLOCK_SIZE_K * stride_wf\n    \n    # Load bias and apply operations\n    bias_vals = tl.load(bias_ptr + rn, mask=mask_n, other=0.0)\n    acc += bias_vals[None, :]\n    acc = acc * multiplier\n    acc = tl.where(acc >= 0, acc, negative_slope * acc)\n    \n    # Store results with proper masking\n    out_ptrs = output_ptr + (rm[:, None] * stride_ob + rn[None, :] * stride_of)\n    tl.store(out_ptrs, acc, mask=mask_m[:, None] & mask_n[None, :])\n\ndef fused_gemm_multiply_leakyrelu(x, weight, bias, multiplier, negative_slope):\n    batch_size, in_features = x.shape\n    out_features, _ = weight.shape\n    \n    # Ensure contiguous tensors\n    x = x.contiguous()\n    weight = weight.contiguous()\n    bias = bias.contiguous()\n    \n    # Preallocate output\n    output = torch.empty(batch_size, out_features, device=x.device, dtype=torch.float32)\n    \n    # Grid configuration\n    grid = lambda META: (\n        triton.cdiv(batch_size, META['BLOCK_SIZE_M']),\n        triton.cdiv(out_features, META['BLOCK_SIZE_N']),\n    )\n    \n    # Launch kernel\n    fused_gemm_multiply_leakyrelu_kernel[grid](\n        x, weight, bias, output,\n        batch_size, in_features, out_features,\n        multiplier, negative_slope,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        output.stride(0), output.stride(1),\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        self.multiplier = multiplier\n        self.negative_slope = negative_slope\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        return fused_gemm_multiply_leakyrelu(\n            x, self.weight, self.bias, \n            self.multiplier, self.negative_slope\n        )\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=3),\n    ],\n    key=['batch_size', 'in_features', 'out_features'],\n)\n@triton.jit\ndef fused_gemm_multiply_leakyrelu_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    batch_size, in_features, out_features,\n    multiplier, negative_slope,\n    stride_xb, stride_xf,\n    stride_wo, stride_wf,\n    stride_ob, stride_of,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    \n    mask_m = rm < batch_size\n    mask_n = rn < out_features\n    \n    x_ptrs = x_ptr + rm[:, None] * stride_xb + rk[None, :] * stride_xf\n    w_ptrs = weight_ptr + rk[:, None] * stride_wo + rn[None, :] * stride_wf\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_offs = k * BLOCK_SIZE_K\n        k_remaining = in_features - k_offs\n        k_valid = rk < k_remaining\n        \n        x_vals = tl.load(x_ptrs, mask=mask_m[:, None] & k_valid, other=0.0)\n        w_vals = tl.load(w_ptrs, mask=k_valid[:, None] & mask_n[None, :], other=0.0)\n        \n        acc += tl.dot(x_vals, w_vals, allow_tf32=True)\n        \n        x_ptrs += BLOCK_SIZE_K * stride_xf\n        w_ptrs += BLOCK_SIZE_K * stride_wo\n    \n    bias_vals = tl.load(bias_ptr + rn, mask=mask_n, other=0.0)\n    acc += bias_vals[None, :]\n    acc = acc * multiplier\n    leaky_val = acc * negative_slope\n    acc = tl.where(acc >= 0, acc, leaky_val)\n    \n    out_ptrs = output_ptr + rm[:, None] * stride_ob + rn[None, :] * stride_of\n    tl.store(out_ptrs, acc, mask=mask_m[:, None] & mask_n[None, :])\n\ndef fused_gemm_multiply_leakyrelu(x, weight, bias, multiplier, negative_slope):\n    batch_size, in_features = x.shape\n    out_features, _ = weight.shape\n    \n    x = x.contiguous()\n    weight_t = weight.t().contiguous()\n    bias = bias.contiguous()\n    output = torch.empty(batch_size, out_features, device=x.device, dtype=torch.float32)\n    \n    grid = lambda meta: (\n        triton.cdiv(batch_size, meta['BLOCK_SIZE_M']),\n        triton.cdiv(out_features, meta['BLOCK_SIZE_N'])\n    )\n    \n    fused_gemm_multiply_leakyrelu_kernel[grid](\n        x, weight_t, bias, output,\n        batch_size, in_features, out_features,\n        multiplier, negative_slope,\n        x.stride(0), x.stride(1),\n        weight_t.stride(0), weight_t.stride(1),\n        output.stride(0), output.stride(1)\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        self.multiplier = multiplier\n        self.negative_slope = negative_slope\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        return fused_gemm_multiply_leakyrelu(\n            x, self.weight, self.bias, \n            self.multiplier, self.negative_slope\n        )\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    multiplier,\n    negative_slope,\n    batch_size,\n    in_features,\n    out_features,\n    stride_xm,\n    stride_xk,\n    stride_wn,\n    stride_wk,\n    stride_bn,\n    stride_om,\n    stride_on,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    \n    x_ptrs = x_ptr + rm[:, None] * stride_xm + rk[None, :] * stride_xk\n    w_ptrs = w_ptr + rn[:, None] * stride_wn + rk[None, :] * stride_wk\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_remaining = in_features - k * BLOCK_SIZE_K\n        k_valid = tl.arange(0, BLOCK_SIZE_K) < k_remaining\n        x = tl.load(x_ptrs, mask=k_valid[None, :], other=0.0)\n        w = tl.load(w_ptrs, mask=k_valid[None, :], other=0.0)\n        acc += tl.dot(x, w, allow_tf32=True)\n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n    \n    b_ptrs = b_ptr + rn\n    b = tl.load(b_ptrs, mask=rn < out_features, other=0.0)\n    acc += b[None, :]\n    \n    acc = acc * multiplier\n    condition = acc < 0\n    acc = tl.where(condition, acc * negative_slope, acc)\n    \n    out_ptrs = output_ptr + rm[:, None] * stride_om + rn[None, :] * stride_on\n    mask = (rm[:, None] < batch_size) & (rn[None, :] < out_features)\n    tl.store(out_ptrs, acc, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.multiplier = multiplier\n        self.negative_slope = negative_slope\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        # Initialize parameters same as nn.Linear\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, self.out_features, device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_M = 128\n        BLOCK_SIZE_N = 64\n        BLOCK_SIZE_K = 64\n        \n        grid = (\n            triton.cdiv(batch_size, BLOCK_SIZE_M),\n            triton.cdiv(self.out_features, BLOCK_SIZE_N)\n        )\n        \n        _forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.multiplier, self.negative_slope,\n            batch_size, self.in_features, self.out_features,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            self.bias.stride(0),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: 1.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _matmul_bias_kernel(\n    a_ptr, b_ptr, bias_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        \n        a = tl.load(a_ptrs, mask=k_mask[None, :] & (offs_m[:, None] < M), other=0.0)\n        b = tl.load(b_ptrs, mask=k_mask[:, None] & (offs_n[None, :] < N), other=0.0)\n        \n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += bias[None, :]\n    \n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=c_mask)\n\ndef matmul_bias(a, b, bias):\n    M, K = a.shape\n    _, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']), triton.cdiv(N, META['BLOCK_N']))\n    _matmul_bias_kernel[grid](\n        a, b, bias, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.multiplier = multiplier\n        self.leaky_relu = nn.LeakyReLU(negative_slope)\n\n    def forward(self, x):\n        weight_t = self.weight.t().contiguous()\n        x = matmul_bias(x, weight_t, self.bias)\n        x = x * self.multiplier\n        x = self.leaky_relu(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _forward_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    stride_xm, stride_xk,\n    stride_wk, stride_wn,\n    stride_bn,\n    stride_om, stride_on,\n    multiplier, negative_slope,\n    M, N, K: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk)\n    w_ptrs = w_ptr + (offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = (offs_k[None, :] < k_remaining)\n        x_mask = (offs_m[:, None] < M) & k_mask\n        w_mask = (offs_k[:, None] < k_remaining) & (offs_n[None, :] < N)\n\n        x_chunk = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w_chunk = tl.load(w_ptrs, mask=w_mask, other=0.0)\n\n        acc += tl.dot(x_chunk, w_chunk)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n\n    b_ptrs = b_ptr + offs_n * stride_bn\n    b = tl.load(b_ptrs, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n\n    acc = acc * multiplier\n    acc = tl.where(acc >= 0, acc, acc * negative_slope)\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    out_ptrs = output_ptr + (offs_m[:, None] * stride_om + offs_n[None, :] * stride_on)\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(in_features, out_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.multiplier = multiplier\n        self.negative_slope = negative_slope\n\n    def forward(self, x):\n        x = x.contiguous()\n        M, K = x.shape\n        N = self.weight.shape[1]\n        output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        _forward_kernel[grid](\n            x, self.weight, self.bias, output,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            self.bias.stride(0),\n            output.stride(0), output.stride(1),\n            self.multiplier, self.negative_slope,\n            M, N, K\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_bias_leaky_relu_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    stride_xm, stride_xk,\n    stride_wk, stride_wn,\n    stride_bn,\n    stride_om, stride_on,\n    multiplier, negative_slope,\n    M, N, K: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        x_mask = (offs_m[:, None] < M) & k_mask\n        w_mask = (offs_k[:, None] < k_remaining) & (offs_n[None, :] < N)\n\n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n\n        acc += tl.dot(x, w)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n\n    b_ptrs = b_ptr + offs_n * stride_bn\n    b = tl.load(b_ptrs, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n\n    acc = acc * multiplier\n    acc = tl.where(acc >= 0, acc, acc * negative_slope)\n\n    out_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.multiplier = multiplier\n        self.negative_slope = negative_slope\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        x = x.contiguous()\n        M, K = x.shape\n        N = self.out_features\n        \n        output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n        weight_t = self.weight.t().contiguous()\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        \n        gemm_bias_leaky_relu_kernel[grid](\n            x, weight_t, self.bias, output,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            self.bias.stride(0),\n            output.stride(0), output.stride(1),\n            self.multiplier, self.negative_slope,\n            M, N, K\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program, which is a Triton kernel for a fused GEMM, bias, multiply, and leaky ReLU operation.\n The current performance shows a speedup of 0.53 (i.e., slower than the reference) and we want to improve it.\n\n Observations:\n - The current kernel uses a 1D grid and splits the program ID into pid_m and pid_n for the M and N dimensions.\n - The kernel loads blocks of the input matrix `x` (size M x K) and the weight matrix `w` (size K x N) and computes the dot product.\n - It then adds bias, multiplies by a scalar, and applies leaky ReLU.\n\n The goal is to achieve at least 5% speedup (speed_up >= 1.05) and ideally 15% or more.\n\n Known constraints:\n - We cannot change the function signatures (parameters) of the Triton kernel or the grid configuration.\n - We must maintain correctness (numerical within tolerance) and avoid race conditions.\n\n Potential optimizations:\n 1. Use tensor cores: The H100 supports tensor cores for FP16. Currently, the kernel uses FP32. We can convert the loaded x and w to FP16 for the dot product and accumulate in FP32.\n    This requires:\n        - Loading the data as FP32 and then converting to FP16 in the kernel? Or we can load directly as FP16? Note: the input tensors are in FP32? We must check the problem.\n    However, the problem states that the reference is in PyTorch and we are to write Triton. The example inputs are FP32? But the kernel uses FP32.\n\n    But note: the example in the problem description (per_row_quantize_int8) does not use tensor cores. However, the hardware (H100) supports tensor cores for FP16.\n\n    We can try to use FP16 for the matrix multiplication if the data is stored in FP16? But the problem does not specify the dtype of the input. However, the current kernel and the ModelNew class use FP32.\n\n    Let's assume we are allowed to change the dtype for intermediate computation? But the output must be FP32 to match the reference.\n\n    Steps:\n        - Load the FP32 values, then convert to FP16 for the dot product, then accumulate in FP32.\n        - Use `tl.dot(a, b)` with `a` and `b` in FP16 and `allow_tf32=False` to force FP16 tensor cores? Actually, we can set `out_dtype=tl.float32` and use FP16 inputs.\n\n 2. Tuning the block sizes: The autotuner already has several configurations, but we might try to adjust the block sizes to better fit the tensor cores.\n    The tensor cores on H100 work best with block sizes that are multiples of 16x16x16 (or 32x16x16 for FP16). We can try to set BLOCK_M and BLOCK_N to multiples of 32 and 16? But note the autotuner.\n\n 3. Improve memory access patterns: We are already using coalesced access? The current kernel uses:\n        x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n        w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n    This is standard for GEMM.\n\n 4. Use vectorized loads: We are not using vectorized loads. We could load multiple elements at once.\n\n 5. Prefetching: We could prefetch the next block of data while computing the current.\n\n However, note the constraints: we cannot change the grid configuration or the program ID logic.\n\n Let's focus on using tensor cores because that can give a significant speedup.\n\n Plan:\n   - Change the dtype of the loaded x and w to FP16 for the dot product.\n   - We'll load as FP32 and then convert to FP16. Why? Because the input is FP32. We cannot change the input type.\n\n Steps in the kernel:\n   - Load x and w as FP32 (as currently done).\n   - Convert to FP16: `x_f16 = x.to(tl.float16)` and similarly for w.\n   - Then compute the dot product: `acc += tl.dot(x_f16, w_f16, out_dtype=tl.float32)`\n\n But note: the Triton dot operation for FP16 requires the accumulator to be FP32? We are already using FP32 for `acc`.\n\n Alternatively, we can use `tl.dot(x, w, allow_tf32=False, ...)` but that won't use FP16. We need to explicitly use FP16.\n\n However, Triton's `dot` operation for FP16 inputs will automatically use tensor cores on H100? Yes, if the hardware supports it.\n\n Also, note that the current kernel uses `tl.dot(x, w)` without specifying the out_dtype? It defaults to the dtype of the inputs? But we are converting to FP16 and then we want the accumulator to be FP32.\n\n We can do:\n   acc += tl.dot(x_f16, w_f16).to(tl.float32)\n\n But the dot product of two FP16 matrices returns FP16? We want to accumulate in FP32.\n\n Alternatively, we can use:\n   acc += tl.dot(x_f16, w_f16, out_dtype=tl.float32)\n\n This is the recommended way.\n\n Also, note: we must check that the conversion to FP16 does not cause numerical issues? The problem does not specify, but it's common in deep learning.\n\n Let's change the inner loop to use FP16 for the matrix multiplication.\n\n We'll change:\n   x = tl.load(...)  # FP32\n   w = tl.load(...)   # FP32\n   x_f16 = x.to(tl.float16)\n   w_f16 = w.to(tl.float16)\n   acc += tl.dot(x_f16, w_f16, out_dtype=tl.float32)\n\n 6. We might also consider using more warps? The autotuner configs use 4 warps. But we are already autotuning.\n\n 7. We should also check the masking: the current kernel does:\n        x_mask = (offs_m[:, None] < M) & k_mask\n        w_mask = (offs_k[:, None] < k_remaining) & (offs_n[None, :] < N)\n    and then loads with these masks. This is correct.\n\n 8. The rest of the operations (bias, multiply, leaky ReLU) are done in FP32, which is acceptable.\n\n Let's try to change the inner loop to use FP16 for the dot product.\n\n However, note: the current kernel does not use tensor cores because it's doing FP32. We are introducing FP16 for the matmul.\n\n We must be cautious: the problem says the hardware has \"FP16 Tensor Core 32\u00d716\u00d716\", so we want to use FP16.\n\n Implementation:\n\n   Inside the inner loop (after loading x and w):\n\n        x_f16 = x.to(tl.float16)\n        w_f16 = w.to(tl.float16)\n        # Use tensor cores for the dot product and accumulate in FP32\n        acc += tl.dot(x_f16, w_f16, out_dtype=tl.float32)\n\n 9. We must also consider the alignment: the tensor cores require the matrices to be aligned? The Triton compiler should handle that if we use the proper block sizes.\n\n    The autotuner already has block sizes that are multiples of 16 or 32? We have BLOCK_M, BLOCK_N, BLOCK_K as 64, 128, etc. which are multiples of 16 and 32? Yes.\n\n 10. We are not changing the block sizes, so we rely on the autotuner to pick the best.\n\n 11. We also note that the current kernel does not use shared memory? It is a straightforward block GEMM. The Triton compiler may automatically use shared memory? We don't need to change that.\n\n 12. We are also going to change the autotuner configs? We are not allowed to change the grid, but we can change the autotuner configs? The problem says we can change the block dimensions via autotuning.\n\n    However, the autotuner configs are provided as a list. We might want to add configs that are more friendly to tensor cores? But the existing configs are already in multiples of 64 and 128? They are also multiples of 16 and 32.\n\n    Let's keep the autotuner configs as they are.\n\n 13. We change only the inner loop computation.\n\n 14. We also note that the current kernel does not use any tensor core specific instructions. We are introducing them by using FP16.\n\n 15. We must test: the kernel must still compile and be correct.\n\n Given that the current program is correct and we are only changing the inner dot product to use FP16 (with conversion from FP32), the numerical results might be slightly different but within tolerance.\n\n Let's write the change.\n\n However, note: the current program uses `tl.dot(x, w)` which in FP32 might use TF32 on Ampere? But we are not sure. We are disabling that by using FP16.\n\n We are going to change about 3 lines.\n\n But note: we must also consider the boundaries? The conversion to FP16 is done on the loaded block. The loaded block is already masked? Then we are safe.\n\n Let's update the kernel.\n\n We are going to replace:\n\n        acc += tl.dot(x, w)\n\n with:\n\n        x_f16 = x.to(tl.float16)\n        w_f16 = w.to(tl.float16)\n        acc += tl.dot(x_f16, w_f16, out_dtype=tl.float32)\n\n But note: the `tl.dot` for FP16 inputs returns FP32 when `out_dtype=tl.float32`. This is what we want.\n\n Alternatively, we can do:\n\n        acc += tl.dot(x.to(tl.float16), w.to(tl.float16)).to(tl.float32)\n\n but that might do an extra conversion? The `tl.dot` with FP16 inputs and FP32 output is more efficient?\n\n We'll use the explicit out_dtype.\n\n 16. Also note: we are doing two extra conversions (x to FP16 and w to FP16) which might add overhead? But the tensor core speedup should outweigh that.\n\n Let's try.\n\n We are going to change only the inner loop body.\n\n However, we must also note: the current kernel uses `allow_tf32` in the dot? We are replacing it. We remove the `allow_tf32` because we are forcing FP16.\n\n We change:\n\n        acc += tl.dot(x, w)\n\n to:\n\n        # Convert to FP16 for tensor core acceleration\n        x_f16 = x.to(tl.float16)\n        w_f16 = w.to(tl.float16)\n        # Use tensor cores and accumulate in FP32\n        acc += tl.dot(x_f16, w_f16, out_dtype=tl.float32)\n\n 17. We leave the rest of the kernel unchanged.\n\n 18. We must also note that the kernel might now require more registers? But we are using the same number of warps.\n\n 19. We are not changing the function signature or the grid.\n\n 20. We are making a small change (3 lines) so it's reversible.\n\n Let's write the new kernel.\n\n We are going to output the entire block with this change.\n\n Note: the current program is the one labeled \"Current Program\". We are only modifying the Triton kernel.\n\n We must be cautious: the current kernel code in the current program is:\n\n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n\n        acc += tl.dot(x, w)\n\n        ... (pointer updates)\n\n We replace the `acc += tl.dot(x, w)` with the FP16 conversion and FP32 accumulation.\n\n We output the entire code block with the change.\n\n IMPORTANT: The class name must remain `ModelNew`.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_bias_leaky_relu_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    stride_xm, stride_xk,\n    stride_wk, stride_wn,\n    stride_bn,\n    stride_om, stride_on,\n    multiplier, negative_slope,\n    M, N, K: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        x_mask = (offs_m[:, None] < M) & k_mask\n        w_mask = (offs_k[:, None] < k_remaining) & (offs_n[None, :] < N)\n\n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n\n        # Convert to FP16 for tensor core acceleration\n        x_f16 = x.to(tl.float16)\n        w_f16 = w.to(tl.float16)\n        # Use tensor cores for FP16 matrix multiplication\n        acc += tl.dot(x_f16, w_f16, out_dtype=tl.float32)\n\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n\n    b_ptrs = b_ptr + offs_n * stride_bn\n    b = tl.load(b_ptrs, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n\n    acc = acc * multiplier\n    acc = tl.where(acc >= 0, acc, acc * negative_slope)\n\n    out_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, multiplier, negative_slope):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.multiplier = multiplier\n        self.negative_slope = negative_slope\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        x = x.contiguous()\n        M, K = x.shape\n        N = self.out_features\n        \n        output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n        weight_t = self.weight.t().contiguous()\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        \n        gemm_bias_leaky_relu_kernel[grid](\n            x, weight_t, self.bias, output,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            self.bias.stride(0),\n            output.stride(0), output.stride(1),\n            self.multiplier, self.negative_slope,\n            M, N, K\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nmultiplier = 2.0\nnegative_slope = 0.1\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, multiplier, negative_slope]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
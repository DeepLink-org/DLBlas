{"id": "3784de0b-8c8d-4afb-ab9a-03956ad6d6bf", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_depth_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    depth: tl.constexpr,\n    height: tl.constexpr,\n    width: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Precompute spatial constants\n    spatial_size = depth * height * width\n    height_width = height * width\n    \n    # Initialize accumulator\n    accum = 0.0\n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    \n    # Process spatial dimension in flattened blocks\n    for offset in range(0, spatial_size, BLOCK_SIZE):\n        idx = offset + tl.arange(0, BLOCK_SIZE)\n        mask = idx < spatial_size\n        \n        # Compute 3D indices from flattened index\n        d = idx // height_width\n        hw = idx % height_width\n        h = hw // width\n        w = hw % width\n        \n        # Compute memory offsets\n        offsets = base_offset + d * input_depth_stride + h * input_height_stride + w * input_width_stride\n        data = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n        accum += tl.sum(data)\n    \n    # Compute average and store result\n    avg = accum / spatial_size\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef global_avg_pool_3d_triton(x):\n    batch_size, channels = x.shape[0], x.shape[1]\n    depth, height, width = x.shape[2], x.shape[3], x.shape[4]\n    \n    # Create output tensor\n    output = torch.empty((batch_size, channels, 1, 1, 1), device=x.device, dtype=torch.float32)\n    \n    # Compute strides\n    input_batch_stride = x.stride(0)\n    input_channel_stride = x.stride(1)\n    input_depth_stride = x.stride(2)\n    input_height_stride = x.stride(3)\n    input_width_stride = x.stride(4)\n    output_batch_stride = output.stride(0)\n    output_channel_stride = output.stride(1)\n    \n    # Kernel configuration - use larger block size for flattened spatial dimension\n    grid = (batch_size, channels)\n    BLOCK_SIZE = 1024\n    \n    # Launch kernel\n    global_avg_pool_kernel[grid](\n        x,\n        output,\n        input_batch_stride,\n        input_channel_stride,\n        input_depth_stride,\n        input_height_stride,\n        input_width_stride,\n        output_batch_stride,\n        output_channel_stride,\n        depth,\n        height,\n        width,\n        BLOCK_SIZE\n    )\n    \n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n        self.batch_norm = nn.BatchNorm3d(out_channels, eps=eps, momentum=momentum)\n        \n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.scale_factor\n        x = self.batch_norm(x)\n        x = global_avg_pool_3d_triton(x)\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "e830870e-a979-4eb4-9e97-2b7dcf48de12", "generation": 4, "timestamp": 1754583545.717657, "iteration_found": 27, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.07, "runtime_ms": 1.65, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.115, "speed_up": 0.07, "custom_timing": 1.65}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.07, "runtime_ms": 1.65, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.115, "speed_up": 0.07, "custom_timing": 1.65}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.0700\n- runtime_ms: 1.6500\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.115, 'speed_up': 0.07, 'custom_timing': 1.65}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: score, speed_up, runtime_ms. Consider continuing with similar changes.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_77_ConvTranspose3d_Scale_BatchNorm_GlobalAvgPool', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmp48ix__2j.py\", line 117, in forward\\n    x = global_avg_pool_3d_triton(x)\\n  File \"/tmp/tmp48ix__2j.py\", line 86, in global_avg_pool_3d_triton\\n    global_avg_pool_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 581, in run\\n    raise KeyError(\"Keyword argument %s was specified but unrecognised\" % k)\\nKeyError: \\'Keyword argument output_batch_stride was specified but unrecognised\\'\\n', 'runtime_error_name': 'builtins.KeyError', 'reference_timing_ms': 0.115, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_77_ConvTranspose3d_Scale_BatchNorm_GlobalAvgPool', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1630, in program_id\\n    return _semantic.program_id(axis)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 40, in program_id\\n    raise ValueError(f\"program_id axis must be 0, 1, or 2 but got {axis}\")\\nValueError: program_id axis must be 0, 1, or 2 but got 3\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpr5pc4anq.py\", line 216, in forward\\n    conv_transpose_scale_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 17:12:\\n    scale_factor,\\n    stride: tl.constexpr, padding: tl.constexpr,\\n    input_bs_stride, input_c_stride, input_d_stride, input_h_stride, input_w_stride,\\n    weight_oc_stride, weight_ic_stride, weight_d_stride, weight_h_stride, weight_w_stride,\\n    output_bs_stride, output_c_stride, output_d_stride, output_h_stride, output_w_stride,\\n    BLOCK_D: tl.constexpr, BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr,\\n    BLOCK_C: tl.constexpr\\n):\\n    pid_batch = tl.program_id(0)\\n    pid_d = tl.program_id(1)\\n    pid_h = tl.program_id(2)\\n    pid_w = tl.program_id(3)\\n            ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1630, in program_id\\n    return _semantic.program_id(axis)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 40, in program_id\\n    raise ValueError(f\"program_id axis must be 0, 1, or 2 but got {axis}\")\\nValueError: program_id axis must be 0, 1, or 2 but got 3\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.115, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.0700, runtime_ms: 1.6500, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.115, 'speed_up': 0.07, 'custom_timing': 1.65}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_depth_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    depth: tl.constexpr,\n    height: tl.constexpr,\n    width: tl.constexpr,\n    BLOCK_D: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n    BLOCK_W: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Initialize accumulator as scalar\n    accum = 0.0\n    total_elements = depth * height * width\n    \n    # Vectorized accumulation\n    for d in range(0, depth, BLOCK_D):\n        d_idx = d + tl.arange(0, BLOCK_D)\n        d_mask = d_idx < depth\n        \n        for h in range(0, height, BLOCK_H):\n            h_idx = h + tl.arange(0, BLOCK_H)\n            h_mask = h_idx < height\n            \n            for w in range(0, width, BLOCK_W):\n                w_idx = w + tl.arange(0, BLOCK_W)\n                w_mask = w_idx < width\n                \n                # Combined mask for valid elements\n                mask = d_mask[:, None, None] & h_mask[None, :, None] & w_mask[None, None, :]\n                \n                # Vectorized memory access\n                offsets = (\n                    pid_batch * input_batch_stride +\n                    pid_channel * input_channel_stride +\n                    d_idx[:, None, None] * input_depth_stride +\n                    h_idx[None, :, None] * input_height_stride +\n                    w_idx[None, None, :] * input_width_stride\n                )\n                \n                # Load data with mask\n                data = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n                accum += tl.sum(data)\n    \n    # Compute average and store result\n    avg = accum / total_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef global_avg_pool_3d_triton(x):\n    batch_size, channels = x.shape[0], x.shape[1]\n    depth, height, width = x.shape[2], x.shape[3], x.shape[4]\n    \n    # Create output tensor\n    output = torch.empty((batch_size, channels, 1, 1, 1), device=x.device, dtype=torch.float32)\n    \n    # Compute strides\n    input_batch_stride = x.stride(0)\n    input_channel_stride = x.stride(1)\n    input_depth_stride = x.stride(2)\n    input_height_stride = x.stride(3)\n    input_width_stride = x.stride(4)\n    output_batch_stride = output.stride(0)\n    output_channel_stride = output.stride(1)\n    \n    # Kernel configuration\n    BLOCK_D, BLOCK_H, BLOCK_W = 8, 16, 16\n    grid = (batch_size, channels)\n    \n    # Launch kernel with fixed arguments\n    global_avg_pool_kernel[grid](\n        x,\n        output,\n        input_batch_stride,\n        input_channel_stride,\n        input_depth_stride,\n        input_height_stride,\n        input_width_stride,\n        output_batch_stride,\n        output_channel_stride,\n        depth,\n        height,\n        width,\n        BLOCK_D,\n        BLOCK_H,\n        BLOCK_W\n    )\n    \n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n        self.batch_norm = nn.BatchNorm3d(out_channels, eps=eps, momentum=momentum)\n        \n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.scale_factor\n        x = self.batch_norm(x)\n        x = global_avg_pool_3d_triton(x)\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.0700), Performs well on runtime_ms (1.6500), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.115, 'speed_up': 0.07, 'custom_timing': 1.65})\n\n\n### Program 2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch.nn import functional as F\n\n@triton.jit\ndef conv_transpose_scale_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    in_channels, out_channels, kernel_size,\n    depth_in, height_in, width_in,\n    depth_out, height_out, width_out,\n    scale_factor,\n    stride: tl.constexpr, padding: tl.constexpr,\n    input_bs_stride, input_c_stride, input_d_stride, input_h_stride, input_w_stride,\n    weight_oc_stride, weight_ic_stride, weight_d_stride, weight_h_stride, weight_w_stride,\n    output_bs_stride, output_c_stride, output_d_stride, output_h_stride, output_w_stride,\n    BLOCK_D: tl.constexpr, BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr,\n    BLOCK_C: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_d = tl.program_id(1)\n    pid_h = tl.program_id(2)\n    pid_w = tl.program_id(3)\n    pid_c = tl.program_id(4)\n    \n    # Create block pointers\n    offsets_d = pid_d * BLOCK_D + tl.arange(0, BLOCK_D)\n    offsets_h = pid_h * BLOCK_H + tl.arange(0, BLOCK_H)\n    offsets_w = pid_w * BLOCK_W + tl.arange(0, BLOCK_W)\n    offsets_c = pid_c * BLOCK_C + tl.arange(0, BLOCK_C)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_D, BLOCK_H, BLOCK_W, BLOCK_C), dtype=tl.float32)\n    \n    # Loop over input channels and kernel dimensions\n    for kd in range(kernel_size):\n        for kh in range(kernel_size):\n            for kw in range(kernel_size):\n                for ic in range(0, in_channels):\n                    # Compute input indices with vectorization\n                    d_in = offsets_d - kd\n                    h_in = offsets_h - kh\n                    w_in = offsets_w - kw\n                    \n                    # Boundary check mask\n                    mask_d = (d_in >= 0) & (d_in < depth_in)\n                    mask_h = (h_in >= 0) & (h_in < height_in)\n                    mask_w = (w_in >= 0) & (w_in < width_in)\n                    mask = mask_d[:, None, None] & mask_h[None, :, None] & mask_w[None, None, :]\n                    \n                    # Load input with vectorization\n                    input_ptr_offset = (\n                        pid_batch * input_bs_stride +\n                        ic * input_c_stride +\n                        d_in * input_d_stride +\n                        h_in * input_h_stride +\n                        w_in * input_w_stride\n                    )\n                    input_val = tl.load(input_ptr + input_ptr_offset, mask=mask, other=0.0)\n                    \n                    # Load weight\n                    weight_ptr_offset = (\n                        pid_c * BLOCK_C + tl.arange(0, BLOCK_C)[:, None, None, None] * weight_oc_stride +\n                        ic * weight_ic_stride +\n                        kd * weight_d_stride +\n                        kh * weight_h_stride +\n                        kw * weight_w_stride\n                    )\n                    weight_val = tl.load(weight_ptr + weight_ptr_offset)\n                    \n                    # Accumulate\n                    acc += input_val * weight_val\n    \n    # Add bias if exists\n    if bias_ptr is not None:\n        bias_val = tl.load(bias_ptr + pid_c * BLOCK_C + tl.arange(0, BLOCK_C)[:, None, None, None])\n        acc += bias_val\n    \n    # Apply scaling\n    acc = acc * scale_factor\n    \n    # Output indices and mask\n    d_out = offsets_d\n    h_out = offsets_h\n    w_out = offsets_w\n    c_out = offsets_c\n    \n    mask_d = d_out < depth_out\n    mask_h = h_out < height_out\n    mask_w = w_out < width_out\n    mask_c = c_out < out_channels\n    mask = mask_d[:, None, None, None] & mask_h[None, :, None, None] & mask_w[None, None, :, None] & mask_c[None, None, None, :]\n    \n    # Store output\n    output_ptr_offset = (\n        pid_batch * output_bs_stride +\n        c_out[None, None, None, :] * output_c_stride +\n        d_out[:, None, None, None] * output_d_stride +\n        h_out[None, :, None, None] * output_h_stride +\n        w_out[None, None, :, None] * output_w_stride\n    )\n    tl.store(output_ptr + output_ptr_offset, acc, mask=mask)\n\n@triton.jit\ndef batch_norm_pool_kernel(\n    input_ptr, output_ptr, \n    weight_ptr, bias_ptr, mean_ptr, var_ptr,\n    depth, height, width, \n    eps, \n    input_bs_stride, input_c_stride, input_d_stride, input_h_stride, input_w_stride,\n    BLOCK_D: tl.constexpr, BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    \n    # Create block pointers\n    offsets_d = tl.arange(0, BLOCK_D)\n    offsets_h = tl.arange(0, BLOCK_H)\n    offsets_w = tl.arange(0, BLOCK_W)\n    \n    # Initialize accumulator for pooling\n    pool_acc = tl.zeros((BLOCK_D, BLOCK_H, BLOCK_W), dtype=tl.float32)\n    count = 0.0\n    \n    # Load batch norm parameters\n    weight = tl.load(weight_ptr + pid_c)\n    bias = tl.load(bias_ptr + pid_c)\n    mean = tl.load(mean_ptr + pid_c)\n    var = tl.load(var_ptr + pid_c)\n    \n    # Compute normalization factor\n    inv_std = 1.0 / tl.sqrt(var + eps)\n    \n    # Process blocks\n    for d in range(0, depth, BLOCK_D):\n        for h in range(0, height, BLOCK_H):\n            for w in range(0, width, BLOCK_W):\n                # Create masks\n                mask_d = (d + offsets_d) < depth\n                mask_h = (h + offsets_h) < height\n                mask_w = (w + offsets_w) < width\n                mask = mask_d[:, None, None] & mask_h[None, :, None] & mask_w[None, None, :]\n                \n                # Load input\n                input_ptr_offset = (\n                    pid_batch * input_bs_stride +\n                    pid_c * input_c_stride +\n                    (d + offsets_d[:, None, None]) * input_d_stride +\n                    (h + offsets_h[None, :, None]) * input_h_stride +\n                    (w + offsets_w[None, None, :]) * input_w_stride\n                )\n                x = tl.load(input_ptr + input_ptr_offset, mask=mask, other=0.0)\n                \n                # Apply batch norm\n                x_norm = (x - mean) * inv_std * weight + bias\n                \n                # Accumulate for pooling\n                pool_acc += tl.sum(x_norm, axis=0)\n                count += tl.sum(mask, axis=0)\n    \n    # Final pooling output\n    pool_val = pool_acc / count\n    tl.store(output_ptr + pid_batch * output_ptr.stride(0) + pid_c, pool_val)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.scale_factor = scale_factor\n        self.eps = eps\n        self.momentum = momentum\n        \n        # Initialize weights and biases\n        self.weight = torch.nn.Parameter(torch.empty(\n            out_channels, in_channels, kernel_size, kernel_size, kernel_size\n        ))\n        self.bias = torch.nn.Parameter(torch.empty(out_channels))\n        self.bn_weight = torch.nn.Parameter(torch.ones(out_channels))\n        self.bn_bias = torch.nn.Parameter(torch.zeros(out_channels))\n        self.register_buffer('running_mean', torch.zeros(out_channels))\n        self.register_buffer('running_var', torch.ones(out_channels))\n        \n        # Initialize parameters\n        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in)\n        torch.nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        # Compute output dimensions\n        stride = 1\n        padding = 0\n        depth_out = (x.size(2)-1)*stride - 2*padding + (self.kernel_size-1) + 1\n        height_out = (x.size(3)-1)*stride - 2*padding + (self.kernel_size-1) + 1\n        width_out = (x.size(4)-1)*stride - 2*padding + (self.kernel_size-1) + 1\n        \n        # Allocate output tensor\n        output = torch.empty(\n            (x.size(0), self.out_channels, depth_out, height_out, width_out),\n            device=x.device, dtype=x.dtype\n        )\n        \n        # Launch ConvTranspose + Scale kernel\n        grid = (\n            x.size(0),  # batch\n            triton.cdiv(depth_out, 4), \n            triton.cdiv(height_out, 4), \n            triton.cdiv(width_out, 4),\n            triton.cdiv(self.out_channels, 16)\n        )\n        \n        conv_transpose_scale_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_channels, self.out_channels, self.kernel_size,\n            x.size(2), x.size(3), x.size(4),\n            depth_out, height_out, width_out,\n            self.scale_factor,\n            stride, padding,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),\n            self.weight.stride(0), self.weight.stride(1), self.weight.stride(2), \n            self.weight.stride(3), self.weight.stride(4),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4),\n            BLOCK_D=4, BLOCK_H=4, BLOCK_W=4, BLOCK_C=16\n        )\n        \n        # BatchNorm + Pooling\n        pooled_output = torch.empty(\n            (output.size(0), self.out_channels),\n            device=output.device, dtype=output.dtype\n        )\n        \n        grid_bn = (output.size(0), self.out_channels)\n        batch_norm_pool_kernel[grid_bn](\n            output, pooled_output,\n            self.bn_weight, self.bn_bias, self.running_mean, self.running_var,\n            depth_out, height_out, width_out,\n            self.eps,\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4),\n            BLOCK_D=8, BLOCK_H=8, BLOCK_W=8\n        )\n        \n        return pooled_output.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n\nbatch_size = 16\nin_channels = 64\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_77_ConvTranspose3d_Scale_BatchNorm_GlobalAvgPool', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1630, in program_id\\n    return _semantic.program_id(axis)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 40, in program_id\\n    raise ValueError(f\"program_id axis must be 0, 1, or 2 but got {axis}\")\\nValueError: program_id axis must be 0, 1, or 2 but got 3\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpr5pc4anq.py\", line 216, in forward\\n    conv_transpose_scale_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 17:12:\\n    scale_factor,\\n    stride: tl.constexpr, padding: tl.constexpr,\\n    input_bs_stride, input_c_stride, input_d_stride, input_h_stride, input_w_stride,\\n    weight_oc_stride, weight_ic_stride, weight_d_stride, weight_h_stride, weight_w_stride,\\n    output_bs_stride, output_c_stride, output_d_stride, output_h_stride, output_w_stride,\\n    BLOCK_D: tl.constexpr, BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr,\\n    BLOCK_C: tl.constexpr\\n):\\n    pid_batch = tl.program_id(0)\\n    pid_d = tl.program_id(1)\\n    pid_h = tl.program_id(2)\\n    pid_w = tl.program_id(3)\\n            ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1630, in program_id\\n    return _semantic.program_id(axis)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 40, in program_id\\n    raise ValueError(f\"program_id axis must be 0, 1, or 2 but got {axis}\")\\nValueError: program_id axis must be 0, 1, or 2 but got 3\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.115, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n### Program 3 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_depth_stride,\n    input_height_stride,\n    input_width_stride,\n    depth: tl.constexpr,\n    height: tl.constexpr,\n    width: tl.constexpr,\n    BLOCK_D: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n    BLOCK_W: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Initialize accumulator\n    accum = tl.zeros((1,), dtype=tl.float32)\n    count = depth * height * width\n    \n    # Loop over depth dimension\n    for d in range(0, depth, BLOCK_D):\n        d_idx = d + tl.arange(0, BLOCK_D)\n        d_mask = d_idx < depth\n        \n        # Loop over height dimension\n        for h in range(0, height, BLOCK_H):\n            h_idx = h + tl.arange(0, BLOCK_H)\n            h_mask = h_idx < height\n            \n            # Loop over width dimension\n            for w in range(0, width, BLOCK_W):\n                w_idx = w + tl.arange(0, BLOCK_W)\n                w_mask = w_idx < width\n                \n                # Create combined mask\n                mask = d_mask[:, None, None] & h_mask[None, :, None] & w_mask[None, None, :]\n                \n                # Compute pointer offsets\n                offsets = (\n                    pid_batch * input_batch_stride +\n                    pid_channel * input_channel_stride +\n                    d_idx[:, None, None] * input_depth_stride +\n                    h_idx[None, :, None] * input_height_stride +\n                    w_idx[None, None, :] * input_width_stride\n                )\n                \n                # Load data with mask\n                data = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n                accum += tl.sum(data)\n    \n    # Compute average and store result\n    avg = accum / count\n    tl.store(output_ptr + pid_batch * output_batch_stride + pid_channel * output_channel_stride, avg)\n\ndef global_avg_pool_3d_triton(x):\n    batch_size, channels = x.shape[0], x.shape[1]\n    depth, height, width = x.shape[2], x.shape[3], x.shape[4]\n    \n    # Create output tensor\n    output = torch.empty((batch_size, channels, 1, 1, 1), device=x.device, dtype=torch.float32)\n    \n    # Compute strides\n    input_batch_stride = x.stride(0)\n    input_channel_stride = x.stride(1)\n    input_depth_stride = x.stride(2)\n    input_height_stride = x.stride(3)\n    input_width_stride = x.stride(4)\n    output_batch_stride = output.stride(0)\n    output_channel_stride = output.stride(1)\n    \n    # Kernel configuration\n    BLOCK_D, BLOCK_H, BLOCK_W = 8, 16, 16\n    grid = (batch_size, channels)\n    \n    # Launch kernel\n    global_avg_pool_kernel[grid](\n        x,\n        output,\n        input_batch_stride,\n        input_channel_stride,\n        input_depth_stride,\n        input_height_stride,\n        input_width_stride,\n        depth,\n        height,\n        width,\n        BLOCK_D,\n        BLOCK_H,\n        BLOCK_W,\n        output_batch_stride=output_batch_stride,\n        output_channel_stride=output_channel_stride\n    )\n    \n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n        self.batch_norm = nn.BatchNorm3d(out_channels, eps=eps, momentum=momentum)\n        \n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.scale_factor\n        x = self.batch_norm(x)\n        x = global_avg_pool_3d_triton(x)\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_77_ConvTranspose3d_Scale_BatchNorm_GlobalAvgPool', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmp48ix__2j.py\", line 117, in forward\\n    x = global_avg_pool_3d_triton(x)\\n  File \"/tmp/tmp48ix__2j.py\", line 86, in global_avg_pool_3d_triton\\n    global_avg_pool_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 581, in run\\n    raise KeyError(\"Keyword argument %s was specified but unrecognised\" % k)\\nKeyError: \\'Keyword argument output_batch_stride was specified but unrecognised\\'\\n', 'runtime_error_name': 'builtins.KeyError', 'reference_timing_ms': 0.115, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch.nn import functional as F\n\n@triton.jit\ndef conv_transpose_scale_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    in_channels, out_channels, kernel_size,\n    depth_in, height_in, width_in,\n    depth_out, height_out, width_out,\n    scale_factor,\n    stride: tl.constexpr, padding: tl.constexpr,\n    input_bs_stride, input_c_stride, input_d_stride, input_h_stride, input_w_stride,\n    weight_oc_stride, weight_ic_stride, weight_d_stride, weight_h_stride, weight_w_stride,\n    output_bs_stride, output_c_stride, output_d_stride, output_h_stride, output_w_stride,\n    BLOCK_D: tl.constexpr, BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr,\n    BLOCK_C: tl.constexpr,\n    num_blocks_d: tl.constexpr, num_blocks_h: tl.constexpr, num_blocks_w: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_spatial = tl.program_id(1)\n    pid_c = tl.program_id(2)\n    \n    # Reconstruct spatial indices\n    total_blocks_hw = num_blocks_h * num_blocks_w\n    d_index = pid_spatial // total_blocks_hw\n    hw_index = pid_spatial % total_blocks_hw\n    h_index = hw_index // num_blocks_w\n    w_index = hw_index % num_blocks_w\n    \n    # Create block pointers\n    offsets_d = d_index * BLOCK_D + tl.arange(0, BLOCK_D)\n    offsets_h = h_index * BLOCK_H + tl.arange(0, BLOCK_H)\n    offsets_w = w_index * BLOCK_W + tl.arange(0, BLOCK_W)\n    offsets_c = pid_c * BLOCK_C + tl.arange(0, BLOCK_C)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_D, BLOCK_H, BLOCK_W, BLOCK_C), dtype=tl.float32)\n    \n    # Loop over input channels and kernel dimensions\n    for kd in range(kernel_size):\n        for kh in range(kernel_size):\n            for kw in range(kernel_size):\n                for ic in range(0, in_channels):\n                    # Compute input indices\n                    d_in = offsets_d - kd\n                    h_in = offsets_h - kh\n                    w_in = offsets_w - kw\n                    \n                    # Boundary check mask\n                    mask_d = (d_in >= 0) & (d_in < depth_in)\n                    mask_h = (h_in >= 0) & (h_in < height_in)\n                    mask_w = (w_in >= 0) & (w_in < width_in)\n                    mask = mask_d[:, None, None] & mask_h[None, :, None] & mask_w[None, None, :]\n                    \n                    # Base offset for batch and channel\n                    base = pid_batch * input_bs_stride + ic * input_c_stride\n                    # Compute spatial offsets as 3D tensor\n                    d_offset = d_in * input_d_stride\n                    h_offset = h_in * input_h_stride\n                    w_offset = w_in * input_w_stride\n                    # Broadcast to 3D\n                    d_offset_3d = d_offset[:, None, None]\n                    h_offset_3d = h_offset[None, :, None]\n                    w_offset_3d = w_offset[None, None, :]\n                    input_ptr_offset = base + d_offset_3d + h_offset_3d + w_offset_3d\n                    \n                    # Load input\n                    input_val = tl.load(input_ptr + input_ptr_offset, mask=mask, other=0.0)\n                    \n                    # Load weight\n                    weight_ptr_offset = (\n                        pid_c * BLOCK_C + tl.arange(0, BLOCK_C)[:, None, None, None] * weight_oc_stride +\n                        ic * weight_ic_stride +\n                        kd * weight_d_stride +\n                        kh * weight_h_stride +\n                        kw * weight_w_stride\n                    )\n                    weight_val = tl.load(weight_ptr + weight_ptr_offset)\n                    \n                    # Accumulate\n                    acc += input_val * weight_val\n    \n    # Add bias if exists\n    if bias_ptr is not None:\n        bias_val = tl.load(bias_ptr + pid_c * BLOCK_C + tl.arange(0, BLOCK_C)[:, None, None, None])\n        acc += bias_val\n    \n    # Apply scaling\n    acc = acc * scale_factor\n    \n    # Output indices and mask\n    d_out = offsets_d\n    h_out = offsets_h\n    w_out = offsets_w\n    c_out = offsets_c\n    \n    mask_d = d_out < depth_out\n    mask_h = h_out < height_out\n    mask_w = w_out < width_out\n    mask_c = c_out < out_channels\n    mask = mask_d[:, None, None, None] & mask_h[None, :, None, None] & mask_w[None, None, :, None] & mask_c[None, None, None, :]\n    \n    # Store output\n    output_ptr_offset = (\n        pid_batch * output_bs_stride +\n        c_out[None, None, None, :] * output_c_stride +\n        d_out[:, None, None, None] * output_d_stride +\n        h_out[None, :, None, None] * output_h_stride +\n        w_out[None, None, :, None] * output_w_stride\n    )\n    tl.store(output_ptr + output_ptr_offset, acc, mask=mask)\n\n@triton.jit\ndef batch_norm_pool_kernel(\n    input_ptr, output_ptr, \n    weight_ptr, bias_ptr, mean_ptr, var_ptr,\n    depth, height, width, \n    eps,\n    input_bs_stride, input_c_stride, input_d_stride, input_h_stride, input_w_stride,\n    output_bs_stride, output_c_stride,\n    BLOCK_D: tl.constexpr, BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    \n    # Create block pointers\n    offsets_d = tl.arange(0, BLOCK_D)\n    offsets_h = tl.arange(0, BLOCK_H)\n    offsets_w = tl.arange(0, BLOCK_W)\n    \n    # Initialize accumulator for pooling\n    pool_acc = tl.zeros((BLOCK_D, BLOCK_H, BLOCK_W), dtype=tl.float32)\n    count = 0.0\n    \n    # Load batch norm parameters\n    weight = tl.load(weight_ptr + pid_c)\n    bias = tl.load(bias_ptr + pid_c)\n    mean = tl.load(mean_ptr + pid_c)\n    var = tl.load(var_ptr + pid_c)\n    \n    # Compute normalization factor\n    inv_std = 1.0 / tl.sqrt(var + eps)\n    \n    # Process blocks\n    for d in range(0, depth, BLOCK_D):\n        for h in range(0, height, BLOCK_H):\n            for w in range(0, width, BLOCK_W):\n                # Create masks\n                mask_d = (d + offsets_d) < depth\n                mask_h = (h + offsets_h) < height\n                mask_w = (w + offsets_w) < width\n                mask = mask_d[:, None, None] & mask_h[None, :, None] & mask_w[None, None, :]\n                \n                # Load input\n                input_ptr_offset = (\n                    pid_batch * input_bs_stride +\n                    pid_c * input_c_stride +\n                    (d + offsets_d[:, None, None]) * input_d_stride +\n                    (h + offsets_h[None, :, None]) * input_h_stride +\n                    (w + offsets_w[None, None, :]) * input_w_stride\n                )\n                x = tl.load(input_ptr + input_ptr_offset, mask=mask, other=0.0)\n                \n                # Apply batch norm\n                x_norm = (x - mean) * inv_std * weight + bias\n                \n                # Accumulate for pooling\n                pool_acc += tl.sum(x_norm, axis=0)\n                count += tl.sum(mask, axis=0)\n    \n    # Final pooling output\n    pool_val = pool_acc / count\n    tl.store(output_ptr + pid_batch * output_bs_stride + pid_c * output_c_stride, pool_val)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.scale_factor = scale_factor\n        self.eps = eps\n        self.momentum = momentum\n        \n        # Initialize weights and biases\n        self.weight = torch.nn.Parameter(torch.empty(\n            out_channels, in_channels, kernel_size, kernel_size, kernel_size\n        ))\n        self.bias = torch.nn.Parameter(torch.empty(out_channels))\n        self.bn_weight = torch.nn.Parameter(torch.ones(out_channels))\n        self.bn_bias = torch.nn.Parameter(torch.zeros(out_channels))\n        self.register_buffer('running_mean', torch.zeros(out_channels))\n        self.register_buffer('running_var', torch.ones(out_channels))\n        \n        # Initialize parameters\n        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in)\n        torch.nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        # Compute output dimensions\n        stride = 1\n        padding = 0\n        depth_out = (x.size(2)-1)*stride - 2*padding + (self.kernel_size-1) + 1\n        height_out = (x.size(3)-1)*stride - 2*padding + (self.kernel_size-1) + 1\n        width_out = (x.size(4)-1)*stride - 2*padding + (self.kernel_size-1) + 1\n        \n        # Allocate output tensor\n        output = torch.empty(\n            (x.size(0), self.out_channels, depth_out, height_out, width_out),\n            device=x.device, dtype=x.dtype\n        )\n        \n        # Compute number of blocks per spatial dimension\n        BLOCK_D, BLOCK_H, BLOCK_W, BLOCK_C = 4, 4, 4, 16\n        num_blocks_d = triton.cdiv(depth_out, BLOCK_D)\n        num_blocks_h = triton.cdiv(height_out, BLOCK_H)\n        num_blocks_w = triton.cdiv(width_out, BLOCK_W)\n        \n        # Launch ConvTranspose + Scale kernel with 3D grid\n        grid = (\n            x.size(0),  # batch\n            num_blocks_d * num_blocks_h * num_blocks_w,  # flattened spatial\n            triton.cdiv(self.out_channels, BLOCK_C)  # channels\n        )\n        \n        conv_transpose_scale_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_channels, self.out_channels, self.kernel_size,\n            x.size(2), x.size(3), x.size(4),\n            depth_out, height_out, width_out,\n            self.scale_factor,\n            stride, padding,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),\n            self.weight.stride(0), self.weight.stride(1), self.weight.stride(2), \n            self.weight.stride(3), self.weight.stride(4),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4),\n            BLOCK_D, BLOCK_H, BLOCK_W, BLOCK_C,\n            num_blocks_d, num_blocks_h, num_blocks_w\n        )\n        \n        # BatchNorm + Pooling\n        pooled_output = torch.empty(\n            (output.size(0), self.out_channels),\n            device=output.device, dtype=output.dtype\n        )\n        \n        grid_bn = (output.size(0), self.out_channels)\n        batch_norm_pool_kernel[grid_bn](\n            output, pooled_output,\n            self.bn_weight, self.bn_bias, self.running_mean, self.running_var,\n            depth_out, height_out, width_out,\n            self.eps,\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4),\n            pooled_output.stride(0), pooled_output.stride(1),\n            BLOCK_D=8, BLOCK_H=8, BLOCK_W=8\n        )\n        \n        return pooled_output.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n\nbatch_size = 16\nin_channels = 64\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef reduce_spatial(\n    x_ptr,\n    out_ptr,\n    spatial_size,\n    n_channels,\n    n_batches,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    base = pid_batch * n_channels * spatial_size + pid_channel * spatial_size\n    total = 0.0\n    for offset in range(0, spatial_size, BLOCK_SIZE):\n        idx = offset + tl.arange(0, BLOCK_SIZE)\n        mask = idx < spatial_size\n        data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n        total += tl.sum(data, axis=0)\n    out_index = pid_batch * n_channels + pid_channel\n    tl.store(out_ptr + out_index, total)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n        self.batch_norm = nn.BatchNorm3d(out_channels, eps=eps, momentum=momentum)\n        self.kernel_size = kernel_size\n        self.kernel_sum = None\n        self.bias_fused = None\n\n    def forward(self, x):\n        if self.kernel_sum is None:\n            conv_weight = self.conv_transpose.weight\n            conv_bias = self.conv_transpose.bias\n            bn_gamma = self.batch_norm.weight\n            bn_beta = self.batch_norm.bias\n            bn_mean = self.batch_norm.running_mean\n            bn_var = self.batch_norm.running_var\n            eps = self.batch_norm.eps\n\n            inv_std = 1.0 / torch.sqrt(bn_var + eps)\n            fused_weight = bn_gamma[None, :, None, None, None] * inv_std[None, :, None, None, None] * self.scale_factor * conv_weight\n            self.kernel_sum = fused_weight.sum(dim=(2,3,4))\n\n            if conv_bias is None:\n                conv_bias = torch.zeros_like(bn_mean)\n            self.bias_fused = bn_gamma * inv_std * (conv_bias * self.scale_factor - bn_mean) + bn_beta\n\n            self.register_buffer('kernel_sum', self.kernel_sum)\n            self.register_buffer('bias_fused', self.bias_fused)\n\n        batch_size, in_channels, depth, height, width = x.shape\n        if isinstance(self.kernel_size, int):\n            kd = kh = kw = self.kernel_size\n        else:\n            kd, kh, kw = self.kernel_size\n\n        total_elements = (depth + kd - 1) * (height + kh - 1) * (width + kw - 1)\n        x_flat = x.contiguous().view(batch_size, in_channels, -1)\n        spatial_size = x_flat.size(2)\n        input_spatial_sum = torch.empty((batch_size, in_channels), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size, in_channels)\n        reduce_spatial[grid](x_flat, input_spatial_sum, spatial_size, in_channels, batch_size, BLOCK_SIZE=1024)\n        \n        output_value = torch.matmul(input_spatial_sum, self.kernel_sum)\n        output_value += self.bias_fused * total_elements\n        output_value /= total_elements\n        return output_value.view(batch_size, -1, 1, 1, 1)\n\nbatch_size = 16\nin_channels = 64\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: -100.0000, Type: Exploratory)\n```python\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_depth_stride,\n    input_height_stride,\n    input_width_stride,\n    depth: tl.constexpr,\n    height: tl.constexpr,\n    width: tl.constexpr,\n    BLOCK_D: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n    BLOCK_W: tl.constexpr\n):\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef reduce_spatial(\n    x_ptr,\n    out_ptr,\n    spatial_size,\n    n_channels,\n    n_batches,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    base = pid_batch * n_channels * spatial_size + pid_channel * spatial_size\n    total = 0.0\n    for offset in range(0, spatial_size, BLOCK_SIZE):\n        idx = offset + tl.arange(0, BLOCK_SIZE)\n        mask = idx < spatial_size\n        data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n        total += tl.sum(data, axis=0)\n    out_index = pid_batch * n_channels + pid_channel\n    tl.store(out_ptr + out_index, total)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n        self.batch_norm = nn.BatchNorm3d(out_channels, eps=eps, momentum=momentum)\n        self.kernel_size = kernel_size\n        self.kernel_sum = None\n        self.bias_fused = None\n\n    def forward(self, x):\n        if self.kernel_sum is None:\n            conv_weight = self.conv_transpose.weight\n            conv_bias = self.conv_transpose.bias\n            bn_gamma = self.batch_norm.weight\n            bn_beta = self.batch_norm.bias\n            bn_mean = self.batch_norm.running_mean\n            bn_var = self.batch_norm.running_var\n            eps = self.batch_norm.eps\n\n            inv_std = 1.0 / torch.sqrt(bn_var + eps)\n            fused_weight = bn_gamma[None, :, None, None, None] * inv_std[None, :, None, None, None] * self.scale_factor * conv_weight\n            self.kernel_sum = fused_weight.sum(dim=(2,3,4))\n\n            if conv_bias is None:\n                conv_bias = torch.zeros_like(bn_mean)\n            self.bias_fused = bn_gamma * inv_std * (conv_bias * self.scale_factor - bn_mean) + bn_beta\n\n            self.register_buffer('kernel_sum', self.kernel_sum)\n            self.register_buffer('bias_fused', self.bias_fused)\n\n        batch_size, in_channels, depth, height, width = x.shape\n        if isinstance(self.kernel_size, int):\n            kd = kh = kw = self.kernel_size\n        else:\n            kd, kh, kw = self.kernel_size\n\n        total_elements = (depth + kd - 1) * (height + kh - 1) * (width + kw - 1)\n        x_flat = x.contiguous().view(batch_size, in_channels, -1)\n        spatial_size = x_flat.size(2)\n        input_spatial_sum = torch.empty((batch_size, in_channels), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size, in_channels)\n        reduce_spatial[grid](x_flat, input_spatial_sum, spatial_size, in_channels, batch_size, BLOCK_SIZE=1024)\n        \n        output_value = torch.matmul(input_spatial_sum, self.kernel_sum)\n        output_value += self.bias_fused * total_elements\n        output_value /= total_elements\n        return output_value.view(batch_size, -1, 1, 1, 1)\n\nbatch_size = 16\nin_channels = 64\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch.nn import functional as F\n\n@triton.jit\ndef conv_transpose_scale_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    in_channels, out_channels, kernel_size,\n    depth_in, height_in, width_in,\n    depth_out, height_out, width_out,\n    scale_factor,\n    stride: tl.constexpr, padding: tl.constexpr,\n    input_bs_stride, input_c_stride, input_d_stride, input_h_stride, input_w_stride,\n    weight_oc_stride, weight_ic_stride, weight_d_stride, weight_h_stride, weight_w_stride,\n    output_bs_stride, output_c_stride, output_d_stride, output_h_stride, output_w_stride,\n    BLOCK_D: tl.constexpr, BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr,\n    BLOCK_C: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_d = tl.program_id(1)\n    pid_h = tl.program_id(2)\n    pid_w = tl.program_id(3)\n    pid_c = tl.program_id(4)\n    \n    # Create block pointers\n    offsets_d = pid_d * BLOCK_D + tl.arange(0, BLOCK_D)\n    offsets_h = pid_h * BLOCK_H + tl.arange(0, BLOCK_H)\n    offsets_w = pid_w * BLOCK_W + tl.arange(0, BLOCK_W)\n    offsets_c = pid_c * BLOCK_C + tl.arange(0, BLOCK_C)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_D, BLOCK_H, BLOCK_W, BLOCK_C), dtype=tl.float32)\n    \n    # Loop over input channels and kernel dimensions\n    for kd in range(kernel_size):\n        for kh in range(kernel_size):\n            for kw in range(kernel_size):\n                for ic in range(0, in_channels):\n                    # Compute input indices with vectorization\n                    d_in = offsets_d - kd\n                    h_in = offsets_h - kh\n                    w_in = offsets_w - kw\n                    \n                    # Boundary check mask\n                    mask_d = (d_in >= 0) & (d_in < depth_in)\n                    mask_h = (h_in >= 0) & (h_in < height_in)\n                    mask_w = (w_in >= 0) & (w_in < width_in)\n                    mask = mask_d[:, None, None] & mask_h[None, :, None] & mask_w[None, None, :]\n                    \n                    # Load input with vectorization\n                    input_ptr_offset = (\n                        pid_batch * input_bs_stride +\n                        ic * input_c_stride +\n                        d_in * input_d_stride +\n                        h_in * input_h_stride +\n                        w_in * input_w_stride\n                    )\n                    input_val = tl.load(input_ptr + input_ptr_offset, mask=mask, other=0.0)\n                    \n                    # Load weight\n                    weight_ptr_offset = (\n                        pid_c * BLOCK_C + tl.arange(0, BLOCK_C)[:, None, None, None] * weight_oc_stride +\n                        ic * weight_ic_stride +\n                        kd * weight_d_stride +\n                        kh * weight_h_stride +\n                        kw * weight_w_stride\n                    )\n                    weight_val = tl.load(weight_ptr + weight_ptr_offset)\n                    \n                    # Accumulate\n                    acc += input_val * weight_val\n    \n    # Add bias if exists\n    if bias_ptr is not None:\n        bias_val = tl.load(bias_ptr + pid_c * BLOCK_C + tl.arange(0, BLOCK_C)[:, None, None, None])\n        acc += bias_val\n    \n    # Apply scaling\n    acc = acc * scale_factor\n    \n    # Output indices and mask\n    d_out = offsets_d\n    h_out = offsets_h\n    w_out = offsets_w\n    c_out = offsets_c\n    \n    mask_d = d_out < depth_out\n    mask_h = h_out < height_out\n    mask_w = w_out < width_out\n    mask_c = c_out < out_channels\n    mask = mask_d[:, None, None, None] & mask_h[None, :, None, None] & mask_w[None, None, :, None] & mask_c[None, None, None, :]\n    \n    # Store output\n    output_ptr_offset = (\n        pid_batch * output_bs_stride +\n        c_out[None, None, None, :] * output_c_stride +\n        d_out[:, None, None, None] * output_d_stride +\n        h_out[None, :, None, None] * output_h_stride +\n        w_out[None, None, :, None] * output_w_stride\n    )\n    tl.store(output_ptr + output_ptr_offset, acc, mask=mask)\n\n@triton.jit\ndef batch_norm_pool_kernel(\n    input_ptr, output_ptr, \n    weight_ptr, bias_ptr, mean_ptr, var_ptr,\n    depth, height, width, \n    eps, \n    input_bs_stride, input_c_stride, input_d_stride, input_h_stride, input_w_stride,\n    BLOCK_D: tl.constexpr, BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    \n    # Create block pointers\n    offsets_d = tl.arange(0, BLOCK_D)\n    offsets_h = tl.arange(0, BLOCK_H)\n    offsets_w = tl.arange(0, BLOCK_W)\n    \n    # Initialize accumulator for pooling\n    pool_acc = tl.zeros((BLOCK_D, BLOCK_H, BLOCK_W), dtype=tl.float32)\n    count = 0.0\n    \n    # Load batch norm parameters\n    weight = tl.load(weight_ptr + pid_c)\n    bias = tl.load(bias_ptr + pid_c)\n    mean = tl.load(mean_ptr + pid_c)\n    var = tl.load(var_ptr + pid_c)\n    \n    # Compute normalization factor\n    inv_std = 1.0 / tl.sqrt(var + eps)\n    \n    # Process blocks\n    for d in range(0, depth, BLOCK_D):\n        for h in range(0, height, BLOCK_H):\n            for w in range(0, width, BLOCK_W):\n                # Create masks\n                mask_d = (d + offsets_d) < depth\n                mask_h = (h + offsets_h) < height\n                mask_w = (w + offsets_w) < width\n                mask = mask_d[:, None, None] & mask_h[None, :, None] & mask_w[None, None, :]\n                \n                # Load input\n                input_ptr_offset = (\n                    pid_batch * input_bs_stride +\n                    pid_c * input_c_stride +\n                    (d + offsets_d[:, None, None]) * input_d_stride +\n                    (h + offsets_h[None, :, None]) * input_h_stride +\n                    (w + offsets_w[None, None, :]) * input_w_stride\n                )\n                x = tl.load(input_ptr + input_ptr_offset, mask=mask, other=0.0)\n                \n                # Apply batch norm\n                x_norm = (x - mean) * inv_std * weight + bias\n                \n                # Accumulate for pooling\n                pool_acc += tl.sum(x_norm, axis=0)\n                count += tl.sum(mask, axis=0)\n    \n    # Final pooling output\n    pool_val = pool_acc / count\n    tl.store(output_ptr + pid_batch * output_ptr.stride(0) + pid_c, pool_val)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.scale_factor = scale_factor\n        self.eps = eps\n        self.momentum = momentum\n        \n        # Initialize weights and biases\n        self.weight = torch.nn.Parameter(torch.empty(\n            out_channels, in_channels, kernel_size, kernel_size, kernel_size\n        ))\n        self.bias = torch.nn.Parameter(torch.empty(out_channels))\n        self.bn_weight = torch.nn.Parameter(torch.ones(out_channels))\n        self.bn_bias = torch.nn.Parameter(torch.zeros(out_channels))\n        self.register_buffer('running_mean', torch.zeros(out_channels))\n        self.register_buffer('running_var', torch.ones(out_channels))\n        \n        # Initialize parameters\n        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in)\n        torch.nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        # Compute output dimensions\n        stride = 1\n        padding = 0\n        depth_out = (x.size(2)-1)*stride - 2*padding + (self.kernel_size-1) + 1\n        height_out = (x.size(3)-1)*stride - 2*padding + (self.kernel_size-1) + 1\n        width_out = (x.size(4)-1)*stride - 2*padding + (self.kernel_size-1) + 1\n        \n        # Allocate output tensor\n        output = torch.empty(\n            (x.size(0), self.out_channels, depth_out, height_out, width_out),\n            device=x.device, dtype=x.dtype\n        )\n        \n        # Launch ConvTranspose + Scale kernel\n        grid = (\n            x.size(0),  # batch\n            triton.cdiv(depth_out, 4), \n            triton.cdiv(height_out, 4), \n            triton.cdiv(width_out, 4),\n            triton.cdiv(self.out_channels, 16)\n        )\n        \n        conv_transpose_scale_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_channels, self.out_channels, self.kernel_size,\n            x.size(2), x.size(3), x.size(4),\n            depth_out, height_out, width_out,\n            self.scale_factor,\n            stride, padding,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),\n            self.weight.stride(0), self.weight.stride(1), self.weight.stride(2), \n            self.weight.stride(3), self.weight.stride(4),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4),\n            BLOCK_D=4, BLOCK_H=4, BLOCK_W=4, BLOCK_C=16\n        )\n        \n        # BatchNorm + Pooling\n        pooled_output = torch.empty(\n            (output.size(0), self.out_channels),\n            device=output.device, dtype=output.dtype\n        )\n        \n        grid_bn = (output.size(0), self.out_channels)\n        batch_norm_pool_kernel[grid_bn](\n            output, pooled_output,\n            self.bn_weight, self.bn_bias, self.running_mean, self.running_var,\n            depth_out, height_out, width_out,\n            self.eps,\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4),\n            BLOCK_D=8, BLOCK_H=8, BLOCK_W=8\n        )\n        \n        return pooled_output.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n\nbatch_size = 16\nin_channels = 64\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef reduce_spatial(\n    x_ptr,\n    out_ptr,\n    spatial_size,\n    n_channels,\n    n_batches,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    base = pid_batch * n_channels * spatial_size + pid_channel * spatial_size\n    total = 0.0\n    for offset in range(0, spatial_size, BLOCK_SIZE):\n        idx = offset + tl.arange(0, BLOCK_SIZE)\n        mask = idx < spatial_size\n        data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n        total += tl.sum(data, axis=0)\n    out_index = pid_batch * n_channels + pid_channel\n    tl.store(out_ptr + out_index, total)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n        self.batch_norm = nn.BatchNorm3d(out_channels, eps=eps, momentum=momentum)\n        self.kernel_size = kernel_size\n\n    def forward(self, x):\n        conv_weight = self.conv_transpose.weight\n        conv_bias = self.conv_transpose.bias\n        bn_gamma = self.batch_norm.weight\n        bn_beta = self.batch_norm.bias\n        bn_mean = self.batch_norm.running_mean\n        bn_var = self.batch_norm.running_var\n        eps = self.batch_norm.eps\n\n        inv_std = 1.0 / torch.sqrt(bn_var + eps)\n        fused_weight = bn_gamma[None, :, None, None, None] * inv_std[None, :, None, None, None] * self.scale_factor * conv_weight\n        kernel_sum = fused_weight.sum(dim=(2,3,4))\n\n        if conv_bias is None:\n            conv_bias = torch.zeros_like(bn_mean)\n        bias_fused = bn_gamma * inv_std * (conv_bias * self.scale_factor - bn_mean) + bn_beta\n\n        batch_size, in_channels, depth, height, width = x.shape\n        if isinstance(self.kernel_size, int):\n            kd = kh = kw = self.kernel_size\n        else:\n            kd, kh, kw = self.kernel_size\n\n        total_elements = (depth + kd - 1) * (height + kh - 1) * (width + kw - 1)\n        x_flat = x.contiguous().view(batch_size, in_channels, -1)\n        spatial_size = x_flat.size(2)\n        input_spatial_sum = torch.empty((batch_size, in_channels), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size, in_channels)\n        reduce_spatial[grid](x_flat, input_spatial_sum, spatial_size, in_channels, batch_size, BLOCK_SIZE=1024)\n        \n        output_value = torch.matmul(input_spatial_sum, kernel_sum.t())\n        output_value += bias_fused * total_elements\n        output_value /= total_elements\n        return output_value.view(batch_size, -1, 1, 1, 1)\n\nbatch_size = 16\nin_channels = 64\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch.nn import functional as F\n\n@triton.jit\ndef conv_transpose_scale_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    in_channels, out_channels, kernel_size,\n    depth_in, height_in, width_in,\n    depth_out, height_out, width_out,\n    scale_factor,\n    stride: tl.constexpr, padding: tl.constexpr,\n    input_bs_stride, input_c_stride, input_d_stride, input_h_stride, input_w_stride,\n    weight_oc_stride, weight_ic_stride, weight_d_stride, weight_h_stride, weight_w_stride,\n    output_bs_stride, output_c_stride, output_d_stride, output_h_stride, output_w_stride,\n    BLOCK_D: tl.constexpr, BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr,\n    BLOCK_C: tl.constexpr,\n    num_blocks_d: tl.constexpr, num_blocks_h: tl.constexpr, num_blocks_w: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_spatial = tl.program_id(1)\n    pid_c = tl.program_id(2)\n    \n    # Reconstruct spatial indices\n    total_blocks_hw = num_blocks_h * num_blocks_w\n    d_index = pid_spatial // total_blocks_hw\n    hw_index = pid_spatial % total_blocks_hw\n    h_index = hw_index // num_blocks_w\n    w_index = hw_index % num_blocks_w\n    \n    # Create block pointers\n    offsets_d = d_index * BLOCK_D + tl.arange(0, BLOCK_D)\n    offsets_h = h_index * BLOCK_H + tl.arange(0, BLOCK_H)\n    offsets_w = w_index * BLOCK_W + tl.arange(0, BLOCK_W)\n    offsets_c = pid_c * BLOCK_C + tl.arange(0, BLOCK_C)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_D, BLOCK_H, BLOCK_W, BLOCK_C), dtype=tl.float32)\n    \n    # Loop over input channels and kernel dimensions\n    for kd in range(kernel_size):\n        for kh in range(kernel_size):\n            for kw in range(kernel_size):\n                for ic in range(0, in_channels):\n                    # Compute input indices\n                    d_in = offsets_d - kd\n                    h_in = offsets_h - kh\n                    w_in = offsets_w - kw\n                    \n                    # Boundary check mask\n                    mask_d = (d_in >= 0) & (d_in < depth_in)\n                    mask_h = (h_in >= 0) & (h_in < height_in)\n                    mask_w = (w_in >= 0) & (w_in < width_in)\n                    mask = mask_d[:, None, None] & mask_h[None, :, None] & mask_w[None, None, :]\n                    \n                    # Base offset for batch and channel\n                    base = pid_batch * input_bs_stride + ic * input_c_stride\n                    # Compute spatial offsets as 3D tensor\n                    d_offset = d_in * input_d_stride\n                    h_offset = h_in * input_h_stride\n                    w_offset = w_in * input_w_stride\n                    # Broadcast to 3D\n                    d_offset_3d = d_offset[:, None, None]\n                    h_offset_3d = h_offset[None, :, None]\n                    w_offset_3d = w_offset[None, None, :]\n                    input_ptr_offset = base + d_offset_3d + h_offset_3d + w_offset_3d\n                    \n                    # Load input\n                    input_val = tl.load(input_ptr + input_ptr_offset, mask=mask, other=0.0)\n                    \n                    # Load weight\n                    weight_ptr_offset = (\n                        pid_c * BLOCK_C + tl.arange(0, BLOCK_C)[:, None, None, None] * weight_oc_stride +\n                        ic * weight_ic_stride +\n                        kd * weight_d_stride +\n                        kh * weight_h_stride +\n                        kw * weight_w_stride\n                    )\n                    weight_val = tl.load(weight_ptr + weight_ptr_offset)\n                    \n                    # Accumulate\n                    acc += input_val * weight_val\n    \n    # Add bias if exists\n    if bias_ptr is not None:\n        bias_val = tl.load(bias_ptr + pid_c * BLOCK_C + tl.arange(0, BLOCK_C)[:, None, None, None])\n        acc += bias_val\n    \n    # Apply scaling\n    acc = acc * scale_factor\n    \n    # Output indices and mask\n    d_out = offsets_d\n    h_out = offsets_h\n    w_out = offsets_w\n    c_out = offsets_c\n    \n    mask_d = d_out < depth_out\n    mask_h = h_out < height_out\n    mask_w = w_out < width_out\n    mask_c = c_out < out_channels\n    mask = mask_d[:, None, None, None] & mask_h[None, :, None, None] & mask_w[None, None, :, None] & mask_c[None, None, None, :]\n    \n    # Store output\n    output_ptr_offset = (\n        pid_batch * output_bs_stride +\n        c_out[None, None, None, :] * output_c_stride +\n        d_out[:, None, None, None] * output_d_stride +\n        h_out[None, :, None, None] * output_h_stride +\n        w_out[None, None, :, None] * output_w_stride\n    )\n    tl.store(output_ptr + output_ptr_offset, acc, mask=mask)\n\n@triton.jit\ndef batch_norm_pool_kernel(\n    input_ptr, output_ptr, \n    weight_ptr, bias_ptr, mean_ptr, var_ptr,\n    depth, height, width, \n    eps,\n    input_bs_stride, input_c_stride, input_d_stride, input_h_stride, input_w_stride,\n    output_bs_stride, output_c_stride,\n    BLOCK_D: tl.constexpr, BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    \n    # Create block pointers\n    offsets_d = tl.arange(0, BLOCK_D)\n    offsets_h = tl.arange(0, BLOCK_H)\n    offsets_w = tl.arange(0, BLOCK_W)\n    \n    # Initialize accumulator for pooling\n    pool_acc = tl.zeros((BLOCK_D, BLOCK_H, BLOCK_W), dtype=tl.float32)\n    count = 0.0\n    \n    # Load batch norm parameters\n    weight = tl.load(weight_ptr + pid_c)\n    bias = tl.load(bias_ptr + pid_c)\n    mean = tl.load(mean_ptr + pid_c)\n    var = tl.load(var_ptr + pid_c)\n    \n    # Compute normalization factor\n    inv_std = 1.0 / tl.sqrt(var + eps)\n    \n    # Process blocks\n    for d in range(0, depth, BLOCK_D):\n        for h in range(0, height, BLOCK_H):\n            for w in range(0, width, BLOCK_W):\n                # Create masks\n                mask_d = (d + offsets_d) < depth\n                mask_h = (h + offsets_h) < height\n                mask_w = (w + offsets_w) < width\n                mask = mask_d[:, None, None] & mask_h[None, :, None] & mask_w[None, None, :]\n                \n                # Load input\n                input_ptr_offset = (\n                    pid_batch * input_bs_stride +\n                    pid_c * input_c_stride +\n                    (d + offsets_d[:, None, None]) * input_d_stride +\n                    (h + offsets_h[None, :, None]) * input_h_stride +\n                    (w + offsets_w[None, None, :]) * input_w_stride\n                )\n                x = tl.load(input_ptr + input_ptr_offset, mask=mask, other=0.0)\n                \n                # Apply batch norm\n                x_norm = (x - mean) * inv_std * weight + bias\n                \n                # Accumulate for pooling\n                pool_acc += tl.sum(x_norm, axis=0)\n                count += tl.sum(mask, axis=0)\n    \n    # Final pooling output\n    pool_val = pool_acc / count\n    tl.store(output_ptr + pid_batch * output_bs_stride + pid_c * output_c_stride, pool_val)\n\nclass ModelNew(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.scale_factor = scale_factor\n        self.eps = eps\n        self.momentum = momentum\n        \n        # Initialize weights and biases\n        self.weight = torch.nn.Parameter(torch.empty(\n            out_channels, in_channels, kernel_size, kernel_size, kernel_size\n        ))\n        self.bias = torch.nn.Parameter(torch.empty(out_channels))\n        self.bn_weight = torch.nn.Parameter(torch.ones(out_channels))\n        self.bn_bias = torch.nn.Parameter(torch.zeros(out_channels))\n        self.register_buffer('running_mean', torch.zeros(out_channels))\n        self.register_buffer('running_var', torch.ones(out_channels))\n        \n        # Initialize parameters\n        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in)\n        torch.nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        # Compute output dimensions\n        stride = 1\n        padding = 0\n        depth_out = (x.size(2)-1)*stride - 2*padding + (self.kernel_size-1) + 1\n        height_out = (x.size(3)-1)*stride - 2*padding + (self.kernel_size-1) + 1\n        width_out = (x.size(4)-1)*stride - 2*padding + (self.kernel_size-1) + 1\n        \n        # Allocate output tensor\n        output = torch.empty(\n            (x.size(0), self.out_channels, depth_out, height_out, width_out),\n            device=x.device, dtype=x.dtype\n        )\n        \n        # Compute number of blocks per spatial dimension\n        BLOCK_D, BLOCK_H, BLOCK_W, BLOCK_C = 4, 4, 4, 16\n        num_blocks_d = triton.cdiv(depth_out, BLOCK_D)\n        num_blocks_h = triton.cdiv(height_out, BLOCK_H)\n        num_blocks_w = triton.cdiv(width_out, BLOCK_W)\n        \n        # Launch ConvTranspose + Scale kernel with 3D grid\n        grid = (\n            x.size(0),  # batch\n            num_blocks_d * num_blocks_h * num_blocks_w,  # flattened spatial\n            triton.cdiv(self.out_channels, BLOCK_C)  # channels\n        )\n        \n        conv_transpose_scale_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_channels, self.out_channels, self.kernel_size,\n            x.size(2), x.size(3), x.size(4),\n            depth_out, height_out, width_out,\n            self.scale_factor,\n            stride, padding,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),\n            self.weight.stride(0), self.weight.stride(1), self.weight.stride(2), \n            self.weight.stride(3), self.weight.stride(4),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4),\n            BLOCK_D, BLOCK_H, BLOCK_W, BLOCK_C,\n            num_blocks_d, num_blocks_h, num_blocks_w\n        )\n        \n        # BatchNorm + Pooling\n        pooled_output = torch.empty(\n            (output.size(0), self.out_channels),\n            device=output.device, dtype=output.dtype\n        )\n        \n        grid_bn = (output.size(0), self.out_channels)\n        batch_norm_pool_kernel[grid_bn](\n            output, pooled_output,\n            self.bn_weight, self.bn_bias, self.running_mean, self.running_var,\n            depth_out, height_out, width_out,\n            self.eps,\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3), output.stride(4),\n            pooled_output.stride(0), pooled_output.stride(1),\n            BLOCK_D=8, BLOCK_H=8, BLOCK_W=8\n        )\n        \n        return pooled_output.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n\nbatch_size = 16\nin_channels = 64\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_depth_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    depth: tl.constexpr,\n    height: tl.constexpr,\n    width: tl.constexpr,\n    BLOCK_D: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n    BLOCK_W: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Initialize accumulator as scalar\n    accum = 0.0\n    total_elements = depth * height * width\n    \n    # Vectorized accumulation\n    for d in range(0, depth, BLOCK_D):\n        d_idx = d + tl.arange(0, BLOCK_D)\n        d_mask = d_idx < depth\n        \n        for h in range(0, height, BLOCK_H):\n            h_idx = h + tl.arange(0, BLOCK_H)\n            h_mask = h_idx < height\n            \n            for w in range(0, width, BLOCK_W):\n                w_idx = w + tl.arange(0, BLOCK_W)\n                w_mask = w_idx < width\n                \n                # Combined mask for valid elements\n                mask = d_mask[:, None, None] & h_mask[None, :, None] & w_mask[None, None, :]\n                \n                # Vectorized memory access\n                offsets = (\n                    pid_batch * input_batch_stride +\n                    pid_channel * input_channel_stride +\n                    d_idx[:, None, None] * input_depth_stride +\n                    h_idx[None, :, None] * input_height_stride +\n                    w_idx[None, None, :] * input_width_stride\n                )\n                \n                # Load data with mask\n                data = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n                accum += tl.sum(data)\n    \n    # Compute average and store result\n    avg = accum / total_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef global_avg_pool_3d_triton(x):\n    batch_size, channels = x.shape[0], x.shape[1]\n    depth, height, width = x.shape[2], x.shape[3], x.shape[4]\n    \n    # Create output tensor\n    output = torch.empty((batch_size, channels, 1, 1, 1), device=x.device, dtype=torch.float32)\n    \n    # Compute strides\n    input_batch_stride = x.stride(0)\n    input_channel_stride = x.stride(1)\n    input_depth_stride = x.stride(2)\n    input_height_stride = x.stride(3)\n    input_width_stride = x.stride(4)\n    output_batch_stride = output.stride(0)\n    output_channel_stride = output.stride(1)\n    \n    # Kernel configuration\n    BLOCK_D, BLOCK_H, BLOCK_W = 8, 16, 16\n    grid = (batch_size, channels)\n    \n    # Launch kernel with fixed arguments\n    global_avg_pool_kernel[grid](\n        x,\n        output,\n        input_batch_stride,\n        input_channel_stride,\n        input_depth_stride,\n        input_height_stride,\n        input_width_stride,\n        output_batch_stride,\n        output_channel_stride,\n        depth,\n        height,\n        width,\n        BLOCK_D,\n        BLOCK_H,\n        BLOCK_W\n    )\n    \n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n        self.batch_norm = nn.BatchNorm3d(out_channels, eps=eps, momentum=momentum)\n        \n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.scale_factor\n        x = self.batch_norm(x)\n        x = global_avg_pool_3d_triton(x)\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a program that is currently functional (compiles and correct) but has a speedup of 0.07 (which is very slow compared to the reference).\n The current program uses a Triton kernel for global average pooling in 3D.\n\n The problem: The current Triton kernel for global average pooling is using nested loops over depth, height, and width with vectorized loads.\n However, the performance is poor (speed_up=0.07). We need to optimize this kernel.\n\n Observation:\n The current kernel has three nested loops (d, h, w) and within the innermost loop, it loads a 3D block of data (BLOCK_D x BLOCK_H x BLOCK_W) and sums it.\n The total number of elements loaded per channel is depth * height * width, and we do this for each channel and batch.\n\n However, note that the kernel is launched with grid = (batch_size, channels). Each program handles one channel in one batch.\n\n The current blocking (BLOCK_D=8, BLOCK_H=16, BLOCK_W=16) might be too small, and the nested loops might cause overhead.\n\n How can we improve?\n 1. We note that the global average pool is a reduction over the entire spatial dimensions. We can use a more efficient reduction strategy.\n 2. Instead of having three nested loops, we can flatten the spatial dimensions and use a single loop with a larger block size.\n\n However, note that the spatial dimensions might be large and we are constrained by the number of registers and shared memory.\n\n Alternative approach:\n We can change the kernel to process the spatial dimensions in a flattened manner. We can let each program handle a batch and channel, and then\n reduce over the spatial dimension by iterating over blocks of flattened spatial indices.\n\n Steps for the new kernel:\n   total_elements = depth * height * width\n   BLOCK_SIZE = the number of elements we process at a time (e.g., 1024)\n\n   for offset in range(0, total_elements, BLOCK_SIZE):\n        idx = offset + tl.arange(0, BLOCK_SIZE)\n        mask = idx < total_elements\n\n        # Convert the flat index to 3D indices (d, h, w)\n        d = idx // (height * width)\n        hw = idx % (height * width)\n        h = hw // width\n        w = hw % width\n\n        # Compute the pointer offset for the input\n        offsets = ... using d, h, w\n        data = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n        accum += tl.sum(data)\n\n   avg = accum / total_elements\n\n But note: converting the flat index to 3D indices inside the kernel might be expensive and involve integer divisions and mods.\n\n Alternatively, we can precompute the 3D indices in the loop? But that might be heavy.\n\n Another idea: use the existing strides and compute the pointer offset without converting to 3D? We can compute:\n\n   offset_3d = d * input_depth_stride + h * input_height_stride + w * input_width_stride\n\n But we have to compute d, h, w from the flat index.\n\n However, note that integer division and mod operations are expensive on GPU.\n\n Considering the constraints, we can try to use a larger block size for the flattened spatial dimension and hope that the reduction is faster.\n\n But note: the current kernel already has a triple loop, which might be inefficient because of the loop overhead and the fact that we are not utilizing the GPU's parallel reduction capabilities within a block.\n\n However, the current kernel does a vectorized load and then a sum over a 3D block. This is efficient for contiguous data? But the data is 3D and we are using strides.\n\n After looking at the top-performing programs, we see that one of the programs (Program D2) uses a reduction over a flattened spatial dimension. Let's adapt that idea.\n\n We are going to change the kernel to:\n\n   @triton.jit\n   def global_avg_pool_kernel(\n        input_ptr,\n        output_ptr,\n        input_batch_stride,\n        input_channel_stride,\n        input_depth_stride,\n        input_height_stride,\n        input_width_stride,\n        output_batch_stride,\n        output_channel_stride,\n        depth: tl.constexpr,\n        height: tl.constexpr,\n        width: tl.constexpr,\n        BLOCK_SIZE: tl.constexpr,   # now for flattened spatial\n   ):\n        pid_batch = tl.program_id(0)\n        pid_channel = tl.program_id(1)\n\n        total_elements = depth * height * width\n        accum = 0.0\n\n        for offset in range(0, total_elements, BLOCK_SIZE):\n            idx = offset + tl.arange(0, BLOCK_SIZE)\n            mask = idx < total_elements\n\n            # Convert flat index to 3D coordinates\n            w = idx % width\n            h = (idx // width) % height\n            d = idx // (width * height)\n\n            # Compute the offset in the input tensor\n            input_offset = (\n                pid_batch * input_batch_stride +\n                pid_channel * input_channel_stride +\n                d * input_depth_stride +\n                h * input_height_stride +\n                w * input_width_stride\n            )\n            data = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n            accum += tl.sum(data)\n\n        avg = accum / total_elements\n        output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n        tl.store(output_ptr + output_offset, avg)\n\n Then, in the function global_avg_pool_3d_triton, we set BLOCK_SIZE to a large number (like 1024 or 2048) and adjust the grid.\n\n However, note: the conversion from flat index to 3D coordinates uses integer divisions and mods. This might be expensive.\n\n We can try to precompute the d, h, w in a vectorized way? But Triton does have these operations.\n\n Alternatively, we can note that the input tensor is contiguous? The function global_avg_pool_3d_triton does not make it contiguous. We can change the input to be contiguous to use a simpler addressing?\n\n But the current kernel uses strides so it should work for non-contiguous too.\n\n Given that the current kernel is slow, we try the flattened version.\n\n But note: the total_elements might be 16*32*32 = 16384. If we set BLOCK_SIZE=1024, then we have 16 iterations per channel. The current kernel had (depth/8) * (height/16) * (width/16) = (16/8)*(32/16)*(32/16)=2*2*2=8 iterations. So the new kernel would have 16 iterations, which is more? But each iteration is simpler? We are only doing one load per element and then a sum.\n\n However, the current kernel does a vectorized load of 8*16*16=2048 elements at a time and then sums them. The new kernel would load 1024 elements at a time. The number of loads per element is the same (one), but the new kernel avoids the triple loop and the complex masking.\n\n But the conversion from flat index to 3D might be heavy. We can try to optimize by precomputing the divisors? Or use the fact that:\n\n   d = idx // (height * width)\n   hw = idx % (height * width)\n   h = hw // width\n   w = hw % width\n\n This requires two divisions and two mod operations per element? Actually, we are doing this for a block of BLOCK_SIZE elements. We can use the Triton's element-wise operations which are vectorized.\n\n However, we must be cautious: the integer divisions are expensive.\n\n Alternatively, we can use the idea from the reduce_spatial kernel in Program D2. In Program D2, they had a kernel that reduced over a flattened spatial dimension without converting to 3D. How? They had the input already flattened? In the D2 program, they did:\n\n   x_flat = x.contiguous().view(batch_size, in_channels, -1)\n\n Then the kernel only needed to do:\n\n   base = pid_batch * n_channels * spatial_size + pid_channel * spatial_size\n   ... and then load from base + idx.\n\n But in our case, we cannot change the input because the function must work for any 5D tensor.\n\n However, note: in the function global_avg_pool_3d_triton, we are given a 5D tensor. We can flatten it to 3D (batch, channel, spatial) and then use a simpler reduction kernel that does not need to compute 3D indices.\n\n Let's restructure:\n\n   In global_avg_pool_3d_triton:\n        x_flat = x.contiguous().view(x.size(0), x.size(1), -1)\n        spatial_size = x_flat.size(2)\n\n        Then call a kernel that reduces over the last dimension.\n\n   The kernel for reduction over the last dimension (spatial) is simpler:\n\n        @triton.jit\n        def reduce_spatial_kernel(\n            input_ptr,\n            output_ptr,\n            input_batch_stride, input_channel_stride, input_spatial_stride,\n            output_batch_stride, output_channel_stride,\n            spatial_size: tl.constexpr,\n            BLOCK_SIZE: tl.constexpr,\n        ):\n            pid_batch = tl.program_id(0)\n            pid_channel = tl.program_id(1)\n\n            base = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n            accum = 0.0\n\n            for offset in range(0, spatial_size, BLOCK_SIZE):\n                idx = offset + tl.arange(0, BLOCK_SIZE)\n                mask = idx < spatial_size\n                ptr = input_ptr + base + idx * input_spatial_stride\n                data = tl.load(ptr, mask=mask, other=0.0)\n                accum += tl.sum(data)\n\n            avg = accum / spatial_size\n            output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n            tl.store(output_ptr + output_offset, avg)\n\n   Then we call this kernel.\n\n   But note: the output must be 5D with shape (batch, channel, 1, 1, 1). We can reshape the output of the kernel accordingly.\n\n   Also note: the input tensor might not be contiguous. By doing `x_flat = x.contiguous().view(...)` we are making a contiguous copy if necessary. This adds an extra copy. We want to avoid that.\n\n   However, the current kernel already uses strides and can handle non-contiguous, but we are trying to optimize. The extra copy might be cheaper than the complex indexing? We need to test.\n\n   Alternatively, we can avoid the copy by using the original strides? But then we cannot flatten arbitrarily.\n\n   Given the time, let's try the flattened version without a copy? We can compute the stride for the spatial dimension as:\n\n        input_spatial_stride = 1   # because we are flattening, but actually we are not changing the memory layout? We are making a view, which is non-contiguous if the original was non-contiguous.\n\n   Actually, the view of a contiguous tensor is contiguous. But if the original tensor is non-contiguous, then the view might not be contiguous. We cannot rely on that.\n\n   So we must use the contiguous copy? Or we can design a kernel that works on non-contiguous 5D without flattening? \n\n   We go back to the first idea: do the conversion from flat index to 3D indices without a copy. But the integer operations might be expensive.\n\n   Considering the performance issue, we try the contiguous approach and hope that the speedup is worth the copy.\n\n   But note: the input tensor might be large. The copy could be expensive.\n\n   Alternatively, we can avoid the copy by using the original strides and compute the pointer offset for the 3D indices without actually making the tensor contiguous. We did that in the original kernel but with a triple loop.\n\n   We are going to try the flattened reduction without a contiguous copy? How?\n\n   We can compute the pointer offset for the element at (d, h, w) as:\n\n        offset = d * input_depth_stride + h * input_height_stride + w * input_width_stride\n\n   and then the base for the batch and channel is:\n\n        base = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n   So the total offset is base + offset.\n\n   In the kernel, we are iterating over a flat index from 0 to total_elements-1. For each flat index i, we compute:\n\n        d = i // (height * width)\n        h = (i % (height * width)) // width\n        w = i % width\n\n        offset = d * input_depth_stride + h * input_height_stride + w * input_width_stride\n\n   Then load from base + offset.\n\n   We do this for a block of BLOCK_SIZE elements.\n\n   We can vectorize the computation of d, h, w? Triton supports element-wise integer operations.\n\n   Let's try this.\n\n   We set:\n        total_elements = depth * height * width\n        BLOCK_SIZE = 1024   # or 2048, but must be a power of two and within the limits.\n\n   We'll change the kernel to:\n\n        for offset_idx in range(0, total_elements, BLOCK_SIZE):\n            i = offset_idx + tl.arange(0, BLOCK_SIZE)\n            mask = i < total_elements\n\n            # Compute d, h, w from i\n            # Let area = height * width\n            area = height * width\n            d = i // area\n            hw = i % area\n            h = hw // width\n            w = hw % width\n\n            # Compute the offset for each element\n            offset_3d = d * input_depth_stride + h * input_height_stride + w * input_width_stride\n            ptrs = input_ptr + base + offset_3d\n            data = tl.load(ptrs, mask=mask, other=0.0)\n            accum += tl.sum(data)\n\n   This might be expensive because of the integer operations, but note that the operations are done in parallel for the entire block.\n\n   We have to try and see.\n\n   We choose a BLOCK_SIZE that is a power of two and as large as possible (but within the register limits).\n\n   Given the trade-offs, we try the flattened kernel without a contiguous copy.\n\n   We'll set BLOCK_SIZE to 1024.\n\n   We also note that the current kernel has two program dimensions (batch, channel). The new kernel will have the same.\n\n   We must be cautious: the integer operations might be slow. But we hope that the reduction in loop overhead (from triple loop to single loop) and the larger block size will help.\n\n   We'll change the kernel accordingly.\n\n   Let's rewrite the kernel:\n\n        @triton.jit\n        def global_avg_pool_kernel(\n            input_ptr,\n            output_ptr,\n            input_batch_stride,\n            input_channel_stride,\n            input_depth_stride,\n            input_height_stride,\n            input_width_stride,\n            output_batch_stride,\n            output_channel_stride,\n            depth: tl.constexpr,\n            height: tl.constexpr,\n            width: tl.constexpr,\n            BLOCK_SIZE: tl.constexpr,   # for flattened spatial\n        ):\n            pid_batch = tl.program_id(0)\n            pid_channel = tl.program_id(1)\n            base = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n            total_elements = depth * height * width\n            area = height * width\n            accum = 0.0\n\n            for offset in range(0, total_elements, BLOCK_SIZE):\n                i = offset + tl.arange(0, BLOCK_SIZE)\n                mask = i < total_elements\n\n                # Compute d, h, w from the flat index i\n                d = i // area\n                hw = i % area\n                h = hw // width\n                w = hw % width\n\n                # Compute the 3D offset\n                offset_3d = d * input_depth_stride + h * input_height_stride + w * input_width_stride\n                ptrs = input_ptr + base + offset_3d\n                data = tl.load(ptrs, mask=mask, other=0.0)\n                accum += tl.sum(data)\n\n            avg = accum / total_elements\n            output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n            tl.store(output_ptr + output_offset, avg)\n\n   Then in the function global_avg_pool_3d_triton, we set BLOCK_SIZE to 1024.\n\n   We remove the triple loops and the old blocking parameters.\n\n   We hope this will be faster.\n\n   However, note: the integer divisions and mod operations might be expensive. But we are doing them in parallel for a block of 1024 elements.\n\n   We try this and see if the performance improves.\n\n   We also note that the current program has a speedup of 0.07, so any improvement is welcome.\n\n   Let's change the kernel accordingly.\n\n   Also, note: we are not changing the rest of the code (the class ModelNew and the helper functions).\n\n   We are only changing the kernel and the function global_avg_pool_3d_triton (to set the new BLOCK_SIZE).\n\n   In the function global_avg_pool_3d_triton, we remove the old BLOCK_D, BLOCK_H, BLOCK_W and use a single BLOCK_SIZE for the flattened dimension.\n\n   We set:\n        BLOCK_SIZE = 1024\n\n   And the grid remains (batch_size, channels).\n\n   We also note: the kernel parameters change.\n\n   Let's write the code accordingly.\n\n   IMPORTANT: We are not allowed to change the function signature of the kernel? But we are allowed to change the internal implementation and the constants.\n\n   We are also allowed to change the arguments we pass to the kernel? Yes, because the kernel is inside the EVOLVE-BLOCK.\n\n   We change the kernel to the flattened version and adjust the call.\n\n   We hope this will improve the performance.\n\n   We also note: the current kernel had a bug? We are not sure. But the current version is correct.\n\n   We are maintaining the same functionality.\n\n   Let's output the new code.\n\n   Note: We are making minimal changes (only the kernel and the call in global_avg_pool_3d_triton).\n\n   We set BLOCK_SIZE to 1024.\n\n   We also note: the total_elements might be 0? We assume not.\n\n   We are ready.\n\n   We output the entire code block.\n</think>\nTo improve the performance of the global average pooling kernel, we'll switch from a triple-nested loop approach to a flattened spatial dimension approach. This reduces loop overhead and allows for more efficient memory access patterns. The key changes include:\n\n1. Flatten the 3D spatial dimensions into a single dimension\n2. Use a single loop with larger block size (1024) for better parallelism\n3. Optimized index calculation using integer arithmetic\n4. Vectorized memory loads\n\nHere's the optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_depth_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    depth: tl.constexpr,\n    height: tl.constexpr,\n    width: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Precompute spatial constants\n    spatial_size = depth * height * width\n    height_width = height * width\n    \n    # Initialize accumulator\n    accum = 0.0\n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    \n    # Process spatial dimension in flattened blocks\n    for offset in range(0, spatial_size, BLOCK_SIZE):\n        idx = offset + tl.arange(0, BLOCK_SIZE)\n        mask = idx < spatial_size\n        \n        # Compute 3D indices from flattened index\n        d = idx // height_width\n        hw = idx % height_width\n        h = hw // width\n        w = hw % width\n        \n        # Compute memory offsets\n        offsets = base_offset + d * input_depth_stride + h * input_height_stride + w * input_width_stride\n        data = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n        accum += tl.sum(data)\n    \n    # Compute average and store result\n    avg = accum / spatial_size\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef global_avg_pool_3d_triton(x):\n    batch_size, channels = x.shape[0], x.shape[1]\n    depth, height, width = x.shape[2], x.shape[3], x.shape[4]\n    \n    # Create output tensor\n    output = torch.empty((batch_size, channels, 1, 1, 1), device=x.device, dtype=torch.float32)\n    \n    # Compute strides\n    input_batch_stride = x.stride(0)\n    input_channel_stride = x.stride(1)\n    input_depth_stride = x.stride(2)\n    input_height_stride = x.stride(3)\n    input_width_stride = x.stride(4)\n    output_batch_stride = output.stride(0)\n    output_channel_stride = output.stride(1)\n    \n    # Kernel configuration - use larger block size for flattened spatial dimension\n    grid = (batch_size, channels)\n    BLOCK_SIZE = 1024\n    \n    # Launch kernel\n    global_avg_pool_kernel[grid](\n        x,\n        output,\n        input_batch_stride,\n        input_channel_stride,\n        input_depth_stride,\n        input_height_stride,\n        input_width_stride,\n        output_batch_stride,\n        output_channel_stride,\n        depth,\n        height,\n        width,\n        BLOCK_SIZE\n    )\n    \n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n        self.batch_norm = nn.BatchNorm3d(out_channels, eps=eps, momentum=momentum)\n        \n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = x * self.scale_factor\n        x = self.batch_norm(x)\n        x = global_avg_pool_3d_triton(x)\n        return x\n\nbatch_size = 16\nin_channels = 64\nout_channels = 32\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
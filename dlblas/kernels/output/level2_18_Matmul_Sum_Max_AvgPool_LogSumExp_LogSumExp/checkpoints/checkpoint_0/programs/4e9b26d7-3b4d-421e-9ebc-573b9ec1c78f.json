{"id": "4e9b26d7-3b4d-421e-9ebc-573b9ec1c78f", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, \n    w_sum_ptr,\n    b_sum,\n    output_ptr,\n    in_features,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    acc = 0.0\n    for k in range(0, in_features, BLOCK_SIZE):\n        offsets = k + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < in_features\n        x_vals = tl.load(x_ptr + row_idx * in_features + offsets, mask=mask, other=0.0)\n        w_vals = tl.load(w_sum_ptr + offsets, mask=mask, other=0.0)\n        acc += tl.sum(x_vals * w_vals)\n    acc += b_sum\n    tl.store(output_ptr + row_idx, acc)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        w_sum = self.linear.weight.sum(dim=0)\n        b_sum = self.linear.bias.sum()\n\n        output_vec = torch.empty(x.shape[0], device=x.device, dtype=x.dtype)\n        if x.shape[0] > 0:\n            grid = (x.shape[0],)\n            BLOCK_SIZE = 128\n            _forward_kernel[grid](x, w_sum, b_sum, output_vec, in_features, BLOCK_SIZE=BLOCK_SIZE)\n        output = output_vec.unsqueeze(1)\n        return output\n\nbatch_size = 128\nin_features = 10\nout_features = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "7bf19ccc-5689-400f-a55e-3f573544e60d", "generation": 1, "timestamp": 1754565943.6694388, "iteration_found": 0, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_18_Matmul_Sum_Max_AvgPool_LogSumExp_LogSumExp", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpnzvtf2ep.py\", line 41, in forward\n    _forward_kernel[grid](x, w_sum, b_sum, output_vec, in_features, BLOCK_SIZE=BLOCK_SIZE)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: def _forward_kernel(\n    x_ptr, \n    w_sum_ptr,\n    b_sum,\n    output_ptr,\n    in_features,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    acc = 0.0\n    for k in range(0, in_features, BLOCK_SIZE):\n        offsets = k + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < in_features\n        x_vals = tl.load(x_ptr + row_idx * in_features + offsets, mask=mask, other=0.0)\n        w_vals = tl.load(w_sum_ptr + offsets, mask=mask, other=0.0)\n        acc += tl.sum(x_vals * w_vals)\n    acc += b_sum\n    tl.store(output_ptr + row_idx, acc)\n\nIncompatibleTypeErrorImpl('invalid operands of type pointer<fp32> and triton.language.float32')\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 900, in __add__\n    return add(self, other, sanitize_overflow=True, _semantic=_semantic)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2468, in add\n    return _semantic.add(x, y, sanitize_overflow)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 226, in add\n    input, other = self.binary_op_type_checking_impl(input, other, True, True)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 185, in binary_op_type_checking_impl\n    self.check_ptr_type_impl(rhs_sca_ty, lhs_sca_ty, allow_rhs_ptr)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 167, in check_ptr_type_impl\n    raise IncompatibleTypeErrorImpl(type_a, type_b)\ntriton.language.semantic.IncompatibleTypeErrorImpl: invalid operands of type pointer<fp32> and triton.language.float32\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.161, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.164, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.161, "speed_up": 0.982, "custom_timing": 0.164}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.1640\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.161, 'speed_up': 0.982, 'custom_timing': 0.164}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.1640, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.161, 'speed_up': 0.982, 'custom_timing': 0.164}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a sequence of operations:\n        - Matrix multiplication\n        - Summation\n        - Max\n        - Average pooling\n        - LogSumExp\n        - LogSumExp\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, 1).\n        \"\"\"\n        x = self.linear(x)  # (batch_size, out_features)\n        x = torch.sum(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.max(x, dim=1, keepdim=True)[0] # (batch_size, 1)\n        x = torch.mean(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.logsumexp(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.logsumexp(x, dim=1, keepdim=True) # (batch_size, 1)\n        return x\n\nbatch_size = 128\nin_features = 10\nout_features = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.1640), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.161, 'speed_up': 0.982, 'custom_timing': 0.164})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a sequence of operations:\n        - Matrix multiplication\n        - Summation\n        - Max\n        - Average pooling\n        - LogSumExp\n        - LogSumExp\n    \"\"\"\n    def __init__(self, in_features, out_features):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, 1).\n        \"\"\"\n        x = self.linear(x)  # (batch_size, out_features)\n        x = torch.sum(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.max(x, dim=1, keepdim=True)[0] # (batch_size, 1)\n        x = torch.mean(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.logsumexp(x, dim=1, keepdim=True) # (batch_size, 1)\n        x = torch.logsumexp(x, dim=1, keepdim=True) # (batch_size, 1)\n        return x\n\nbatch_size = 128\nin_features = 10\nout_features = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does a sequence of operations. The current implementation is using high-level PyTorch operations. The goal is to rewrite the forward pass using Triton to speed it up.\n\n However, note that the current model does:\n  1. Linear layer (matrix multiplication plus bias)\n  2. Sum over the second dimension (dim=1) -> reduces to (batch_size, 1)\n  3. Max over the second dimension (which is now size 1, so it's a no-op) -> still (batch_size, 1)\n  4. Mean over the second dimension (again, size 1, so no-op) -> still (batch_size, 1)\n  5. LogSumExp over the second dimension (no-op) -> (batch_size, 1)\n  6. Another LogSumExp (no-op) -> (batch_size, 1)\n\n Actually, steps 2-6 are all reducing a tensor of shape (batch_size, 1) along dimension 1, which doesn't change the tensor. So the only real operation is the linear layer and then the sum. The rest are redundant.\n\n But note: the current model does:\n   x = self.linear(x)  # (batch_size, out_features)  [say (128,5)]\n   Then: torch.sum(x, dim=1, keepdim=True) -> (128,1)\n   Then: torch.max(x, dim=1, keepdim=True)[0] -> on a (128,1) tensor, this returns the same tensor? Because we are taking max along the 1 dimension which has only one element.\n   Similarly, the mean and logsumexp on a single element are just that element.\n\n So the entire model is equivalent to:\n   x = self.linear(x).sum(dim=1, keepdim=True)\n\n However, we are asked to rewrite the forward function. We cannot change the model architecture (i.e., the sequence of operations) because we must be functionally equivalent.\n\n But note: the operations after the sum are redundant. However, we must compute them because the reference does.\n\n However, the current implementation is very inefficient because it does multiple passes over the data (each reduction is a separate pass). We can fuse all these operations in one Triton kernel.\n\n Steps in the forward pass:\n  1. Linear layer: we have a weight matrix W (out_features x in_features) and a bias vector b (out_features). Then: x @ W.T + b.\n  2. Then we do a sum reduction along the columns (dim=1) -> output of shape (batch_size, 1)\n  3. Then we do a max reduction on the result of step2 (which is a vector of size 1 per row) -> same as the input.\n  4. Then we do a mean reduction on the same -> same.\n  5. Then logsumexp (which for a single element x is log(exp(x)) = x) -> same.\n  6. Then again logsumexp -> same.\n\n Therefore, the only non-redundant operations are the linear layer and the initial sum. But note: the linear layer is followed by a sum over the out_features. So we can combine:\n\n   For each row i (batch element):\n      y_i = sum_{j=1}^{out_features} ( (x_i @ W_j) + b_j )\n\n   But note: the linear layer is: for each row i and each column j: \n        z_{i,j} = (x_i @ W_j) + b_j\n   Then we do: y_i = sum_j z_{i,j}\n\n   We can rewrite: \n        y_i = (x_i @ (sum_j W_j)) + (sum_j b_j)\n\n   Let:\n        W_sum = W.sum(dim=0)   # (in_features,)\n        b_sum = b.sum()         # scalar\n\n        Then: y_i = x_i @ W_sum + b_sum\n\n   So we can replace the linear layer and the sum by a single matrix-vector product and then add a scalar.\n\n   However, note that the rest of the operations (max, mean, logsumexp twice) are applied on a tensor of shape (batch_size, 1). Since each row has only one element, they are identity operations.\n\n   Therefore, the entire forward pass is equivalent to:\n        output = (x @ W_sum) + b_sum   [then unsqueeze to (batch_size,1)]\n\n   But wait: the linear layer is defined as: nn.Linear(in_features, out_features). We cannot change the parameters (W and b) because they are part of the model. However, we can precompute the sum of the weights and the sum of the bias.\n\n   However, note: the model is in the training mode? We are not told. But the problem says we are to rewrite the forward function. We are allowed to precompute the W_sum and b_sum in the forward pass? But note that the weights might change during training. So we have to do it every time.\n\n   Alternatively, we can write a Triton kernel that does:\n        For each row i:\n            acc = 0\n            for k in range(in_features):\n                acc += x[i, k] * (sum_{j} W[j, k])   # but note: W is stored as (out_features, in_features)\n            then add b_sum.\n\n   This is a matrix-vector multiplication (x of shape (batch, in_features) with W_sum of shape (in_features,)) -> output shape (batch,).\n\n   Then we add b_sum and then we do the rest of the operations (which are no-ops) but we must do them to be equivalent.\n\n   However, note: the original code does the linear layer and then the sum. This is mathematically equivalent to what we described.\n\n   But what about the rest? They are done on a tensor of shape (batch,1). We can just do:\n\n        output = (x @ W_sum) + b_sum   # shape (batch,)\n        output = output.unsqueeze(1)    # shape (batch,1)\n\n   Then the max, mean, logsumexp, logsumexp on dim=1 will leave it unchanged.\n\n   So the entire forward pass can be rewritten as:\n\n        W_sum = self.linear.weight.sum(dim=0)   # (in_features,)\n        b_sum = self.linear.bias.sum()           # scalar\n        x = x @ W_sum + b_sum\n        x = x.unsqueeze(1)\n\n        # Then do the rest? But they are no-ops? Actually, they are identity.\n\n        # But note: the original code does:\n        #   x = self.linear(x) -> (batch, out_features)\n        #   x = torch.sum(x, dim=1, keepdim=True) -> (batch,1)\n        #   ... then the rest.\n\n        # We are replacing the first two steps by the above.\n\n        # Then we must do the max, mean, logsumexp, logsumexp? But they are on a tensor of shape (batch,1) and they are reducing the second dimension (which is 1). They return the same tensor.\n\n        # So we can skip them? But the problem says: functional equivalence. We must produce the same output.\n\n        # However, note that the max and logsumexp might have different numerical properties? But for a single element, they are the same.\n\n        # Therefore, we can simply return x.unsqueeze(1) and skip the rest? But that would change the computation graph (if we are in training) and might break autograd.\n\n   Given the problem, we are only concerned with the forward pass and numerical equivalence. The problem says \"functionally equivalent\". We are allowed to skip operations that are identity? But note: the original code does them. We must do the same sequence? The problem does not say we can remove operations.\n\n   However, the problem says: \"Generate functionally equivalent but faster Triton implementations\". So we can change the implementation as long as the output is the same.\n\n   Since the operations after the sum are identity, we can skip them. But note: the output of the linear layer and the sum is a float tensor. Then we do max, mean, logsumexp, logsumexp? They are all differentiable? We are in a Module. We must preserve the computation graph for backward? But the problem does not specify training or inference.\n\n   The evaluation pipeline only checks the output. So if we can prove that the output is the same, we can skip the redundant operations.\n\n   But note: the problem says \"functional equivalence\". The operations after the sum are identity, so the output is the same.\n\n   Therefore, we can replace the entire forward pass by:\n\n        W_sum = self.linear.weight.sum(dim=0)\n        b_sum = self.linear.bias.sum()\n        return (x @ W_sum + b_sum).unsqueeze(1)\n\n   This is much faster because we avoid the large matrix multiplication (batch x in_features x out_features) and replace it by a matrix-vector multiplication (batch x in_features). Then we avoid the subsequent reductions (which were on a small tensor but still multiple passes).\n\n   However, note: the current model does a linear layer that produces a (batch, out_features) and then reduces it. The total FLOPs of the linear layer is batch * in_features * out_features, and the reduction is batch * out_features. Our method: batch * in_features (for the matrix-vector) plus the cost of summing the weights (which is in_features * out_features) and the bias (out_features). This might be faster if out_features is large.\n\n   But in the given example: in_features=10, out_features=5. The cost of the linear layer is 128*10*5 = 6400, and the reduction is 128*5=640. Our method: the precomputation of W_sum: 5*10=50, and the matrix-vector: 128*10=1280. So total 50+1280=1330, which is less than 6400+640=7040.\n\n   Therefore, we can do that.\n\n   However, the problem says we are to rewrite using Triton. But note: we are allowed to use PyTorch as well? The example shows that we are to replace the entire model? But the problem says: \"Generate functionally equivalent but faster Triton implementations (custom_code)\".\n\n   But note: the current code is a PyTorch model. We are allowed to rewrite the forward function using Triton? Or we can use a combination?\n\n   The example provided (with the quantize) shows a Triton kernel. We are expected to write a Triton kernel for the entire forward pass? Or for the heavy part?\n\n   The heavy part is the linear layer and the initial sum. We can do that in Triton? But note: the rest are no-ops. So we can write a Triton kernel that does the linear layer and the sum in one go? That is, for each row i, we compute:\n\n        z_i = 0\n        for j in range(out_features):\n            for k in range(in_features):\n                z_i += x[i,k] * W[j,k] \n            z_i += b[j]\n        # but then we are not storing the intermediate j, we are summing over j.\n\n   Actually, we can break:\n\n        z_i = (sum_j W[j,:]) @ x[i,:] + (sum_j b[j])\n\n   So we can precompute the sums of the weights and the bias on the CPU/GPU and then do a simple matrix-vector product? Then we don't need Triton? But the problem might require Triton.\n\n   Alternatively, we can write a Triton kernel that does the entire forward pass? But note: the rest of the operations are no-ops. We can ignore them.\n\n   The problem says: \"Modify ONLY the Triton kernel source within this block\". But in the current code, there is no Triton kernel. So we are to introduce one?\n\n   The current code does not have a Triton kernel. We are to rewrite the forward function using a Triton kernel? But note: the example we were given (the quantize) also defined a new function with a Triton kernel.\n\n   Let's look at the constraints:\n\n      \u274c NEVER change @triton.jit function signatures or parameter names\n\n      But we are introducing a new kernel? Then we can define the signature.\n\n   Steps:\n\n      We will write a Triton kernel that does the equivalent of:\n\n          output = (x @ W_sum) + b_sum   [and then unsqueeze to (batch,1)]\n\n      Then we return that.\n\n      We don't need to do the rest because they are identity.\n\n   However, note: the original code returns a tensor of shape (batch,1). Our kernel must output the same.\n\n   We can write a kernel that:\n\n        @triton.jit\n        def _forward_kernel(\n            x_ptr, \n            w_sum_ptr,\n            b_sum,\n            output_ptr,\n            in_features,\n            BLOCK_SIZE: tl.constexpr,\n        ):\n            pid = tl.program_id(0)\n            offsets = pid * in_features + tl.arange(0, BLOCK_SIZE)\n            mask = tl.arange(0, BLOCK_SIZE) < in_features\n\n            x_row = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n            w_row = tl.load(w_sum_ptr + tl.arange(0, BLOCK_SIZE), mask=mask, other=0.0)\n\n            dot = tl.sum(x_row * w_row) + b_sum\n            tl.store(output_ptr + pid, dot)\n\n        Then we call this kernel for each row.\n\n        Then we unsqueeze the output to (batch,1) and then we do the rest of the operations? But we skip the rest.\n\n   However, note: the rest of the operations (max, mean, logsumexp, logsumexp) are applied on the result of the sum (which is a tensor of shape (batch,1)). But in our kernel, we are returning a tensor of shape (batch,) and then we unsqueeze to (batch,1). Then we must apply the operations? But they are identity. So we can skip.\n\n   But to be safe, we can do:\n\n        output = output.unsqueeze(1)\n        # Then do the rest? But they are no-ops. We can skip.\n\n   However, the problem says: functional equivalence. We must do the same operations? But note: the operations are identity. We can skip.\n\n   But what about numerical stability? The logsumexp of a single element is the element. So we can skip.\n\n   Therefore, we can do:\n\n        # Precompute w_sum and b_sum\n        w_sum = self.linear.weight.sum(dim=0)   # (in_features,)\n        b_sum = self.linear.bias.sum()           # scalar\n\n        # Allocate output for the kernel: (batch_size,)\n        output = torch.empty(x.shape[0], device=x.device, dtype=x.dtype)\n\n        # Launch kernel\n        grid = (x.shape[0],)\n        BLOCK_SIZE = triton.next_power_of_2(in_features)\n        _forward_kernel[grid](x, w_sum, b_sum, output, in_features, BLOCK_SIZE=BLOCK_SIZE)\n\n        # Then unsqueeze to (batch_size,1)\n        output = output.unsqueeze(1)\n\n        # Then we do the rest? They are identity, so we return output.\n\n        return output\n\n   But note: the rest of the operations in the original code (max, mean, logsumexp, logsumexp) are not done? We are skipping them. However, they are identity. So the output is the same.\n\n   However, what if the tensor after the linear and sum had NaN? Then the max and logsumexp might behave differently? But for a single element, they are the same.\n\n   Therefore, we skip.\n\n   But note: the problem says \"functional equivalence\". We are allowed to skip operations that do nothing.\n\n   However, the problem says: \"functional equivalence\" meaning same output for the same input. We are producing the same output.\n\n   But wait: the original code does:\n\n        x = self.linear(x)  # (batch, out_features)\n        x = torch.sum(x, dim=1, keepdim=True)   # (batch,1)\n\n        Then: max, mean, logsumexp, logsumexp on dim=1.\n\n        Each of these operations on a tensor of shape (batch,1) returns the same tensor.\n\n        So the output is the same as after the sum.\n\n        We are producing the same as after the sum (but without the subsequent no-ops). So the output is the same.\n\n   Therefore, we can do that.\n\n   However, note: the original model might have been designed for a different purpose and the operations might be placeholders? But the problem says we are to rewrite for performance and functional equivalence.\n\n   But the problem says: \"functional equivalence\" meaning same output. We are producing the same output.\n\n   So we write:\n\n        class ModelNew(nn.Module):\n            def __init__(self, in_features, out_features):\n                super(ModelNew, self).__init__()\n                self.linear = nn.Linear(in_features, out_features)\n\n            def forward(self, x):\n                # Precompute the sums\n                w_sum = self.linear.weight.sum(dim=0)   # (in_features,)\n                b_sum = self.linear.bias.sum()           # scalar\n\n                # We are going to do: x @ w_sum + b_sum, then unsqueeze\n                # But we do it with a Triton kernel for the matrix-vector product?\n\n                # However, note: the matrix-vector product is already efficient in PyTorch? \n                #   x: (batch, in_features), w_sum: (in_features,)\n                #   torch.matmul(x, w_sum) is efficient.\n\n                # Why use Triton? Because the problem requires Triton? And we are to demonstrate Triton.\n\n                # But the problem says: \"Generate functionally equivalent but faster Triton implementations (custom_code)\".\n\n                # So we must use a Triton kernel.\n\n                # Let's write the kernel.\n\n                # Alternatively, we can do the entire operation in the kernel? We can write a kernel that does:\n\n                #   for each row i in batch:\n                #       acc = 0\n                #       for k in range(in_features):\n                #           acc += x[i,k] * w_sum[k]\n                #       acc += b_sum\n                #       output[i] = acc\n\n                # Then we unsqueeze.\n\n                # We'll write that kernel.\n\n                # Allocate output: (batch,)\n                output_vec = torch.empty(x.shape[0], device=x.device, dtype=x.dtype)\n\n                # We need to set the grid and block.\n                # We'll use one program per row.\n\n                # Note: in_features might not be a power of two? We can use masking.\n\n                # Define the kernel:\n\n                # We'll use the following kernel:\n\n                @triton.jit\n                def _forward_kernel(\n                    x_ptr, \n                    w_sum_ptr,\n                    b_sum,\n                    output_ptr,\n                    in_features,\n                    BLOCK_SIZE: tl.constexpr,\n                ):\n                    row_idx = tl.program_id(0)\n                    # Pointers to the current row of x\n                    x_row_ptr = x_ptr + row_idx * in_features\n                    # We'll load the entire row of x and the entire w_sum in chunks.\n\n                    acc = 0.0\n                    # Loop over the features in blocks.\n                    for k in range(0, in_features, BLOCK_SIZE):\n                        offsets = k + tl.arange(0, BLOCK_SIZE)\n                        mask = offsets < in_features\n                        x_val = tl.load(x_row_ptr + offsets, mask=mask, other=0.0)\n                        w_val = tl.load(w_sum_ptr + offsets, mask=mask, other=0.0)\n                        acc += tl.sum(x_val * w_val)\n\n                    # Add bias\n                    acc += b_sum\n\n                    # Store the result for this row\n                    tl.store(output_ptr + row_idx, acc)\n\n                # But note: we cannot define the kernel inside the forward function? Because Triton kernels must be top-level? \n                # Actually, they can be defined inside but then we must avoid recompilation? We can move the kernel to the top.\n\n                # Alternatively, we can define it outside.\n\n                # Given the constraints, we are to output the entire block. We can define the kernel at the top of the block.\n\n                # However, the problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\n                # We are to write the entire code block.\n\n                # We'll define the kernel at the top of the class? Or outside the class? The example had the kernel outside.\n\n                # We'll follow the example.\n\n                # But note: the example had:\n\n                #   @triton.autotune(...)\n                #   @triton.jit\n                #   def _quantize_rowwise(...)\n\n                #   then a function per_row_quantize_int8\n\n                # In our case, we are to rewrite the entire model. We are to replace the forward method.\n\n                # We'll define the kernel outside the class.\n\n                # However, the problem says: the output structure class is named ModelNew.\n\n                # We must output the ModelNew class and the other functions (get_inputs, get_init_inputs) and the batch_size, etc.\n\n                # We'll do:\n\n                #   First, we define the kernel (with a unique name) outside the class.\n\n                #   Then we define the ModelNew class.\n\n                #   Then the batch_size, get_inputs, get_init_inputs.\n\n                # But note: the kernel must be compiled for different BLOCK_SIZE? We can use autotune? But the problem says: small iterative steps.\n\n                # We'll skip autotune for now and use a fixed BLOCK_SIZE.\n\n                # How to choose BLOCK_SIZE? It should be at most the in_features, and a power of two.\n\n                # We'll set BLOCK_SIZE to the next power of two of in_features? But if in_features is 10, then 16.\n\n                # However, we are looping. We can set BLOCK_SIZE to 128? But in_features is 10, so we can set it to 16.\n\n                # But note: the kernel uses a loop over blocks. We can set BLOCK_SIZE to min(1024, triton.next_power_of_2(in_features))? But we are looping.\n\n                # Actually, we can set BLOCK_SIZE to any value. We'll let it be a tunable parameter? But we are not required to autotune.\n\n                # We'll set BLOCK_SIZE as a constant. We can set it to 128? But if in_features is 10, then we only need one block.\n\n                # We'll set it to 128.\n\n                # But note: the kernel must be defined at the top of the block.\n\n                # We are going to write:\n\n                #   import triton\n                #   import triton.language as tl\n\n                #   @triton.jit\n                #   def ... \n\n                # But the original code did not import triton? We must add the imports.\n\n                # The problem says: the output must be the entire block. The current block does not have triton imports. We must add them.\n\n                # We'll add the necessary imports.\n\n                # However, note: the example had:\n                #   import math\n                #   import torch\n                #   import triton\n                #   import triton.language as tl\n                #   from triton.language.extra import libdevice   # but we don't need libdevice.\n\n                # We don't need libdevice.\n\n                # We'll add:\n                #   import triton\n                #   import triton.language as tl\n\n                # Then the kernel.\n\n                # Then the ModelNew class.\n\n                # Then the batch_size, etc.\n\n                # But note: the current program does not have these imports. We are allowed to add them? The problem says: \"Modify ONLY the Triton kernel source within this block\". But we are introducing Triton, so we must import.\n\n                # We are allowed to change the entire block? The block starts with the current code. We are to rewrite the entire block? Yes, because the problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\n                # So we can restructure the entire block.\n\n   However, note: the current block does not have any Triton code. We are introducing it.\n\n   Therefore, we write:\n\n        import math\n        import torch\n        import triton\n        import triton.language as tl\n        import torch.nn as nn\n\n        # Define the kernel\n        @triton.jit\n        def _forward_kernel(\n            x_ptr, \n            w_sum_ptr,\n            b_sum,\n            output_ptr,\n            in_features,\n            BLOCK_SIZE: tl.constexpr,\n        ):\n            row_idx = tl.program_id(0)\n            # We'll process a row. We break the row into blocks of size BLOCK_SIZE.\n            acc = 0.0\n            # We'll loop over the features in blocks of BLOCK_SIZE\n            for k in range(0, in_features, BLOCK_SIZE):\n                offsets = k + tl.arange(0, BLOCK_SIZE)\n                mask = offsets < in_features\n                # Load a block of x\n                x_vals = tl.load(x_ptr + row_idx * in_features + offsets, mask=mask, other=0.0)\n                # Load the same block of w_sum\n                w_vals = tl.load(w_sum_ptr + offsets, mask=mask, other=0.0)\n                acc += tl.sum(x_vals * w_vals)\n            acc += b_sum\n            tl.store(output_ptr + row_idx, acc)\n\n        class ModelNew(nn.Module):\n            def __init__(self, in_features, out_features):\n                super(ModelNew, self).__init__()\n                self.linear = nn.Linear(in_features, out_features)\n\n            def forward(self, x):\n                # Precompute the sums of the weights and bias\n                w_sum = self.linear.weight.sum(dim=0)   # (in_features,)\n                b_sum = self.linear.bias.sum()           # scalar\n\n                # Allocate output vector (batch_size,)\n                output_vec = torch.empty(x.shape[0], device=x.device, dtype=x.dtype)\n\n                # Check if we have a valid grid\n                if x.shape[0] == 0:\n                    return output_vec.unsqueeze(1)\n\n                # Grid is (batch_size,)\n                grid = (x.shape[0],)\n\n                # BLOCK_SIZE for the kernel: we choose a power of two that is <= in_features? But we are looping. We can set it to 128 or 256.\n                BLOCK_SIZE = 128\n                # But note: if in_features is 0, then we skip. We assume at least 1.\n\n                # Call the kernel\n                _forward_kernel[grid](x, w_sum, b_sum, output_vec, in_features, BLOCK_SIZE=BLOCK_SIZE)\n\n                # Now, the rest of the operations are on a tensor of shape (batch,1). We create that tensor.\n                output = output_vec.unsqueeze(1)\n\n                # Then we do the rest: max, mean, logsumexp, logsumexp? They are identity. So we return output.\n                return output\n\n        batch_size = 128\n        in_features = 10\n        out_features = 5\n\n        def get_inputs():\n            return [torch.randn(batch_size, in_features)]\n\n        def get_init_inputs():\n            return [in_features, out_features]\n\n   However, note: the kernel does not handle the case where in_features is 0? But the problem says in_features=10.\n\n   But we should be safe.\n\n   Also note: the kernel uses a loop. We are using a fixed BLOCK_SIZE=128. If in_features is 10, then the loop runs once (k=0, then k=128 which is beyond 10). So it works.\n\n   Let's test with in_features=10: \n        k=0: offsets = [0,1,...,127] -> but we mask beyond 10. So we load 10 elements.\n\n   This is correct.\n\n   But note: the kernel does not require the entire w_sum to be in shared memory? It loads contiguous blocks. This is efficient.\n\n   However, we can tune BLOCK_SIZE? But the problem says small steps. We leave it at 128.\n\n   But note: the kernel might be inefficient for very small in_features? But 10 is small. We are only doing one iteration. It's fine.\n\n   We are replacing:\n        original: linear (128*10*5 = 6400 mults and 6400 adds) and then sum (128*5=640 adds) -> total about 12800 operations.\n        new: \n            precomputation: w_sum: 5*10=50 adds, b_sum: 5 adds -> 55 adds.\n            kernel: 128 rows * 10 mults and 10 adds -> 128*10*2 = 2560 operations.\n\n        So total 55 + 2560 = 2615 operations.\n\n   So we expect a speedup.\n\n   But note: the kernel launches and memory access. We are doing two loads per element (x and w_sum) and then a store per row. The original linear layer does: for each row and each output feature: 10 mults and 10 adds? Then the sum does 5 adds per row.\n\n   The kernel is doing 10 mults and 10 adds per row? Then plus the precomputation.\n\n   So it should be faster.\n\n   However, note: the original linear layer is implemented with highly optimized GEMM. But our kernel is a simple vector dot. Also, we are precomputing w_sum and b_sum. The precomputation is done on the GPU? It is a reduction of the weight matrix (5x10) -> 10 elements. That is 50 adds. Then the kernel: 1280 mults and 1280 adds? Plus 128 adds for the bias? Then the precomputation and the kernel together are about 50+1280+1280+128=2738 operations. The original: 128*10*5*2 = 12800 operations? Actually, the linear layer does: for each element in the output (128*5=640 elements): 10 mult-adds (so 20 operations per element) -> 12800 operations. Then the sum: 128*5=640 operations. Total 13440. So we are doing less.\n\n   Therefore, we expect a speedup.\n\n   But note: the kernel is written in Triton and might not be as optimized as cuBLAS? But for small matrices, cuBLAS might not be optimal. And our kernel is doing a simple dot product per row.\n\n   We'll hope for a speedup.\n\n   However, the problem says: the current speedup is 0.982 (which is less than 1). We are to improve it.\n\n   We'll output the code accordingly.\n\n   One more note: the kernel must be compiled. We are using a fixed BLOCK_SIZE=128. This is a constant. But what if in_features is larger than 128? The kernel loops. It should work.\n\n   But note: the kernel uses the same BLOCK_SIZE for every row? And we are using the same BLOCK_SIZE for the entire grid. It is a constant.\n\n   We'll write the code accordingly.\n\n   Let's write the entire block.\n\n   IMPORTANT: We must not change the class name (ModelNew) and the functions get_inputs, get_init_inputs.\n\n   We are replacing the forward method.\n\n   We are adding the Triton kernel and the necessary imports.\n\n   We are removing the redundant operations (max, mean, logsumexp, logsumexp) because they are identity and we skip them.\n\n   We hope the output is the same.\n\n   If there are any issues, we might have to do the identity operations? But they are no-ops. We skip.\n\n   We output the code block.\n\n   Note: We must include the imports at the top.\n\n   The current block does not have the Triton imports. We add them.\n\n   Also note: the original block had:\n        import torch\n        import torch.nn as nn\n\n   We are adding:\n        import math\n        import triton\n        import triton.language as tl\n\n   But note: we are not using math? We can remove it? Or leave it.\n\n   We'll leave it because it was in the example.\n\n   Alternatively, we can remove it. But the problem says we can restructure the entire block.\n\n   We'll do:\n\n        import math\n        import torch\n        import triton\n        import triton.language as tl\n        import torch.nn as nn\n\n   Then the kernel.\n\n   Then the class.\n\n   Then the batch_size, get_inputs, get_init_inputs.\n\n   We write:\n\n        # ================== EVOLVE-BLOCK-START ==================\n        import math\n        import torch\n        import triton\n        import triton.language as tl\n        import torch.nn as nn\n\n        @triton.jit\n        def _forward_kernel(\n            x_ptr, \n            w_sum_ptr,\n            b_sum,\n            output_ptr,\n            in_features,\n            BLOCK_SIZE: tl.constexpr,\n        ):\n            row_idx = tl.program_id(0)\n            acc = 0.0\n            # Loop over the features in blocks of BLOCK_SIZE\n            for k in range(0, in_features, BLOCK_SIZE):\n                offsets = k + tl.arange(0, BLOCK_SIZE)\n                mask = offsets < in_features\n                # Load a block of x for this row\n                x_vals = tl.load(x_ptr + row_idx * in_features + offsets, mask=mask, other=0.0)\n                # Load the same block of w_sum (which is of length in_features)\n                w_vals = tl.load(w_sum_ptr + offsets, mask=mask, other=0.0)\n                acc += tl.sum(x_vals * w_vals)\n            acc += b_sum\n            tl.store(output_ptr + row_idx, acc)\n\n        class ModelNew(nn.Module):\n            def __init__(self, in_features, out_features):\n                super(ModelNew, self).__init__()\n                self.linear = nn.Linear(in_features, out_features)\n\n            def forward(self, x):\n                # Precompute the sum of weights and bias\n                w_sum = self.linear.weight.sum(dim=0)   # (in_features,)\n                b_sum = self.linear.bias.sum()           # scalar\n\n                # Allocate output vector (batch_size,)\n                output_vec = torch.empty(x.shape[0], device=x.device, dtype=x.dtype)\n\n                # If batch_size is 0, return empty tensor with an extra dimension\n                if x.shape[0] == 0:\n                    return output_vec.unsqueeze(1)\n\n                # Grid is (batch_size,)\n                grid = (x.shape[0],)\n\n                # We choose a block size for the kernel. We'll use 128.\n                BLOCK_SIZE = 128\n\n                # Call the kernel\n                _forward_kernel[grid](x, w_sum, b_sum, output_vec, in_features, BLOCK_SIZE=BLOCK_SIZE)\n\n                # Reshape to (batch_size, 1)\n                output = output_vec.unsqueeze(1)\n\n                # We skip the redundant operations (max, mean, logsumexp, logsumexp) because they are identity on a tensor of shape (batch,1)\n                return output\n\n        batch_size = 128\n        in_features = 10\n        out_features = 5\n\n        def get_inputs():\n            return [torch.randn(batch_size, in_features)]\n\n        def get_init_inputs():\n            return [in_features, out_features]\n        # =================== EVOLVE-BLOCK-END ===================\n\n   However, note: the original model had the operations after the sum. We are skipping them. But they are identity. We hope this is acceptable.\n\n   If the evaluator requires exact operation-by-operation equivalence, then we might have to do them? But they are identity. We skip.\n\n   We'll submit and see.\n\n   But note: the problem says \"functional equivalence\". We are confident.\n\n   Alternatively, we can do:\n\n        output = output.unsqueeze(1)\n        output = torch.max(output, dim=1, keepdim=True)[0]\n        output = torch.mean(output, dim=1, keepdim=True)\n        output = torch.logsumexp(output, dim=1, keepdim=True)\n        output = torch.logsumexp(output, dim=1, keepdim=True)\n\n   But that would be inefficient? It would do four passes over the data? And they are no-ops. We can skip.\n\n   We skip.\n\n   We output the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, \n    w_sum_ptr,\n    b_sum,\n    output_ptr,\n    in_features,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    acc = 0.0\n    for k in range(0, in_features, BLOCK_SIZE):\n        offsets = k + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < in_features\n        x_vals = tl.load(x_ptr + row_idx * in_features + offsets, mask=mask, other=0.0)\n        w_vals = tl.load(w_sum_ptr + offsets, mask=mask, other=0.0)\n        acc += tl.sum(x_vals * w_vals)\n    acc += b_sum\n    tl.store(output_ptr + row_idx, acc)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        w_sum = self.linear.weight.sum(dim=0)\n        b_sum = self.linear.bias.sum()\n\n        output_vec = torch.empty(x.shape[0], device=x.device, dtype=x.dtype)\n        if x.shape[0] > 0:\n            grid = (x.shape[0],)\n            BLOCK_SIZE = 128\n            _forward_kernel[grid](x, w_sum, b_sum, output_vec, in_features, BLOCK_SIZE=BLOCK_SIZE)\n        output = output_vec.unsqueeze(1)\n        return output\n\nbatch_size = 128\nin_features = 10\nout_features = 5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
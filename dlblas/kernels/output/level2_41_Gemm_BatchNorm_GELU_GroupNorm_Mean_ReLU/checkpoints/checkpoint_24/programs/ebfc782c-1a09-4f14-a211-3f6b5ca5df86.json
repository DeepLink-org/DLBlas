{"id": "ebfc782c-1a09-4f14-a211-3f6b5ca5df86", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Triton kernel for fused GEMM + BatchNorm\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_bn_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N)\n        accumulator += bias[None, :]\n    \n    offs_m = tl.minimum(offs_m, M - 1)\n    offs_n = tl.minimum(offs_n, N - 1)\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def forward(self, x):\n        if not self.training:\n            if self.fused_weight is None or self.fused_bias is None:\n                with torch.no_grad():\n                    eps = self.batch_norm.eps\n                    running_var = self.batch_norm.running_var\n                    weight_bn = self.batch_norm.weight\n                    bias_bn = self.batch_norm.bias\n                    total_scale = weight_bn / torch.sqrt(running_var + eps)\n                    fused_bias = (self.gemm.bias * total_scale) + (bias_bn - self.batch_norm.running_mean * total_scale)\n                    self.fused_weight = (self.gemm.weight * total_scale.unsqueeze(1)).t().contiguous()\n                    self.fused_bias = fused_bias.contiguous()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[1]\n            output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_SIZE_M) * triton.cdiv(N, opt.BLOCK_SIZE_N),)\n            fused_gemm_bn_kernel[grid](\n                x, self.fused_weight, output, self.fused_bias,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                output.stride(0), output.stride(1),\n            )\n            x = output\n        else:\n            x = self.gemm(x)\n            x = self.batch_norm(x)\n        \n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "88416bcd-96f2-425e-a70d-293fc3d77c1c", "generation": 1, "timestamp": 1754571546.7490785, "iteration_found": 1, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.799, "runtime_ms": 0.149, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.119, "speed_up": 0.799, "custom_timing": 0.149}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.143, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.119, "speed_up": 0.832, "custom_timing": 0.143}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.1430\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.832, 'custom_timing': 0.143}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.1430, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.832, 'custom_timing': 0.143}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, BatchNorm, GELU, GroupNorm, Mean, and ReLU operations in sequence.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = self.batch_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.1430), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.832, 'custom_timing': 0.143})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, BatchNorm, GELU, GroupNorm, Mean, and ReLU operations in sequence.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = self.batch_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear (GEMM)\n  2. BatchNorm1d\n  3. GELU\n  4. GroupNorm\n  5. Mean (over dim=1, keepdim=True)\n  6. ReLU\n\nThe current implementation is using high-level PyTorch operations. We are to rewrite the forward pass using Triton to speed it up.\n\nHowever, note that the problem is about rewriting the entire model's forward pass. The original model uses several operations that are not easily fused in a single Triton kernel. Moreover, the problem states that we are in the \"reference code\" stage and the current speed_up is 0.832 (which is worse than the reference).\n\nWe are allowed to change the Triton kernel source only. But note that the current code does not contain any Triton kernel. Therefore, we are to introduce Triton kernels to replace the high-level operations.\n\nBut note: the constraints say:\n  \u274c NEVER change @triton.jit function signatures or parameter names\n  \u274c NEVER modify grid configuration, output tensor shapes, or PID logic\n\nHowever, we are starting from a non-Triton code. So we have to design the Triton kernel from scratch.\n\nBut note: the problem says we are in Phase 1 (Compilation Pass) and Phase 2 (Numerical Correctness) and then Phase 3 (Performance). Currently, the code is in the \"reference code\" stage and has passed correctness but is slower.\n\nWe are to rewrite the forward function using Triton. However, the model has multiple operations. We cannot write one kernel for everything because of the complexity and the fact that BatchNorm and GroupNorm require statistics over the whole tensor.\n\nGiven the complexity, we might break it down:\n\nOption 1: Write a Triton kernel for the entire forward pass? That would be very complex because of the multiple operations and the reduction operations (mean, variance for norms).\n\nOption 2: Optimize each operation separately? But note that the operations are sequential and we might benefit from fusion.\n\nHowever, the problem states: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" meaning we can only change the Triton kernel? But currently there is none.\n\nSo let's reexamine the task: We are to generate Triton implementations for the given PyTorch reference. The example provided was a quantization function that was replaced by a Triton kernel.\n\nIn this case, the entire forward pass is the reference. We are to replace the entire forward pass with a Triton kernel? That might be too heavy.\n\nAlternatively, we can replace the most expensive parts. The GEMM is the most expensive, but note that the GEMM is done by nn.Linear. We cannot change that without rewriting the linear layer with Triton.\n\nBut note: the problem says we can replace the entire reference_code with custom_code (Triton). So we are allowed to write a custom forward that uses Triton kernels.\n\nWe are going to write a custom forward function that uses Triton kernels for the operations that are performance critical.\n\nHowever, the BatchNorm and GroupNorm are already optimized in PyTorch. The GELU and ReLU are pointwise. The mean reduction is also a reduction.\n\nThe problem: the current implementation is slower than the reference? Actually the meta says: \n  'reference_timing_ms': 0.119, 'speed_up': 0.832, 'custom_timing': 0.143\n\nSo the custom code (the current ModelNew) is taking 0.143 ms vs the reference 0.119 ms? That doesn't make sense because the current code is the reference? \n\nWait, the meta says: \n  \"stage\": \"reference code\", \n  \"custom_timing\": 0.143, \n  \"reference_timing_ms\": 0.119\n\nThis suggests that the current program (the one we are to evolve) is being treated as the custom code? And it is slower than the reference? \n\nBut the problem says: the reference is in the dataset as a PyTorch reference. So what is the reference? It might be a hand-optimized version? \n\nLooking at the problem statement: the dataset contains PyTorch reference (reference_code) and we are to generate Triton (custom_code). \n\nTherefore, the current program we are given (the ModelNew) is the reference_code? And we are to replace it with custom_code (Triton) to make it faster.\n\nBut the current program is using high-level PyTorch. We are to rewrite the forward function using Triton.\n\nGiven the complexity, we might focus on the most expensive operation: the GEMM.\n\nBut note: the problem says we are allowed to do:\n  - Memory coalescing, vectorized loads/stores\n  - Adaptive tuning of block sizes\n  - Stride precomputation, pointer caching\n\nSo let's design:\n\nWe are going to replace the entire forward pass with a sequence of Triton kernels? But note that the problem says we must maintain the same inputs and outputs.\n\nAlternatively, we might start by replacing the linear layer with a Triton GEMM. Then leave the rest in PyTorch? But then we have to transfer data between GPU and CPU? No, the kernels run on GPU.\n\nBut note: the BatchNorm and GroupNorm are also expensive and might be fused with the GEMM? That is complex.\n\nGiven the constraints and the fact that we are only allowed to change the Triton kernel source, and the current code does not have any, we are to introduce Triton kernels.\n\nWe can do:\n\n1. Write a Triton kernel for the GEMM (the linear layer).\n2. Then do the rest in PyTorch? But that might not be faster because of the data movement.\n\nAlternatively, we can write a fused kernel for GEMM + pointwise operations? But we have BatchNorm and GroupNorm which are not pointwise.\n\nGiven the time, we might focus on the GEMM.\n\nBut note: the problem says we are to generate Triton implementations for the entire reference. So we are allowed to break the forward pass into multiple kernels.\n\nLet's write:\n\n  - A Triton GEMM kernel for the linear layer.\n  - Then use PyTorch for the rest? But that might not give enough speedup.\n\nAlternatively, we can write:\n\n  - A Triton kernel for the linear layer and then in the same kernel do the BatchNorm? But BatchNorm requires the entire output of the linear layer to compute the mean and variance? We cannot do that in a single kernel without multiple passes.\n\nTherefore, we break down:\n\n  Step 1: GEMM (Triton)\n  Step 2: BatchNorm (Triton) - but BatchNorm requires statistics over the entire feature dimension for each example? Actually, BatchNorm1d(out_features) normalizes over the batch and the spatial dimension? In this case, the input to BatchNorm is (batch_size, out_features). The BatchNorm1d normalizes over the batch? Actually, no: the BatchNorm1d normalizes per feature over the batch and the single spatial dimension (which doesn't exist). So it computes the mean and variance for each feature (over the batch). \n\n  But note: the BatchNorm1d layer in the model is applied to the output of the linear layer which is (batch_size, out_features). The BatchNorm1d(out_features) has learnable parameters (weight and bias) and running mean and variance. During training, it uses the batch statistics, and during inference, the running statistics.\n\n  We are not given if we are in training or inference. We must assume both? But the problem does not specify.\n\n  Step 3: GELU (pointwise) - easy to fuse with the previous or next kernel.\n  Step 4: GroupNorm - which normalizes over groups of channels. It requires computing mean and variance per group per example.\n  Step 5: Mean over the features (dim=1) - reducing the feature dimension to 1, so output becomes (batch_size, 1).\n  Step 6: ReLU - pointwise.\n\nGiven the complexity, we might start by writing a Triton kernel for the GEMM and see if that helps.\n\nBut note: the current GEMM is done by nn.Linear, which is already highly optimized (using cuBLAS). So we might not beat it with a Triton kernel unless we fuse it with the next operations.\n\nWe note that the next operation is BatchNorm, which is an affine transformation per feature. So we can fuse the linear layer and the BatchNorm? \n\nThe fused operation would be:\n\n  y = (x * W^T + b - running_mean) * (weight / sqrt(running_var + eps)) + bias\n\nBut note: during training, we use the batch statistics, so we cannot fuse. Therefore, we must assume inference? The problem does not specify.\n\nBut the problem says: \"Evaluation pipeline\" runs /verify_plus and /execute. It doesn't specify training or inference.\n\nGiven the ambiguity, we assume we are in inference mode. Then we can fuse the linear and the BatchNorm.\n\nSo the plan:\n\n  Fuse the linear and BatchNorm into one kernel? Then do the rest in separate steps.\n\nBut note: the BatchNorm layer in the model has running_mean and running_var and parameters (weight and bias). We can precompute the scaling and offset for inference:\n\n  scale = (weight / sqrt(running_var + eps))\n  offset = (bias - running_mean * scale)   [but note: the linear layer has its own bias?]\n\nActually, the fused operation:\n\n  linear_out = x @ W.t() + linear_bias\n  batch_norm_out = (linear_out - running_mean) * scale + batch_norm_bias\n\nBut wait, the BatchNorm layer has two parameters: weight (called gamma) and bias (called beta). And the formula is:\n\n  y = (x - running_mean) * (weight / sqrt(running_var + eps)) + bias\n\nSo we can precompute:\n\n  total_scale = weight / sqrt(running_var + eps)\n  total_bias = bias - running_mean * total_scale\n\nBut then the fused operation becomes:\n\n  y = (x @ W.t() + linear_bias) * total_scale + total_bias\n\nBut note: the linear_bias is added first, then the BatchNorm scaling and shifting. So:\n\n  y = ( (x @ W.t() + linear_bias) * total_scale ) + total_bias\n\nBut wait, that is not exactly the same because:\n\n  Without fusion:\n      linear_out = x @ W.t() + linear_bias\n      batch_norm_out = (linear_out - running_mean) * total_scale + bias\n\n  = linear_out * total_scale - running_mean * total_scale + bias\n  = (x @ W.t() + linear_bias) * total_scale + (bias - running_mean * total_scale)\n\nSo we can precompute:\n  fused_scale = total_scale\n  fused_bias = (bias - running_mean * total_scale) + total_scale * linear_bias\n\nBut note: the BatchNorm formula does not include the linear_bias in the statistics? Actually, the BatchNorm is applied after the linear, so the running_mean and running_var are computed from the linear outputs (which include the bias). So the running_mean and running_var are for the distribution of (x @ W.t() + linear_bias). Therefore, we can fuse as:\n\n  y = (x @ W.t()) * fused_scale + fused_bias\n\nwhere:\n  fused_scale = total_scale\n  fused_bias = (bias - running_mean * total_scale) + total_scale * linear_bias\n\nBut wait, we are missing the linear_bias in the scaling? Actually, we have:\n\n  (x @ W.t() + linear_bias) * total_scale = (x @ W.t()) * total_scale + linear_bias * total_scale\n\nSo the fused_bias = linear_bias * total_scale + (bias - running_mean * total_scale)\n\nTherefore, we can combine the linear and BatchNorm into a single linear-like operation.\n\nSo we can write a Triton kernel that does:\n\n  y = x @ (W.t() * fused_scale[None, :]) + fused_bias\n\nBut note: we are scaling the weights by fused_scale per output feature. Then we add the fused_bias.\n\nThis is mathematically equivalent to the fused operation.\n\nSo steps:\n\n  1. Precompute fused_scale and fused_bias from the BatchNorm parameters and the linear layer's bias.\n  2. Write a Triton kernel for the linear operation (GEMM) that uses the scaled weights and the fused_bias.\n\nBut note: the problem says we cannot change the model's __init__? We are only allowed to change the Triton kernel source? Actually, the entire code block is given and we can change it arbitrarily as long as the inputs and outputs are the same.\n\nSo we can change the __init__ and the forward? But note: the problem says \"maintain the same inputs and outputs\". The class name must be ModelNew.\n\nWe are going to:\n\n  - In __init__: we will still create the linear and batch_norm layers, but in the forward we are going to fuse them.\n\n  - Precompute the fused_scale and fused_bias for the BatchNorm in inference mode? But during training, we cannot fuse? \n\n  However, the problem does not specify training or inference. We must pass the correctness test. The correctness test runs the model and compares the output. It might run in training mode? But BatchNorm in training mode uses batch statistics, not running statistics.\n\n  We are in a dilemma.\n\nGiven the complexity and the fact that the problem says we are in the reference code stage and we are to generate Triton, we assume the model is in eval mode for inference.\n\nBut note: the problem does not specify. We have to pass correctness. So we must handle both? \n\nAlternatively, we can only fuse in inference mode. We can check the mode of the model.\n\nBut the problem says we are to generate a faster implementation. We can do:\n\n  - If in training, use the original unfused operations.\n  - If in inference, use the fused kernel.\n\nBut the problem does not specify the mode. The evaluation pipeline might run in training mode? We don't know.\n\nGiven the ambiguity, we will assume inference mode for now. We note that the problem says the model is in the dataset and we are to generate Triton. We are to focus on performance.\n\nSo we do:\n\n  Precompute the fused parameters in the __init__ for inference? But the running_mean and running_var are updated during training. So we must update the fused parameters after every training step? That is not possible without hooks.\n\nTherefore, we cannot change the model's state during training. So we decide to do the fusion on the fly in the forward pass? But then we are doing the same computation as before.\n\nAlternatively, we can write the fused operation without precomputation? That is, in the forward pass:\n\n  if training:\n      do the original operations\n  else:\n      use the fused kernel with the current running_mean and running_var\n\nBut then we have to do the precomputation for fused_scale and fused_bias in every forward? That is cheap.\n\nSo the plan for the forward:\n\n  if self.training:\n      x = self.gemm(x)\n      x = self.batch_norm(x)\n      ... rest ...\n  else:\n      # Inference: fuse gemm and batch_norm\n      # Precompute fused_scale and fused_bias for batch_norm\n      with torch.no_grad():\n          eps = self.batch_norm.eps\n          running_mean = self.batch_norm.running_mean\n          running_var = self.batch_norm.running_var\n          weight = self.batch_norm.weight\n          bias_bn = self.batch_norm.bias\n          # total_scale for batch_norm: weight / (sqrt(running_var + eps))\n          total_scale = weight / torch.sqrt(running_var + eps)\n          # For the linear layer: we have weight and bias\n          linear_weight = self.gemm.weight\n          linear_bias = self.gemm.bias\n          # Fused weight: linear_weight * total_scale[None, :]   [because each output feature scaled by total_scale[feature]]\n          fused_weight = linear_weight * total_scale[None, :]\n          fused_bias = (linear_bias * total_scale) + (bias_bn - running_mean * total_scale)\n      # Now do the fused GEMM: x @ fused_weight.t() + fused_bias\n      # We can use a Triton kernel for this GEMM.\n\nBut note: the original linear layer is x @ linear_weight.t() + linear_bias.\n\nSo we can write a Triton kernel for the fused GEMM.\n\nBut is this faster? The original GEMM in PyTorch is highly optimized. We are replacing it with a Triton GEMM that does the same thing? We might not be faster.\n\nHowever, we are going to write a well-tuned Triton GEMM kernel for this specific matrix size? The problem gives the batch_size=128, in_features=512, out_features=1024.\n\nWe can write a kernel that is tuned for these sizes? But note: the problem says we must use autotune.\n\nAlternatively, we can use the Triton matmul kernel from the tutorials.\n\nLet me recall the Triton matmul kernel:\n\n  We can write a kernel that does:\n\n    for each block in the output matrix (M, N), we load a block of A (M, K) and a block of B (K, N) and compute the dot product.\n\n  We can tune the block sizes for the given hardware (H100).\n\nGiven the constraints, we are going to write a Triton kernel for the fused GEMM (only for inference).\n\nBut note: the rest of the operations (GELU, GroupNorm, Mean, ReLU) are not fused. We are only fusing the linear and batchnorm.\n\nWe might see a speedup if the Triton GEMM is faster than the cuBLAS GEMM? That is unlikely for large matrices, but for small ones we might.\n\nBut the matrices: batch_size=128, in_features=512, out_features=1024. This is a medium size.\n\nAlternatively, we can fuse more operations? For example, after the fused GEMM, we can do the GELU and then the GroupNorm? But GroupNorm is complex.\n\nGiven the time, we will start by writing a Triton kernel for the fused GEMM for inference.\n\nWe are going to:\n\n  1. In the forward, for inference, compute the fused_weight and fused_bias.\n  2. Then call a Triton kernel that does: output = x @ fused_weight.t() + fused_bias\n\n  3. Then do the rest in PyTorch: \n        x = torch.nn.functional.gelu(output)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n\nBut note: the rest might become the bottleneck.\n\nAlternatively, we can write a Triton kernel that does the entire forward pass? That would be very complex.\n\nGiven the complexity and the constraints (only 20 lines per revision), we start with the fused GEMM.\n\nWe will write:\n\n  @triton.autotune(...)\n  @triton.jit\n  def fused_gemm_kernel(...)\n\nBut note: we are also to do the GELU? No, we are only fusing the linear and batch_norm.\n\nWe will do:\n\n  Step 1: Write the Triton kernel for the fused GEMM (without activation).\n\n  Step 2: Call it in the forward for inference.\n\n  Step 3: For training, we use the original.\n\nBut note: the problem says we must not change the function signatures? We are introducing a new kernel. That is allowed.\n\nWe must be cautious: the problem says \"Modify ONLY the Triton kernel source within this block\". We are adding a Triton kernel.\n\nWe are going to write the kernel inside the ModelNew class? Or outside? The example had the kernel outside the class.\n\nWe will write the kernel outside the class, inside the EVOLVE-BLOCK.\n\nLet's structure the code:\n\n  We are going to:\n    - Write the Triton kernel for the fused_gemm.\n    - Change the forward to use it in inference.\n\nBut note: the problem says we must not change the grid configuration? We are defining the grid for the kernel.\n\nWe are allowed to do that because we are writing the kernel from scratch.\n\nWe design the kernel for the matrix multiplication: \n  Input: x (batch_size, in_features)\n  Weight: fused_weight (out_features, in_features)\n  Bias: fused_bias (out_features)\n\n  Output: y (batch_size, out_features)\n\nWe can write a kernel that uses:\n\n  pid = tl.program_id(0)   # we'll use 2D grid: (out_features // BLOCK_SIZE_N, batch_size // BLOCK_SIZE_M)\n  but note: batch_size might be small (128) and out_features is 1024.\n\nAlternatively, we can do a 1D grid over the batch and output features? \n\n  Let M = batch_size, N = out_features, K = in_features.\n\n  We can have a 2D grid: (ceil(M / BLOCK_SIZE_M), ceil(N / BLOCK_SIZE_N))\n\n  Each program will compute a block of (BLOCK_SIZE_M, BLOCK_SIZE_N) of the output.\n\n  We will load a block of A (M, K) and a block of B (K, N) and compute the dot product.\n\nBut note: we are also adding bias. The bias is per output feature (N). So we add it after the dot product.\n\nWe'll write:\n\n  for m in range(BLOCK_SIZE_M):\n      for n in range(BLOCK_SIZE_N):\n          acc[m, n] = 0\n          for k in range(0, K, BLOCK_SIZE_K):\n              a = load from x[m, k:k+BLOCK_SIZE_K]\n              b = load from fused_weight[k:k+BLOCK_SIZE_K, n]   # but note: we need to transpose? because fused_weight is (N, K)\n          # then add bias for the column n.\n\nBut note: the weight matrix is stored in row-major (out_features, in_features). We want to do x @ W.t(), so we can treat the weight matrix as (in_features, out_features) without transposing? \n\nActually, we are doing: x (M, K) @ W (K, N) -> (M, N)   [if we use W as (K, N)]\n\nBut our weight is (N, K). So we need to transpose it to (K, N). We can pre-transpose the weight matrix? Or we can load it transposed.\n\nIn the kernel, we can load the weight matrix by swapping the indices? \n\n  We can have:\n      b_ptr = fused_weight_ptr + (offs_n * stride_nt + offs_k * stride_kt)\n\n  where stride_nt is the stride for the output feature dimension? Actually, the weight matrix is (N, K) and stored in row-major: so stride_nt = K, stride_kt = 1.\n\n  But we want to load a block of size (BLOCK_SIZE_K, BLOCK_SIZE_N) from the weight matrix? And we want to load it in a way that the k dimension is contiguous? \n\n  We can store the weight matrix in column-major? But that is not efficient.\n\nAlternatively, we can pre-transpose the weight matrix to (K, N) and then store it in row-major? Then we can load it without transposing.\n\nBut that would be: \n  fused_weight = fused_weight.t()   # now (K, N)\n\nThen we do: x (M, K) @ fused_weight (K, N) -> (M, N)\n\nBut we don't want to do the transpose every forward pass because it is expensive.\n\nSo we can precompute and store the weight in transposed form in the model? \n\nIn the __init__, we can store the weight as (in_features, out_features). But then we would have to transpose it for the original linear layer? We are not using the original linear layer in inference.\n\nWe can do:\n\n  In __init__: \n      self.gemm = nn.Linear(...)\n      ... \n      # for inference, we will have a transposed weight for the fused kernel?\n      self.register_buffer('fused_weight', None)\n      self.register_buffer('fused_bias', None)\n\n  In the forward, when in inference, we compute the fused_weight and fused_bias and then transpose the fused_weight to (K, N) and store it? But then we do it every forward? \n\nAlternatively, we can avoid the transpose by loading the weight matrix in a transposed manner in the kernel? \n\n  We can load the weight matrix by having a different stride pattern. We can use the stride of the original weight (which is (K, 1) for the last dimension) and then stride of K for the next row? \n\n  Actually, the original weight is (N, K). We want to load a block of size (BLOCK_SIZE_K, BLOCK_SIZE_N) from it. We can do:\n\n      b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n\n  where stride_bk = 1? and stride_bn = K? because moving to the next output feature (n) we skip K elements? \n\n  But wait: the weight matrix is stored in row-major: the first row is the first output feature (all K elements), then the second row, etc. So the stride between two consecutive features in the same column is K? \n\n  Actually, the element at (n, k) is at index n * K + k. So stride for n is K, stride for k is 1.\n\n  Therefore, we can set:\n      stride_bk = 1\n      stride_bn = K\n\n  Then the pointer for a block starting at (k_start, n_start) is:\n\n      b_ptr + k_start * stride_bk + n_start * stride_bn = b_ptr + k_start + n_start * K\n\n  And then within the block, we have:\n      b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n\n  which is: offs_k (a vector of size BLOCK_SIZE_K) and offs_n (a vector of size BLOCK_SIZE_N) -> a matrix of pointers of size (BLOCK_SIZE_K, BLOCK_SIZE_N)\n\n  Then we can load a block of the weight matrix in column-major order? Actually, we are loading a contiguous block in the original row-major order? \n\n  But note: we want to multiply a block of x (M, K) with a block of weight (K, N). The weight block we loaded is for a fixed range of k and n. However, the weight block we loaded is stored in row-major? But we want to use it in the multiplication as (K, N). We can use it as is? \n\n  The multiplication of two blocks: \n      a_block (BLOCK_SIZE_M, BLOCK_SIZE_K) \n      b_block (BLOCK_SIZE_K, BLOCK_SIZE_N)   [because we loaded it as (BLOCK_SIZE_K, BLOCK_SIZE_N) and it is stored in row-major? But actually, the loaded block is stored in the same order as the original: so the first row of the block is the first k elements of the first output feature in the block?]\n\n  We can use tl.dot(a_block, b_block) if we have the b_block in column-major? \n\n  Alternatively, we can transpose the b_block to (BLOCK_SIZE_N, BLOCK_SIZE_K) and then do a_block @ b_block_transposed? \n\n  But note: the dot product in Triton expects the second matrix to be in column-major? Actually, Triton's dot product supports row-major for both? \n\n  The Triton dot product: \n      c = tl.dot(a, b)\n\n  where a is (M, K) and b is (K, N) -> c (M, N). The b matrix is expected to be in column-major? Actually, no: the dot product in Triton is like any other: it multiplies row of a by column of b. The b matrix should be stored in row-major for efficient access? \n\n  But we have the b matrix stored in row-major: each row is a contiguous block. When we load a block of size (BLOCK_SIZE_K, BLOCK_SIZE_N) from the weight matrix (which is row-major), we get a block that is row-major: the first row in the block is the first k elements of the first output feature in the block? \n\n  Actually, we are loading a block that is not contiguous in memory? We are loading a block of size (BLOCK_SIZE_K, BLOCK_SIZE_N) from a row-major matrix. That is not contiguous because the matrix has K columns, and we are loading BLOCK_SIZE_K consecutive rows? But wait, we are loading a contiguous block in memory? \n\n  The block we are loading is a contiguous block of size (BLOCK_SIZE_K, BLOCK_SIZE_N) in memory? That is only true if we set the block in the row dimension to be 1? Because the next row is K elements away.\n\n  Actually, we cannot load a 2D block that is contiguous if the matrix is row-major and we want a block that spans multiple rows? \n\n  We have to use a different approach: we can load the weight matrix in a transposed way by having each program load a row of the weight matrix? That is not efficient.\n\nGiven the complexity, we will transpose the weight matrix once in the forward pass for inference. It is O(N*K) and N=1024, K=512, so 524288 elements, which is acceptable.\n\nSo we do:\n\n  fused_weight = fused_weight.t()   # now (in_features, out_features)\n\n  Then we can write a kernel that does:\n\n      a_ptr: points to x (M, K) in row-major\n      b_ptr: points to fused_weight (K, N) in row-major (each row is a contiguous block of N elements)\n\n  Then the kernel can load blocks of a and b easily.\n\n  We will write a standard Triton matmul kernel.\n\nWe will base it on the Triton matmul tutorial.\n\nWe will also add the bias.\n\nThe kernel:\n\n  @triton.jit\n  def fused_gemm_kernel(\n      # Pointers to matrices\n      a_ptr, b_ptr, c_ptr, bias_ptr,\n      # Matrix dimensions\n      M, N, K,\n      # The stride variables\n      stride_am, stride_ak,\n      stride_bk, stride_bn,\n      stride_cm, stride_cn,\n      # Meta-parameters\n      BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n      GROUP_SIZE_M: tl.constexpr,\n  ):\n      pid = tl.program_id(0)\n      num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n      num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n      num_pid_in_group = GROUP_SIZE_M * num_pid_n\n      group_id = pid // num_pid_in_group\n      first_pid_m = group_id * GROUP_SIZE_M\n      group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n      pid_m = first_pid_m + (pid % group_size_m)\n      pid_n = (pid % num_pid_in_group) // group_size_m\n\n      # [BLOCK_SIZE_M, BLOCK_SIZE_K]\n      offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n      offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n      offs_k = tl.arange(0, BLOCK_SIZE_K)\n      a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n      b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n      accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n      for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n          a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n          b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n          accumulator = tl.dot(a, b, accumulator)\n          a_ptrs += BLOCK_SIZE_K * stride_ak\n          b_ptrs += BLOCK_SIZE_K * stride_bk\n\n      # Add bias\n      if bias_ptr is not None:\n          bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n          accumulator += bias[None, :]\n\n      c = accumulator\n      offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n      offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n      c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n      mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n      tl.store(c_ptrs, c, mask=mask)\n\nThen we call this kernel in the forward for inference.\n\nBut note: the input x is (M, K) and the output is (M, N). We are using float32 for the accumulator? The original might be float32.\n\nWe will assume float32.\n\nHowever, the original model might be using float16? The problem does not specify. But the hardware has FP16 Tensor Core. We might want to use float16.\n\nBut the problem does not specify the dtype of the input. We will assume float32 for now.\n\nBut note: the example in the problem used FP16 Tensor Core. We might want to use float16 if the input is float16.\n\nGiven the complexity, we will assume float32.\n\nNow, we are going to change the code:\n\n  We will write the Triton kernel outside the class.\n\n  We will change the forward to use the kernel in inference.\n\n  For training, we use the original.\n\nBut note: the GroupNorm, GELU, etc. are still in PyTorch.\n\nWe hope that the fused GEMM in Triton will be faster than the two separate layers in PyTorch.\n\nBut note: the original linear layer is in PyTorch and is already fast. The BatchNorm is also fast. The fusion might save us one read/write of the intermediate tensor.\n\nWe will try.\n\nGiven the complexity and the fact that we are limited to 20 lines per revision, we will do the minimal change.\n\nWe will write the kernel and then call it in the forward.\n\nBut note: the kernel is long. We might exceed the 20 lines? The problem says \"Modify \u226420 lines per revision\". But we are adding a kernel and changing the forward. The kernel is new and not counted as modification? The problem says \"small iterative steps: Modify \u226420 lines per revision\". We are adding many lines for the kernel, but the modification to the existing code is only in the forward.\n\nWe will count the lines of change in the existing code (the class and functions) to be <= 20.\n\nLet's write the code.\n\nNote: We must not change the grid configuration? But we are defining it for the kernel. We are allowed.\n\nWe will also include the autotune for the kernel.\n\nBut note: the problem says we are allowed to do adaptive tuning.\n\nGiven the hardware (H100), we can use:\n\n  BLOCK_SIZE_M = 64, BLOCK_SIZE_N = 64, BLOCK_SIZE_K = 64, but we will let the autotune decide.\n\nWe will write the autotune with several configs.\n\nBut note: the problem says we must not change the grid configuration? We are defining it for the kernel, which is new.\n\nWe are safe.\n\nLet's code.\n\nIMPORTANT: We must not change the get_inputs and get_init_inputs? We are not changing them.\n\nWe are only changing the ModelNew class and adding the kernel.\n\nBut note: the problem says \"all other code is LOCKED\". We are only allowed to change the Triton kernel source within the block? But the entire block is the current program. We are allowed to change the entire block.\n\nWe will output the entire block with the new kernel and the modified ModelNew.\n\nDue to the complexity and length, we will only show the final code.\n\nNote: We are also to maintain the same inputs and outputs. The output of the forward is (batch_size, 1). We are not changing that.\n\nLet's write.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Triton kernel for fused GEMM + BatchNorm\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_bn_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N)\n        accumulator += bias[None, :]\n    \n    offs_m = tl.minimum(offs_m, M - 1)\n    offs_n = tl.minimum(offs_n, N - 1)\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def forward(self, x):\n        if not self.training:\n            if self.fused_weight is None or self.fused_bias is None:\n                with torch.no_grad():\n                    eps = self.batch_norm.eps\n                    running_var = self.batch_norm.running_var\n                    weight_bn = self.batch_norm.weight\n                    bias_bn = self.batch_norm.bias\n                    total_scale = weight_bn / torch.sqrt(running_var + eps)\n                    fused_bias = (self.gemm.bias * total_scale) + (bias_bn - self.batch_norm.running_mean * total_scale)\n                    self.fused_weight = (self.gemm.weight * total_scale.unsqueeze(1)).t().contiguous()\n                    self.fused_bias = fused_bias.contiguous()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[1]\n            output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_SIZE_M) * triton.cdiv(N, opt.BLOCK_SIZE_N),)\n            fused_gemm_bn_kernel[grid](\n                x, self.fused_weight, output, self.fused_bias,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                output.stride(0), output.stride(1),\n            )\n            x = output\n        else:\n            x = self.gemm(x)\n            x = self.batch_norm(x)\n        \n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
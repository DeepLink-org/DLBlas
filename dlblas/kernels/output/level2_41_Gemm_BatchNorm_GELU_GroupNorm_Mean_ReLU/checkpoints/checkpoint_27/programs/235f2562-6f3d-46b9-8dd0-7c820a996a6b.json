{"id": "235f2562-6f3d-46b9-8dd0-7c820a996a6b", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Optimized Triton kernel with FP16 Tensor Core utilization\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n        # New configurations for better tiling\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 512, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 512, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_bn_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Compute masks for M and N boundaries\n    mask_m = offs_m < M\n    mask_n = offs_n < N\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        mask_k = offs_k < k_remaining\n        # Apply full masks including M/N boundaries\n        mask_a = mask_m[:, None] & mask_k[None, :]\n        mask_b = mask_k[:, None] & mask_n[None, :]\n        # Use FP16 inputs with Tensor Core optimized dot product\n        a = tl.load(a_ptrs, mask=mask_a, other=0.0, dtype=tl.float16)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0, dtype=tl.float16)\n        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)\n        accumulator += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask = mask_m[:, None] & mask_n[None, :]\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def forward(self, x):\n        if not self.training:\n            if self.fused_weight is None or self.fused_bias is None:\n                with torch.no_grad():\n                    eps = self.batch_norm.eps\n                    running_var = self.batch_norm.running_var\n                    weight_bn = self.batch_norm.weight\n                    bias_bn = self.batch_norm.bias\n                    total_scale = weight_bn / torch.sqrt(running_var + eps)\n                    fused_bias = (self.gemm.bias * total_scale) + (bias_bn - self.batch_norm.running_mean * total_scale)\n                    self.fused_weight = (self.gemm.weight * total_scale.unsqueeze(1)).t().contiguous()\n                    self.fused_bias = fused_bias.contiguous()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[1]\n            output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_SIZE_M) * triton.cdiv(N, opt.BLOCK_SIZE_N),)\n            fused_gemm_bn_kernel[grid](\n                x, self.fused_weight, output, self.fused_bias,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                output.stride(0), output.stride(1),\n            )\n            x = output\n        else:\n            x = self.gemm(x)\n            x = self.batch_norm(x)\n        \n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "e96b1253-b727-470c-ab3e-5e2a59bb8fa2", "generation": 3, "timestamp": 1754575253.010107, "iteration_found": 15, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.915, "runtime_ms": 0.13, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.119, "speed_up": 0.915, "custom_timing": 0.13}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.922, "runtime_ms": 0.129, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.119, "speed_up": 0.922, "custom_timing": 0.129}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.9220\n- runtime_ms: 0.1290\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.922, 'custom_timing': 0.129}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9220, runtime_ms: 0.1290, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.922, 'custom_timing': 0.129}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.6040, runtime_ms: 0.1970, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.604, 'custom_timing': 0.197}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.6760, runtime_ms: 0.1760, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.676, 'custom_timing': 0.176}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride values\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    offs_cm = offs_m[:, None]\n    offs_cn = offs_n[None, :]\n    c_ptrs = c_ptr + stride_cm * offs_cm + stride_cn * offs_cn\n    tl.store(c_ptrs, accumulator, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\ndef triton_gemm(a, b, bias=None):\n    assert a.dim() == 2 and b.dim() == 2\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    gemm_kernel[grid](\n        a, b, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        \n        # Initialize parameters\n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)  # Fixed: replaced math.sqrt(5)\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / (fan_in**0.5) if fan_in > 0 else 0  # Fixed: replaced math.sqrt\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        # Pre-transpose weight for efficient access\n        weight_t = self.weight.t().contiguous()\n        x = triton_gemm(x, weight_t, self.bias)\n        x = self.batch_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.6760), Performs well on runtime_ms (0.1760), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.676, 'custom_timing': 0.176})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=8, num_stages=3),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    mask_m = offs_m < M\n    mask_n = offs_n < N\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        mask_k = (k * BLOCK_SIZE_K + offs_k) < K\n        a_mask = mask_m[:, None] & mask_k[None, :]\n        b_mask = mask_k[:, None] & mask_n[None, :]\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)\n        accumulator += bias[None, :]\n    \n    c_ptrs = c_ptr + stride_cm * offs_m[:, None] + stride_cn * offs_n[None, :]\n    c_mask = mask_m[:, None] & mask_n[None, :]\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\ndef triton_gemm(a, b, bias=None):\n    assert a.dim() == 2 and b.dim() == 2\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    gemm_kernel[grid](\n        a, b, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1)\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        \n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / (fan_in**0.5) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        weight_t = self.weight.t().contiguous()\n        x = triton_gemm(x, weight_t, self.bias)\n        x = self.batch_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.6040), Performs well on runtime_ms (0.1970), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.604, 'custom_timing': 0.197})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Optimized Triton kernel with FP16 Tensor Core utilization\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_bn_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        mask_k = offs_k < k_remaining\n        # Use FP16 inputs with Tensor Core optimized dot product\n        a = tl.load(a_ptrs, mask=mask_k[None, :], other=0.0, dtype=tl.float16)\n        b = tl.load(b_ptrs, mask=mask_k[:, None], other=0.0, dtype=tl.float16)\n        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N)\n        accumulator += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def forward(self, x):\n        if not self.training:\n            if self.fused_weight is None or self.fused_bias is None:\n                with torch.no_grad():\n                    eps = self.batch_norm.eps\n                    running_var = self.batch_norm.running_var\n                    weight_bn = self.batch_norm.weight\n                    bias_bn = self.batch_norm.bias\n                    total_scale = weight_bn / torch.sqrt(running_var + eps)\n                    fused_bias = (self.gemm.bias * total_scale) + (bias_bn - self.batch_norm.running_mean * total_scale)\n                    self.fused_weight = (self.gemm.weight * total_scale.unsqueeze(1)).t().contiguous()\n                    self.fused_bias = fused_bias.contiguous()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[1]\n            output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_SIZE_M) * triton.cdiv(N, opt.BLOCK_SIZE_N),)\n            fused_gemm_bn_kernel[grid](\n                x, self.fused_weight, output, self.fused_bias,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                output.stride(0), output.stride(1),\n            )\n            x = output\n        else:\n            x = self.gemm(x)\n            x = self.batch_norm(x)\n        \n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9220), Performs well on runtime_ms (0.1290), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.922, 'custom_timing': 0.129})\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride values\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    offs_cm = offs_m[:, None]\n    offs_cn = offs_n[None, :]\n    c_ptrs = c_ptr + stride_cm * offs_cm + stride_cn * offs_cn\n    tl.store(c_ptrs, accumulator, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\ndef triton_gemm(a, b, bias=None):\n    assert a.dim() == 2 and b.dim() == 2\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    gemm_kernel[grid](\n        a, b, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        \n        # Initialize parameters\n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)  # Fixed: replaced math.sqrt(5)\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / (fan_in**0.5) if fan_in > 0 else 0  # Fixed: replaced math.sqrt\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        # Pre-transpose weight for efficient access\n        weight_t = self.weight.t().contiguous()\n        x = triton_gemm(x, weight_t, self.bias)\n        x = self.batch_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Optimized Triton kernel with FP16 Tensor Core utilization\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_bn_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        mask_k = offs_k < k_remaining\n        # Use FP16 inputs with Tensor Core optimized dot product\n        a = tl.load(a_ptrs, mask=mask_k[None, :], other=0.0, dtype=tl.float16)\n        b = tl.load(b_ptrs, mask=mask_k[:, None], other=0.0, dtype=tl.float16)\n        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N)\n        accumulator += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def forward(self, x):\n        if not self.training:\n            if self.fused_weight is None or self.fused_bias is None:\n                with torch.no_grad():\n                    eps = self.batch_norm.eps\n                    running_var = self.batch_norm.running_var\n                    weight_bn = self.batch_norm.weight\n                    bias_bn = self.batch_norm.bias\n                    total_scale = weight_bn / torch.sqrt(running_var + eps)\n                    fused_bias = (self.gemm.bias * total_scale) + (bias_bn - self.batch_norm.running_mean * total_scale)\n                    self.fused_weight = (self.gemm.weight * total_scale.unsqueeze(1)).t().contiguous()\n                    self.fused_bias = fused_bias.contiguous()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[1]\n            output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_SIZE_M) * triton.cdiv(N, opt.BLOCK_SIZE_N),)\n            fused_gemm_bn_kernel[grid](\n                x, self.fused_weight, output, self.fused_bias,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                output.stride(0), output.stride(1),\n            )\n            x = output\n        else:\n            x = self.gemm(x)\n            x = self.batch_norm(x)\n        \n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program, which uses a fused GEMM and batch normalization kernel for inference (when not training).\n The current speedup is 0.922 (which is less than 1, meaning it's slower than the reference). We need to improve performance.\n\n The current kernel uses FP16 for the matrix multiplication (with Tensor Cores) and accumulates in FP32.\n However, note that the input and weight tensors are in FP16? Actually, the example uses `dtype=x.dtype` for the output, and the input `x` is from `get_inputs` which is `torch.randn` (FP32 by default). But in the kernel, we are loading as `tl.float16`.\n\n Let's check the current program:\n   - The kernel loads `a` and `b` as `tl.float16` but the accumulator is in `tl.float32`.\n   - The input tensor `x` is from `torch.randn(batch_size, in_features)` -> which is FP32.\n\n So we are converting FP32 to FP16 on the fly? That might be costly and also we are storing the result as FP32.\n\n However, the Triton kernel does not support mixed input types for `tl.dot` (both must be the same). We have two options:\n   Option 1: Convert the input to FP16 before passing to the kernel and adjust the model to use FP16 for the entire computation.\n   Option 2: Keep the kernel in FP32 and use FP32 for the dot product (but then we cannot use Tensor Cores for FP16).\n\n But note: the current program uses a `ModelNew` that has an `nn.Linear` and `nn.BatchNorm1d` that are in FP32. The fusion is only for inference.\n\n The reference implementation (PyTorch) is in FP32. We must match the numerical precision.\n\n How about we change the kernel to use FP32 for the dot product? But then we lose the Tensor Core acceleration for FP16.\n\n Alternatively, we can pre-convert the input and weights to FP16? But the reference uses FP32 and we must match.\n\n However, the problem says: \"Output must match PyTorch reference within float32 tolerance\". So we can do the entire computation in FP16? But that might reduce precision.\n\n Actually, the current kernel does the dot product in FP16 (with FP32 accumulation) and then adds the bias in FP32. This is standard practice and should be within tolerance.\n\n Why is it slow then?\n\n Let's look at the kernel:\n\n   - It uses an autotuner with several configurations.\n   - The loop is over `k in range(0, K, BLOCK_SIZE_K)`, and inside we load a block of A and B and do a dot product.\n\n Another issue: the current kernel does not use shared memory for blocking and may not be optimized for memory access.\n\n Steps to improve:\n\n 1. We note that the current kernel does not have any shared memory (software-managed cache) for reusing data. We can try to use shared memory to reduce global memory accesses.\n\n 2. We can also try to adjust the block sizes to better fit the GPU. The current autotune configurations are provided, but maybe we can add more or change the existing ones.\n\n 3. Another idea: use vectorized loads and stores to reduce the number of memory operations.\n\n 4. Also, note that the kernel currently loads the entire bias vector for each block. This might be inefficient. But the bias is stored in global memory and we load it once per column.\n\n However, the current kernel does:\n\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N)\n        accumulator += bias[None, :]\n\n This is a broadcasted load and should be efficient.\n\n 5. We are not using any shared memory to buffer the blocks of A and B. Let's try to add that.\n\n How the kernel works without shared memory? It loads directly from global memory. For Tensor Cores, it is important to have contiguous and aligned memory accesses.\n\n 6. We can also try to increase the block sizes to improve the arithmetic intensity.\n\n Given the constraints (we cannot change the function signature or the grid configuration), we can try to optimize within the kernel.\n\n Let's restructure the kernel to use shared memory for blocks of A and B.\n\n However, note: Triton does not have explicit shared memory. Instead, we can use `tl.cache` to cache the blocks in shared memory? Actually, in Triton we can use `tl.make_block_ptr` and `tl.advance` for efficient memory access, but that is for version 2.1.0. Alternatively, we can use `tl.load` with `cache_modifier` to cache in shared memory.\n\n But the current Triton version in the environment might not support that. Let's stick to the basics.\n\n Alternatively, we can try to use the `fp16_acc=True` option in `tl.dot` to use Tensor Cores with FP32 accumulation? Actually, the current `tl.dot` already uses Tensor Cores when the inputs are FP16 and the output is FP32.\n\n After analysis, the main issue might be the block sizes and the autotune configurations.\n\n Since the current program already uses autotune, we might try to adjust the configurations to better fit the problem size.\n\n The problem size: M = batch_size (128), N = out_features (1024), K = in_features (512).\n\n We are using a grid of (ceil(M / BLOCK_SIZE_M) * ceil(N / BLOCK_SIZE_N), ...). For M=128, N=1024, K=512.\n\n We note that 128 is a small dimension. We should choose BLOCK_SIZE_M that divides 128. Similarly, BLOCK_SIZE_N should divide 1024? Not necessarily, but it helps.\n\n Let me try to design configurations that are more aligned with the problem:\n\n   - BLOCK_SIZE_M: 128 (so that we have only one block in M dimension) and BLOCK_SIZE_N: 256, then we have 1024/256=4 blocks in N -> total 4 blocks.\n\n   - BLOCK_SIZE_M: 128, BLOCK_SIZE_N: 128 -> then 1024/128=8 blocks.\n\n   - BLOCK_SIZE_M: 64, then we have 2 blocks in M -> then with BLOCK_SIZE_N=256, we have 4 blocks in N -> total 8 blocks.\n\n We can also adjust BLOCK_SIZE_K to be 64 or 128 to reduce the number of steps.\n\n Since the K dimension is 512, we want to split it into chunks of BLOCK_SIZE_K.\n\n We are going to add new configurations that are tailored to the problem size.\n\n But note: the autotuner is already provided with several configurations. However, the current best configuration might not be optimal.\n\n Alternatively, we can try to use a different tiling strategy: use a larger block for K to reduce the outer loop.\n\n However, the kernel is currently written to load a block of K at a time.\n\n Another idea: use the `num_warps` and `num_stages` more appropriately. The H100 has more warps and we can increase the number of warps to 8 or 16.\n\n But the current configurations already use up to 8 warps.\n\n Considering the above, I propose to:\n\n  1. Change the kernel to use vectorized loads. We can load multiple elements at a time to reduce the number of memory instructions.\n\n  2. Increase the block sizes to improve arithmetic intensity.\n\n  3. Add new autotune configurations that are more tailored to the problem (M=128, N=1024, K=512).\n\n However, note that the kernel must work for any M, N, K? The autotune key uses ['M','N','K'] so it will choose the best for the given size.\n\n Let's try to add a configuration that uses:\n   - BLOCK_SIZE_M=128, BLOCK_SIZE_N=256, BLOCK_SIZE_K=64   (already present)\n   - BLOCK_SIZE_M=128, BLOCK_SIZE_N=512, BLOCK_SIZE_K=32   (new) -> then we have 2 blocks in N (1024/512=2) and 512/32=16 steps in K.\n\n   - BLOCK_SIZE_M=128, BLOCK_SIZE_N=1024, BLOCK_SIZE_K=16  -> one block in N, 32 steps in K.\n\n But note: the block size in N cannot exceed the maximum block size (which is 1024? but the GPU has constraints). Also, the number of threads per block is (BLOCK_SIZE_M * BLOCK_SIZE_N) / (number of elements per thread). Actually, the kernel uses 2D blocks? No, the kernel is 1D in program_id, but the block is 2D in M and N.\n\n The kernel uses:\n   offs_m = ... of size BLOCK_SIZE_M\n   offs_n = ... of size BLOCK_SIZE_N\n   offs_k = ... of size BLOCK_SIZE_K\n\n The total number of threads per block is (BLOCK_SIZE_M * BLOCK_SIZE_N) / (number of elements per thread) but actually, we are using a single thread to compute a whole block? No, the kernel is written in a way that each thread block (in the CUDA sense) handles a block of size BLOCK_SIZE_M x BLOCK_SIZE_N. The inner loop over K is sequential per thread block.\n\n How is the parallelism? The grid has one element per (pid_m, pid_n) which corresponds to a block in the output matrix.\n\n The kernel is launched with a 1D grid of size (ceil(M/BLOCK_SIZE_M) * ceil(N/BLOCK_SIZE_N)).\n\n Each thread block (in Triton) is responsible for a block of size BLOCK_SIZE_M x BLOCK_SIZE_N. The inner loop over K is done sequentially within the thread block.\n\n Therefore, the total number of threads per block is (BLOCK_SIZE_M * BLOCK_SIZE_N) / (number of elements each thread computes). But in the current code, each thread computes one element? No, actually the kernel uses a 2D block of threads? No, it uses 1D program_id and then uses vector operations over BLOCK_SIZE_M and BLOCK_SIZE_N.\n\n Actually, the kernel is written for a single thread per output block? That would be inefficient because BLOCK_SIZE_M and BLOCK_SIZE_N are large (like 128x256=32768 elements). That cannot be.\n\n I see: the kernel uses `tl.arange(0, BLOCK_SIZE_M)` and `tl.arange(0, BLOCK_SIZE_N)`. This means that the entire block is processed by a single thread? That is not possible. I think there is a misunderstanding.\n\n Actually, in Triton, the kernel is executed by a block of threads. The `tl.arange` creates a vector of indices, and the operations are vectorized. The Triton compiler then maps these vectors to threads.\n\n But the current kernel does not use `pid` to split the work within a block. It uses the program_id to determine the output block, and then the entire output block is processed by one Triton program (which is mapped to a thread block). The inner loop over K is done by the entire thread block.\n\n How about we change the kernel to use two levels of blocking? That is, split the output block into smaller tiles that are assigned to different threads? But that would require a different kernel structure.\n\n Alternatively, we can rely on Triton's automatic parallelization within the block. But the current code is standard.\n\n Given the complexity, and since we are only allowed to change the Triton kernel and the autotune configurations, we will focus on:\n\n  a) Adding more autotune configurations that are tailored to the problem (M=128, N=1024, K=512).\n\n  b) Using vectorized loads.\n\n  c) Prefetching the next block of A and B to hide memory latency.\n\n However, note that the current kernel already does:\n\n        a = tl.load(a_ptrs, mask=mask_k[None, :], other=0.0, dtype=tl.float16)\n        b = tl.load(b_ptrs, mask=mask_k[:, None], other=0.0, dtype=tl.float16)\n\n  This is a vector load for the entire block. But we can try to split the block into smaller chunks and use multiple threads to load? That is already handled by Triton.\n\n After careful thought, I propose to try the following:\n\n  1. Add configurations that have larger BLOCK_SIZE_N (like 512, 1024) to reduce the number of blocks and increase the work per block.\n\n  2. Also, try to set BLOCK_SIZE_M to 128 (the entire M dimension) to have one block in M.\n\n  3. We can also try to use a larger BLOCK_SIZE_K to reduce the number of steps.\n\n  4. Additionally, we can try to use the `num_warps=8` and `num_stages=4` for these large blocks.\n\n Let me design new configurations:\n\n   - Config 1: BLOCK_SIZE_M=128, BLOCK_SIZE_N=1024, BLOCK_SIZE_K=64, num_warps=8, num_stages=4\n        Then: steps in K: ceil(512/64)=8, and one block in M and one in N -> total 1 block.\n\n   - Config 2: BLOCK_SIZE_M=128, BLOCK_SIZE_N=512, BLOCK_SIZE_K=64, num_warps=8, num_stages=4 -> 2 blocks in N, total 2 blocks.\n\n   - Config 3: BLOCK_SIZE_M=128, BLOCK_SIZE_N=256, BLOCK_SIZE_K=64, num_warps=8, num_stages=4 -> 4 blocks.\n\n   - Config 4: BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=64, num_warps=8, num_stages=4 -> 8 blocks.\n\n   - Config 5: BLOCK_SIZE_M=64, BLOCK_SIZE_N=1024, BLOCK_SIZE_K=64, num_warps=8, num_stages=4 -> 2 blocks in M, 1 in N -> total 2 blocks.\n\n   - We also keep the existing ones.\n\n 5. We also note that the kernel currently uses `mask_k` for the load. This mask is computed as `offs_k < k_remaining`. We can precompute the mask for the entire iteration? Actually, it is already done per step.\n\n 6. We can try to use the `eviction_policy` in `tl.load` to cache the data in L2? But that is not available in the current Triton version? We don't know.\n\n 7. We can also try to use the `libdevice` for the square root in the batch norm fusion? But that is done on the host in the `forward` function.\n\n Implementation:\n\n   - We will change the autotune configs to include the new configurations.\n\n   - We will also try to vectorize the loads? Actually, the `tl.load` for a block is already vectorized by Triton.\n\n   - We will change the kernel to use a larger block size for K? That is already in the autotune.\n\n   - We will also try to increase the number of warps and stages.\n\n But note: the kernel must be general for any M, N, K? The autotune will choose the best configuration for the given M, N, K.\n\n However, the current M, N, K are fixed in the problem? Actually, the `get_inputs` returns a tensor of (128,512). But the model is initialized with (in_features, out_features) = (512,1024). So the kernel is called with M=128, N=1024, K=512.\n\n So we can tune specifically for that.\n\n Let's update the autotune configs:\n\n   We will replace the current configs with:\n\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 512, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 512, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n        ... and so on.\n\n But note: the kernel must also work for other sizes? The autotune will pick the best for the given size.\n\n However, the problem says the dataset has 250 kernelbench files. So we don't know the exact sizes. But the current benchmark is for (128,512) and (512,1024). We can hope that these configs are good for other sizes too.\n\n Alternatively, we can keep some of the old configs for generality.\n\n We will try to replace the autotune configs with a set that includes both large and small block sizes, with a focus on the current problem.\n\n Since we are limited to 20 lines of change, we will only change the autotune configs and try to adjust the kernel to use a larger BLOCK_SIZE_K when possible.\n\n But note: the kernel loop is:\n\n        for k in range(0, K, BLOCK_SIZE_K):\n\n This is independent of the autotune config.\n\n We will change the autotune configs to the following:\n\n    configs=[\n        # New configs tailored for M=128, N=1024, K=512\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 512, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 512, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        # Also include some with smaller K block\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 512, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 512, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n    ],\n\n But note: the kernel also has a constraint on the total number of threads per block? The BLOCK_SIZE_M * BLOCK_SIZE_N must be divisible by the number of warps? Actually, Triton handles that.\n\n However, the number of elements in a block is (BLOCK_SIZE_M * BLOCK_SIZE_N) and that must be processed by the thread block. If the block is too large, it might not fit in the GPU.\n\n The H100 has a maximum threads per block of 1024? Actually, Triton abstracts this. But we must be cautious.\n\n Since we are using a vectorized approach, the block sizes are not the number of threads. The number of threads is determined by the Triton compiler.\n\n We will try the above.\n\n Alternatively, we can also try to use a smaller BLOCK_SIZE_M and BLOCK_SIZE_N and a larger BLOCK_SIZE_K to reduce the number of steps.\n\n Given the time, we will change the autotune configs and hope that one of them gives better performance.\n\n We also note that the current kernel might have an issue: the mask for `a` and `b` is computed as:\n\n        mask_k = offs_k < k_remaining\n        a_mask = mask_m[:, None] & mask_k[None, :]\n        b_mask = mask_k[:, None] & mask_n[None, :]\n\n But the current kernel does not have `mask_m` and `mask_n` defined. Actually, the current kernel does:\n\n        mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n\n but that is only for the store. For the loads, we only use `mask_k`? And we don't check the boundaries in M and N for the loads? That is a bug!\n\n Actually, the kernel does:\n\n        a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n        b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n        ... inside the loop:\n        a = tl.load(a_ptrs, mask=mask_k[None, :], ...)   -> but we should also mask by the row (M) and column (N) boundaries?\n\n The problem: if `offs_m` goes beyond M, then we are reading out of bounds. Similarly for `offs_n` in B.\n\n So we should have:\n\n        mask_a = (offs_m[:, None] < M) & (mask_k[None, :])\n        mask_b = (mask_k[:, None]) & (offs_n[None, :] < N)\n\n But the current kernel does not do that. This might be the reason for correctness issues? But the current program passed correctness.\n\n However, the current program might have passed because in the benchmark M=128 and we set BLOCK_SIZE_M=128, so offs_m is [0..127] which is <128. But if we use a smaller block, then we might have boundary issues.\n\n We must fix this.\n\n We will change the kernel to use:\n\n        mask_a = (offs_m[:, None] < M) & (mask_k[None, :])\n        mask_b = (mask_k[:, None]) & (offs_n[None, :] < N)\n\n and then:\n\n        a = tl.load(a_ptrs, mask=mask_a, other=0.0, dtype=tl.float16)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0, dtype=tl.float16)\n\n This will fix any potential boundary issues.\n\n But note: the current kernel does not have `offs_m` and `offs_n` defined at the load? Actually, they are defined at the beginning and do not change. So we can compute:\n\n        mask_m = offs_m < M\n        mask_n = offs_n < N\n\n and then:\n\n        mask_a = mask_m[:, None] & mask_k[None, :]\n        mask_b = mask_k[:, None] & mask_n[None, :]\n\n This is more efficient.\n\n We will add this.\n\n So the plan:\n\n  1. Add the mask for M and N dimensions in the loads of A and B.\n  2. Change the autotune configs to the new set.\n\n We hope that the combination of better block sizes and correct masking will improve performance and maintain correctness.\n\n Let's rewrite the kernel accordingly.\n\n However, note that the kernel already has:\n\n        mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n\n for the store. We can compute `mask_m` and `mask_n` at the beginning.\n\n We will add:\n\n        mask_m = offs_m < M\n        mask_n = offs_n < N\n\n and then use them in the loop.\n\n Code changes:\n\n        pid = tl.program_id(0)\n        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n        pid_m = pid // num_pid_n\n        pid_n = pid % num_pid_n\n\n        offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n        mask_m = offs_m < M\n        mask_n = offs_n < N\n\n        a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n        b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, K, BLOCK_SIZE_K):\n            k_remaining = K - k\n            mask_k = offs_k < k_remaining\n            mask_a = mask_m[:, None] & mask_k[None, :]\n            mask_b = mask_k[:, None] & mask_n[None, :]\n\n            a = tl.load(a_ptrs, mask=mask_a, other=0.0, dtype=tl.float16)\n            b = tl.load(b_ptrs, mask=mask_b, other=0.0, dtype=tl.float16)\n            accumulator += tl.dot(a, b, out_dtype=tl.float32)\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n\n        ... rest unchanged.\n\n We must do this to ensure correctness.\n\n Given the above, we will rewrite the kernel and the autotune configs.\n\n Since the current program is long, we will only show the changed parts.\n\n Let's write the entire kernel and autotune with the changes.\n\n But note: the current program in the EVOLVE-BLOCK is the one labeled as \"Current Program\". We are going to change the kernel and the autotune configs.\n\n We are allowed to change up to 20 lines? We are changing about 20 lines.\n\n Let's do it.\n</think>\nBased on the analysis, I'll optimize the Triton kernel by:\n1. Adding proper boundary checks for M/N dimensions in the load operations\n2. Expanding the autotune configurations to include larger block sizes\n3. Improving mask handling for better correctness and performance\n\nHere's the optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Optimized Triton kernel with FP16 Tensor Core utilization\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n        # New configurations for better tiling\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 512, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 512, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_bn_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Compute masks for M and N boundaries\n    mask_m = offs_m < M\n    mask_n = offs_n < N\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        mask_k = offs_k < k_remaining\n        # Apply full masks including M/N boundaries\n        mask_a = mask_m[:, None] & mask_k[None, :]\n        mask_b = mask_k[:, None] & mask_n[None, :]\n        # Use FP16 inputs with Tensor Core optimized dot product\n        a = tl.load(a_ptrs, mask=mask_a, other=0.0, dtype=tl.float16)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0, dtype=tl.float16)\n        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)\n        accumulator += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask = mask_m[:, None] & mask_n[None, :]\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def forward(self, x):\n        if not self.training:\n            if self.fused_weight is None or self.fused_bias is None:\n                with torch.no_grad():\n                    eps = self.batch_norm.eps\n                    running_var = self.batch_norm.running_var\n                    weight_bn = self.batch_norm.weight\n                    bias_bn = self.batch_norm.bias\n                    total_scale = weight_bn / torch.sqrt(running_var + eps)\n                    fused_bias = (self.gemm.bias * total_scale) + (bias_bn - self.batch_norm.running_mean * total_scale)\n                    self.fused_weight = (self.gemm.weight * total_scale.unsqueeze(1)).t().contiguous()\n                    self.fused_bias = fused_bias.contiguous()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[1]\n            output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_SIZE_M) * triton.cdiv(N, opt.BLOCK_SIZE_N),)\n            fused_gemm_bn_kernel[grid](\n                x, self.fused_weight, output, self.fused_bias,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                output.stride(0), output.stride(1),\n            )\n            x = output\n        else:\n            x = self.gemm(x)\n            x = self.batch_norm(x)\n        \n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\nKey improvements made:\n1. Added proper mask handling for M/N dimensions in load operations\n2. Expanded autotune configurations with larger block sizes (up to 1024) \n3. Optimized mask computation for better boundary handling\n4. Maintained FP16 Tensor Core utilization while ensuring correctness\n5. Kept all other functionality identical to original implementation\n\nThe changes focus on improving memory access patterns and better utilizing the H100's Tensor Cores through optimized tiling configurations."]}}}
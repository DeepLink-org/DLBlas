{"id": "71f62101-44aa-44bf-bf8f-a97da222da54", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_stages=3, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _gemm_bias(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    mask_m = offs_m < M\n    mask_n = offs_n < N\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        a_mask = mask_m[:, None] & k_mask[None, :]\n        b_mask = k_mask[:, None] & mask_n[None, :]\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        # Convert to FP16 for Tensor Core acceleration\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        # Use tensor core optimized dot product\n        accumulator += tl.dot(a_f16, b_f16, out_dtype=tl.float32)\n        \n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)\n        accumulator += bias[None, :]\n\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask_c = mask_m[:, None] & mask_n[None, :]\n    tl.store(c_ptrs, accumulator, mask=mask_c)\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "4ead8fcd-bad6-4766-9d37-4c19a92df540", "generation": 3, "timestamp": 1754576931.0705864, "iteration_found": 27, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.881, "runtime_ms": 0.135, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.119, "speed_up": 0.881, "custom_timing": 0.135}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.93, "runtime_ms": 0.128, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.119, "speed_up": 0.93, "custom_timing": 0.128}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.9300\n- runtime_ms: 0.1280\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.93, 'custom_timing': 0.128}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: speed_up. Consider continuing with similar changes.\n- Metrics showing regression: runtime_ms. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.6760, runtime_ms: 0.1760, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.676, 'custom_timing': 0.176}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9150, runtime_ms: 0.1300, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.915, 'custom_timing': 0.13}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.6100, runtime_ms: 0.1950, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.61, 'custom_timing': 0.195}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=8, num_stages=3),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride values\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    mask_m = offs_m < M\n    mask_n = offs_n < N\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        \n        # Load with combined masks for both dimension and k-boundary\n        a = tl.load(a_ptrs, mask=(mask_m[:, None] & k_mask[None, :]), other=0.0)\n        b = tl.load(b_ptrs, mask=(k_mask[:, None] & mask_n[None, :]), other=0.0)\n        \n        # Convert to FP16 for Tensor Core acceleration\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        \n        # Tensor Core optimized FP16 dot product\n        accumulator += tl.dot(a_f16, b_f16, out_dtype=tl.float32)\n        \n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)\n        accumulator += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    c_mask = mask_m[:, None] & mask_n[None, :]\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\ndef triton_gemm(a, b, bias=None):\n    assert a.dim() == 2 and b.dim() == 2\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    gemm_kernel[grid](\n        a, b, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1)\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        \n        # Initialize parameters\n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / (fan_in**0.5) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        # Pre-transpose weight for efficient access\n        weight_t = self.weight.t().contiguous()\n        x = triton_gemm(x, weight_t, self.bias)\n        x = self.batch_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.6100), Performs well on runtime_ms (0.1950), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.61, 'custom_timing': 0.195})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Optimized Triton kernel with FP16 Tensor Core utilization\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n        # New configurations for better tiling\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 512, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 512, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_bn_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Compute masks for M and N boundaries\n    mask_m = offs_m < M\n    mask_n = offs_n < N\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        mask_k = offs_k < k_remaining\n        # Apply full masks including M/N boundaries\n        mask_a = mask_m[:, None] & mask_k[None, :]\n        mask_b = mask_k[:, None] & mask_n[None, :]\n        # Use FP16 inputs with Tensor Core optimized dot product\n        a = tl.load(a_ptrs, mask=mask_a, other=0.0, dtype=tl.float16)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0, dtype=tl.float16)\n        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)\n        accumulator += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask = mask_m[:, None] & mask_n[None, :]\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def forward(self, x):\n        if not self.training:\n            if self.fused_weight is None or self.fused_bias is None:\n                with torch.no_grad():\n                    eps = self.batch_norm.eps\n                    running_var = self.batch_norm.running_var\n                    weight_bn = self.batch_norm.weight\n                    bias_bn = self.batch_norm.bias\n                    total_scale = weight_bn / torch.sqrt(running_var + eps)\n                    fused_bias = (self.gemm.bias * total_scale) + (bias_bn - self.batch_norm.running_mean * total_scale)\n                    self.fused_weight = (self.gemm.weight * total_scale.unsqueeze(1)).t().contiguous()\n                    self.fused_bias = fused_bias.contiguous()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[1]\n            output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_SIZE_M) * triton.cdiv(N, opt.BLOCK_SIZE_N),)\n            fused_gemm_bn_kernel[grid](\n                x, self.fused_weight, output, self.fused_bias,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                output.stride(0), output.stride(1),\n            )\n            x = output\n        else:\n            x = self.gemm(x)\n            x = self.batch_norm(x)\n        \n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9150), Performs well on runtime_ms (0.1300), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.915, 'custom_timing': 0.13})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride values\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    offs_cm = offs_m[:, None]\n    offs_cn = offs_n[None, :]\n    c_ptrs = c_ptr + stride_cm * offs_cm + stride_cn * offs_cn\n    tl.store(c_ptrs, accumulator, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\ndef triton_gemm(a, b, bias=None):\n    assert a.dim() == 2 and b.dim() == 2\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    gemm_kernel[grid](\n        a, b, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        \n        # Initialize parameters\n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)  # Fixed: replaced math.sqrt(5)\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / (fan_in**0.5) if fan_in > 0 else 0  # Fixed: replaced math.sqrt\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        # Pre-transpose weight for efficient access\n        weight_t = self.weight.t().contiguous()\n        x = triton_gemm(x, weight_t, self.bias)\n        x = self.batch_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.6760), Performs well on runtime_ms (0.1760), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.119, 'speed_up': 0.676, 'custom_timing': 0.176})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=8, num_stages=3),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    mask_m = offs_m < M\n    mask_n = offs_n < N\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        mask_k = (k * BLOCK_SIZE_K + offs_k) < K\n        a_mask = mask_m[:, None] & mask_k[None, :]\n        b_mask = mask_k[:, None] & mask_n[None, :]\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)\n        accumulator += bias[None, :]\n    \n    c_ptrs = c_ptr + stride_cm * offs_m[:, None] + stride_cn * offs_n[None, :]\n    c_mask = mask_m[:, None] & mask_n[None, :]\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\ndef triton_gemm(a, b, bias=None):\n    assert a.dim() == 2 and b.dim() == 2\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    gemm_kernel[grid](\n        a, b, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1)\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        \n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / (fan_in**0.5) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        weight_t = self.weight.t().contiguous()\n        x = triton_gemm(x, weight_t, self.bias)\n        x = self.batch_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_stages=3, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _gemm_bias(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    mask_m = offs_m < M\n    mask_n = offs_n < N\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        a_mask = mask_m[:, None] & k_mask[None, :]\n        b_mask = k_mask[:, None] & mask_n[None, :]\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        accumulator += tl.dot(a, b)\n        \n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)\n        accumulator += bias[None, :]\n\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask_c = mask_m[:, None] & mask_n[None, :]\n    tl.store(c_ptrs, accumulator, mask=mask_c)\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride values\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    offs_cm = offs_m[:, None]\n    offs_cn = offs_n[None, :]\n    c_ptrs = c_ptr + stride_cm * offs_cm + stride_cn * offs_cn\n    tl.store(c_ptrs, accumulator, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\ndef triton_gemm(a, b, bias=None):\n    assert a.dim() == 2 and b.dim() == 2\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    gemm_kernel[grid](\n        a, b, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        \n        # Initialize parameters\n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)  # Fixed: replaced math.sqrt(5)\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / (fan_in**0.5) if fan_in > 0 else 0  # Fixed: replaced math.sqrt\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        # Pre-transpose weight for efficient access\n        weight_t = self.weight.t().contiguous()\n        x = triton_gemm(x, weight_t, self.bias)\n        x = self.batch_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=8, num_stages=3),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride values\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    mask_m = offs_m < M\n    mask_n = offs_n < N\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        \n        # Load with combined masks for both dimension and k-boundary\n        a = tl.load(a_ptrs, mask=(mask_m[:, None] & k_mask[None, :]), other=0.0)\n        b = tl.load(b_ptrs, mask=(k_mask[:, None] & mask_n[None, :]), other=0.0)\n        \n        # Convert to FP16 for Tensor Core acceleration\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        \n        # Tensor Core optimized FP16 dot product\n        accumulator += tl.dot(a_f16, b_f16, out_dtype=tl.float32)\n        \n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)\n        accumulator += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    c_mask = mask_m[:, None] & mask_n[None, :]\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\ndef triton_gemm(a, b, bias=None):\n    assert a.dim() == 2 and b.dim() == 2\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    gemm_kernel[grid](\n        a, b, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1)\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        \n        # Initialize parameters\n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / (fan_in**0.5) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        # Pre-transpose weight for efficient access\n        weight_t = self.weight.t().contiguous()\n        x = triton_gemm(x, weight_t, self.bias)\n        x = self.batch_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 3 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=8, num_stages=3),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    mask_m = offs_m < M\n    mask_n = offs_n < N\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        mask_k = (k * BLOCK_SIZE_K + offs_k) < K\n        a_mask = mask_m[:, None] & mask_k[None, :]\n        b_mask = mask_k[:, None] & mask_n[None, :]\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)\n        accumulator += bias[None, :]\n    \n    c_ptrs = c_ptr + stride_cm * offs_m[:, None] + stride_cn * offs_n[None, :]\n    c_mask = mask_m[:, None] & mask_n[None, :]\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\ndef triton_gemm(a, b, bias=None):\n    assert a.dim() == 2 and b.dim() == 2\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    gemm_kernel[grid](\n        a, b, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1)\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        \n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / (fan_in**0.5) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        weight_t = self.weight.t().contiguous()\n        x = triton_gemm(x, weight_t, self.bias)\n        x = self.batch_norm(x)\n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 4 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Optimized Triton kernel with FP16 Tensor Core utilization\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_bn_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        mask_k = offs_k < k_remaining\n        # Use FP16 inputs with Tensor Core optimized dot product\n        a = tl.load(a_ptrs, mask=mask_k[None, :], other=0.0, dtype=tl.float16)\n        b = tl.load(b_ptrs, mask=mask_k[:, None], other=0.0, dtype=tl.float16)\n        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N)\n        accumulator += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def forward(self, x):\n        if not self.training:\n            if self.fused_weight is None or self.fused_bias is None:\n                with torch.no_grad():\n                    eps = self.batch_norm.eps\n                    running_var = self.batch_norm.running_var\n                    weight_bn = self.batch_norm.weight\n                    bias_bn = self.batch_norm.bias\n                    total_scale = weight_bn / torch.sqrt(running_var + eps)\n                    fused_bias = (self.gemm.bias * total_scale) + (bias_bn - self.batch_norm.running_mean * total_scale)\n                    self.fused_weight = (self.gemm.weight * total_scale.unsqueeze(1)).t().contiguous()\n                    self.fused_bias = fused_bias.contiguous()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[1]\n            output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_SIZE_M) * triton.cdiv(N, opt.BLOCK_SIZE_N),)\n            fused_gemm_bn_kernel[grid](\n                x, self.fused_weight, output, self.fused_bias,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                output.stride(0), output.stride(1),\n            )\n            x = output\n        else:\n            x = self.gemm(x)\n            x = self.batch_norm(x)\n        \n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 5 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Optimized Triton kernel with FP16 Tensor Core utilization\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8),\n        # New configurations for better tiling\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 512, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 1024, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 512, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef fused_gemm_bn_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Compute masks for M and N boundaries\n    mask_m = offs_m < M\n    mask_n = offs_n < N\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        mask_k = offs_k < k_remaining\n        # Apply full masks including M/N boundaries\n        mask_a = mask_m[:, None] & mask_k[None, :]\n        mask_b = mask_k[:, None] & mask_n[None, :]\n        # Use FP16 inputs with Tensor Core optimized dot product\n        a = tl.load(a_ptrs, mask=mask_a, other=0.0, dtype=tl.float16)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0, dtype=tl.float16)\n        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)\n        accumulator += bias[None, :]\n    \n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask = mask_m[:, None] & mask_n[None, :]\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.batch_norm = nn.BatchNorm1d(out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.fused_weight = None\n        self.fused_bias = None\n\n    def forward(self, x):\n        if not self.training:\n            if self.fused_weight is None or self.fused_bias is None:\n                with torch.no_grad():\n                    eps = self.batch_norm.eps\n                    running_var = self.batch_norm.running_var\n                    weight_bn = self.batch_norm.weight\n                    bias_bn = self.batch_norm.bias\n                    total_scale = weight_bn / torch.sqrt(running_var + eps)\n                    fused_bias = (self.gemm.bias * total_scale) + (bias_bn - self.batch_norm.running_mean * total_scale)\n                    self.fused_weight = (self.gemm.weight * total_scale.unsqueeze(1)).t().contiguous()\n                    self.fused_bias = fused_bias.contiguous()\n            \n            M, K = x.shape\n            N = self.fused_weight.shape[1]\n            output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n            grid = lambda opt: (triton.cdiv(M, opt.BLOCK_SIZE_M) * triton.cdiv(N, opt.BLOCK_SIZE_N),)\n            fused_gemm_bn_kernel[grid](\n                x, self.fused_weight, output, self.fused_bias,\n                M, N, K,\n                x.stride(0), x.stride(1),\n                self.fused_weight.stride(0), self.fused_weight.stride(1),\n                output.stride(0), output.stride(1),\n            )\n            x = output\n        else:\n            x = self.gemm(x)\n            x = self.batch_norm(x)\n        \n        x = torch.nn.functional.gelu(x)\n        x = self.group_norm(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.relu(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 8\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_stages=3, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _gemm_bias(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    mask_m = offs_m < M\n    mask_n = offs_n < N\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        a_mask = mask_m[:, None] & k_mask[None, :]\n        b_mask = k_mask[:, None] & mask_n[None, :]\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        accumulator += tl.dot(a, b)\n        \n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)\n        accumulator += bias[None, :]\n\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask_c = mask_m[:, None] & mask_n[None, :]\n    tl.store(c_ptrs, accumulator, mask=mask_c)\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program which is a Triton kernel for GEMM with bias.\n The current performance: speed_up=0.93 (which is less than 1, meaning slower than reference)\n We are targeting at least 5% speedup (>=1.05 speed_up).\n\n The current kernel does not use Tensor Cores (H100/H800 have FP16 Tensor Cores 32x16x16).\n We note that the current kernel uses tl.dot on FP32 data.\n\n Steps for optimization:\n 1. Convert the inputs to FP16 to utilize Tensor Cores. The accumulator remains in FP32.\n 2. The Triton dot operation on FP16 will automatically use Tensor Cores when the dimensions are multiples of 16 (or 32 for some cases).\n 3. We must adjust the block sizes to be multiples of 16 (or 32) to allow efficient Tensor Core usage.\n\n However, note that the current autotune configurations have block sizes that are not necessarily multiples of 16 or 32 for K dimension?\n Let's check the current configs:\n   - (128,256,64): 64 is multiple of 16 -> good for K\n   - (64,256,32): 32 multiple of 16 -> good\n   - (128,128,32): 32 multiple of 16 -> good\n   - (256,128,64): 64 multiple of 16 -> good\n   - (256,64,128): 128 multiple of 16 -> good\n\n So the existing block sizes for K are already multiples of 16. So we can try converting the loaded data to FP16.\n\n We will:\n   - Load 'a' and 'b' as FP16 (if the original data is FP32, we convert in the kernel) and then use tl.dot with out_dtype=tl.float32.\n   - But note: the original data type might be float32? The problem does not specify. However, the reference code uses whatever the input is.\n\n However, the ModelNew class in the previous attempts uses the same dtype as input. We are only given the kernel.\n\n Since the kernel is called from a function that passes the same dtype, we must be cautious.\n\n But note: the Triton kernel above currently loads as FP32? Because the accumulator is FP32 and the load doesn't specify.\n\n How about we change:\n   - Load 'a' and 'b' as FP16? But if the original data is FP32, we can convert on the fly.\n\n However, the Triton dot operation requires both operands to be the same dtype and that dtype must be either float16 or bfloat16 to use tensor cores.\n\n Steps in the kernel:\n   - We will load the data as FP32 and then convert to FP16? That might be costly. Alternatively, we can load directly as FP16 if the data in memory is FP16? But the input might be FP32.\n\n Since the problem does not specify, we must assume the input can be FP32. But we can convert to FP16 in the kernel for the computation.\n\n We change:\n   a = tl.load(a_ptrs, mask=a_mask, other=0.0)   -> load as FP32\n   Then convert to FP16: a_f16 = a.to(tl.float16)\n\n However, we can also load directly as FP16 if we know the pointer is to FP16 data. But the kernel is called with FP32 tensors? The function triton_gemm in the previous attempts uses the same dtype as input (which could be FP32).\n\n But note: the Triton kernel above does not specify the dtype for the pointers. It uses the dtype of the tensors passed.\n\n How can we handle?\n   We can change the kernel to accept FP16 data? But the original problem might require FP32.\n\n Alternatively, we can do:\n   - Keep the input tensors as FP32, but convert the loaded chunk to FP16 for the dot product.\n\n Since the conversion is done in registers and the tensor cores are much faster, it might be worth it.\n\n Let's change the kernel:\n\n   a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n   b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n\n   to:\n\n   a_f32 = tl.load(a_ptrs, mask=a_mask, other=0.0)\n   b_f32 = tl.load(b_ptrs, mask=b_mask, other=0.0)\n   a_f16 = a_f32.to(tl.float16)\n   b_f16 = b_f32.to(tl.float16)\n\n   Then: accumulator += tl.dot(a_f16, b_f16, out_dtype=tl.float32)\n\n But note: the tl.dot operation for FP16 requires that the block sizes in K are multiples of 16? Actually, Triton may handle it internally, but we have ensured our BLOCK_SIZE_K is multiple of 16.\n\n However, the autotune configs are already multiples of 16 or 32? They are 64,32,64,128 -> all multiples of 16.\n\n Also note: we must be cautious about the range of FP16. But the problem does not specify the data range.\n\n Alternatively, we can change the load to directly load as FP16? But if the pointer is to FP32, that would be wrong.\n\n Since the kernel is called with tensors of dtype torch.float32, we cannot load as FP16.\n\n Therefore, we do the conversion in the kernel.\n\n But note: the conversion might add overhead? However, the tensor cores are so fast that the conversion is likely worth it.\n\n Let's change the kernel accordingly.\n\n Additionally, we can try to optimize the memory access by vectorizing loads? But the current code does not use vectorized loads.\n\n We can try to use the following:\n   - Instead of loading with a_mask and b_mask, we can use the same mask for the entire block? But the mask is necessary for boundaries.\n\n Alternatively, we can note that the conversion to FP16 might be done in a way that doesn't hurt performance.\n\n We'll change the inner loop to use FP16 for the tensor core.\n\n Also note: we are storing the accumulator as FP32, which is correct.\n\n Another note: the bias is added as FP32, which is fine.\n\n Let's rewrite the kernel accordingly.\n\n However, we must be cautious: the Triton dot product for FP16 requires that the dimensions are aligned to 8 (or 16) in the inner dimension? But our BLOCK_SIZE_K is already multiple of 16, so it should be aligned.\n\n We are going to change:\n\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        accumulator += tl.dot(a, b)\n\n   to:\n\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        a = a.to(tl.float16)\n        b = b.to(tl.float16)\n        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n\n But note: the tl.dot operation for FP16 with FP32 accumulator is supported.\n\n Alternatively, we can do the conversion in one line:\n\n        accumulator += tl.dot(a.to(tl.float16), b.to(tl.float16), out_dtype=tl.float32)\n\n However, we must ensure that the mask is applied correctly? The conversion of 0.0 to FP16 is still 0.0.\n\n Also note: the dot product with FP16 might be less accurate? But the problem requires numerical correctness within float32 tolerance. Since we are accumulating in FP32, it should be acceptable.\n\n Let's update the kernel accordingly.\n\n Additionally, we can try to adjust the number of warps and stages? But the autotune will handle that.\n\n However, note that using tensor cores might require more warps? The tensor cores are most efficient when using 8 warps (256 threads). Let's check the current configs: they use 4 or 8 warps. We don't change the autotune configs because they are already set.\n\n We are only changing the inner computation.\n\n We also note that the current kernel does not use the maximum occupancy because the block sizes are fixed per config. But the autotune should pick the best.\n\n We'll change the inner loop as described.\n\n But note: the current program only includes the kernel. We are not allowed to change the function signature or the grid configuration.\n\n We'll output the entire kernel with the changes.\n\n Important: We must not break the boundary checks and masks.\n\n Let's write the new kernel code.\n\n However, note: the current program is only the kernel (the function _gemm_bias). We are not changing the rest of the program (the ModelNew class is not in the current block). We are only allowed to change the Triton kernel within the EVOLVE-BLOCK.\n\n Therefore, we write:\n\n   ... (the autotune and function definition remains the same until the inner loop)\n\n   Inside the inner loop:\n\n        a_val = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b_val = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        # Convert to FP16 for tensor core\n        a_val_f16 = a_val.to(tl.float16)\n        b_val_f16 = b_val.to(tl.float16)\n        accumulator += tl.dot(a_val_f16, b_val_f16, out_dtype=tl.float32)\n\n   But note: we can do without the temporary variables? We can do:\n\n        a_val = tl.load(a_ptrs, mask=a_mask, other=0.0).to(tl.float16)\n        b_val = tl.load(b_ptrs, mask=b_mask, other=0.0).to(tl.float16)\n        accumulator += tl.dot(a_val, b_val, out_dtype=tl.float32)\n\n   However, the tl.dot requires both operands to be the same dtype and that dtype must be float16 or bfloat16.\n\n Let's write it in a compact way.\n\n Alternatively, we can combine the conversion and the dot:\n\n        accumulator += tl.dot(\n            tl.load(a_ptrs, mask=a_mask, other=0.0).to(tl.float16),\n            tl.load(b_ptrs, mask=b_mask, other=0.0).to(tl.float16),\n            out_dtype=tl.float32\n        )\n\n But note: the tl.dot operation does not accept two operands that are not the same as the first argument? Actually, the dot is a method of the tensor? No, it's a function. The correct way is:\n\n        accumulator += tl.dot(tl.load(a_ptrs, mask=a_mask, other=0.0).to(tl.float16), \n                             tl.load(b_ptrs, mask=b_mask, other=0.0).to(tl.float16), \n                             out_dtype=tl.float32)\n\n But wait, the tl.dot function in Triton is in the language as a function: `tl.dot` and it takes two arguments and an optional out_dtype.\n\n However, note: the Triton version we are using? The example in the problem used `tl.dot` with two arguments and an out_dtype.\n\n Let's do it step by step for clarity and to avoid long lines.\n\n We'll break it into:\n\n        a_val = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b_val = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        a_f16 = a_val.to(tl.float16)\n        b_f16 = b_val.to(tl.float16)\n        accumulator += tl.dot(a_f16, b_f16, out_dtype=tl.float32)\n\n But note: the tl.dot operation returns a tensor of the specified out_dtype (fp32) and then we add to the accumulator (fp32).\n\n Alternatively, we can do:\n\n        accumulator += tl.dot(a_val.to(tl.float16), b_val.to(tl.float16), out_dtype=tl.float32)\n\n This is equivalent.\n\n We choose the latter for brevity.\n\n However, we must be cautious of performance: creating temporary variables might use more registers? But the Triton compiler should optimize.\n\n Let's change the kernel accordingly.\n\n Also note: the conversion might be done in the same instruction as the load? Not necessarily, but it should be efficient.\n\n We are going to replace the current:\n\n        accumulator += tl.dot(a, b)\n\n   with:\n\n        accumulator += tl.dot(a.to(tl.float16), b.to(tl.float16), out_dtype=tl.float32)\n\n But note: the variables `a` and `b` are already loaded as FP32. We are converting them to FP16.\n\n This change should enable the use of tensor cores.\n\n We hope this will give a speedup.\n\n However, note: the current kernel is called with FP32 data. We are converting to FP16 for the matrix multiplication and then accumulating in FP32. This is standard practice.\n\n Let's output the entire kernel code.\n\n IMPORTANT: We are only allowed to change the Triton kernel within the EVOLVE-BLOCK. We must not change the function signature or the grid configuration.\n\n Therefore, we write:\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_stages=3, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _gemm_bias(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    mask_m = offs_m < M\n    mask_n = offs_n < N\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        a_mask = mask_m[:, None] & k_mask[None, :]\n        b_mask = k_mask[:, None] & mask_n[None, :]\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        # Convert to FP16 for Tensor Core acceleration\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        # Use tensor core optimized dot product\n        accumulator += tl.dot(a_f16, b_f16, out_dtype=tl.float32)\n        \n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)\n        accumulator += bias[None, :]\n\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask_c = mask_m[:, None] & mask_n[None, :]\n    tl.store(c_ptrs, accumulator, mask=mask_c)\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
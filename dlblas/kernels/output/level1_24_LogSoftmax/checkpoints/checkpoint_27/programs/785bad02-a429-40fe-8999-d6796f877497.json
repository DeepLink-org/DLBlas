{"id": "785bad02-a429-40fe-8999-d6796f877497", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _log_softmax_kernel(\n    x_ptr,\n    output_ptr,\n    stride_x,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    row_start = pid * stride_x\n    \n    # Initialize reduction variables\n    row_max = tl.full((), -float('inf'), dtype=tl.float32)\n    row_sum = tl.zeros((), dtype=tl.float32)\n    \n    # First pass: online max and sum computation\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        col_offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = col_offsets < n_cols\n        abs_offsets = row_start + col_offsets\n        x_vals = tl.load(x_ptr + abs_offsets, mask=mask, other=-float('inf'))\n        \n        # Update max and sum with numerical stability\n        local_max = tl.max(x_vals, axis=0)\n        new_max = tl.maximum(row_max, local_max)\n        scale = tl.exp(row_max - new_max)\n        row_sum = row_sum * scale\n        exp_vals = tl.exp(x_vals - new_max)\n        block_sum = tl.sum(exp_vals, axis=0)\n        row_sum = row_sum + block_sum\n        row_max = new_max\n    \n    # Compute log-sum-exp\n    log_sum = tl.log(row_sum) + row_max\n    \n    # Second pass: compute and store log_softmax\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        col_offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = col_offsets < n_cols\n        abs_offsets = row_start + col_offsets\n        x_vals = tl.load(x_ptr + abs_offsets, mask=mask)\n        output_vals = x_vals - log_sum\n        tl.store(output_ptr + abs_offsets, output_vals, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.dim != 1:\n            return torch.log_softmax(x, dim=self.dim)\n            \n        x = x.contiguous()\n        output = torch.empty_like(x)\n        n_rows, n_cols = x.shape\n        \n        if n_cols == 0:\n            return output\n            \n        # Use larger block size (capped at 16384) to reduce iterations\n        BLOCK_SIZE = min(16384, triton.next_power_of_2(n_cols))\n        grid = (n_rows,)\n        _log_softmax_kernel[grid](x, output, x.stride(0), n_cols, BLOCK_SIZE)\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "48c1307a-f927-437d-8c72-39ad43d1ca4e", "generation": 3, "timestamp": 1754639137.6010711, "iteration_found": 27, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.411, "runtime_ms": 0.0421, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0173, "speed_up": 0.411, "custom_timing": 0.0421}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.404, "runtime_ms": 0.0428, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0173, "speed_up": 0.404, "custom_timing": 0.0428}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.4040\n- runtime_ms: 0.0428\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0173, 'speed_up': 0.404, 'custom_timing': 0.0428}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: runtime_ms. Consider continuing with similar changes.\n- Metrics showing regression: speed_up. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.4120, runtime_ms: 0.0420, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0173, 'speed_up': 0.412, 'custom_timing': 0.042}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.4190, runtime_ms: 0.0413, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0173, 'speed_up': 0.419, 'custom_timing': 0.0413}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.3170, runtime_ms: 0.0546, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0173, 'speed_up': 0.317, 'custom_timing': 0.0546}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=8),\n        triton.Config({\"BLOCK_SIZE\": 2048}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 2048}, num_warps=8),\n        triton.Config({\"BLOCK_SIZE\": 4096}, num_warps=8),\n        triton.Config({\"BLOCK_SIZE\": 8192}, num_warps=8),\n    ],\n    key=[\"n_cols\"],\n)\n@triton.jit\ndef _log_softmax_kernel(\n    x_ptr,\n    output_ptr,\n    stride_x,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    row_start = pid * stride_x\n    row_ptr = x_ptr + row_start\n    \n    # First pass: compute row max and row sum (online algorithm)\n    row_max = tl.full((), -float('inf'), dtype=tl.float32)\n    row_sum = tl.zeros((), dtype=tl.float32)\n    \n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n        x_vals = tl.load(row_ptr + offsets, mask=mask, other=-float('inf'))\n        \n        # Compute local max and update global max\n        local_max = tl.max(x_vals, axis=0)\n        new_max = tl.maximum(row_max, local_max)\n        \n        # Compute exponentials with new max for numerical stability\n        exp_vals = tl.exp(x_vals - new_max)\n        block_sum = tl.sum(exp_vals, axis=0)\n        \n        # Update row sum with rescaling\n        scale = tl.exp(row_max - new_max)\n        row_sum = row_sum * scale + block_sum\n        \n        # Update max for next iteration\n        row_max = new_max\n    \n    # Compute log sum\n    log_sum = tl.log(row_sum) + row_max\n    \n    # Second pass: compute log softmax\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n        x_vals = tl.load(row_ptr + offsets, mask=mask)\n        output = x_vals - log_sum\n        tl.store(output_ptr + row_start + offsets, output, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.dim != 1:\n            return torch.log_softmax(x, dim=self.dim)\n            \n        x = x.contiguous()\n        output = torch.empty_like(x)\n        n_rows, n_cols = x.shape\n        grid = (n_rows,)\n        _log_softmax_kernel[grid](x, output, x.stride(0), n_cols)\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.3170), Performs well on runtime_ms (0.0546), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0173, 'speed_up': 0.317, 'custom_timing': 0.0546})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _log_softmax_kernel(\n    x_ptr,\n    output_ptr,\n    stride_x,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    row_start = pid * stride_x\n    \n    # First pass: compute row max and row sum with online algorithm\n    row_max = tl.full((), -float('inf'), dtype=tl.float32)\n    row_sum = tl.zeros((), dtype=tl.float32)\n    \n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        offsets = row_start + block_start + tl.arange(0, BLOCK_SIZE)\n        mask = block_start + tl.arange(0, BLOCK_SIZE) < n_cols\n        x_vals = tl.load(x_ptr + offsets, mask=mask, other=-float('inf'))\n        \n        # Compute local max and update global max\n        local_max = tl.max(x_vals, axis=0)\n        new_max = tl.maximum(row_max, local_max)\n        \n        # Compute exponentials with new max for numerical stability\n        exp_vals = tl.exp(x_vals - new_max)\n        block_sum = tl.sum(exp_vals, axis=0)\n        \n        # Update row sum with rescaling\n        scale = tl.exp(row_max - new_max)\n        row_sum = row_sum * scale + block_sum\n        \n        # Update max for next iteration\n        row_max = new_max\n    \n    # Compute log sum\n    log_sum = tl.log(row_sum) + row_max\n    \n    # Second pass: compute log softmax\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        offsets = row_start + block_start + tl.arange(0, BLOCK_SIZE)\n        mask = block_start + tl.arange(0, BLOCK_SIZE) < n_cols\n        x_vals = tl.load(x_ptr + offsets, mask=mask)\n        output = x_vals - log_sum\n        tl.store(output_ptr + offsets, output, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.dim != 1:\n            return torch.log_softmax(x, dim=self.dim)\n            \n        x = x.contiguous()\n        output = torch.empty_like(x)\n        n_rows, n_cols = x.shape\n        grid = (n_rows,)\n        BLOCK_SIZE = min(2048, triton.next_power_of_2(n_cols))\n        _log_softmax_kernel[grid](x, output, x.stride(0), n_cols, BLOCK_SIZE)\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.4190), Performs well on runtime_ms (0.0413), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0173, 'speed_up': 0.419, 'custom_timing': 0.0413})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _log_softmax_forward(\n    x_ptr,\n    output_ptr,\n    n_cols,\n    x_row_stride,\n    output_row_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    x_row_ptr = x_ptr + row_idx * x_row_stride\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n\n    # Compute max value in row\n    row_max = tl.full((BLOCK_SIZE,), -float('inf'), dtype=tl.float32)\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        values = tl.load(x_row_ptr + col_idx, mask=mask, other=-float('inf'))\n        row_max = tl.maximum(row_max, values)\n    max_val = tl.max(row_max, axis=0)\n\n    # Compute sum of exponentials\n    sum_exp = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        values = tl.load(x_row_ptr + col_idx, mask=mask, other=0)\n        exp_vals = tl.exp(values - max_val)\n        sum_exp += tl.where(mask, exp_vals, 0)\n    log_sum_exp = tl.log(tl.sum(sum_exp, axis=0))\n\n    # Compute log_softmax\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        values = tl.load(x_row_ptr + col_idx, mask=mask)\n        out = (values - max_val) - log_sum_exp\n        tl.store(output_row_ptr + col_idx, out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Ensure contiguous tensor and check dimensions\n        x = x.contiguous()\n        output = torch.empty_like(x)\n        \n        # Kernel configuration\n        n_rows, n_cols = x.shape\n        BLOCK_SIZE = triton.next_power_of_2(n_cols)\n        if BLOCK_SIZE > 32768:\n            BLOCK_SIZE = 16384\n        \n        grid = (n_rows,)\n        _log_softmax_forward[grid](\n            x, output, n_cols, \n            x.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim, device='cuda')\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.4120), Performs well on runtime_ms (0.0420), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0173, 'speed_up': 0.412, 'custom_timing': 0.042})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _log_softmax_kernel(\n    x_ptr,\n    output_ptr,\n    stride_x,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    row_start = pid * stride_x\n    row_max = tl.full((), -float('inf'), dtype=tl.float32)\n    \n    # First pass: Compute row max\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        offsets = row_start + block_start + tl.arange(0, BLOCK_SIZE)\n        mask = block_start + tl.arange(0, BLOCK_SIZE) < n_cols\n        x_vals = tl.load(x_ptr + offsets, mask=mask, other=-float('inf'))\n        row_max = tl.maximum(row_max, tl.max(x_vals, 0))\n    \n    # Second pass: Compute row sum of exponentials\n    row_sum = tl.zeros((), dtype=tl.float32)\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        offsets = row_start + block_start + tl.arange(0, BLOCK_SIZE)\n        mask = block_start + tl.arange(0, BLOCK_SIZE) < n_cols\n        x_vals = tl.load(x_ptr + offsets, mask=mask)\n        shifted = x_vals - row_max\n        exp_vals = tl.exp(shifted)\n        row_sum += tl.sum(exp_vals, 0)\n    \n    log_sum = tl.log(row_sum)\n    \n    # Third pass: Compute log softmax\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        offsets = row_start + block_start + tl.arange(0, BLOCK_SIZE)\n        mask = block_start + tl.arange(0, BLOCK_SIZE) < n_cols\n        x_vals = tl.load(x_ptr + offsets, mask=mask)\n        output_vals = x_vals - row_max - log_sum\n        tl.store(output_ptr + offsets, output_vals, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.dim != 1:\n            return torch.log_softmax(x, dim=self.dim)\n            \n        x = x.contiguous()\n        output = torch.empty_like(x)\n        n_rows, n_cols = x.shape\n        \n        if n_cols == 0:\n            return output\n            \n        BLOCK_SIZE = min(1024, triton.next_power_of_2(n_cols))\n        grid = (n_rows,)\n        _log_softmax_kernel[grid](x, output, x.stride(0), n_cols, BLOCK_SIZE)\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _log_softmax_forward(\n    x_ptr,\n    output_ptr,\n    n_cols,\n    x_row_stride,\n    output_row_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    x_row_ptr = x_ptr + row_idx * x_row_stride\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n\n    # Compute max value in row using scalar reduction\n    row_max = -float('inf')\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        values = tl.load(x_row_ptr + col_idx, mask=mask, other=-float('inf'))\n        block_max = tl.max(tl.where(mask, values, -float('inf')), axis=0)\n        row_max = tl.maximum(row_max, block_max)\n    max_val = row_max\n\n    # Compute sum of exponentials using scalar reduction\n    total = 0.0\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        values = tl.load(x_row_ptr + col_idx, mask=mask, other=0)\n        exp_vals = tl.exp(values - max_val)\n        block_sum = tl.sum(tl.where(mask, exp_vals, 0), axis=0)\n        total += block_sum\n    log_sum_exp = tl.log(total)\n\n    # Compute log_softmax\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        values = tl.load(x_row_ptr + col_idx, mask=mask)\n        out = (values - max_val) - log_sum_exp\n        tl.store(output_row_ptr + col_idx, out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.contiguous()\n        output = torch.empty_like(x)\n        \n        n_rows, n_cols = x.shape\n        # Use fixed block size for better register utilization\n        BLOCK_SIZE = 1024\n        \n        grid = (n_rows,)\n        _log_softmax_forward[grid](\n            x, output, n_cols, \n            x.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim, device='cuda')\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _log_softmax_forward(\n    x_ptr,\n    output_ptr,\n    n_cols,\n    x_row_stride,\n    output_row_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    x_row_ptr = x_ptr + row_idx * x_row_stride\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n\n    # Compute max value in row\n    row_max = tl.full((BLOCK_SIZE,), -float('inf'), dtype=tl.float32)\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        values = tl.load(x_row_ptr + col_idx, mask=mask, other=-float('inf'))\n        row_max = tl.maximum(row_max, values)\n    max_val = tl.max(row_max, axis=0)\n\n    # Compute sum of exponentials\n    sum_exp = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        values = tl.load(x_row_ptr + col_idx, mask=mask, other=0)\n        exp_vals = tl.exp(values - max_val)\n        sum_exp += tl.where(mask, exp_vals, 0)\n    log_sum_exp = tl.log(tl.sum(sum_exp, axis=0))\n\n    # Compute log_softmax\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        values = tl.load(x_row_ptr + col_idx, mask=mask)\n        out = (values - max_val) - log_sum_exp\n        tl.store(output_row_ptr + col_idx, out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Ensure contiguous tensor and check dimensions\n        x = x.contiguous()\n        output = torch.empty_like(x)\n        \n        # Kernel configuration\n        n_rows, n_cols = x.shape\n        BLOCK_SIZE = triton.next_power_of_2(n_cols)\n        if BLOCK_SIZE > 32768:\n            BLOCK_SIZE = 16384\n        \n        grid = (n_rows,)\n        _log_softmax_forward[grid](\n            x, output, n_cols, \n            x.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim, device='cuda')\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=8),\n        triton.Config({\"BLOCK_SIZE\": 2048}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 2048}, num_warps=8),\n        triton.Config({\"BLOCK_SIZE\": 4096}, num_warps=8),\n        triton.Config({\"BLOCK_SIZE\": 8192}, num_warps=8),\n    ],\n    key=[\"n_cols\"],\n)\n@triton.jit\ndef _log_softmax_kernel(\n    x_ptr,\n    output_ptr,\n    stride_x,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    row_start = pid * stride_x\n    row_ptr = x_ptr + row_start\n    \n    # First pass: compute row max and row sum (online algorithm)\n    row_max = tl.full((), -float('inf'), dtype=tl.float32)\n    row_sum = tl.zeros((), dtype=tl.float32)\n    \n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n        x_vals = tl.load(row_ptr + offsets, mask=mask, other=-float('inf'))\n        \n        # Compute local max and update global max\n        local_max = tl.max(x_vals, axis=0)\n        new_max = tl.maximum(row_max, local_max)\n        \n        # Compute exponentials with new max for numerical stability\n        exp_vals = tl.exp(x_vals - new_max)\n        block_sum = tl.sum(exp_vals, axis=0)\n        \n        # Update row sum with rescaling\n        scale = tl.exp(row_max - new_max)\n        row_sum = row_sum * scale + block_sum\n        \n        # Update max for next iteration\n        row_max = new_max\n    \n    # Compute log sum\n    log_sum = tl.log(row_sum) + row_max\n    \n    # Second pass: compute log softmax\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n        x_vals = tl.load(row_ptr + offsets, mask=mask)\n        output = x_vals - log_sum\n        tl.store(output_ptr + row_start + offsets, output, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.dim != 1:\n            return torch.log_softmax(x, dim=self.dim)\n            \n        x = x.contiguous()\n        output = torch.empty_like(x)\n        n_rows, n_cols = x.shape\n        grid = (n_rows,)\n        _log_softmax_kernel[grid](x, output, x.stride(0), n_cols)\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 3 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 2048}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 4096}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 2048}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 4096}, num_warps=8),\n    ],\n    key=['n_cols'],\n)\n@triton.jit\ndef _log_softmax_forward(\n    x_ptr,\n    output_ptr,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    row_start = pid * n_cols\n    num_blocks = tl.cdiv(n_cols, BLOCK_SIZE)\n    \n    # Compute row max with efficient reduction\n    row_max = -float('inf')\n    for block_idx in range(num_blocks):\n        offset = block_idx * BLOCK_SIZE\n        col_offsets = tl.arange(0, BLOCK_SIZE)\n        valid_mask = col_offsets < (n_cols - offset)\n        segment = tl.load(x_ptr + row_start + offset + col_offsets, \n                         mask=valid_mask, other=-float('inf'))\n        row_max = tl.maximum(row_max, tl.max(segment, axis=0))\n    \n    # Compute sum of exponentials with stable computation\n    exp_sum = 0.0\n    for block_idx in range(num_blocks):\n        offset = block_idx * BLOCK_SIZE\n        col_offsets = tl.arange(0, BLOCK_SIZE)\n        valid_mask = col_offsets < (n_cols - offset)\n        segment = tl.load(x_ptr + row_start + offset + col_offsets, \n                         mask=valid_mask, other=0)\n        exp_val = tl.exp(segment - row_max)\n        exp_sum += tl.sum(exp_val, axis=0)\n    \n    # Compute log-sum-exp and final values\n    log_sum_exp = tl.log(exp_sum) + row_max\n    \n    # Store results with efficient vectorization\n    for block_idx in range(num_blocks):\n        offset = block_idx * BLOCK_SIZE\n        col_offsets = tl.arange(0, BLOCK_SIZE)\n        valid_mask = col_offsets < (n_cols - offset)\n        segment = tl.load(x_ptr + row_start + offset + col_offsets, \n                         mask=valid_mask)\n        log_softmax_val = segment - log_sum_exp\n        tl.store(output_ptr + row_start + offset + col_offsets, \n                log_softmax_val, mask=valid_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.dim != 1 or x.dim() != 2:\n            return torch.log_softmax(x, dim=self.dim)\n            \n        x = x.contiguous()\n        output = torch.empty_like(x)\n        n_rows, n_cols = x.shape\n        \n        grid = (n_rows,)\n        _log_softmax_forward[grid](x, output, n_cols)\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim, device='cuda')\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _log_softmax_forward(\n    x_ptr,\n    output_ptr,\n    n_cols,\n    x_row_stride,\n    output_row_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    x_row_ptr = x_ptr + row_idx * x_row_stride\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n\n    # Initialize reduction variables\n    row_max = tl.full((BLOCK_SIZE,), -float('inf'), dtype=tl.float32)\n    row_sum = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    global_max = -float('inf')\n    \n    # First pass: compute max and sum of exponentials\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        values = tl.load(x_row_ptr + col_idx, mask=mask, other=-float('inf'))\n        \n        # Update max and sum using online algorithm\n        new_max = tl.maximum(global_max, tl.max(values, 0))\n        scale = tl.exp(global_max - new_max)\n        row_sum = row_sum * scale + tl.sum(tl.exp(values - new_max), 0)\n        global_max = new_max\n\n    # Compute log-sum-exp\n    log_sum_exp = tl.log(tl.sum(row_sum, 0)) + global_max\n\n    # Second pass: compute log_softmax\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_idx = col_offset + tl.arange(0, BLOCK_SIZE)\n        mask = col_idx < n_cols\n        values = tl.load(x_row_ptr + col_idx, mask=mask)\n        out = values - global_max - log_sum_exp\n        tl.store(output_row_ptr + col_idx, out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.contiguous()\n        output = torch.empty_like(x)\n        \n        n_rows, n_cols = x.shape\n        # Use adaptive block size with safe cap\n        BLOCK_SIZE = min(triton.next_power_of_2(n_cols), 16384)\n        \n        grid = (n_rows,)\n        _log_softmax_forward[grid](\n            x, output, n_cols, \n            x.stride(0), output.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim, device='cuda')\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _log_softmax_forward(\n    x_ptr,\n    output_ptr,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    row_start = pid * n_cols\n    num_blocks = tl.cdiv(n_cols, BLOCK_SIZE)\n    \n    row_max = -float('inf')\n    for block_idx in range(num_blocks):\n        offset = block_idx * BLOCK_SIZE\n        col_offsets = tl.arange(0, BLOCK_SIZE)\n        valid_mask = col_offsets < (n_cols - offset)\n        segment = tl.load(x_ptr + row_start + offset + col_offsets, \n                         mask=valid_mask, other=-float('inf'))\n        row_max = tl.maximum(row_max, tl.max(segment, axis=0))\n    \n    exp_sum = 0.0\n    for block_idx in range(num_blocks):\n        offset = block_idx * BLOCK_SIZE\n        col_offsets = tl.arange(0, BLOCK_SIZE)\n        valid_mask = col_offsets < (n_cols - offset)\n        segment = tl.load(x_ptr + row_start + offset + col_offsets, \n                         mask=valid_mask, other=0)\n        exp_val = tl.exp(segment - row_max)\n        exp_sum += tl.sum(exp_val, axis=0)\n    \n    log_sum_exp = tl.log(exp_sum) + row_max\n    \n    for block_idx in range(num_blocks):\n        offset = block_idx * BLOCK_SIZE\n        col_offsets = tl.arange(0, BLOCK_SIZE)\n        valid_mask = col_offsets < (n_cols - offset)\n        segment = tl.load(x_ptr + row_start + offset + col_offsets, \n                         mask=valid_mask)\n        log_softmax_val = segment - log_sum_exp\n        tl.store(output_ptr + row_start + offset + col_offsets, \n                log_softmax_val, mask=valid_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.dim != 1 or x.dim() != 2:\n            return torch.log_softmax(x, dim=self.dim)\n            \n        x = x.contiguous()\n        output = torch.empty_like(x)\n        n_rows, n_cols = x.shape\n        \n        grid = (n_rows,)\n        BLOCK_SIZE = 1024\n        _log_softmax_forward[grid](x, output, n_cols, BLOCK_SIZE)\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim, device='cuda')\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _log_softmax_kernel(\n    x_ptr,\n    output_ptr,\n    stride_x,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    row_start = pid * stride_x\n    row_max = tl.full((), -float('inf'), dtype=tl.float32)\n    \n    # First pass: Compute row max\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        offsets = row_start + block_start + tl.arange(0, BLOCK_SIZE)\n        mask = block_start + tl.arange(0, BLOCK_SIZE) < n_cols\n        x_vals = tl.load(x_ptr + offsets, mask=mask, other=-float('inf'))\n        row_max = tl.maximum(row_max, tl.max(x_vals, 0))\n    \n    # Second pass: Compute row sum of exponentials\n    row_sum = tl.zeros((), dtype=tl.float32)\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        offsets = row_start + block_start + tl.arange(0, BLOCK_SIZE)\n        mask = block_start + tl.arange(0, BLOCK_SIZE) < n_cols\n        x_vals = tl.load(x_ptr + offsets, mask=mask)\n        shifted = x_vals - row_max\n        exp_vals = tl.exp(shifted)\n        row_sum += tl.sum(exp_vals, 0)\n    \n    log_sum = tl.log(row_sum)\n    \n    # Third pass: Compute log softmax\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        offsets = row_start + block_start + tl.arange(0, BLOCK_SIZE)\n        mask = block_start + tl.arange(0, BLOCK_SIZE) < n_cols\n        x_vals = tl.load(x_ptr + offsets, mask=mask)\n        output_vals = x_vals - row_max - log_sum\n        tl.store(output_ptr + offsets, output_vals, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.dim != 1:\n            return torch.log_softmax(x, dim=self.dim)\n            \n        x = x.contiguous()\n        output = torch.empty_like(x)\n        n_rows, n_cols = x.shape\n        \n        if n_cols == 0:\n            return output\n            \n        BLOCK_SIZE = min(1024, triton.next_power_of_2(n_cols))\n        grid = (n_rows,)\n        _log_softmax_kernel[grid](x, output, x.stride(0), n_cols, BLOCK_SIZE)\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program which uses three passes (max, sum_exp, then log_softmax) and has a speed_up of 0.404 (which is less than 1, meaning it's slower than the reference).\n The goal is to improve the performance (speed_up) to be at least 1.05 (5% faster) and ideally 15% or more.\n\n Observations from the current program:\n   - It uses three passes over the data, which results in more memory reads and writes.\n   - The top-performing and inspiration programs often use an online algorithm that computes the max and the sum of exponentials in one pass (with rescaling) and then a second pass for the log_softmax.\n\n Proposed changes:\n   - We will change the kernel to use an online algorithm for the max and the sum of exponentials in a single pass (as seen in some of the top programs and inspiration programs).\n   - This online algorithm updates the max and rescales the running sum appropriately, which avoids having to do a separate max pass and then a sum pass.\n\n Steps for the kernel:\n   - Initialize row_max to -inf and row_sum to 0.\n   - For each block in the row:\n        - Load the block of data.\n        - Compute the local max of the block and update the global max (new_max = max(row_max, local_max)).\n        - Rescale the current row_sum by exp(row_max - new_max) and then add the sum of exponentials (with the new_max) of the current block.\n        - Update row_max to new_max.\n   - Then compute log_sum = log(row_sum) + row_max.\n   - In the second pass, compute the log_softmax as (x - log_sum) and store.\n\n This reduces the number of passes from three to two.\n\n Additionally, we can consider:\n   - Using triton.autotune to find the best BLOCK_SIZE and num_warps configuration (as in some of the top programs).\n   - However, note that the current program does not use autotune and we are allowed to change only the Triton kernel source within the block.\n\n Constraints:\n   - We must not change the function signature of the kernel (so we keep the same parameters).\n   - We must not change the grid configuration (so we keep the same grid and the same way of computing the row index).\n\n Let's rewrite the kernel accordingly.\n\n Also note: The current program uses a BLOCK_SIZE that is the minimum between 1024 and the next power of two of n_cols. We might want to let the autotuner choose the block size, but note that the autotune decorator is allowed.\n\n However, the current program does not have autotune. We can introduce it if it's within the kernel block and does not change the function signature.\n\n But note: The problem says \"Modify ONLY the Triton kernel source within this block\". We are allowed to change the kernel and the class ModelNew? Actually, the entire code within the EVOLVE-BLOCK is modifiable. However, note that the grid configuration and output tensor shapes are locked.\n\n But wait: the problem says \"\u274c NEVER change @triton.jit function signatures or parameter names\" and \"\u274c NEVER modify grid configuration, output tensor shapes, or PID logic\".\n\n We are going to:\n   - Change the kernel to the two-pass online algorithm.\n   - Add autotune to the kernel (if it's not there) to try different block sizes and num_warps.\n\n However, the current program does not use autotune. We can add it as long as we don't change the function signature (the parameters remain the same).\n\n Let's look at the signature: \n    _log_softmax_kernel(x_ptr, output_ptr, stride_x, n_cols, BLOCK_SIZE: tl.constexpr)\n\n We are going to add an autotune decorator above the kernel and then adjust the call accordingly? But note that the autotune requires that we don't pass BLOCK_SIZE as a fixed argument? Actually, the autotune will generate multiple versions and choose the best at runtime.\n\n How to adjust the call?\n   - Currently, the kernel is called with BLOCK_SIZE as an argument. But if we use autotune, we must not pass BLOCK_SIZE as an argument? Actually, in the autotune examples, the BLOCK_SIZE is a config parameter and is passed as a keyword argument to the kernel.\n\n   - We can change the kernel to have BLOCK_SIZE as a config parameter and then in the call we remove the BLOCK_SIZE argument? But note: the kernel function signature must remain the same. However, we are allowed to change the internal implementation.\n\n   - Alternatively, note that the autotune examples in the inspiration programs have the BLOCK_SIZE as a tl.constexpr and then in the autotune configs they set it.\n\n   - We can do:\n\n        @triton.autotune(\n            configs=[\n                triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=4),\n                triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=8),\n                ...,\n            ],\n            key=[\"n_cols\"],\n        )\n        @triton.jit\n        def _log_softmax_kernel(...): ...\n\n   - And then in the call, we do not pass BLOCK_SIZE? Actually, the autotune will override the BLOCK_SIZE from the config. But note that the kernel still expects BLOCK_SIZE as a parameter. So we must pass it? Actually, the autotune will set the BLOCK_SIZE from the config and ignore the one we pass? This is not the case.\n\n   - Alternatively, we can remove the BLOCK_SIZE argument from the kernel? But that would change the signature. We are not allowed to change the signature.\n\n   - How the autotune example in the inspiration programs does it? They have the kernel with BLOCK_SIZE as a tl.constexpr and then in the call they do not pass it? Actually, they do pass it. But note: the autotune will override the value we pass? No, the autotune will generate multiple kernels and choose the one that matches the config. The call must pass the same arguments, but the autotune will intercept and use the tuned config.\n\n   - We can keep the call the same? Actually, the autotune will use the value of BLOCK_SIZE that we pass as the default? Or it will be overridden by the config? The triton.autotune will try the configs and then for the call, it will use the best config. The BLOCK_SIZE we pass in the call might be ignored? But note: the kernel is compiled for each config, and the call will use the compiled kernel for the chosen config.\n\n   - However, the problem says: \"NEVER change @triton.jit function signatures or parameter names\". So we must leave the parameters as they are.\n\n   - We can try to use autotune and still pass BLOCK_SIZE? Actually, the autotune will set the BLOCK_SIZE from the config and ignore the one we pass? That is not the case. We are passing BLOCK_SIZE as an argument. The autotune will use the config to set the value of BLOCK_SIZE and then ignore the passed value? I think we should not pass it if we use autotune? But then the kernel expects it.\n\n   - Looking at the inspiration program \"Program 1\" (the top one) and \"Inspiration 2\", they do:\n\n        @triton.autotune(...)\n        @triton.jit\n        def _log_softmax_kernel(x_ptr, output_ptr, stride_x, n_cols, BLOCK_SIZE: tl.constexpr):\n            ...\n\n        and then in the call:\n\n        _log_softmax_kernel[grid](x, output, x.stride(0), n_cols)\n\n        They are not passing BLOCK_SIZE! But the kernel expects 5 arguments and they are passing 4.\n\n   - That means that in the autotune version, the BLOCK_SIZE is provided by the autotune config and is not passed as an argument? Then we must remove the BLOCK_SIZE from the call.\n\n   - However, our current call is:\n\n        _log_softmax_kernel[grid](x, output, x.stride(0), n_cols, BLOCK_SIZE)\n\n   - We have two options:\n        Option 1: Remove the BLOCK_SIZE from the kernel parameters and the call, and rely on autotune to set it via config.\n        Option 2: Keep the BLOCK_SIZE as a parameter and try to use autotune? But then we would be passing two BLOCK_SIZEs? That doesn't work.\n\n   - Actually, the autotune examples in the inspiration programs do not have BLOCK_SIZE in the kernel parameters? Let me check:\n\n        Inspiration 2 has:\n            def _log_softmax_kernel(x_ptr, output_ptr, stride_x, n_cols, BLOCK_SIZE: tl.constexpr)\n\n        and then in the call: \n            _log_softmax_kernel[grid](x, output, x.stride(0), n_cols)\n\n        This is 4 arguments, but the kernel expects 5. That doesn't match.\n\n   - I see: in Inspiration 2, the kernel is defined with 5 parameters, but the call passes 4. This is an error? Or perhaps the example was miswritten.\n\n   - Actually, in the example I provided earlier (the quantize example), the call passes the BLOCK_SIZE.\n\n   - After rechecking: the autotune does not remove the need to pass the BLOCK_SIZE if it is in the signature. So we must pass it. But then the autotune will override it? That is not the intended behavior.\n\n   - The correct way is to let the autotune set the BLOCK_SIZE via the config, so we remove the BLOCK_SIZE from the kernel parameters? But we cannot because the problem says we cannot change the function signature.\n\n   - Alternatively, we can avoid using autotune and instead choose a fixed BLOCK_SIZE that we know is good? Or we can try to set the BLOCK_SIZE to a value that is the next power of two and then cap it, as in some programs.\n\n   - Given the constraints, we decide to forgo autotune for now and focus on the two-pass online algorithm. We can try to set the BLOCK_SIZE to a fixed value that is known to be efficient, such as 1024 or 2048, and then see if that helps.\n\n   - However, note that the current program sets BLOCK_SIZE = min(1024, triton.next_power_of_2(n_cols)). For n_cols=16384, the next power of two is 16384, so BLOCK_SIZE=1024. That might be too small? We could try a larger block size to reduce the number of passes.\n\n   - Alternatively, we can set BLOCK_SIZE to 4096 or 8192? But note that the maximum number of threads in a block is 1024 for Triton? Actually, Triton can have 1D blocks of up to 1024 elements? Or more? The block size here is the number of elements processed by the kernel in one iteration? It is not the thread block size? Actually, in Triton, the BLOCK_SIZE is the number of elements processed by the program in one step? And we can have any BLOCK_SIZE as long as we have enough shared memory? But note that the kernel uses a loop over blocks.\n\n   - We can try increasing the BLOCK_SIZE to 4096 or 8192? But then we might run into resource limits? The online algorithm uses scalar accumulators, so it should be fine.\n\n   - Alternatively, we can remove the BLOCK_SIZE argument and use a fixed value? But the problem says we cannot change the signature. So we must leave the BLOCK_SIZE as an argument.\n\n   - We are going to do the following:\n\n        Step 1: Rewrite the kernel to use the online algorithm for max and sum of exponentials in the first pass (so two passes total: one for max+sum and one for the log_softmax).\n        Step 2: Keep the BLOCK_SIZE as passed in the call.\n\n        But we change the call to use a larger BLOCK_SIZE? Currently, it uses min(1024, triton.next_power_of_2(n_cols)). For n_cols=16384, that gives 1024. We can change it to a larger value? For example, 4096? Then we have 16384/4096=4 iterations per pass.\n\n        However, note that the autotune in the inspiration programs suggests that larger block sizes (like 4096, 8192) with more warps might be faster.\n\n        We are not using autotune, so we can try to set the BLOCK_SIZE to 4096? But we must be cautious because the kernel might use too many registers and cause spilling.\n\n        Alternatively, we can set it to the next power of two without capping at 1024? But note that the current program caps at 1024. We can remove the cap? \n\n        We change in the ModelNew.forward:\n\n            BLOCK_SIZE = triton.next_power_of_2(n_cols)   # without the min(1024, ...)\n\n        But then for very large n_cols, we might set BLOCK_SIZE to a huge number? We must cap it to a safe value? The inspiration programs cap at 16384.\n\n        We can do: BLOCK_SIZE = min(triton.next_power_of_2(n_cols), 16384)\n\n        This will be 16384 for n_cols=16384.\n\n        Then we have only one iteration per pass? That would be efficient.\n\n        However, note that the kernel uses a loop over blocks. If we set BLOCK_SIZE to 16384, then the loop will run only once. That is efficient because we avoid the loop overhead.\n\n        But note: the kernel must work for any n_cols? If n_cols is not a power of two, then we set BLOCK_SIZE to the next power of two? And then the last block will be masked? That is acceptable.\n\n        However, if n_cols is 100000, then the next power of two is 131072? And we cap at 16384? Then we set BLOCK_SIZE=16384 and the loop will run ceil(100000/16384)=7 times.\n\n        This is acceptable.\n\n        So we change the BLOCK_SIZE in the call to:\n\n            BLOCK_SIZE = min(16384, triton.next_power_of_2(n_cols))\n\n        And then we use the two-pass online algorithm in the kernel.\n\n Implementation of the kernel:\n\n    pid = tl.program_id(0)\n    row_start = pid * stride_x\n    row_ptr = x_ptr + row_start   # base pointer for the row\n\n    # Online algorithm for max and sum of exponentials\n    row_max = tl.full((), -float('inf'), dtype=tl.float32)\n    row_sum = tl.zeros((), dtype=tl.float32)\n\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n        x_vals = tl.load(row_ptr + offsets, mask=mask, other=-float('inf'))\n\n        # Compute the local max in the block\n        local_max = tl.max(x_vals, axis=0)\n        new_max = tl.maximum(row_max, local_max)\n\n        # Rescale the current row_sum and add the new exponentials\n        # Note: exp_vals = exp(x_vals - new_max) but we can compute without storing the entire block\n        #        by breaking the exponential into two parts: the old part is scaled, the new part is computed.\n        scale = tl.exp(row_max - new_max)\n        row_sum = row_sum * scale\n        # Now compute the exponentials for the current block with the new_max\n        exp_vals = tl.exp(x_vals - new_max)\n        block_sum = tl.sum(exp_vals, axis=0)\n        row_sum = row_sum + block_sum\n\n        row_max = new_max\n\n    log_sum = tl.log(row_sum) + row_max\n\n    # Second pass: compute log_softmax\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n        x_vals = tl.load(row_ptr + offsets, mask=mask)\n        output_vals = x_vals - log_sum\n        tl.store(output_ptr + row_start + offsets, output_vals, mask=mask)\n\n Note: We are storing to output_ptr at row_start+offsets? But the output_ptr is the base of the entire output tensor? And the row in the output is at the same row index? Yes.\n\n However, note that the current program uses:\n\n        tl.store(output_ptr + offsets, ...)\n\n    but that is not correct because the output_ptr is for the entire tensor? Actually, the output_ptr for the row is at row_start? But in the current program, the kernel uses:\n\n        offsets = row_start + block_start + tl.arange(0, BLOCK_SIZE)   [for the input]\n        and then stores at output_ptr + offsets.\n\n    But in the online version, we have:\n\n        row_ptr = x_ptr + row_start\n        then we load from row_ptr + offsets (where offsets = block_start + ...)\n\n        and then we store at output_ptr + row_start + offsets? \n\n    But note: the output_ptr is the base of the entire output tensor, so the row in the output is at row_start? and then the column is at offsets? Yes.\n\n    However, the current kernel in the program we are replacing does:\n\n        tl.store(output_ptr + offsets, ...)   [where offsets = row_start + block_start + ...]\n\n    So we must do the same? Actually, the output_ptr is the entire tensor, so we must store at:\n\n        output_ptr + row_start + block_start + tl.arange(0, BLOCK_SIZE)\n\n    But note: the kernel parameters are:\n\n        x_ptr, output_ptr, stride_x, n_cols, BLOCK_SIZE\n\n    The output_ptr is the base of the entire output tensor? And the row for the current pid is at output_ptr + pid * stride_x? But note: the output tensor is contiguous? The ModelNew.forward makes the output contiguous? And we are using the same shape? So the stride of the output is the same as the input? Actually, the input is made contiguous and the output is created as empty_like(x), which is contiguous? So the stride for the output in the row dimension is n_cols? But the kernel uses stride_x (which is the input stride) for the input. For the output, we don't have a stride? We are storing at row_start (which is pid * stride_x) plus the column offset? But that might not be the same for the output? Actually, the output is contiguous and has the same shape, so the row stride is n_cols? But the input stride (stride_x) is the stride for the rows? For a contiguous tensor, stride(0) is n_cols? So row_start = pid * stride_x = pid * n_cols? Then the column offset is just the index in the row? Then we can store at output_ptr + row_start + offsets? But note: in the online version, we computed offsets without the row_start? We have:\n\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)   [for the column]\n\n    So the absolute address for the output is:\n\n        output_ptr + row_start + (block_start + tl.arange(0, BLOCK_SIZE))\n\n    which is the same as output_ptr + pid * stride_x + block_start + tl.arange(0, BLOCK_SIZE)\n\n    That is correct.\n\n    But in the current program, the kernel uses:\n\n        offsets = row_start + block_start + tl.arange(0, BLOCK_SIZE)   [for the input and output]\n\n    We are going to change the kernel to use:\n\n        row_ptr = x_ptr + row_start   [for the input]\n        and then for the output, we use output_ptr + row_start + ...?\n\n    Alternatively, we can keep the same addressing as the current program? The current program uses:\n\n        offsets = row_start + block_start + tl.arange(0, BLOCK_SIZE)\n\n    for both the input and output.\n\n    We can do:\n\n        for block_start in range(0, n_cols, BLOCK_SIZE):\n            col_offsets = tl.arange(0, BLOCK_SIZE)\n            mask = col_offsets < (n_cols - block_start)   # or mask = (block_start + col_offsets) < n_cols\n            abs_offsets = row_start + block_start + col_offsets\n\n    Then we load from x_ptr + abs_offsets and store to output_ptr + abs_offsets.\n\n    That is consistent with the current program.\n\n    We will use that.\n\n    But note: the online algorithm requires that we compute the max and sum for the entire row? So we need to iterate over the entire row? We are already doing that.\n\n    Let's write the kernel accordingly.\n\n    We also note that the current program uses:\n\n        mask = block_start + tl.arange(0, BLOCK_SIZE) < n_cols\n\n    That is the same as:\n\n        mask = (block_start + col_offsets) < n_cols   [where col_offsets = tl.arange(0, BLOCK_SIZE)]\n\n    We can compute the mask as:\n\n        col_offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = col_offsets < n_cols\n\n    But note: the col_offsets might be beyond n_cols? We are using a loop that goes from 0 to n_cols in steps of BLOCK_SIZE, so the last block might be partial.\n\n    We'll do:\n\n        col_offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = col_offsets < n_cols\n\n    Then the absolute address is row_start + col_offsets? Actually, row_start is the start of the row? Then the absolute address for the input is x_ptr + row_start + col_offsets? But wait: row_start is the byte offset for the row? Actually, row_start = pid * stride_x, and stride_x is in terms of elements? Actually, the stride_x is the number of elements between rows? So row_start is the element index for the start of the row? Then we can do:\n\n        x_vals = tl.load(x_ptr + row_start + col_offsets, mask=mask, ...)\n\n    But note: the current program does:\n\n        offsets = row_start + block_start + tl.arange(0, BLOCK_SIZE)   [so the absolute address is x_ptr + offsets]\n\n    So we can do:\n\n        offsets = row_start + block_start + tl.arange(0, BLOCK_SIZE)\n        mask = (block_start + tl.arange(0, BLOCK_SIZE)) < n_cols   # but note: block_start+arange might be beyond n_cols? We can do:\n        mask = (block_start + tl.arange(0, BLOCK_SIZE)) < n_cols   # same as the current program\n\n    However, note: the row_start is added to the block_start and the arange? Then the mask is only on the column part? That is acceptable.\n\n    We'll keep the same addressing as the current program.\n\n    Revised kernel:\n\n        pid = tl.program_id(0)\n        row_start = pid * stride_x   # in elements\n\n        # Initialize reduction variables\n        row_max = -float('inf')\n        row_sum = 0.0\n\n        # First pass: online max and sum of exponentials\n        for block_start in range(0, n_cols, BLOCK_SIZE):\n            # Compute absolute offsets for the current block\n            col_offsets = block_start + tl.arange(0, BLOCK_SIZE)\n            mask = col_offsets < n_cols\n            abs_offsets = row_start + col_offsets\n            x_vals = tl.load(x_ptr + abs_offsets, mask=mask, other=-float('inf'))\n\n            # Update the max and the running sum\n            local_max = tl.max(x_vals, axis=0)\n            new_max = tl.maximum(row_max, local_max)\n\n            # Rescale the current row_sum and add the new block sum\n            scale = tl.exp(row_max - new_max)\n            row_sum = row_sum * scale\n            exp_vals = tl.exp(x_vals - new_max)\n            block_sum = tl.sum(exp_vals, axis=0)\n            row_sum = row_sum + block_sum\n\n            row_max = new_max\n\n        log_sum = tl.log(row_sum) + row_max\n\n        # Second pass: compute log_softmax and store\n        for block_start in range(0, n_cols, BLOCK_SIZE):\n            col_offsets block =_start + tl.arange(0, BLOCK_SIZE)\n            mask = col_offsets < n_cols\n            abs_offsets = row_start + col_offsets\n            x_vals = tl.load(x_ptr + abs_offsets, mask=mask)\n            output_vals = x_vals - log_sum\n            tl.store(output_ptr + abs_offsets, output_vals, mask=mask)\n\n    But note: the row_sum is a scalar? Yes, we are using tl.sum to reduce the block to a scalar.\n\n    This kernel uses two passes: one for the online max+sum and one for the log_softmax.\n\n    We also change the ModelNew.forward to set:\n\n        BLOCK_SIZE = min(16384, triton.next_power_of_2(n_cols))\n\n    This will reduce the number of iterations for the loops.\n\n    We hope this will be faster.\n\n    However, note that the current program uses three passes and we are replacing with two passes? But each pass in the online algorithm does more work? But the number of memory accesses is the same: two passes over the data.\n\n    We are also using a larger block size to reduce the number of iterations? That should help.\n\n    Let's code accordingly.\n\n    Important: We must not break the correctness. We are using the same numerical method as the inspiration programs.\n\n    We'll try to run this and see if it improves the speed.\n\n    We also note that the online algorithm is numerically stable.\n\n    Now, let's write the entire code block.\n\n    We are replacing the entire kernel and the BLOCK_SIZE computation in the forward function.\n\n    Note: The kernel function name and parameters remain the same.\n\n    We also note that the current program has an if statement for the dimension and returns the torch.log_softmax if dim != 1. We leave that unchanged.\n\n    We also note that the current program handles n_cols==0? We leave that.\n\n    We'll write the new code.\n</think>\nTo improve the performance of the Triton kernel for log_softmax, I've implemented a more efficient two-pass algorithm with online max and sum computation. Key optimizations include:\n\n1. Replaced the three-pass approach with a two-pass algorithm that computes max and sum of exponentials in a single online pass\n2. Used numerical stabilization techniques to avoid overflow/underflow\n3. Increased BLOCK_SIZE to 16384 to reduce loop iterations for the given input size (16384 columns)\n4. Maintained all function signatures and boundary checks while improving memory access patterns\n\nThe changes reduce memory passes and improve arithmetic intensity while maintaining correctness. The larger block size minimizes loop overhead and better utilizes the GPU.\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _log_softmax_kernel(\n    x_ptr,\n    output_ptr,\n    stride_x,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    row_start = pid * stride_x\n    \n    # Initialize reduction variables\n    row_max = tl.full((), -float('inf'), dtype=tl.float32)\n    row_sum = tl.zeros((), dtype=tl.float32)\n    \n    # First pass: online max and sum computation\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        col_offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = col_offsets < n_cols\n        abs_offsets = row_start + col_offsets\n        x_vals = tl.load(x_ptr + abs_offsets, mask=mask, other=-float('inf'))\n        \n        # Update max and sum with numerical stability\n        local_max = tl.max(x_vals, axis=0)\n        new_max = tl.maximum(row_max, local_max)\n        scale = tl.exp(row_max - new_max)\n        row_sum = row_sum * scale\n        exp_vals = tl.exp(x_vals - new_max)\n        block_sum = tl.sum(exp_vals, axis=0)\n        row_sum = row_sum + block_sum\n        row_max = new_max\n    \n    # Compute log-sum-exp\n    log_sum = tl.log(row_sum) + row_max\n    \n    # Second pass: compute and store log_softmax\n    for block_start in range(0, n_cols, BLOCK_SIZE):\n        col_offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = col_offsets < n_cols\n        abs_offsets = row_start + col_offsets\n        x_vals = tl.load(x_ptr + abs_offsets, mask=mask)\n        output_vals = x_vals - log_sum\n        tl.store(output_ptr + abs_offsets, output_vals, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, dim: int = 1):\n        super(ModelNew, self).__init__()\n        self.dim = dim\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.dim != 1:\n            return torch.log_softmax(x, dim=self.dim)\n            \n        x = x.contiguous()\n        output = torch.empty_like(x)\n        n_rows, n_cols = x.shape\n        \n        if n_cols == 0:\n            return output\n            \n        # Use larger block size (capped at 16384) to reduce iterations\n        BLOCK_SIZE = min(16384, triton.next_power_of_2(n_cols))\n        grid = (n_rows,)\n        _log_softmax_kernel[grid](x, output, x.stride(0), n_cols, BLOCK_SIZE)\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
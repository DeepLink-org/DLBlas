{"id": "b6043500-2ccb-43cd-a4e0-1808104b3951", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=['batch_size'],\n)\n@triton.jit\ndef _hinge_loss_kernel(\n    pred_ptr, \n    target_ptr, \n    output_ptr, \n    batch_size: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    idx = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = idx < batch_size\n    \n    # Vectorized loads for better memory throughput\n    pred = tl.load(pred_ptr + idx, mask=mask, other=0.0)\n    target = tl.load(target_ptr + idx, mask=mask, other=0.0)\n    \n    product = pred * target\n    hinge = 1.0 - product\n    hinge = tl.where(hinge > 0, hinge, 0.0)\n    \n    # Efficient reduction with tree aggregation\n    partial_sum = tl.sum(hinge, axis=0)\n    tl.store(output_ptr + pid, partial_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, predictions, targets):\n        predictions_flat = predictions.contiguous().view(-1)\n        targets_flat = targets.contiguous().view(-1)\n        batch_size = predictions_flat.numel()\n        \n        if predictions_flat.is_cuda and batch_size > 0:\n            # Optimized block sizing for H100 architecture\n            BLOCK_SIZE = min(triton.next_power_of_2(batch_size), 1024)\n            grid = (triton.cdiv(batch_size, BLOCK_SIZE),)\n            partial_sums = torch.zeros(grid[0], device=predictions.device, dtype=torch.float32)\n            \n            _hinge_loss_kernel[grid](\n                predictions_flat, \n                targets_flat, \n                partial_sums, \n                batch_size, \n                BLOCK_SIZE=BLOCK_SIZE\n            )\n            \n            # Final reduction optimized for GPU\n            total_loss = torch.sum(partial_sums) / batch_size\n            return total_loss\n        else:\n            return torch.mean(torch.clamp(1 - predictions * targets, min=0))\n\nbatch_size = 128\ninput_shape = (1,)\ndim = 1\n\ndef get_inputs():\n    return [torch.randn(batch_size, *input_shape), torch.randint(0, 2, (batch_size, 1)).float() * 2 - 1]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "327a4491-0952-4e1d-888f-a6c67a490023", "generation": 3, "timestamp": 1754638212.37948, "iteration_found": 27, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.556, "runtime_ms": 0.086, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0478, "speed_up": 0.556, "custom_timing": 0.086}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.728, "runtime_ms": 0.0657, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0478, "speed_up": 0.728, "custom_timing": 0.0657}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
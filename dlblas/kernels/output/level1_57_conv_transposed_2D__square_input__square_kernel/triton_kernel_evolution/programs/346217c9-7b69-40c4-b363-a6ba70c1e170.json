{"id": "346217c9-7b69-40c4-b363-a6ba70c1e170", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef conv_transpose2d_kernel(\n    # Input pointers\n    input_ptr,\n    weight_ptr,\n    bias_ptr,\n    output_ptr,\n    # Tensor dimensions\n    batch_size,\n    in_channels,\n    out_channels,\n    height,\n    width,\n    kernel_size,\n    stride,\n    padding,\n    output_padding,\n    groups,\n    height_out,\n    width_out,\n    # Blocking\n    BLOCK_SIZE: tl.constexpr,\n    # Optional parameters\n    USE_BIAS: tl.constexpr,\n    # Meta-parameters\n    tl.constexpr\n):\n    # Calculate output element index\n    pid = tl.program_id(0)\n    pid_batch = pid // (out_channels * height_out * width_out)\n    pid_oc = (pid // (height_out * width_out)) % out_channels\n    pid_hw = pid % (height_out * width_out)\n    pid_h = pid_hw // width_out\n    pid_w = pid_hw % width_out\n    \n    # Compute group parameters\n    group_size_oc = out_channels // groups\n    group_index = pid_oc // group_size_oc\n    oc_in_group = pid_oc % group_size_oc\n    in_channels_per_group = in_channels // groups\n    group_start = group_index * in_channels_per_group\n    \n    # Initialize accumulator\n    accumulator = 0.0\n    \n    # Vectorized computation parameters\n    VEC_SIZE = 16  # Optimize for H100 vector units\n    num_vectors = tl.cdiv(in_channels_per_group, VEC_SIZE)\n    \n    # Loop over kernel positions\n    for ky in range(kernel_size):\n        for kx in range(kernel_size):\n            # Calculate input position\n            h_in = pid_h + padding - ky\n            w_in = pid_w + padding - kx\n            \n            # Check if position is in valid range and strided\n            if h_in % stride == 0 and w_in % stride == 0:\n                h_in //= stride\n                w_in //= stride\n                if 0 <= h_in < height and 0 <= w_in < width:\n                    # Vectorized channel processing\n                    for vec_idx in range(num_vectors):\n                        # Create channel range mask\n                        c_offset = vec_idx * VEC_SIZE\n                        c_mask = tl.arange(0, VEC_SIZE) < (in_channels_per_group - c_offset)\n                        \n                        # Compute input pointer offsets\n                        input_offset = (\n                            pid_batch * in_channels * height * width +\n                            (group_start + c_offset) * height * width +\n                            h_in * width +\n                            w_in\n                        )\n                        input_vals = tl.load(\n                            input_ptr + input_offset + tl.arange(0, VEC_SIZE) * height * width,\n                            mask=c_mask,\n                            other=0.0\n                        )\n                        \n                        # Compute weight pointer offsets\n                        weight_offset = (\n                            (group_start + c_offset) * (out_channels // groups) * kernel_size * kernel_size +\n                            oc_in_group * kernel_size * kernel_size +\n                            ky * kernel_size +\n                            kx\n                        )\n                        weight_vals = tl.load(\n                            weight_ptr + weight_offset + tl.arange(0, VEC_SIZE) * (out_channels // groups) * kernel_size * kernel_size,\n                            mask=c_mask,\n                            other=0.0\n                        )\n                        \n                        # FMA operations\n                        accumulator += tl.sum(input_vals * weight_vals)\n    \n    # Add bias if enabled\n    if USE_BIAS:\n        bias_val = tl.load(bias_ptr + pid_oc)\n        accumulator += bias_val\n    \n    # Store result\n    tl.store(output_ptr + pid, accumulator)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):\n        super(ModelNew, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.output_padding = output_padding\n        self.groups = groups\n        \n        # Initialize weights\n        self.weight = nn.Parameter(torch.empty(\n            in_channels,\n            out_channels // groups,\n            kernel_size,\n            kernel_size\n        ))\n        \n        # Initialize bias if requested\n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_channels))\n        else:\n            self.register_parameter('bias', None)\n            \n        # Reset parameters\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in)\n            nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Compute output dimensions\n        batch_size, _, height, width = x.shape\n        height_out = (height - 1) * self.stride - 2 * self.padding + self.kernel_size + self.output_padding\n        width_out = (width - 1) * self.stride - 2 * self.padding + self.kernel_size + self.output_padding\n        \n        # Allocate output tensor\n        output = torch.empty(\n            batch_size,\n            self.out_channels,\n            height_out,\n            width_out,\n            device=x.device,\n            dtype=x.dtype\n        )\n        \n        # Prepare kernel launch parameters\n        total_elements = batch_size * self.out_channels * height_out * width_out\n        grid = (total_elements,)\n        \n        # Launch kernel\n        conv_transpose2d_kernel[grid](\n            x, self.weight, \n            self.bias if self.bias is not None else tl.zeros(1, dtype=tl.float32),\n            output,\n            batch_size,\n            self.in_channels,\n            self.out_channels,\n            height,\n            width,\n            self.kernel_size,\n            self.stride,\n            self.padding,\n            self.output_padding,\n            self.groups,\n            height_out,\n            width_out,\n            USE_BIAS=(self.bias is not None),\n            BLOCK_SIZE=1024\n        )\n        \n        return output\n\n# Test code\nimport math\nbatch_size = 16\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nwidth = 128\nheight = 128\n\ndef get_inputs():\n    x = torch.randn(batch_size, in_channels, height, width)\n    return [x]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "d69f4461-681e-40c4-b6e0-2b9d5f4aeaf5", "generation": 1, "timestamp": 1754640708.6500556, "iteration_found": 3, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "", "exec_log": "[Child] \u7ed1\u5b9a GPU 7, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 32\n    tl.constexpr\n      ^\nSyntaxError: invalid syntax\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.175, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0558, "speed_up": 0.319, "custom_timing": 0.175}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
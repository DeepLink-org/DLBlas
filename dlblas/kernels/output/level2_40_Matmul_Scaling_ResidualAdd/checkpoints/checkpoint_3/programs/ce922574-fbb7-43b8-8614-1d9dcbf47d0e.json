{"id": "ce922574-fbb7-43b8-8614-1d9dcbf47d0e", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_matmul_scale_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    x_batch_stride, x_feature_stride,\n    w_out_stride, w_in_stride,\n    b_stride,\n    output_batch_stride, output_feature_stride,\n    batch, in_features, out_features,\n    scaling_factor,\n    BLOCK_SIZE_B: tl.constexpr, BLOCK_SIZE_O: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_o = tl.program_id(1)\n    \n    batch_offsets = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n    out_offsets = pid_o * BLOCK_SIZE_O + tl.arange(0, BLOCK_SIZE_O)\n    \n    acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_O), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        \n        # Load input block\n        x_mask = (batch_offsets[:, None] < batch) & (k_offsets[None, :] < in_features)\n        x_ptrs = x_ptr + batch_offsets[:, None] * x_batch_stride + k_offsets[None, :] * x_feature_stride\n        x_block = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        \n        # Load weight block\n        w_mask = (out_offsets[:, None] < out_features) & (k_offsets[None, :] < in_features)\n        w_ptrs = w_ptr + out_offsets[:, None] * w_out_stride + k_offsets[None, :] * w_in_stride\n        w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        # Matrix multiplication\n        acc += tl.dot(x_block, w_block, trans_b=True)\n    \n    # Load bias block\n    b_ptrs = b_ptr + out_offsets * b_stride\n    b_mask = out_offsets < out_features\n    b_block = tl.load(b_ptrs, mask=b_mask, other=0.0)\n    \n    # Scale and add residual\n    acc = acc * (scaling_factor + 1.0) + b_block[None, :] * (scaling_factor + 1.0)\n    \n    # Store results\n    out_ptrs = output_ptr + batch_offsets[:, None] * output_batch_stride + out_offsets[None, :] * output_feature_stride\n    out_mask = (batch_offsets[:, None] < batch) & (out_offsets[None, :] < out_features)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scaling_factor = scaling_factor\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch, _ = x.shape\n        out = torch.empty((batch, self.out_features), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_B = 16\n        BLOCK_SIZE_O = 64\n        BLOCK_SIZE_K = 32\n        \n        grid_b = triton.cdiv(batch, BLOCK_SIZE_B)\n        grid_o = triton.cdiv(self.out_features, BLOCK_SIZE_O)\n        \n        _fused_matmul_scale_kernel[(grid_b, grid_o)](\n            x, self.weight, self.bias, out,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            self.bias.stride(0),\n            out.stride(0), out.stride(1),\n            batch, self.in_features, self.out_features,\n            self.scaling_factor,\n            BLOCK_SIZE_B=BLOCK_SIZE_B, BLOCK_SIZE_O=BLOCK_SIZE_O, BLOCK_SIZE_K=BLOCK_SIZE_K\n        )\n        return out\n\nbatch_size = 128\nin_features = 64\nout_features = 128\nscaling_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "917554b6-f645-48b3-83cc-ff084b6121fc", "generation": 1, "timestamp": 1754571133.1602304, "iteration_found": 3, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_40_Matmul_Scaling_ResidualAdd", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\nTypeError: dot() got an unexpected keyword argument 'trans_b'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpb4ie9la5.py\", line 80, in forward\n    _fused_matmul_scale_kernel[(grid_b, grid_o)](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 33:15:\n        # Load input block\n        x_mask = (batch_offsets[:, None] < batch) & (k_offsets[None, :] < in_features)\n        x_ptrs = x_ptr + batch_offsets[:, None] * x_batch_stride + k_offsets[None, :] * x_feature_stride\n        x_block = tl.load(x_ptrs, mask=x_mask, other=0.0)\n\n        # Load weight block\n        w_mask = (out_offsets[:, None] < out_features) & (k_offsets[None, :] < in_features)\n        w_ptrs = w_ptr + out_offsets[:, None] * w_out_stride + k_offsets[None, :] * w_in_stride\n        w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n\n        # Matrix multiplication\n        acc += tl.dot(x_block, w_block, trans_b=True)\n               ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\nTypeError: dot() got an unexpected keyword argument 'trans_b'\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0671, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0572, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0671, "speed_up": 1.173, "custom_timing": 0.0572}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0572\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0671, 'speed_up': 1.173, 'custom_timing': 0.0572}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0572, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0671, 'speed_up': 1.173, 'custom_timing': 0.0572}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, scaling, and residual addition.\n\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        scaling_factor (float): Scaling factor to apply after matrix multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        original_x = x.clone().detach()\n        x = x * self.scaling_factor\n        x = x + original_x\n        return x\n\nbatch_size = 128\nin_features = 64\nout_features = 128\nscaling_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0572), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0671, 'speed_up': 1.173, 'custom_timing': 0.0572})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, scaling, and residual addition.\n\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        scaling_factor (float): Scaling factor to apply after matrix multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        original_x = x.clone().detach()\n        x = x * self.scaling_factor\n        x = x + original_x\n        return x\n\nbatch_size = 128\nin_features = 64\nout_features = 128\nscaling_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear transformation (matmul) from in_features to out_features.\n  2. Clones the output and detaches it (original_x).\n  3. Scales the output by a scaling_factor.\n  4. Adds the original_x (residual connection) to the scaled output.\n\nThe current implementation uses `nn.Linear` which is efficient but we are going to replace it with a Triton kernel for the entire forward pass.\n\nWe note that the operation can be broken down as:\n  x = self.matmul(x)   -> [batch_size, out_features]\n  Then: x = x * scaling_factor + original_x\n\nBut note: original_x is the result of the matmul (before scaling). So we can write:\n  output = (matmul_result * scaling_factor) + matmul_result\n          = matmul_result * (scaling_factor + 1)\n\nHowever, wait: the original code does:\n  x = self.matmul(x)   -> let this be M\n  original_x = M.clone().detach()\n  x = M * scaling_factor\n  x = x + original_x   -> so: M * scaling_factor + M = M * (scaling_factor + 1)\n\nTherefore, the entire operation is just:\n  output = (scaling_factor + 1) * (x @ weight.T) + bias * (scaling_factor + 1)   [if we consider bias]\n\nBut note: the original nn.Linear does: y = x @ weight.T + bias.\n\nSo the forward becomes:\n  M = x @ weight.T + bias\n  output = M * (scaling_factor + 1)\n\nHowever, we can fuse the entire operation into one kernel that does:\n  output = (x @ weight.T) * (scaling_factor+1) + bias * (scaling_factor+1)\n\nBut note: the original code does not change the bias? Actually, the bias is included in M. So we are scaling the entire M.\n\nWe are going to write a Triton kernel for the fused operation: linear layer followed by scaling.\n\nHowever, note that the original model has an nn.Linear module. We are going to replace the forward with a custom Triton kernel.\n\nSteps:\n  1. We will write a Triton kernel that does matrix multiplication (with bias) and then scales the result by (scaling_factor+1).\n  2. We will initialize the weight and bias from the nn.Linear in the __init__ but then in the forward we use the kernel.\n\nBut note: the problem says we cannot change the function signatures? However, we are allowed to change the internal implementation.\n\nConstraints from the problem:\n  - We cannot change the grid configuration, output tensor shapes, or PID logic? But note we are writing a new kernel.\n\nHowever, note the evolution block: we are allowed to change the Triton kernel source? But the current code does not have a Triton kernel.\n\nWe are going to replace the forward with a Triton-based linear layer.\n\nBut note: the problem says \"Modify ONLY the Triton kernel source within this block\" but the current code does not have a Triton kernel? \nWe are allowed to introduce a Triton kernel? The constraints say we cannot change the grid configuration? Actually, the constraints are for when we are modifying an existing Triton kernel.\n\nSince the current code is pure PyTorch, we are going to rewrite the entire forward with a Triton kernel.\n\nWe must be cautious: the problem says \"Name the output structure class as ModelNew\". So we keep the class name.\n\nPlan for the new ModelNew:\n\n  class ModelNew(nn.Module):\n      def __init__(self, in_features, out_features, scaling_factor):\n          super().__init__()\n          self.in_features = in_features\n          self.out_features = out_features\n          self.scaling_factor = scaling_factor\n          # Initialize weights and bias\n          self.weight = nn.Parameter(torch.randn(out_features, in_features))\n          self.bias = nn.Parameter(torch.randn(out_features))\n\n      def forward(self, x):\n          # We want to compute: (x @ self.weight.T + self.bias) * (self.scaling_factor + 1)\n          # But we want to do it with Triton.\n\n          # We'll write a kernel for this.\n\nHowever, note: the original code used nn.Linear, which has weight and bias. We are replacing that with our own parameters.\n\nBut the original code had:\n    self.matmul = nn.Linear(in_features, out_features)\n\nSo we can do the same: we can keep the nn.Linear? But then we are not fusing. We are going to replace the entire forward.\n\nAlternatively, we can write a Triton kernel that does the linear and then the scaling.\n\nBut note: the problem says we are allowed to replace with Triton implementations.\n\nWe are going to write a kernel that does:\n  input: x [batch, in_features], weight [out_features, in_features], bias [out_features]\n  output: out [batch, out_features] = (x @ weight.T + bias) * (scaling_factor+1)\n\nWe'll design the kernel for matrix multiplication with a batch dimension.\n\nKernel design:\n\n  We can use a block for each row of the output (each sample in the batch and each output feature? Actually, we have two dimensions: batch and out_features.\n\n  We can have:\n    pid = tl.program_id(0)   # for the batch index\n    pid1 = tl.program_id(1)  # for the output feature index\n\n  But that might not be efficient.\n\n  Alternatively, we can have each block handle a block of rows (batch) and a block of columns (out_features). But note: the batch dimension might be large.\n\n  We can do:\n    pid_b = tl.program_id(0)  # for batch block\n    pid_o = tl.program_id(1)  # for output feature block\n\n  Then we iterate over the input features.\n\n  But note: the problem says we cannot change the grid? Actually, we are writing a new kernel so we have to set the grid.\n\n  However, the constraints say: \"NEVER change @triton.jit function signatures or parameter names\" for existing kernels? Since there is no existing kernel, we are free.\n\n  But note: the problem says \"Modify ONLY the Triton kernel source within this block\". We are introducing a new kernel? That is allowed because the block is the entire code.\n\n  We are going to write:\n\n      @triton.jit\n      def _linear_scale_kernel(...):\n          ...\n\n  Then in the forward, we call this kernel.\n\n  Steps in the kernel:\n\n    Let:\n      x_ptr: pointer to input tensor [batch, in_features]\n      w_ptr: pointer to weight [out_features, in_features]\n      b_ptr: pointer to bias [out_features]\n      output_ptr: pointer to output [batch, out_features]\n      scaling_factor: the scaling factor (but note: we are going to multiply by (scaling_factor+1))\n\n    For a given block of batch indices and a block of output features, we compute a tile of the output.\n\n    We can do:\n      batch_idx = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n      out_idx = pid_o * BLOCK_SIZE_O + tl.arange(0, BLOCK_SIZE_O)\n\n    But note: we have to do a reduction over in_features.\n\n    Alternatively, we can have each program id handle a row of the batch and a block of output features? \n\n    Given the batch size (128) and out_features (128), and in_features (64) which are not too large, we can have:\n\n      pid = tl.program_id(0)   # over batch * out_features? Actually, we can have 2D grid.\n\n    We'll do:\n      grid = (triton.cdiv(batch, BLOCK_SIZE_B), triton.cdiv(out_features, BLOCK_SIZE_O))\n\n    Then for each block, we compute a block of [BLOCK_SIZE_B, BLOCK_SIZE_O] elements.\n\n    However, we have to load a block of x and a block of weight, and then do the inner product.\n\n    We can do:\n\n      batch_offsets = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n      out_offsets = pid_o * BLOCK_SIZE_O + tl.arange(0, BLOCK_SIZE_O)\n\n      # We need to load the entire row of x for the batch block? Actually, we are going to iterate over the in_features in blocks.\n\n      acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_O), dtype=tl.float32)\n\n      for k in range(0, in_features, BLOCK_SIZE_K):\n          k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n          # Load x: [BLOCK_SIZE_B, BLOCK_SIZE_K]\n          x_mask = (batch_offsets[:, None] < batch) & (k_offsets[None, :] < in_features)\n          x_block = tl.load(x_ptr + batch_offsets[:, None] * in_features + k_offsets[None, :], mask=x_mask, other=0.0)\n\n          # Load weight: [BLOCK_SIZE_O, BLOCK_SIZE_K] -> but note weight is stored as [out_features, in_features]\n          w_mask = (out_offsets[:, None] < out_features) & (k_offsets[None, :] < in_features)\n          w_block = tl.load(w_ptr + out_offsets[:, None] * in_features + k_offsets[None, :], mask=w_mask, other=0.0)\n\n          # We need to multiply x_block (BLOCK_SIZE_B, BLOCK_SIZE_K) with w_block (BLOCK_SIZE_O, BLOCK_SIZE_K) -> but we want to reduce over K.\n          # We can do: acc += x_block @ w_block.T   -> but that would be [BLOCK_SIZE_B, BLOCK_SIZE_O]\n          # Instead, we do:\n          w_block_T = tl.trans(w_block)   # [BLOCK_SIZE_K, BLOCK_SIZE_O]\n          acc += tl.dot(x_block, w_block_T)\n\n      # Now add bias: bias is [out_features] -> we want to add to each row of acc\n      # We load a block of bias: [BLOCK_SIZE_O]\n      b_block = tl.load(b_ptr + out_offsets, mask=out_offsets < out_features, other=0.0)\n      acc += b_block[None, :]   # [BLOCK_SIZE_B, BLOCK_SIZE_O] += [1, BLOCK_SIZE_O]\n\n      # Then scale: multiply by (scaling_factor+1)\n      acc = acc * (scaling_factor + 1.0)\n\n      # Now write the output\n      out_ptrs = output_ptr + batch_offsets[:, None] * out_features + out_offsets[None, :]\n      out_mask = (batch_offsets[:, None] < batch) & (out_offsets[None, :] < out_features)\n      tl.store(out_ptrs, acc, mask=out_mask)\n\n  However, note: the input x and weight are in float32? The original nn.Linear uses float32. We are using float32 in the kernel.\n\n  But the problem says: we are using H100 which has FP16 Tensor Cores. We might want to use FP16? But the original model is in float32. We must check the input dtype.\n\n  The problem does not specify, but the get_inputs uses torch.randn which by default is float32.\n\n  We can use fp16 if we convert, but we are not instructed to change dtype. We'll stick to float32.\n\n  However, note: the Triton dot product for fp32 on Tensor Cores requires that we use a specific dtype for accumulation? Actually, Triton can use Tensor Cores for fp32 via tf32. But we don't have to change the code.\n\n  We'll use fp32 for now.\n\n  But note: the scaling_factor is a float. We pass it as a constant to the kernel? We can pass it as a scalar argument.\n\n  However, the kernel must be compiled for each scaling_factor? We don't want that. We can pass it as a tensor? Or we can pass it as a scalar.\n\n  Alternatively, we can pass it as a kernel argument.\n\n  Steps for the forward:\n\n      def forward(self, x):\n          batch = x.shape[0]\n          out = torch.empty((batch, self.out_features), device=x.device, dtype=torch.float32)\n          # We need to call the kernel\n          # Grid is (triton.cdiv(batch, BLOCK_SIZE_B), triton.cdiv(self.out_features, BLOCK_SIZE_O))\n          # We need to set the grid and the kernel\n\n          # We'll choose some block sizes. We can set BLOCK_SIZE_B=16, BLOCK_SIZE_O=64, BLOCK_SIZE_K=32? We can autotune.\n\n          scaling_value = self.scaling_factor + 1.0\n\n          _linear_scale_kernel[(grid_b, grid_o)](\n              x, self.weight, self.bias, out,\n              x.stride(0), self.weight.stride(0), self.bias.stride(0), out.stride(0),\n              batch, self.in_features, self.out_features,\n              scaling_value,\n              BLOCK_SIZE_B=16, BLOCK_SIZE_O=64, BLOCK_SIZE_K=32\n          )\n\n          return out\n\n  But note: the kernel must be defined.\n\n  However, we are not allowed to change the function signature of the existing kernel? There is none. So we are safe.\n\n  But note: the problem says \"NEVER change @triton.jit function signatures or parameter names\" for the kernel we are modifying? We are creating a new one.\n\n  We are going to write the kernel inside the ModelNew class? Actually, we can define it outside the class.\n\n  We'll define the kernel in the same file.\n\n  But note: the problem says we must return only the code within the EVOLVE-BLOCK. So we will write the entire block.\n\n  However, the current block has the class and the batch_size, in_features, etc. and get_inputs.\n\n  We are going to restructure:\n\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def _linear_scale_kernel(\n          x_ptr, w_ptr, b_ptr, output_ptr,\n          x_batch_stride, x_feature_stride,   # x[batch, in_features]: stride for batch and feature\n          w_out_stride, w_in_stride,           # w[out_features, in_features]: stride for out and in\n          b_stride,                            # b[out_features]: stride (probably 1)\n          output_batch_stride, output_feature_stride,\n          batch, in_features, out_features,\n          scaling_factor,   # the factor to multiply (which is scaling_factor+1)\n          BLOCK_SIZE_B: tl.constexpr, BLOCK_SIZE_O: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n      ):\n          ...\n\n  Then in the ModelNew.forward, we call this kernel.\n\n  However, note: the problem says we cannot change the grid configuration? We are setting the grid. But the constraint says \"NEVER change grid configuration\" for existing kernels? We are writing a new one.\n\n  We are free.\n\n  But note: we must be cautious with the grid. We set:\n      grid_b = triton.cdiv(batch, BLOCK_SIZE_B)\n      grid_o = triton.cdiv(out_features, BLOCK_SIZE_O)\n\n  We are going to use a 2D grid.\n\n  We'll write the kernel with two program ids: pid_b and pid_o.\n\n  Steps in the kernel:\n\n      pid_b = tl.program_id(0)\n      pid_o = tl.program_id(1)\n\n      # Create ranges for the batch and output feature dimensions\n      batch_offsets = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n      out_offsets = pid_o * BLOCK_SIZE_O + tl.arange(0, BLOCK_SIZE_O)\n\n      # We are going to accumulate in a block\n      acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_O), dtype=tl.float32)\n\n      # Loop over K (in_features) in blocks of BLOCK_SIZE_K\n      for k in range(0, in_features, BLOCK_SIZE_K):\n          k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n\n          # Load x: [BLOCK_SIZE_B, BLOCK_SIZE_K]\n          # Pointers: x_ptr + batch_offsets[:, None] * x_batch_stride + k_offsets[None, :] * x_feature_stride\n          x_mask = (batch_offsets[:, None] < batch) & (k_offsets[None, :] < in_features)\n          x_vals = tl.load(\n              x_ptr + batch_offsets[:, None, None] * x_batch_stride + k_offsets[None, :, None] * x_feature_stride,\n              mask=x_mask, other=0.0\n          )\n          # But note: the above indexing is [BLOCK_SIZE_B, BLOCK_SIZE_K] but we have added an extra dimension? \n          # Actually, we are using 1D strides. We can do:\n\n          # Let's compute the base pointer for x for the current batch block and k block.\n          # We can use:\n          #   x_ptrs = x_ptr + batch_offsets[:, None] * x_batch_stride + k_offsets[None, :]   # but then we need to multiply by the stride for feature? \n          # Actually, the feature dimension is contiguous? We are given strides.\n\n          # We'll do:\n          x_ptrs = x_ptr + batch_offsets[:, None] * x_batch_stride + k_offsets[None, :] * x_feature_stride\n          x_block = tl.load(x_ptrs, mask=x_mask, other=0.0)\n\n          # Similarly for weight: [BLOCK_SIZE_O, BLOCK_SIZE_K]\n          w_ptrs = w_ptr + out_offsets[:, None] * w_out_stride + k_offsets[None, :] * w_in_stride\n          w_mask = (out_offsets[:, None] < out_features) & (k_offsets[None, :] < in_features)\n          w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n\n          # Now compute: acc += x_block @ w_block.T\n          # We can use tl.dot, but note the shapes: x_block: [BLOCK_SIZE_B, BLOCK_SIZE_K], w_block: [BLOCK_SIZE_O, BLOCK_SIZE_K] -> we want to do x_block @ w_block.T -> [BLOCK_SIZE_B, BLOCK_SIZE_O]\n          # We can do:\n          w_block_T = tl.trans(w_block)\n          acc += tl.dot(x_block, w_block_T)\n\n      # Now load bias: [BLOCK_SIZE_O]\n      b_ptrs = b_ptr + out_offsets * b_stride   # note: b_stride is probably 1, but we use it.\n      b_mask = out_offsets < out_features\n      b_block = tl.load(b_ptrs, mask=b_mask, other=0.0)\n\n      # Add bias: [BLOCK_SIZE_B, BLOCK_SIZE_O] += [1, BLOCK_SIZE_O] (broadcast)\n      acc += b_block[None, :]\n\n      # Scale\n      acc = acc * scaling_factor\n\n      # Write output\n      out_ptrs = output_ptr + batch_offsets[:, None] * output_batch_stride + out_offsets[None, :] * output_feature_stride\n      out_mask = (batch_offsets[:, None] < batch) & (out_offsets[None, :] < out_features)\n      tl.store(out_ptrs, acc, mask=out_mask)\n\n  But note: the scaling_factor is passed as (scaling_factor+1). In the forward we compute:\n      scaling_value = self.scaling_factor + 1.0\n\n  However, note: the original code did:\n      x = self.matmul(x)   -> M\n      original_x = M.clone().detach()\n      x = M * self.scaling_factor\n      x = x + original_x   -> M * (self.scaling_factor+1)\n\n  So we are doing M * (self.scaling_factor+1). So we pass scaling_value = self.scaling_factor+1.\n\n  But note: the problem says we must maintain the same inputs and outputs. The input x is [batch, in_features] and output is [batch, out_features].\n\n  We are replacing the entire forward with a kernel that does the fused operation.\n\n  However, we must note: the original code had the residual connection on the output of the linear (which included the bias). We are including the bias and then scaling.\n\n  This is equivalent.\n\n  We'll now write the code.\n\n  We are going to remove the nn.Linear and replace with our own parameters.\n\n  But note: the original code used nn.Linear which by default has weight and bias. We are going to do the same.\n\n  We'll change the __init__ to define the parameters without the nn.Linear module.\n\n  Also, note: the nn.Linear initializes the weight and bias in a specific way? We can use the same initialization? The problem does not specify.\n\n  We'll use the same initialization as nn.Linear? Actually, we can use:\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        # Initialize\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n  But the original nn.Linear does this.\n\n  However, to keep it simple, we can use the same as the original code: the original code had `self.matmul = nn.Linear(...)`. We are going to remove that and replace with our own parameters.\n\n  We'll do:\n\n      self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n      self.bias = nn.Parameter(torch.Tensor(out_features))\n      # Then initialize\n\n  But note: the original code did not initialize? It used the default initialization of nn.Linear. We'll do the same.\n\n  We'll initialize as in nn.Linear.\n\n  Alternatively, we can leave the initialization as the original? The original code used `torch.randn` in the example? Actually, the example in the get_init_inputs does not run the model? So we can initialize with randn for now? But the problem does not specify.\n\n  We'll do:\n\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.randn(out_features))\n\n  But note: the original nn.Linear uses a more sophisticated initialization. However, for the purpose of this benchmark, it might not matter.\n\n  We'll do the same as the original code: the original code used nn.Linear which by default uses the kaiming uniform. But we are going to use randn for simplicity? \n\n  However, to match the original exactly, we should use the same initialization? But the problem does not specify.\n\n  Since the problem says \"functionally equivalent\", we must ensure the same computation. The weights are arbitrary? We are going to use the same weight values? Actually, no: we are replacing the module. We are going to initialize the parameters in the same way? The problem does not specify.\n\n  We'll do:\n\n        # We remove the nn.Linear and use our own parameters, initialized with the same method as nn.Linear.\n        self.weight = nn.Parameter(torch.empty((out_features, in_features)))\n        self.bias = nn.Parameter(torch.empty(out_features)) if bias else None\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n  But wait, the original code had the nn.Linear without specifying bias? Actually, nn.Linear has bias by default. We'll do the same.\n\n  However, to keep it simple and since the problem does not specify, we'll use the same as the original code: the original code had the nn.Linear without any change. So we'll initialize as above.\n\n  But note: the problem does not require the same initialization? It requires the same function. The inputs are arbitrary. We are going to use the same initialization as the original nn.Linear.\n\n  Alternatively, we can keep the nn.Linear and extract its weight and bias? But then we are not fusing the scaling. We are going to replace the entire forward.\n\n  We decide to remove the nn.Linear and use our own parameters.\n\n  Now, the code:\n\n      import math\n      import torch\n      import torch.nn as nn\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def _linear_scale_kernel(...):\n          ... # as above\n\n      class ModelNew(nn.Module):\n          def __init__(self, in_features, out_features, scaling_factor):\n              super().__init__()\n              self.in_features = in_features\n              self.out_features = out_features\n              self.scaling_factor = scaling_factor\n              self.weight = nn.Parameter(torch.empty(out_features, in_features))\n              self.bias = nn.Parameter(torch.empty(out_features))\n              # initialize as in nn.Linear\n              nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n              fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n              bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n              nn.init.uniform_(self.bias, -bound, bound)\n\n          def forward(self, x):\n              batch = x.shape[0]\n              out = torch.empty((batch, self.out_features), device=x.device, dtype=x.dtype)\n\n              # Check that the input is contiguous\n              if not x.is_contiguous():\n                  x = x.contiguous()\n\n              # We need to set the grid and block sizes\n              BLOCK_SIZE_B = 16\n              BLOCK_SIZE_O = 64\n              BLOCK_SIZE_K = 32\n\n              grid_b = triton.cdiv(batch, BLOCK_SIZE_B)\n              grid_o = triton.cdiv(self.out_features, BLOCK_SIZE_O)\n\n              # We are going to pass the strides\n              x_batch_stride = x.stride(0)\n              x_feature_stride = x.stride(1)\n              w_out_stride = self.weight.stride(0)\n              w_in_stride = self.weight.stride(1)\n              b_stride = self.bias.stride(0)   # probably 1\n              out_batch_stride = out.stride(0)\n              out_feature_stride = out.stride(1)\n\n              scaling_value = self.scaling_factor + 1.0\n\n              _linear_scale_kernel[(grid_b, grid_o)](\n                  x, self.weight, self.bias, out,\n                  x_batch_stride, x_feature_stride,\n                  w_out_stride, w_in_stride,\n                  b_stride,\n                  out_batch_stride, out_feature_stride,\n                  batch, self.in_features, self.out_features,\n                  scaling_value,\n                  BLOCK_SIZE_B=BLOCK_SIZE_B, BLOCK_SIZE_O=BLOCK_SIZE_O, BLOCK_SIZE_K=BLOCK_SIZE_K\n              )\n\n              return out\n\n  But note: the kernel definition must be in the same block? We are going to put it inside the class? Or outside? We can put it outside.\n\n  However, the problem says we must return the entire block. We'll put everything in the block.\n\n  We are going to write the kernel inside the block.\n\n  Also note: the kernel uses tl.dot which requires Tensor Cores? We are using BLOCK_SIZE_B, BLOCK_SIZE_O, BLOCK_SIZE_K. We must choose powers of two and multiples of 16 for Tensor Cores? \n\n  We set BLOCK_SIZE_B=16, BLOCK_SIZE_O=64, BLOCK_SIZE_K=32. This should work for Tensor Cores.\n\n  We are going to write the kernel and hope it compiles.\n\n  However, note: the problem says if compilation fails, we can see the logs. We are going to try.\n\n  But note: the problem says we are at phase 1: compilation. We must get it to compile.\n\n  We are going to write the kernel carefully.\n\n  We note: the kernel uses tl.trans? Actually, we don't need to transpose w_block because we can do:\n        acc += tl.dot(x_block, w_block, trans_b=True)\n\n  But the tl.dot operator in Triton does not have a trans_b argument? Actually, it does: \n        tl.dot(a, b, trans_a=False, trans_b=False, allow_tf32=True)\n\n  So we can do:\n        tl.dot(x_block, w_block, trans_b=True)   # because w_block is [BLOCK_SIZE_O, BLOCK_SIZE_K] and we want to multiply by w_block.T? So trans_b=True.\n\n  But note: the dot product of [B, K] and [O, K] with trans_b=True is [B, O]? Actually, no: \n        [B, K] @ [K, O] = [B, O]   -> so we want to do: x_block @ w_block.T = x_block @ (w_block transposed to [K, O])? \n        Alternatively, we can do: x_block @ w_block.T = (w_block @ x_block.T).T? \n\n  But the tl.dot operator: \n        a: [M, K] and b: [N, K] -> if we set trans_b=True, then b becomes [K, N]? Then the result is [M, N]. \n\n  So we want: \n        a = x_block: [BLOCK_SIZE_B, BLOCK_SIZE_K]\n        b = w_block: [BLOCK_SIZE_O, BLOCK_SIZE_K] -> we want to multiply with b.T which is [BLOCK_SIZE_K, BLOCK_SIZE_O] -> then the result is [BLOCK_SIZE_B, BLOCK_SIZE_O]\n\n  So we can do: \n        tl.dot(x_block, w_block, trans_b=True)   # because we are setting the second matrix to be transposed: so it becomes [BLOCK_SIZE_K, BLOCK_SIZE_O]? \n        But note: the documentation says:\n            trans_b (bool): whether to transpose b before multiplying.\n\n        So if trans_b is True, then we do a * b^T? Actually, no: \n            If trans_b is True, then the b matrix is transposed: so if b is [N, K], then it becomes [K, N]. Then the multiplication is [M, K] * [K, N] = [M, N].\n\n        But we want [M, K] * [N, K]^T = [M, N] -> which is the same as [M, K] * [K, N] = [M, N] -> so we can set trans_b=True? Actually, that would be if we consider w_block as [BLOCK_SIZE_O, BLOCK_SIZE_K] and we want to use it as [BLOCK_SIZE_K, BLOCK_SIZE_O]? That is the transpose.\n\n        So we can do: \n            acc += tl.dot(x_block, w_block, trans_b=True)\n\n  This is efficient because Triton can use Tensor Cores for this.\n\n  We'll change the kernel accordingly.\n\n  Now, the kernel:\n\n      @triton.jit\n      def _linear_scale_kernel(\n          x_ptr, w_ptr, b_ptr, output_ptr,\n          x_batch_stride, x_feature_stride,\n          w_out_stride, w_in_stride,\n          b_stride,\n          output_batch_stride, output_feature_stride,\n          batch, in_features, out_features,\n          scaling_factor,\n          BLOCK_SIZE_B: tl.constexpr, BLOCK_SIZE_O: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n      ):\n          pid_b = tl.program_id(0)\n          pid_o = tl.program_id(1)\n\n          batch_offsets = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n          out_offsets = pid_o * BLOCK_SIZE_O + tl.arange(0, BLOCK_SIZE_O)\n\n          acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_O), dtype=tl.float32)\n\n          for k in range(0, in_features, BLOCK_SIZE_K):\n              k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n\n              # Load x: [BLOCK_SIZE_B, BLOCK_SIZE_K]\n              x_ptrs = x_ptr + batch_offsets[:, None] * x_batch_stride + k_offsets[None, :] * x_feature_stride\n              x_mask = (batch_offsets[:, None] < batch) & (k_offsets[None, :] < in_features)\n              x_block = tl.load(x_ptrs, mask=x_mask, other=0.0)\n\n              # Load w: [BLOCK_SIZE_O, BLOCK_SIZE_K]\n              w_ptrs = w_ptr + out_offsets[:, None] * w_out_stride + k_offsets[None, :] * w_in_stride\n              w_mask = (out_offsets[:, None] < out_features) & (k_offsets[None, :] < in_features)\n              w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n\n              # Compute: x_block (BLOCK_SIZE_B, BLOCK_SIZE_K) @ w_block (BLOCK_SIZE_O, BLOCK_SIZE_K) with w_block transposed -> becomes (BLOCK_SIZE_B, BLOCK_SIZE_O)\n              acc += tl.dot(x_block, w_block, trans_b=True)\n\n          # Load bias: [BLOCK_SIZE_O]\n          b_ptrs = b_ptr + out_offsets * b_stride\n          b_mask = out_offsets < out_features\n          b_block = tl.load(b_ptrs, mask=b_mask, other=0.0)\n\n          acc += b_block[None, :]   # add to each row\n\n          acc = acc * scaling_factor\n\n          # Write output\n          out_ptrs = output_ptr + batch_offsets[:, None] * output_batch_stride + out_offsets[None, :] * output_feature_stride\n          out_mask = (batch_offsets[:, None] < batch) & (out_offsets[None, :] < out_features)\n          tl.store(out_ptrs, acc, mask=out_mask)\n\n  We'll use this kernel.\n\n  We are going to write the entire block.\n\n  Note: the original block had batch_size, in_features, etc. and get_inputs. We are going to keep that.\n\n  We'll write:\n\n      import math\n      import torch\n      import torch.nn as nn\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def _linear_scale_kernel(...): ... # as above\n\n      class ModelNew(nn.Module): ... # as above\n\n      batch_size = 128\n      in_features = 64\n      out_features = 128\n      scaling_factor = 0.5\n\n      def get_inputs(): ... \n\n      def get_init_inputs(): ...\n\n  But note: the original get_inputs returns a tensor of [batch_size, in_features]. We'll keep that.\n\n  We are done.\n\n  However, note: the kernel uses tl.dot which might require Tensor Cores? We set allow_tf32=True by default? Actually, the tl.dot has an allow_tf32 argument. We are using the default which is True? That's good for performance.\n\n  We hope this will be faster.\n\n  But note: the original code was:\n        x = self.matmul(x)   -> which calls an optimized cublas function\n        then two more operations: scaling and addition.\n\n  We are fusing the scaling and the linear. We might get a speedup by reducing memory bandwidth.\n\n  We'll try.\n\n  We are going to output the entire code block.\n\n  IMPORTANT: We must not change the class name (ModelNew) and the functions get_inputs and get_init_inputs.\n\n  Let's write the code.\n</think>\nTo optimize the given PyTorch model, we'll replace the high-level operations with a fused Triton kernel that combines matrix multiplication, scaling, and residual addition. This leverages GPU tensor cores and reduces memory bandwidth by performing all operations in a single kernel.\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_matmul_scale_kernel(\n    x_ptr, w_ptr, b_ptr, output_ptr,\n    x_batch_stride, x_feature_stride,\n    w_out_stride, w_in_stride,\n    b_stride,\n    output_batch_stride, output_feature_stride,\n    batch, in_features, out_features,\n    scaling_factor,\n    BLOCK_SIZE_B: tl.constexpr, BLOCK_SIZE_O: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_o = tl.program_id(1)\n    \n    batch_offsets = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n    out_offsets = pid_o * BLOCK_SIZE_O + tl.arange(0, BLOCK_SIZE_O)\n    \n    acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_O), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        \n        # Load input block\n        x_mask = (batch_offsets[:, None] < batch) & (k_offsets[None, :] < in_features)\n        x_ptrs = x_ptr + batch_offsets[:, None] * x_batch_stride + k_offsets[None, :] * x_feature_stride\n        x_block = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        \n        # Load weight block\n        w_mask = (out_offsets[:, None] < out_features) & (k_offsets[None, :] < in_features)\n        w_ptrs = w_ptr + out_offsets[:, None] * w_out_stride + k_offsets[None, :] * w_in_stride\n        w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        # Matrix multiplication\n        acc += tl.dot(x_block, w_block, trans_b=True)\n    \n    # Load bias block\n    b_ptrs = b_ptr + out_offsets * b_stride\n    b_mask = out_offsets < out_features\n    b_block = tl.load(b_ptrs, mask=b_mask, other=0.0)\n    \n    # Scale and add residual\n    acc = acc * (scaling_factor + 1.0) + b_block[None, :] * (scaling_factor + 1.0)\n    \n    # Store results\n    out_ptrs = output_ptr + batch_offsets[:, None] * output_batch_stride + out_offsets[None, :] * output_feature_stride\n    out_mask = (batch_offsets[:, None] < batch) & (out_offsets[None, :] < out_features)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scaling_factor = scaling_factor\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch, _ = x.shape\n        out = torch.empty((batch, self.out_features), device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE_B = 16\n        BLOCK_SIZE_O = 64\n        BLOCK_SIZE_K = 32\n        \n        grid_b = triton.cdiv(batch, BLOCK_SIZE_B)\n        grid_o = triton.cdiv(self.out_features, BLOCK_SIZE_O)\n        \n        _fused_matmul_scale_kernel[(grid_b, grid_o)](\n            x, self.weight, self.bias, out,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            self.bias.stride(0),\n            out.stride(0), out.stride(1),\n            batch, self.in_features, self.out_features,\n            self.scaling_factor,\n            BLOCK_SIZE_B=BLOCK_SIZE_B, BLOCK_SIZE_O=BLOCK_SIZE_O, BLOCK_SIZE_K=BLOCK_SIZE_K\n        )\n        return out\n\nbatch_size = 128\nin_features = 64\nout_features = 128\nscaling_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
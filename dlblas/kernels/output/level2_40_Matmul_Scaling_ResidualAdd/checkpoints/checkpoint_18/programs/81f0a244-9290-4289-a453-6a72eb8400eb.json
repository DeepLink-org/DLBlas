{"id": "81f0a244-9290-4289-a453-6a72eb8400eb", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_scale_residual_kernel(\n    y_ptr,\n    y_detached_ptr,\n    output_ptr,\n    scaling_factor,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    y = tl.load(y_ptr + offsets, mask=mask)\n    y_detached = tl.load(y_detached_ptr + offsets, mask=mask)\n    output_val = y * scaling_factor + y_detached\n    tl.store(output_ptr + offsets, output_val, mask=mask)\n\nclass ScaledResidual(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, y, scaling_factor):\n        ctx.save_for_backward(scaling_factor)\n        y_detached = y.detach()\n        y = y.contiguous()\n        y_detached = y_detached.contiguous()\n        output = torch.empty_like(y)\n        n_elements = y.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_scale_residual_kernel[grid](\n            y, y_detached, output, scaling_factor.item(), n_elements, \n            BLOCK_SIZE=1024\n        )\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        scaling_factor, = ctx.saved_tensors\n        return grad_output * scaling_factor, None\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.register_buffer('scaling_factor_tensor', torch.tensor(scaling_factor, dtype=torch.float32))\n\n    def forward(self, x):\n        x = self.matmul(x)\n        return ScaledResidual.apply(x, self.scaling_factor_tensor)\n\nbatch_size = 128\nin_features = 64\nout_features = 128\nscaling_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "57be9915-9a01-4c23-9d1d-f71a4d1fee8c", "generation": 2, "timestamp": 1754572084.6290965, "iteration_found": 5, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.681, "runtime_ms": 0.0985, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0671, "speed_up": 0.681, "custom_timing": 0.0985}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 0.0000\n- correctness: 0.0000\n- score: -100.0000\n- stage: no_valid_triton_code\n- feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 0.0000, correctness: 0.0000, score: -100.0000, stage: no_valid_triton_code, feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0572, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0671, 'speed_up': 1.173, 'custom_timing': 0.0572}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: -1.0000, runtime_ms: -1.0000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'error_during_performance': 'save_for_backward can only save variables, but argument 0 is of type float', 'reference_timing_ms': 0.0671, 'speed_up': -1.0, 'custom_timing': -1.0}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_scale_residual_kernel(\n    y_ptr,\n    output_ptr,\n    scaling_factor,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    y = tl.load(y_ptr + offsets, mask=mask)\n    scaled = y * scaling_factor\n    output = scaled + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n\nclass ScaledResidual(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, y, scaling_factor):\n        ctx.save_for_backward(scaling_factor)\n        output = torch.empty_like(y)\n        n_elements = y.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_scale_residual_kernel[grid](\n            y, output, scaling_factor, n_elements, \n            BLOCK_SIZE=1024\n        )\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        scaling_factor, = ctx.saved_tensors\n        return grad_output * scaling_factor, None\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.matmul(x)\n        return ScaledResidual.apply(x, self.scaling_factor)\n\nbatch_size = 128\nin_features = 64\nout_features = 128\nscaling_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (-1.0000), Performs well on runtime_ms (-1.0000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'error_during_performance': 'save_for_backward can only save variables, but argument 0 is of type float', 'reference_timing_ms': 0.0671, 'speed_up': -1.0, 'custom_timing': -1.0})\n\n\n### Program 2 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, scaling, and residual addition.\n\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        scaling_factor (float): Scaling factor to apply after matrix multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        original_x = x.clone().detach()\n        x = x * self.scaling_factor\n        x = x + original_x\n        return x\n\nbatch_size = 128\nin_features = 64\nout_features = 128\nscaling_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0572), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0671, 'speed_up': 1.173, 'custom_timing': 0.0572})\n\n\n### Program 3 (Score: -100.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, scaling, and residual addition.\n\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        scaling_factor (float): Scaling factor to apply after matrix multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        # Use detach() instead of clone().detach() to avoid unnecessary memory copy\n        original_x = x.detach()\n        x = x * self.scaling_factor\n        x = x + original_x\n        return x\n\nbatch_size = 128\nin_features = 64\nout_features = 128\nscaling_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (0.0000), Performs well on correctness (0.0000), Performs well on score (-100.0000), Performs well on stage (no_valid_triton_code), Performs well on feedback (You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_scale_residual_kernel(\n    y_ptr,\n    output_ptr,\n    scaling_factor,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    y = tl.load(y_ptr + offsets, mask=mask)\n    scaled = y * scaling_factor\n    output = scaled + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n\nclass ScaledResidual(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, y, scaling_factor):\n        ctx.save_for_backward(scaling_factor)\n        output = torch.empty_like(y)\n        n_elements = y.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_scale_residual_kernel[grid](\n            y, output, scaling_factor, n_elements, \n            BLOCK_SIZE=1024\n        )\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        scaling_factor, = ctx.saved_tensors\n        return grad_output * scaling_factor, None\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        x = self.matmul(x)\n        return ScaledResidual.apply(x, self.scaling_factor)\n\nbatch_size = 128\nin_features = 64\nout_features = 128\nscaling_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, scaling, and residual addition.\n\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        scaling_factor (float): Scaling factor to apply after matrix multiplication.\n    \"\"\"\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        # Use detach() instead of clone().detach() to avoid unnecessary memory copy\n        original_x = x.detach()\n        x = x * self.scaling_factor\n        x = x + original_x\n        return x\n\nbatch_size = 128\nin_features = 64\nout_features = 128\nscaling_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear transformation (matmul)\n  2. Then, for the result (let's call it `x`), it does:\n        original_x = x.detach()\n        x = x * scaling_factor\n        x = x + original_x\n\n This is equivalent to: \n        x = x * scaling_factor + x.detach()\n\n But note: detach() breaks the gradient flow. However, observe that the gradient for the original_x should not be backpropagated because it's detached.\n\n However, the current implementation in the ModelNew class is entirely in PyTorch and does not use Triton.\n\n We are to rewrite the core operation (the scaling and residual) in Triton to speed it up.\n\n The previous top-performing program (Program 1) used a Triton kernel for the scaling and residual. However, it had an issue in the backward pass because it saved a float (scaling_factor) as a tensor for backward, but in the backward it used `grad_output * scaling_factor` which is correct. However, the error in Program 1 was: \n        'save_for_backward can only save variables, but argument 0 is of type float'\n\n This is because in the forward, `scaling_factor` was passed as a float, not a tensor. We must make sure that `scaling_factor` is a tensor.\n\n Also, note the original reference code uses `x.detach()` so that the residual branch does not get gradients. In the Triton version, we are only writing the forward kernel. We also need to write the backward? Actually, the autograd function handles both.\n\n Let's design:\n\n We'll create a `ScaledResidual` autograd function that uses a Triton kernel for the forward.\n\n Steps:\n\n  1. The forward kernel:\n        Input: y (tensor), scaling_factor (float, but we will make it a tensor to save for backward)\n        Output: output = y * scaling_factor + y.detach()   ??? \n\n  But note: the operation in the reference is:\n        original_x = x.detach()   # so this is the same as x but without the gradient\n        x = x * scaling_factor\n        x = x + original_x\n\n  This is equivalent to: \n        x = x * scaling_factor + x.detach()\n\n  However, observe that if we break down:\n        x = (x * scaling_factor) + x.detach()\n\n  But note: the gradient of this operation with respect to x should be:\n        d(output)/dx = scaling_factor   (for the first term) and 0 for the second term? \n        But wait: the second term is detached, so in the backward pass, we only get the gradient from the first term.\n\n  Actually, the entire output is:\n        output = x * scaling_factor + x.detach()\n\n  Then, the gradient of output w.r.t. x is:\n        d(output)/dx = scaling_factor   (because the second term is constant w.r.t. x)\n\n  Therefore, in the backward pass we do:\n        grad_input = grad_output * scaling_factor\n\n  This matches the backward in the top-performing program.\n\n  However, the issue in Program 1 was that `scaling_factor` was a float and not a tensor when saved by `ctx.save_for_backward`. Therefore, we must convert it to a tensor if it's a float.\n\n  But note: the model uses `self.scaling_factor` which is a float. We can change the model to hold it as a tensor? Or we can convert it to a tensor in the forward.\n\n  We can do in the forward of `ScaledResidual`:\n        scaling_factor_tensor = torch.tensor(scaling_factor, device=y.device, dtype=y.dtype)\n        Then save scaling_factor_tensor.\n\n  Alternatively, we can store `scaling_factor` as a tensor in the model.\n\n  Let's look at the model initialization: `scaling_factor` is a float. We can change the model to store it as a tensor.\n\n  But note: the problem states that we cannot change the inputs and outputs. The model is initialized with `in_features, out_features, scaling_factor` (a float). We can convert it to a tensor in the model.\n\n  We'll change the model to:\n        self.scaling_factor = torch.tensor(scaling_factor)\n\n  However, we must be cautious because if the model is moved to GPU, we must move the tensor too. We can register it as a buffer.\n\n  Alternatively, we can avoid changing the model and instead in the forward of `ScaledResidual` we convert the float to a tensor.\n\n  Since the backward only uses the scaling_factor (which is a scalar) and we are going to use it in the kernel, it's better to have it as a tensor.\n\n  Revised plan:\n\n      class ModelNew(nn.Module):\n          def __init__(self, in_features, out_features, scaling_factor):\n              super().__init__()\n              self.matmul = nn.Linear(in_features, out_features)\n              # register scaling_factor as a buffer so it's moved to device\n              self.register_buffer('scaling_factor_tensor', torch.tensor(scaling_factor))\n\n          def forward(self, x):\n              x = self.matmul(x)\n              # Now call the autograd function with the tensor version\n              return ScaledResidual.apply(x, self.scaling_factor_tensor)\n\n  But note: the autograd function `ScaledResidual` expects two arguments: y and scaling_factor (which is now a tensor).\n\n  Then in the forward of `ScaledResidual`:\n        ctx.save_for_backward(scaling_factor)   # now it's a tensor, so it's safe.\n\n  The kernel remains the same.\n\n  However, note: the reference code uses detach. In the Triton kernel, we are doing:\n        output = y * scaling_factor + y   ??? \n\n  But wait, that would be: \n        output = y * (scaling_factor + 1)\n\n  That is not the same as the reference.\n\n  We must do: \n        output = (y * scaling_factor) + y_detached\n\n  But note: we cannot get the detached version of `y` inside the kernel? Because the kernel is just doing element-wise operations.\n\n  Actually, the entire point of the detach is to break the gradient. We can do:\n\n        In the forward, we can split the tensor into two parts for autograd? \n        But note: the kernel is just doing:\n            output = y * scaling_factor + y\n\n        And then in the backward, we are returning:\n            grad_output * scaling_factor\n\n        This backward ignores the second term (because we treat it as constant). So the autograd function is effectively:\n\n            forward: output = y * (scaling_factor + 1)   [but with the backward only propagating the scaling_factor term]\n\n        That is not the same as the reference.\n\n  Let me clarify:\n\n        Reference forward:\n            x = ... (from matmul)   [with gradient]\n            original_x = x.detach()   [no gradient]\n            output = x * scaling_factor + original_x\n\n        This is: output = x * scaling_factor + x_detached\n\n        The gradient of output w.r.t. x is only scaling_factor (because the x_detached is constant).\n\n        Now, if we write:\n\n            output = x * scaling_factor + x   (without detach)\n\n        Then the gradient would be: scaling_factor + 1.\n\n        So we must not add the gradient from the second term.\n\n  Therefore, we cannot simply do `y + y*scaling_factor` in the kernel and then rely on the backward to be `scaling_factor` because that would be wrong.\n\n  How to fix?\n\n  We have two options:\n\n    Option 1: Do two kernels? One to compute the detached version and then the fused operation? But that would be two passes.\n\n    Option 2: We design the kernel to only compute `y * scaling_factor` and then we do an addition with the detached version outside? But then we are not fusing the addition.\n\n  Alternatively, we can note that the operation is:\n\n        output = y * scaling_factor + y.detach()\n\n        = (y - y.detach()) * scaling_factor + y.detach() * (1 + scaling_factor)   ??? \n\n        Actually, that doesn't help.\n\n  Another idea: We can split the tensor in the autograd function:\n\n        We do:\n            y_detached = y.detach()\n            Then we call a kernel that does: output = y * scaling_factor + y_detached\n\n        But then we are creating a new tensor y_detached, and then the kernel does the fused operation.\n\n        The backward of this autograd function would be:\n\n            We know that y_detached has no gradient, so we only get the gradient from the first term.\n\n            Therefore, backward: \n                grad_y = grad_output * scaling_factor\n\n        This matches.\n\n        So in the forward of `ScaledResidual`:\n\n            y_detached = y.detach()\n            output = ... (result of kernel that does: y * scaling_factor + y_detached)\n\n        But note: we are creating y_detached, which is a full copy of y. That might be expensive.\n\n        However, the reference also does `x.detach()` which creates a copy? Actually, detach() is a view that does not require a copy until modified. But then when we do the addition, it will be materialized.\n\n        So we are not worse than the reference.\n\n  We can write a kernel that takes two inputs: y and y_detached? But note: they are the same values. So we can just pass y and then in the kernel we do:\n\n        output = y * scaling_factor + y   ??? \n\n        But wait, that would be: output = y * (scaling_factor+1) and then we break the gradient by the autograd function? \n\n        How? The autograd function would have:\n\n            forward: \n                y_detached = y.detach()   # this is a constant\n                then we call the kernel with y and y_detached? \n\n        Actually, we can avoid passing two copies by just passing y and then having two loads? That would double the memory reads.\n\n        Alternatively, we can do:\n\n            We pass y and then in the kernel we do:\n                output = y * scaling_factor + y   [but then we break the gradient by the autograd function's backward?]\n\n        But note: the kernel is just doing the computation. The autograd function's backward defines the gradient.\n\n        The backward function returns: \n            grad_y = grad_output * scaling_factor\n\n        So the autograd function is telling that the gradient of the output with respect to y is scaling_factor, regardless of the fact that we added y in the forward.\n\n        This is inconsistent because the forward computation used y (the non-detached) in two places: one scaled and one not. But the backward only accounts for the scaled part.\n\n        This is exactly what we want: the unscaled part (the second y) is treated as constant (because of detach) in the reference.\n\n        However, in the kernel we are using the same y for both. The autograd function's backward does not know about the kernel's internals. It only knows that the gradient is scaling_factor.\n\n        Therefore, we can do in the kernel:\n\n            output = y * scaling_factor + y   [without detaching inside the kernel]\n\n        and then rely on the backward to only return the scaling_factor part? \n\n        This would be wrong because the backward would be:\n\n            The forward computation graph would have two contributions from y: one scaled and one not. Without detach, the autograd would naturally compute the gradient as scaling_factor+1.\n\n        So we cannot do that.\n\n  Therefore, we must pass the detached version as a separate tensor. We can do:\n\n        In the forward of the autograd function:\n\n            y_detached = y.detach()\n            output = ... (kernel that does: y * scaling_factor + y_detached)\n\n        Then the kernel will have two pointers: y_ptr and y_detached_ptr.\n\n        The kernel:\n            @triton.jit\n            def kernel(y_ptr, y_detached_ptr, output_ptr, scaling_factor, n_elements, BLOCK_SIZE: tl.constexpr):\n                ...\n                y_val = tl.load(y_ptr + offsets, mask=mask)\n                y_detached_val = tl.load(y_detached_ptr + offsets, mask=mask)\n                output_val = y_val * scaling_factor + y_detached_val\n                tl.store(output_ptr + offsets, output_val, mask=mask)\n\n        This is correct.\n\n  However, we are doing two loads and one store. The reference also does two loads (one for x and one for original_x) and one store? Actually, the reference does:\n\n        original_x = x.detach()   # this is a view, no copy of data\n        x = x * scaling_factor    # this writes a new tensor? or in-place? \n        Then x = x + original_x   # another operation.\n\n        So it does two kernels: one for multiplication and one for addition.\n\n        We are fusing the two operations into one kernel, so it should be faster.\n\n  But note: we are creating a new tensor y_detached, which is a view. Then we pass it to the kernel. The kernel then reads two tensors. This is two memory accesses per element.\n\n  The original reference does:\n        Step1: multiply: reads x, writes to a new tensor (or in-place? The reference code does not do in-place because it does `x = x * ...` which is a new tensor? Then the addition is another new tensor.\n\n        So the reference does:\n            temp = x * scaling_factor   [read x, write temp]\n            output = temp + original_x   [read temp, read original_x (which is x), write output]\n\n        Total: 2 reads and 1 write for the multiply, then 2 reads and 1 write for the addition? Actually, the addition reads two tensors and writes one.\n\n        So total: 3 reads and 2 writes? \n\n        Actually, the temp tensor is intermediate and then discarded? \n\n        Our Triton kernel: \n            We do: read y, read y_detached (which is the same as y), then write output.\n            So 2 reads and 1 write.\n\n        And we avoid the intermediate temp.\n\n        So it should be faster.\n\n  Implementation:\n\n        We'll create the autograd function:\n\n            class ScaledResidual(torch.autograd.Function):\n                @staticmethod\n                def forward(ctx, y, scaling_factor):\n                    # scaling_factor is a tensor (with one element)\n                    ctx.save_for_backward(scaling_factor)\n                    y_detached = y.detach()   # creates a view that shares storage but without gradient\n\n                    # We must make sure that y and y_detached are contiguous? \n                    output = torch.empty_like(y)\n                    n_elements = y.numel()\n                    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n                    fused_scale_residual_kernel[grid](\n                        y, y_detached, output, \n                        scaling_factor, \n                        n_elements, \n                        BLOCK_SIZE=1024\n                    )\n                    return output\n\n                @staticmethod\n                def backward(ctx, grad_output):\n                    scaling_factor, = ctx.saved_tensors\n                    return grad_output * scaling_factor, None\n\n        Then the model:\n\n            class ModelNew(nn.Module):\n                def __init__(self, in_features, out_features, scaling_factor):\n                    super().__init__()\n                    self.matmul = nn.Linear(in_features, out_features)\n                    # We register the scaling_factor as a buffer so it's a tensor and moved to the device\n                    self.register_buffer('scaling_factor_tensor', torch.tensor(scaling_factor))\n\n                def forward(self, x):\n                    x = self.matmul(x)\n                    return ScaledResidual.apply(x, self.scaling_factor_tensor)\n\n  But note: the kernel must be defined.\n\n  Let's write the kernel:\n\n        @triton.jit\n        def fused_scale_residual_kernel(\n            y_ptr,\n            y_detached_ptr,\n            output_ptr,\n            scaling_factor,\n            n_elements,\n            BLOCK_SIZE: tl.constexpr,\n        ):\n            pid = tl.program_id(0)\n            block_start = pid * BLOCK_SIZE\n            offsets = block_start + tl.arange(0, BLOCK_SIZE)\n            mask = offsets < n_elements\n\n            y = tl.load(y_ptr + offsets, mask=mask)\n            y_detached = tl.load(y_detached_ptr + offsets, mask=mask)\n            output_val = y * scaling_factor + y_detached\n            tl.store(output_ptr + offsets, output_val, mask=mask)\n\n  However, note: `scaling_factor` is a tensor with one element. We can load it as a scalar? \n\n        We can pass it as a scalar value? But the kernel expects a pointer? \n\n        Alternatively, we can pass it as a scalar by value. We can do:\n\n            scaling_factor_value = scaling_factor.item()   # but then we lose the device and type? \n\n        We want the kernel to be type-specific. We can pass it as a scalar argument of the kernel? \n\n        The kernel can have a scalar argument? Yes.\n\n        We change the kernel signature:\n\n            def fused_scale_residual_kernel(\n                y_ptr,\n                y_detached_ptr,\n                output_ptr,\n                scaling_factor,   # now a scalar, not a pointer\n                n_elements,\n                BLOCK_SIZE: tl.constexpr,\n            ):\n\n        Then when calling the kernel, we pass `scaling_factor` as the scalar value? But note: the kernel expects a scalar of a specific type. We can do:\n\n            scaling_factor_value = scaling_factor   # but it's a tensor of one element? How to pass?\n\n        In Triton, we can pass a scalar by using the value of the tensor. We can do:\n\n            scaling_factor_value = scaling_factor.item()   # but this is a Python float. Then the kernel will use that float.\n\n        But note: the kernel is compiled for the specific type of the scalar? It will be a float32? \n\n        We can also pass the tensor and then load the scaling factor once at the beginning? \n\n        We can do:\n\n            @triton.jit\n            def fused_scale_residual_kernel(\n                y_ptr,\n                y_detached_ptr,\n                output_ptr,\n                scaling_factor_ptr,   # pointer to a tensor of one element\n                n_elements,\n                BLOCK_SIZE: tl.constexpr,\n            ):\n                scaling_factor = tl.load(scaling_factor_ptr)\n\n        Then we pass the scaling_factor tensor's pointer.\n\n        This way, we avoid having to convert to a scalar and we keep the type.\n\n        Let's do that.\n\n  Revised kernel:\n\n        @triton.jit\n        def fused_scale_residual_kernel(\n            y_ptr,\n            y_detached_ptr,\n            output_ptr,\n            scaling_factor_ptr,\n            n_elements,\n            BLOCK_SIZE: tl.constexpr,\n        ):\n            pid = tl.program_id(0)\n            block_start = pid * BLOCK_SIZE\n            offsets = block_start + tl.arange(0, BLOCK_SIZE)\n            mask = offsets < n_elements\n\n            # Load the scaling factor (a scalar, at the beginning of the tensor)\n            scaling_factor = tl.load(scaling_factor_ptr)\n            y = tl.load(y_ptr + offsets, mask=mask)\n            y_detached = tl.load(y_detached_ptr + offsets, mask=mask)\n            output_val = y * scaling_factor + y_detached\n            tl.store(output_ptr + offsets, output_val, mask=mask)\n\n  Then in the forward call:\n\n        fused_scale_residual_kernel[grid](\n            y, y_detached, output, \n            scaling_factor,   # this is the tensor, but we need to pass a pointer? \n            n_elements, \n            BLOCK_SIZE=1024\n        )\n\n        How do we pass the pointer? We can get the pointer by: `scaling_factor.data_ptr()`\n\n        But note: the kernel expects a pointer. We can do:\n\n            scaling_factor_ptr = triton.reinterpret(scaling_factor, tl.float32)   # if it's float32? \n\n        Actually, we can pass the tensor and then in the kernel we use `tl.tensor`? \n\n        Alternatively, we can pass the pointer as the data pointer? \n\n        We can do:\n\n            scaling_factor_ptr = scaling_factor.data_ptr()\n\n        But we must make sure it's on the same device and the type matches.\n\n        We'll assume scaling_factor is a float32 tensor.\n\n        So in the call:\n\n            fused_scale_residual_kernel[grid](\n                y, y_detached, output, \n                scaling_factor.data_ptr(),   # pass the pointer\n                n_elements, \n                BLOCK_SIZE=1024\n            )\n\n  But note: the kernel expects a pointer to a float32. We are passing an integer (the pointer) and then inside the kernel we do:\n\n        scaling_factor_ptr is an integer? We can use it as:\n\n            tl.load(scaling_factor_ptr)   # but that would load from the base address? \n\n        Yes, because it's a pointer to a single float.\n\n  However, we must make sure that the scaling_factor tensor is in the same device as the kernel. It will be because it's registered in the model and the model is on the same device as the input.\n\n  Also, note: we are using `tl.load` at the beginning of the kernel. This load is not masked because it's a single element. But we must make sure that the pointer is valid.\n\n  We can also pass the scaling factor by value? \n\n        We can do: scaling_factor_value = scaling_factor.item()\n        and then pass it as a scalar. Then the kernel signature would be:\n\n            scaling_factor: tl.constexpr,   # or without tl.constexpr? \n\n        But note: the scaling factor might change between runs? It is a constant for the kernel launch? \n\n        We can do:\n\n            @triton.jit\n            def fused_scale_residual_kernel(\n                y_ptr,\n                y_detached_ptr,\n                output_ptr,\n                scaling_factor,   # scalar, not constexpr\n                n_elements,\n                BLOCK_SIZE: tl.constexpr,\n            ):\n                ... then use scaling_factor as a scalar.\n\n        Then we call:\n\n            fused_scale_residual_kernel[grid](\n                y, y_detached, output, \n                scaling_factor.item(),   # pass the float\n                n_elements, \n                BLOCK_SIZE=1024\n            )\n\n        This is simpler and avoids the pointer load.\n\n        But note: the kernel will be recompiled for different scaling_factor values? \n\n        Actually, the kernel is compiled for the specific type (float32) and the value of scaling_factor is passed as an argument. It is not a compile-time constant. So we don't need to mark it as constexpr.\n\n        This should work.\n\n  Let's choose this simpler way.\n\n  Therefore, we change the kernel to:\n\n        @triton.jit\n        def fused_scale_residual_kernel(\n            y_ptr,\n            y_detached_ptr,\n            output_ptr,\n            scaling_factor,\n            n_elements,\n            BLOCK_SIZE: tl.constexpr,\n        ):\n            pid = tl.program_id(0)\n            block_start = pid * BLOCK_SIZE\n            offsets = block_start + tl.arange(0, BLOCK_SIZE)\n            mask = offsets < n_elements\n\n            y = tl.load(y_ptr + offsets, mask=mask)\n            y_detached = tl.load(y_detached_ptr + offsets, mask=mask)\n            output_val = y * scaling_factor + y_detached\n            tl.store(output_ptr + offsets, output_val, mask=mask)\n\n  And then in the forward:\n\n            scaling_factor_value = scaling_factor.item()   # if we are sure it's a one-element tensor? \n\n        But note: `scaling_factor` is a tensor of one element. We can use `.item()` to get the float.\n\n        However, we can also pass the tensor and then do `scaling_factor[0]`? \n\n        But in the kernel, we are passing a scalar. So we must extract the value.\n\n        We do:\n\n            scaling_factor_value = scaling_factor.item()\n\n  But what if the tensor has more than one element? It shouldn't because we created it as a scalar tensor.\n\n  Alternatively, we can pass `scaling_factor` as a tensor and then in the kernel we load the first element? \n\n        We don't want to do that because it would require a pointer and we are trying to avoid the extra load.\n\n        So we'll use .item().\n\n  However, note: the kernel expects a scalar of type float32. We must make sure that the scaling_factor tensor is float32? \n\n        In the model, we did: \n            self.register_buffer('scaling_factor_tensor', torch.tensor(scaling_factor))\n\n        This will be a float tensor? Yes, because scaling_factor is a float.\n\n        But we can specify dtype=torch.float32.\n\n  We'll do:\n\n        self.register_buffer('scaling_factor_tensor', torch.tensor(scaling_factor, dtype=torch.float32))\n\n  Then scaling_factor_value = scaling_factor.item() is a float.\n\n  Now, note: the kernel must be compiled for the type of the pointers. We are passing:\n\n        y_ptr: pointer to y, which is the output of the linear layer -> probably float32? \n        y_detached_ptr: same as y -> float32\n        output_ptr: same as y -> float32\n\n        scaling_factor: a float (which in the kernel will be a tl.float32)\n\n  So we are safe.\n\n  Finally, we must note: the kernel may be called with non-contiguous tensors? \n\n        We can make sure that the tensors are contiguous by calling .contiguous().\n\n        In the forward:\n\n            y = y.contiguous()\n            y_detached = y_detached.contiguous()\n\n        But note: y_detached is a view of y (detached). If y is contiguous, then y_detached is contiguous? \n\n        Yes. So we can assume they are contiguous if we ensure y is contiguous.\n\n        Alternatively, we can do:\n\n            y_detached = y.detach().contiguous()\n\n        But that would cause a copy. We want to avoid that.\n\n        Since we are reading the tensor in a contiguous block, we can rely on the fact that the original y is contiguous? \n\n        The output of the linear layer is contiguous by default.\n\n        So we can skip the contiguous().\n\n  But to be safe, we can do:\n\n        output = torch.empty_like(y)   # contiguous\n        and we assume y and y_detached are contiguous? \n\n        We can enforce:\n\n            y = y.contiguous()\n            y_detached = y_detached.contiguous()\n\n        But note: y_detached = y.detach() and if y is contiguous, then y_detached is contiguous? \n\n        Yes.\n\n        So we can do:\n\n            y = y.contiguous()\n            y_detached = y_detached.contiguous()\n\n        This will not cause a copy if already contiguous.\n\n  We'll do that.\n\n  Now, let's code:\n\n        We must define the kernel and the autograd function and the model.\n\n  But note: the problem says that we must not change the function signatures of the Triton kernel. We are defining a new kernel with a different signature? \n\n        The previous top-performing program had a kernel with the same name but different arguments? \n\n        We are allowed to change the kernel as long as we are within the EVOLVE-BLOCK.\n\n  We'll write the entire code within the EVOLVE-BLOCK.\n\n  Important: The model class must be named `ModelNew`.\n\n  We'll also include the batch_size, in_features, etc. and the get_inputs and get_init_inputs.\n\n  Let's code accordingly.\n\n  One more thing: the kernel uses BLOCK_SIZE. We can set it to 1024? But we can also use autotune? \n\n        The problem says we can do adaptive tuning. But we are in the first step to get it compiled and correct.\n\n        We'll leave it at 1024 for now.\n\n  We are ready.\n\n  Code:\n\n      import torch\n      import torch.nn as nn\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def fused_scale_residual_kernel(\n          y_ptr,\n          y_detached_ptr,\n          output_ptr,\n          scaling_factor,\n          n_elements,\n          BLOCK_SIZE: tl.constexpr,\n      ):\n          pid = tl.program_id(0)\n          block_start = pid * BLOCK_SIZE\n          offsets = block_start + tl.arange(0, BLOCK_SIZE)\n          mask = offsets < n_elements\n\n          y = tl.load(y_ptr + offsets, mask=mask)\n          y_detached = tl.load(y_detached_ptr + offsets, mask=mask)\n          output_val = y * scaling_factor + y_detached\n          tl.store(output_ptr + offsets, output_val, mask=mask)\n\n      class ScaledResidual(torch.autograd.Function):\n          @staticmethod\n          def forward(ctx, y, scaling_factor):\n              # Save scaling_factor for backward\n              ctx.save_for_backward(scaling_factor)\n              # We detach y to break the gradient for the residual part\n              y_detached = y.detach()\n              # Make sure the tensors are contiguous\n              y = y.contiguous()\n              y_detached = y_detached.contiguous()\n              output = torch.empty_like(y)\n              n_elements = y.numel()\n              grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n              # Extract the scalar value from the scaling_factor tensor\n              scaling_factor_value = scaling_factor.item()\n              fused_scale_residual_kernel[grid](\n                  y, y_detached, output, scaling_factor_value, n_elements, BLOCK_SIZE=1024\n              )\n              return output\n\n          @staticmethod\n          def backward(ctx, grad_output):\n              scaling_factor, = ctx.saved_tensors\n              return grad_output * scaling_factor, None\n\n      class ModelNew(nn.Module):\n          def __init__(self, in_features, out_features, scaling_factor):\n              super(ModelNew, self).__init__()\n              self.matmul = nn.Linear(in_features, out_features)\n              # Register the scaling_factor as a buffer (tensor) so it's on the device\n              self.register_buffer('scaling_factor_tensor', torch.tensor(scaling_factor, dtype=torch.float32))\n\n          def forward(self, x):\n              x = self.matmul(x)\n              return ScaledResidual.apply(x, self.scaling_factor_tensor)\n\n      batch_size = 128\n      in_features = 64\n      out_features = 128\n      scaling_factor = 0.5\n\n      def get_inputs():\n          return [torch.randn(batch_size, in_features)]\n\n      def get_init_inputs():\n          return [in_features, out_features, scaling_factor]\n\n  This should work.\n\n  Let's hope it compiles and is correct.\n\n  Note: the previous Program 1 had an issue because it saved a float. We are saving a tensor now.\n\n  Also, note: the backward returns two values: one for y and one for scaling_factor. We return None for scaling_factor because we don't want to backpropagate to it? \n\n        The scaling_factor is a fixed constant? \n\n        In the model, it's a buffer (not a parameter) so we don't need its gradient.\n\n        So we return None for the scaling_factor gradient.\n\n  We are good.\n\n  We output the entire block.\n\n  IMPORTANT: The EVOLVE-BLOCK markers must be included.\n\n  We are replacing the entire block.\n\n  But note: the current program is a pure PyTorch model without Triton. We are replacing it with a Triton version.\n\n  We write:\n\n      # ================== EVOLVE-BLOCK-START ==================\n      ... our code ...\n      # =================== EVOLVE-BLOCK-END ===================\n\n  We must not change the markers.\n\n  Let's write the code accordingly.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_scale_residual_kernel(\n    y_ptr,\n    y_detached_ptr,\n    output_ptr,\n    scaling_factor,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    y = tl.load(y_ptr + offsets, mask=mask)\n    y_detached = tl.load(y_detached_ptr + offsets, mask=mask)\n    output_val = y * scaling_factor + y_detached\n    tl.store(output_ptr + offsets, output_val, mask=mask)\n\nclass ScaledResidual(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, y, scaling_factor):\n        ctx.save_for_backward(scaling_factor)\n        y_detached = y.detach()\n        y = y.contiguous()\n        y_detached = y_detached.contiguous()\n        output = torch.empty_like(y)\n        n_elements = y.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        fused_scale_residual_kernel[grid](\n            y, y_detached, output, scaling_factor.item(), n_elements, \n            BLOCK_SIZE=1024\n        )\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        scaling_factor, = ctx.saved_tensors\n        return grad_output * scaling_factor, None\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.register_buffer('scaling_factor_tensor', torch.tensor(scaling_factor, dtype=torch.float32))\n\n    def forward(self, x):\n        x = self.matmul(x)\n        return ScaledResidual.apply(x, self.scaling_factor_tensor)\n\nbatch_size = 128\nin_features = 64\nout_features = 128\nscaling_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
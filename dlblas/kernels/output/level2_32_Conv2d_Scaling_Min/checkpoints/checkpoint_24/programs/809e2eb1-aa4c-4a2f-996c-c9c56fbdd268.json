{"id": "809e2eb1-aa4c-4a2f-996c-c9c56fbdd268", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 64}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 64}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n    ],\n    key=['spatial_size']\n)\n@triton.jit\ndef _min_reduction_kernel(\n    x_ptr,\n    output_ptr,\n    scale_factor,\n    spatial_size,\n    x_batch_stride,\n    x_channel_stride,\n    x_spatial_stride,\n    output_batch_stride,\n    output_spatial_stride,\n    channels: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_spatial_block = tl.program_id(1)\n    \n    spatial_start = pid_spatial_block * BLOCK_SIZE\n    spatial_offsets = spatial_start + tl.arange(0, BLOCK_SIZE)\n    spatial_mask = spatial_offsets < spatial_size\n    \n    min_values = tl.full((BLOCK_SIZE,), float('inf'), dtype=tl.float32)\n    base_batch = pid_batch * x_batch_stride\n    \n    # Unrolled channel loop for better instruction scheduling\n    for c in tl.static_range(0, channels):\n        ptr = x_ptr + base_batch + c * x_channel_stride + spatial_offsets * x_spatial_stride\n        vals = tl.load(ptr, mask=spatial_mask, other=float('inf'))\n        vals = vals * scale_factor\n        min_values = tl.minimum(min_values, vals)\n    \n    out_ptr = output_ptr + pid_batch * output_batch_stride + spatial_offsets * output_spatial_stride\n    tl.store(out_ptr, min_values, mask=spatial_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        batch_size, channels, height, width = x.shape\n        spatial_size = height * width\n        \n        x_flat = x.contiguous().view(batch_size, channels, spatial_size)\n        output = torch.empty(batch_size, 1, spatial_size, device=x.device, dtype=x.dtype)\n        \n        # Dynamic grid configuration based on autotuned BLOCK_SIZE\n        grid = lambda meta: (batch_size, triton.cdiv(spatial_size, meta['BLOCK_SIZE']))\n        \n        _min_reduction_kernel[grid](\n            x_flat, \n            output,\n            self.scale_factor,\n            spatial_size,\n            x_flat.stride(0),\n            x_flat.stride(1),\n            x_flat.stride(2),\n            output.stride(0),\n            output.stride(2),\n            channels\n        )\n        \n        return output.view(batch_size, 1, height, width)\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "293cebf2-9b29-491b-8a2c-437abb4ad39f", "generation": 3, "timestamp": 1754570315.6077547, "iteration_found": 25, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.675, "runtime_ms": 0.104, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0702, "speed_up": 0.675, "custom_timing": 0.104}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.791, "runtime_ms": 0.0888, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0702, "speed_up": 0.791, "custom_timing": 0.0888}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.7910\n- runtime_ms: 0.0888\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0702, 'speed_up': 0.791, 'custom_timing': 0.0888}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7910, runtime_ms: 0.0888, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0702, 'speed_up': 0.791, 'custom_timing': 0.0888}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.6820, runtime_ms: 0.1030, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0702, 'speed_up': 0.682, 'custom_timing': 0.103}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.4980, runtime_ms: 0.1410, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0702, 'speed_up': 0.498, 'custom_timing': 0.141}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _min_scale_kernel(\n    x_ptr,\n    output_ptr,\n    scale_factor,\n    out_channels,\n    height, width,\n    stride_batch, stride_channel, stride_height, stride_width,\n    output_stride_batch, output_stride_height, output_stride_width,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_h = tl.program_id(1)\n    pid_w = tl.program_id(2)\n    \n    min_val = float('inf')\n    base = pid_batch * stride_batch + pid_h * stride_height + pid_w * stride_width\n    \n    for channel_offset in range(0, out_channels, BLOCK_SIZE):\n        c_offsets = channel_offset + tl.arange(0, BLOCK_SIZE)\n        mask = c_offsets < out_channels\n        ptrs = x_ptr + base + c_offsets * stride_channel\n        vals = tl.load(ptrs, mask=mask, other=float('inf'))\n        vals = vals * scale_factor\n        block_min = tl.min(vals, axis=0)\n        min_val = tl.minimum(min_val, block_min)\n        \n    output_idx = pid_batch * output_stride_batch + pid_h * output_stride_height + pid_w * output_stride_width\n    tl.store(output_ptr + output_idx, min_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        batch, channels, height, width = x.shape\n        output = torch.empty(batch, 1, height, width, device=x.device, dtype=x.dtype)\n        \n        if not x.is_contiguous():\n            x = x.contiguous()\n        output = output.contiguous()\n        \n        grid = (batch, height, width)\n        _min_scale_kernel[grid](\n            x,\n            output,\n            self.scale_factor,\n            channels,\n            height, width,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(2), output.stride(3),\n            BLOCK_SIZE=128\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.4980), Performs well on runtime_ms (0.1410), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0702, 'speed_up': 0.498, 'custom_timing': 0.141})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n    ],\n    key=['spatial_size'],\n)\n@triton.jit\ndef _min_reduction_kernel(\n    x_ptr,\n    output_ptr,\n    scale_factor,\n    spatial_size,\n    x_batch_stride,\n    x_channel_stride,\n    x_spatial_stride,\n    output_batch_stride,\n    output_spatial_stride,\n    channels: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_spatial_block = tl.program_id(1)\n    \n    spatial_start = pid_spatial_block * BLOCK_SIZE\n    spatial_offsets = spatial_start + tl.arange(0, BLOCK_SIZE)\n    spatial_mask = spatial_offsets < spatial_size\n    \n    base_batch = pid_batch * x_batch_stride\n    min_values = tl.full((BLOCK_SIZE,), float('inf'), dtype=tl.float32)\n    \n    for c in range(0, channels):\n        ptr = x_ptr + base_batch + c * x_channel_stride + spatial_offsets * x_spatial_stride\n        vals = tl.load(ptr, mask=spatial_mask, other=float('inf'))\n        scaled_vals = vals * scale_factor\n        min_values = tl.minimum(min_values, scaled_vals)\n    \n    out_ptr = output_ptr + pid_batch * output_batch_stride + spatial_offsets * output_spatial_stride\n    tl.store(out_ptr, min_values, mask=spatial_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        batch_size, channels, height, width = x.shape\n        spatial_size = height * width\n        \n        x_flat = x.contiguous().view(batch_size, channels, spatial_size)\n        output = torch.empty(batch_size, 1, spatial_size, device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (batch_size, triton.cdiv(spatial_size, META['BLOCK_SIZE']))\n        \n        _min_reduction_kernel[grid](\n            x_flat, \n            output,\n            self.scale_factor,\n            spatial_size,\n            x_flat.stride(0),\n            x_flat.stride(1),\n            x_flat.stride(2),\n            output.stride(0),\n            output.stride(2),\n            channels\n        )\n        \n        return output.view(batch_size, 1, height, width)\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.6820), Performs well on runtime_ms (0.1030), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0702, 'speed_up': 0.682, 'custom_timing': 0.103})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _min_reduction_kernel(\n    x_ptr,\n    output_ptr,\n    scale_factor,\n    spatial_size,\n    x_batch_stride,\n    x_channel_stride,\n    x_spatial_stride,\n    output_batch_stride,\n    output_spatial_stride,\n    channels: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_spatial_block = tl.program_id(1)\n    \n    spatial_start = pid_spatial_block * BLOCK_SIZE\n    spatial_offsets = spatial_start + tl.arange(0, BLOCK_SIZE)\n    spatial_mask = spatial_offsets < spatial_size\n    \n    min_values = tl.full((BLOCK_SIZE,), float('inf'), dtype=tl.float32)\n    base_batch = pid_batch * x_batch_stride\n    \n    for c in range(0, channels):\n        ptr = x_ptr + base_batch + c * x_channel_stride + spatial_offsets * x_spatial_stride\n        vals = tl.load(ptr, mask=spatial_mask, other=float('inf'))\n        vals = vals * scale_factor\n        min_values = tl.minimum(min_values, vals)\n    \n    out_ptr = output_ptr + pid_batch * output_batch_stride + spatial_offsets * output_spatial_stride\n    tl.store(out_ptr, min_values, mask=spatial_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        batch_size, channels, height, width = x.shape\n        spatial_size = height * width\n        \n        x_flat = x.contiguous().view(batch_size, channels, spatial_size)\n        output = torch.empty(batch_size, 1, spatial_size, device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE = 128\n        grid = (batch_size, triton.cdiv(spatial_size, BLOCK_SIZE))\n        \n        _min_reduction_kernel[grid](\n            x_flat, \n            output,\n            self.scale_factor,\n            spatial_size,\n            x_flat.stride(0),\n            x_flat.stride(1),\n            x_flat.stride(2),\n            output.stride(0),\n            output.stride(2),\n            channels,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output.view(batch_size, 1, height, width)\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7910), Performs well on runtime_ms (0.0888), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0702, 'speed_up': 0.791, 'custom_timing': 0.0888})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _min_reduction_kernel(\n    x_ptr,\n    output_ptr,\n    spatial_size,\n    x_batch_stride,\n    x_channel_stride,\n    x_spatial_stride,\n    output_batch_stride,\n    output_spatial_stride,\n    channels: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_spatial_block = tl.program_id(1)\n    \n    spatial_start = pid_spatial_block * BLOCK_SIZE\n    spatial_offsets = spatial_start + tl.arange(0, BLOCK_SIZE)\n    spatial_mask = spatial_offsets < spatial_size\n    \n    batch_base = pid_batch * x_batch_stride\n    ptr0 = x_ptr + batch_base + 0 * x_channel_stride + spatial_offsets * x_spatial_stride\n    min_values = tl.load(ptr0, mask=spatial_mask, other=float('inf'))\n    \n    for c in range(1, channels):\n        ptr = x_ptr + batch_base + c * x_channel_stride + spatial_offsets * x_spatial_stride\n        vals = tl.load(ptr, mask=spatial_mask, other=float('inf'))\n        min_values = tl.minimum(min_values, vals)\n    \n    out_ptr = output_ptr + pid_batch * output_batch_stride + spatial_offsets * output_spatial_stride\n    tl.store(out_ptr, min_values, mask=spatial_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x * self.scale_factor\n        \n        batch_size, channels, height, width = x.shape\n        spatial_size = height * width\n        x_flat = x.contiguous().view(batch_size, channels, spatial_size)\n        output = torch.empty(batch_size, 1, spatial_size, device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE = 128\n        grid = (batch_size, triton.cdiv(spatial_size, BLOCK_SIZE))\n        \n        _min_reduction_kernel[grid](\n            x_flat, output,\n            spatial_size,\n            x_flat.stride(0),\n            x_flat.stride(1),\n            x_flat.stride(2),\n            output.stride(0),\n            output.stride(2),\n            channels,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output.view(batch_size, 1, height, width)\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n    ],\n    key=['spatial_size'],\n)\n@triton.jit\ndef _min_reduction_kernel(\n    x_ptr,\n    output_ptr,\n    spatial_size,\n    x_batch_stride,\n    x_channel_stride,\n    x_spatial_stride,\n    output_batch_stride,\n    output_spatial_stride,\n    channels: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_spatial_block = tl.program_id(1)\n    \n    spatial_start = pid_spatial_block * BLOCK_SIZE\n    spatial_offsets = spatial_start + tl.arange(0, BLOCK_SIZE)\n    spatial_mask = spatial_offsets < spatial_size\n    \n    base_batch = pid_batch * x_batch_stride\n    ptr0 = x_ptr + base_batch + 0 * x_channel_stride + spatial_offsets\n    min_values = tl.load(ptr0, mask=spatial_mask, other=float('inf'))\n    \n    for c in tl.static_range(1, channels):\n        ptr = x_ptr + base_batch + c * x_channel_stride + spatial_offsets\n        vals = tl.load(ptr, mask=spatial_mask, other=float('inf'))\n        min_values = tl.minimum(min_values, vals)\n    \n    out_ptr = output_ptr + pid_batch * output_batch_stride + spatial_offsets\n    tl.store(out_ptr, min_values, mask=spatial_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x * self.scale_factor\n        \n        batch_size, channels, height, width = x.shape\n        spatial_size = height * width\n        x_flat = x.contiguous().view(batch_size, channels, spatial_size)\n        output = torch.empty(batch_size, 1, spatial_size, device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (batch_size, triton.cdiv(spatial_size, META['BLOCK_SIZE']))\n        \n        _min_reduction_kernel[grid](\n            x_flat, output,\n            spatial_size,\n            x_flat.stride(0),\n            x_flat.stride(1),\n            x_flat.stride(2),\n            output.stride(0),\n            output.stride(2),\n            channels\n        )\n        \n        return output.view(batch_size, 1, height, width)\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _min_reduction_kernel(\n    x_ptr,\n    output_ptr,\n    spatial_size,\n    x_batch_stride,\n    x_channel_stride,\n    x_spatial_stride,\n    output_batch_stride,\n    output_spatial_stride,\n    channels: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_spatial_block = tl.program_id(1)\n    \n    spatial_start = pid_spatial_block * BLOCK_SIZE\n    spatial_offsets = spatial_start + tl.arange(0, BLOCK_SIZE)\n    spatial_mask = spatial_offsets < spatial_size\n    \n    base_batch = pid_batch * x_batch_stride\n    ptr0 = x_ptr + base_batch + 0 * x_channel_stride + spatial_offsets\n    min_values = tl.load(ptr0, mask=spatial_mask, other=float('inf'))\n    \n    for c in range(1, channels):\n        ptr = x_ptr + base_batch + c * x_channel_stride + spatial_offsets\n        vals = tl.load(ptr, mask=spatial_mask, other=float('inf'))\n        min_values = tl.minimum(min_values, vals)\n    \n    out_ptr = output_ptr + pid_batch * output_batch_stride + spatial_offsets\n    tl.store(out_ptr, min_values, mask=spatial_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x * self.scale_factor\n        \n        batch_size, channels, height, width = x.shape\n        spatial_size = height * width\n        x_flat = x.contiguous().view(batch_size, channels, spatial_size)\n        output = torch.empty(batch_size, 1, spatial_size, device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE = 128\n        grid = (batch_size, triton.cdiv(spatial_size, BLOCK_SIZE))\n        \n        _min_reduction_kernel[grid](\n            x_flat, output,\n            spatial_size,\n            x_flat.stride(0),\n            x_flat.stride(1),\n            x_flat.stride(2),\n            output.stride(0),\n            output.stride(2),\n            channels,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output.view(batch_size, 1, height, width)\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _min_scale_kernel(\n    x_ptr,\n    output_ptr,\n    scale_factor,\n    out_channels,\n    height, width,\n    stride_batch, stride_channel, stride_height, stride_width,\n    output_stride_batch, output_stride_height, output_stride_width,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_h = tl.program_id(1)\n    pid_w = tl.program_id(2)\n    \n    min_val = float('inf')\n    base = pid_batch * stride_batch + pid_h * stride_height + pid_w * stride_width\n    \n    for channel_offset in range(0, out_channels, BLOCK_SIZE):\n        c_offsets = channel_offset + tl.arange(0, BLOCK_SIZE)\n        mask = c_offsets < out_channels\n        ptrs = x_ptr + base + c_offsets * stride_channel\n        vals = tl.load(ptrs, mask=mask, other=float('inf'))\n        vals = vals * scale_factor\n        block_min = tl.min(vals, axis=0)\n        min_val = tl.minimum(min_val, block_min)\n        \n    output_idx = pid_batch * output_stride_batch + pid_h * output_stride_height + pid_w * output_stride_width\n    tl.store(output_ptr + output_idx, min_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        batch, channels, height, width = x.shape\n        output = torch.empty(batch, 1, height, width, device=x.device, dtype=x.dtype)\n        \n        if not x.is_contiguous():\n            x = x.contiguous()\n        output = output.contiguous()\n        \n        grid = (batch, height, width)\n        _min_scale_kernel[grid](\n            x,\n            output,\n            self.scale_factor,\n            channels,\n            height, width,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(2), output.stride(3),\n            BLOCK_SIZE=128\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 3 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n    ],\n    key=['spatial_size'],\n)\n@triton.jit\ndef _min_reduction_kernel(\n    x_ptr,\n    output_ptr,\n    scale_factor,\n    spatial_size,\n    x_batch_stride,\n    x_channel_stride,\n    x_spatial_stride,\n    output_batch_stride,\n    output_spatial_stride,\n    channels: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_spatial_block = tl.program_id(1)\n    \n    spatial_start = pid_spatial_block * BLOCK_SIZE\n    spatial_offsets = spatial_start + tl.arange(0, BLOCK_SIZE)\n    spatial_mask = spatial_offsets < spatial_size\n    \n    base_batch = pid_batch * x_batch_stride\n    min_values = tl.full((BLOCK_SIZE,), float('inf'), dtype=tl.float32)\n    \n    for c in range(0, channels):\n        ptr = x_ptr + base_batch + c * x_channel_stride + spatial_offsets * x_spatial_stride\n        vals = tl.load(ptr, mask=spatial_mask, other=float('inf'))\n        scaled_vals = vals * scale_factor\n        min_values = tl.minimum(min_values, scaled_vals)\n    \n    out_ptr = output_ptr + pid_batch * output_batch_stride + spatial_offsets * output_spatial_stride\n    tl.store(out_ptr, min_values, mask=spatial_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        batch_size, channels, height, width = x.shape\n        spatial_size = height * width\n        \n        x_flat = x.contiguous().view(batch_size, channels, spatial_size)\n        output = torch.empty(batch_size, 1, spatial_size, device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (batch_size, triton.cdiv(spatial_size, META['BLOCK_SIZE']))\n        \n        _min_reduction_kernel[grid](\n            x_flat, \n            output,\n            self.scale_factor,\n            spatial_size,\n            x_flat.stride(0),\n            x_flat.stride(1),\n            x_flat.stride(2),\n            output.stride(0),\n            output.stride(2),\n            channels\n        )\n        \n        return output.view(batch_size, 1, height, width)\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 4 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 64}, num_warps=1, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 64}, num_warps=2, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 64}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 64}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=1, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=2, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=1, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=2, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=1, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=2, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8, num_stages=2),\n    ],\n    key=['spatial_size', 'channels']\n)\n@triton.jit\ndef _min_reduction_kernel(\n    x_ptr,\n    output_ptr,\n    spatial_size,\n    x_batch_stride,\n    x_channel_stride,\n    x_spatial_stride,\n    output_batch_stride,\n    output_spatial_stride,\n    channels: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_spatial_block = tl.program_id(1)\n    \n    spatial_start = pid_spatial_block * BLOCK_SIZE\n    spatial_offsets = spatial_start + tl.arange(0, BLOCK_SIZE)\n    spatial_mask = spatial_offsets < spatial_size\n    \n    base_batch = pid_batch * x_batch_stride\n    ptr0 = x_ptr + base_batch + 0 * x_channel_stride + spatial_offsets * x_spatial_stride\n    min_values = tl.load(ptr0, mask=spatial_mask, other=float('inf'))\n    \n    # Unroll channel loop for better instruction scheduling\n    for c in tl.static_range(1, channels):\n        ptr = x_ptr + base_batch + c * x_channel_stride + spatial_offsets * x_spatial_stride\n        vals = tl.load(ptr, mask=spatial_mask, other=float('inf'))\n        min_values = tl.minimum(min_values, vals)\n    \n    out_ptr = output_ptr + pid_batch * output_batch_stride + spatial_offsets * output_spatial_stride\n    tl.store(out_ptr, min_values, mask=spatial_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x * self.scale_factor\n        \n        batch_size, channels, height, width = x.shape\n        spatial_size = height * width\n        x_flat = x.contiguous().view(batch_size, channels, spatial_size)\n        output = torch.empty(batch_size, 1, spatial_size, device=x.device, dtype=x.dtype)\n        \n        # Autotuned grid configuration with dynamic BLOCK_SIZE\n        grid = lambda meta: (batch_size, triton.cdiv(spatial_size, meta['BLOCK_SIZE']))\n        \n        _min_reduction_kernel[grid](\n            x_flat, output,\n            spatial_size,\n            x_flat.stride(0),\n            x_flat.stride(1),\n            x_flat.stride(2),\n            output.stride(0),\n            output.stride(2),\n            channels\n        )\n        \n        return output.view(batch_size, 1, height, width)\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 5 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _min_reduction_kernel(\n    x_ptr,\n    output_ptr,\n    spatial_size,\n    x_batch_stride,\n    x_channel_stride,\n    x_spatial_stride,\n    output_batch_stride,\n    output_spatial_stride,\n    channels: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_spatial_block = tl.program_id(1)\n    \n    spatial_start = pid_spatial_block * BLOCK_SIZE\n    spatial_offsets = spatial_start + tl.arange(0, BLOCK_SIZE)\n    spatial_mask = spatial_offsets < spatial_size\n    \n    batch_base = pid_batch * x_batch_stride\n    ptr0 = x_ptr + batch_base + 0 * x_channel_stride + spatial_offsets * x_spatial_stride\n    min_values = tl.load(ptr0, mask=spatial_mask, other=float('inf'))\n    \n    for c in range(1, channels):\n        ptr = x_ptr + batch_base + c * x_channel_stride + spatial_offsets * x_spatial_stride\n        vals = tl.load(ptr, mask=spatial_mask, other=float('inf'))\n        min_values = tl.minimum(min_values, vals)\n    \n    out_ptr = output_ptr + pid_batch * output_batch_stride + spatial_offsets * output_spatial_stride\n    tl.store(out_ptr, min_values, mask=spatial_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x * self.scale_factor\n        \n        batch_size, channels, height, width = x.shape\n        spatial_size = height * width\n        x_flat = x.contiguous().view(batch_size, channels, spatial_size)\n        output = torch.empty(batch_size, 1, spatial_size, device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE = 128\n        grid = (batch_size, triton.cdiv(spatial_size, BLOCK_SIZE))\n        \n        _min_reduction_kernel[grid](\n            x_flat, output,\n            spatial_size,\n            x_flat.stride(0),\n            x_flat.stride(1),\n            x_flat.stride(2),\n            output.stride(0),\n            output.stride(2),\n            channels,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output.view(batch_size, 1, height, width)\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _min_reduction_kernel(\n    x_ptr,\n    output_ptr,\n    scale_factor,\n    spatial_size,\n    x_batch_stride,\n    x_channel_stride,\n    x_spatial_stride,\n    output_batch_stride,\n    output_spatial_stride,\n    channels: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_spatial_block = tl.program_id(1)\n    \n    spatial_start = pid_spatial_block * BLOCK_SIZE\n    spatial_offsets = spatial_start + tl.arange(0, BLOCK_SIZE)\n    spatial_mask = spatial_offsets < spatial_size\n    \n    min_values = tl.full((BLOCK_SIZE,), float('inf'), dtype=tl.float32)\n    base_batch = pid_batch * x_batch_stride\n    \n    for c in range(0, channels):\n        ptr = x_ptr + base_batch + c * x_channel_stride + spatial_offsets * x_spatial_stride\n        vals = tl.load(ptr, mask=spatial_mask, other=float('inf'))\n        vals = vals * scale_factor\n        min_values = tl.minimum(min_values, vals)\n    \n    out_ptr = output_ptr + pid_batch * output_batch_stride + spatial_offsets * output_spatial_stride\n    tl.store(out_ptr, min_values, mask=spatial_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        batch_size, channels, height, width = x.shape\n        spatial_size = height * width\n        \n        x_flat = x.contiguous().view(batch_size, channels, spatial_size)\n        output = torch.empty(batch_size, 1, spatial_size, device=x.device, dtype=x.dtype)\n        \n        BLOCK_SIZE = 128\n        grid = (batch_size, triton.cdiv(spatial_size, BLOCK_SIZE))\n        \n        _min_reduction_kernel[grid](\n            x_flat, \n            output,\n            self.scale_factor,\n            spatial_size,\n            x_flat.stride(0),\n            x_flat.stride(1),\n            x_flat.stride(2),\n            output.stride(0),\n            output.stride(2),\n            channels,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return output.view(batch_size, 1, height, width)\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program which has a speed_up of 0.791 (which is less than 1, meaning slower than the reference).\n The goal is to achieve at least 5% speedup (speed_up >= 1.05).\n\n The current implementation uses a Triton kernel that does a min reduction over channels for each spatial location.\n The kernel is launched with:\n   grid = (batch_size, triton.cdiv(spatial_size, BLOCK_SIZE))\n   where BLOCK_SIZE is fixed at 128.\n\n The kernel iterates over each channel (from 0 to channels-1) and loads a block of spatial values for that channel.\n It then multiplies by a scale_factor and does a min reduction across channels (but note: the reduction is per spatial location).\n\n However, note that the current kernel does:\n   min_values = tl.full(...)  # initialized to infinity\n   for c in range(channels):\n        load a block of spatial values for channel c\n        multiply by scale_factor\n        update min_values = min(min_values, current channel values)\n\n Then it stores the min_values for the spatial block.\n\n The problem is that the kernel is doing a lot of memory accesses and the inner loop is over channels, which might not be efficient.\n\n Let's consider the memory access pattern:\n   For a fixed batch and a fixed spatial block, we are iterating over channels. For each channel, we load a contiguous block of spatial data (if the tensor is contiguous in the spatial dimension?).\n\n But note: the tensor x_flat has shape [batch, channels, spatial_size]. The stride for the spatial dimension is the smallest (x_flat.stride(2)=1). So when we load for a fixed channel and a block of spatial indices, we are loading contiguous memory. That is good.\n\n However, the kernel is currently not vectorized because we are doing a scalar min operation per spatial element. We are using tl.minimum which is element-wise, but note that the reduction is over channels, not over the spatial block. Actually, we are doing a per-spatial-element reduction over channels.\n\n The current approach does:\n   For each spatial element in the block (indexed by i in [0, BLOCK_SIZE-1]), we have a separate min reduction over channels.\n\n The issue: the inner loop over channels is sequential and cannot be parallelized. Also, we are doing one load per channel per spatial element.\n\n Alternative approach: we can use a reduction tree in the channel dimension? But note: the channel dimension (16 in the example) is small. So the sequential loop might not be the main bottleneck.\n\n However, note that the current kernel does a lot of memory loads: for each channel, we load a block of BLOCK_SIZE elements. The total number of loads per kernel instance is channels * BLOCK_SIZE.\n\n The spatial_size is 32*32=1024, and BLOCK_SIZE=128, so the grid for spatial is 1024/128=8 per batch. The total number of kernel launches is batch * 8 = 128*8=1024.\n\n The total memory loaded is: 1024 (kernels) * channels (16) * 128 (spatial block) * 4 (bytes) = 8MB. This is not huge, but the overhead of launching 1024 kernels might be significant.\n\n Another approach: instead of having the kernel reduce over channels for a block of spatial elements, we could have the kernel reduce over channels for a single spatial element? But that would be worse because we would have more kernel launches (spatial_size=1024 per batch -> 128*1024=131072 kernels).\n\n Alternatively, we can try to process multiple channels per thread by vectorizing the loads and stores? But note that the min reduction is independent per spatial element.\n\n Another idea: we can change the kernel to have two levels of reduction? But note that the channel dimension is small (16) so it's not worth parallelizing over channels.\n\n However, we note that the current kernel does not use shared memory. Since the same spatial block is accessed for each channel, we could load a block of channels for a spatial block? But the min reduction is over channels, so we need to combine across channels.\n\n Actually, we can try to load multiple channels at once by vectorizing the load. For example, we can load a vector of 4 consecutive channels for the same spatial index? Then we can do a vector min reduction. This would reduce the number of load instructions by a factor of 4.\n\n Steps for the vectorized approach:\n\n  1. We change the kernel to process a block of spatial indices and a block of channels? But note: the reduction is over channels, so we cannot split the channel dimension across threads? Actually, we can: we can have each thread reduce a subset of channels and then do a shared memory reduction? But the channel dimension is small (16) and we have 128 threads per block? That might be overkill.\n\n  2. Alternatively, we can use vectorized loads. We can load multiple channels for the same spatial index at once. For example, we can load 4 channels at a time. We would then have a vector of 4 values per spatial index. Then we can do a vector min to reduce these 4 values to one, and then combine with the current min.\n\n How to do that in Triton?\n\n  We can change the inner loop to step by, say, 4 channels. Then we use tl.arange(0, 4) to create a vector of channel offsets.\n\n  Then we load a vector of 4 values for the same spatial index? Actually, note: the memory layout is [batch, channels, spatial]. So for a fixed spatial index, the channels are consecutive. Therefore, we can load 4 consecutive channels for a fixed spatial index with a vector load.\n\n  However, in our kernel, we are processing a block of spatial indices. So we want to load for a block of spatial indices and a vector of channels? That would be a 2D load? Triton supports 2D loads? Actually, we can use tl.make_block_ptr to do 2D loads? Or we can use a vector of pointers? It's easier to change the order of the loops.\n\n  Alternatively, we can restructure the kernel to have the inner loop over spatial be vectorized per channel block? But that might complicate.\n\n  Let me propose:\n\n    We change the inner loop to step over channels in chunks of 4 (or whatever vector size). Then for a given chunk of 4 channels, we load a [4, BLOCK_SIZE] block of data? But that would be non-contiguous? Actually, if we load for a fixed set of 4 channels and a block of spatial indices, then the data is stored in contiguous memory? No, because the tensor is [batch, channels, spatial], so the spatial dimension is contiguous. Therefore, for a fixed set of 4 channels and a contiguous block of spatial indices, the data is stored in 4 contiguous segments, each of length BLOCK_SIZE? That is not contiguous.\n\n  We can use a loop over the spatial block and for each spatial index, we load a vector of 4 channels. Then we do a vector min? But that would be a loop over the spatial block (BLOCK_SIZE=128) and then a loop over channels in steps of 4. The total operations would be the same, but the loads would be vectorized.\n\n  How to do that in Triton?\n\n    We can have two nested loops: \n      for c in range(0, channels, 4):\n        for i in tl.static_range(0, BLOCK_SIZE):\n            # load 4 channels at spatial_offsets[i] for channels [c, c+3]\n            ptr = ... + (c + tl.arange(0,4)) * x_channel_stride + (spatial_offsets[i]) * x_spatial_stride\n            vals = tl.load(ptr, mask=(c+tl.arange(0,4))<channels, ...)\n            # then reduce the 4 values to one min and update min_value[i]\n\n    But note: tl.static_range is for compile-time unrolling. BLOCK_SIZE is 128, which is not too large? But unrolling 128 might be heavy.\n\n  Alternatively, we can use vectorization in Triton by having each thread process multiple elements? We can change the kernel to have a vector of BLOCK_SIZE elements and then process 4 channels at a time? But the min reduction would then be done in vectors.\n\n  However, note that the current kernel already uses a vector of BLOCK_SIZE elements (min_values is a vector). We want to extend that to also vectorize over channels? That would be a 2D vector? We don't want to do that.\n\n  Alternatively, we can change the kernel to have the channel reduction as a parallel reduction? But the channel dimension is small (16) so we can use a sequential loop that steps by 4 and does vector loads.\n\n  Let me try to change the inner channel loop to step by a vector size (let's say 4) and then do a vector load for the same spatial block? But note: the data for 4 channels and a contiguous spatial block is not contiguous. We would have to do 4 separate loads? Or we can use a 2D load? \n\n  In Triton, we can use tl.load with a broadcast mask? Actually, we can do:\n\n      c_offsets = c + tl.arange(0,4)\n      mask = (c_offsets < channels) & spatial_mask   # spatial_mask is [BLOCK_SIZE]\n      ptrs = base_batch + c_offsets[None,:] * x_channel_stride + spatial_offsets[:,None] * x_spatial_stride\n\n      # This would create a [BLOCK_SIZE, 4] array of pointers? Then we can load with:\n      #   vals = tl.load(ptrs, mask=mask)   # [BLOCK_SIZE, 4]\n\n  But note: Triton does not support 2D arrays in the kernel. We can only have 1D arrays.\n\n  Alternatively, we can do:\n\n      for c in range(0, channels, 4):\n          # for each of the 4 channels in the vector\n          for j in range(4):\n              cc = c + j\n              if cc < channels:\n                  ptr = ... + cc * x_channel_stride + spatial_offsets\n                  val = tl.load(ptr, mask=spatial_mask, ...)\n                  # then we have a vector of 4 values for each spatial element? We don't.\n\n  This doesn't help.\n\n  Another idea: we can use tl.dot for reduction? Not for min.\n\n  Given the complexity and the fact that the channel dimension is small (16), the current sequential loop might not be the main bottleneck. The main bottleneck might be the kernel launch overhead and the fact that we are not using shared memory to reduce the min across channels? But the min reduction is per spatial element and independent, so shared memory won't help.\n\n  We should look at the top performing programs for inspiration.\n\n  The top performing program (Program 3) has a speed_up of 0.791 (which is the same as the current). But we see that the best we've had is 0.791 (which is still less than 1).\n\n  We need to try a different approach.\n\n  Let's look at the inspiration programs:\n\n      Inspiration 1: It does the scaling after the convolution and then does a min reduction without scaling in the kernel. But note: the current kernel does the scaling inside.\n\n      Inspiration 2: It uses a kernel that iterates over channels in blocks and does a min reduction per spatial location. Similar to our current.\n\n      Inspiration 3: It uses autotuning and does the same as our current.\n\n      Inspiration 4: It uses static unrolling of the channel loop? That might help.\n\n      Inspiration 5: It does the scaling outside the kernel and then does the min reduction without scaling.\n\n  We note that the current kernel does the scaling inside the kernel. But the reference might be doing:\n\n        x = self.conv(x)\n        # then the min reduction over channels (with scaling?) \n\n  Actually, the problem says: the reference code is in PyTorch. We don't have it, but the Triton kernel multiplies by scale_factor.\n\n  But note: the reference might be doing:\n\n        output = torch.min(x * scale_factor, dim=1, keepdim=True)\n\n  So we are doing the same.\n\n  However, we can try to do the scaling outside the kernel? That would be one element-wise operation and then the min reduction. But that would require an extra pass over the data? We don't want that.\n\n  Alternatively, we can fuse the scaling with the min reduction? We are already doing that.\n\n  What if we use a different kernel structure? The initial approach that did not flatten the spatial dimension (like in Program 1) had a worse performance (speed_up=0.498).\n\n  The current approach (flattening the spatial dimension) is better.\n\n  How about we try to use autotuning? The current program does not use autotuning. We can add autotuning for the BLOCK_SIZE and the number of warps.\n\n  Also, note that the current program uses a fixed BLOCK_SIZE=128. We can try different block sizes and also different numbers of warps.\n\n  Let's add autotuning:\n\n      We will define several configurations for BLOCK_SIZE and num_warps.\n\n      We note that the spatial_size is 1024 (for 32x32). We can try BLOCK_SIZE of 64, 128, 256, 512, 1024.\n\n      Also, we can try different numbers of warps (1,2,4,8).\n\n  We also note that the kernel does not use shared memory. We might consider using shared memory to reduce the number of global memory writes? But we only write once per spatial element.\n\n  Alternatively, we can change the kernel to do the min reduction in shared memory? But the min reduction is per spatial element and independent, so it doesn't require communication between threads. So shared memory won't help.\n\n  Therefore, I propose:\n\n      Add autotuning for BLOCK_SIZE and num_warps.\n\n  Steps:\n\n      1. Add the @triton.autotune decorator above the kernel.\n      2. Define a set of configs with different BLOCK_SIZE and num_warps.\n      3. Change the kernel to take BLOCK_SIZE as a config parameter (it already does).\n      4. In the grid function, use triton.cdiv(spatial_size, BLOCK_SIZE) but now BLOCK_SIZE is from the config.\n\n  Also, note that the kernel uses a fixed BLOCK_SIZE=128 in the call. We should remove that and let autotuning choose.\n\n  We change the kernel call to:\n\n        grid = (batch_size, triton.cdiv(spatial_size, BLOCK_SIZE))\n\n        _min_reduction_kernel[grid](\n            ...,\n            BLOCK_SIZE=BLOCK_SIZE,   # pass the BLOCK_SIZE from the config\n            ...)\n\n  But note: in the current program, the kernel call uses BLOCK_SIZE=BLOCK_SIZE (which is 128). We want to make it dynamic.\n\n  We also note that the kernel uses a fixed BLOCK_SIZE in the kernel signature. We already have it as tl.constexpr.\n\n  Let's rewrite the program with autotuning.\n\n  We'll also remove the fixed BLOCK_SIZE=128 in the ModelNew.forward and instead let the autotuner choose.\n\n  We'll define the autotuner with:\n\n      configs = [\n          triton.Config({'BLOCK_SIZE': 64}, num_warps=1),\n          triton.Config({'BLOCK_SIZE': 64}, num_warps=2),\n          triton.Config({'BLOCK_SIZE': 64}, num_warps=4),\n          triton.Config({'BLOCK_SIZE': 64}, num_warps=8),\n          triton.Config({'BLOCK_SIZE': 128}, num_warps=1),\n          triton.Config({'BLOCK_SIZE': 128}, num_warps=2),\n          triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n          triton.Config({'BLOCK_SIZE': 128}, num_warps=8),\n          triton.Config({'BLOCK_SIZE': 256}, num_warps=1),\n          triton.Config({'BLOCK_SIZE': 256}, num_warps=2),\n          triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n          triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n          triton.Config({'BLOCK_SIZE': 512}, num_warps=1),\n          triton.Config({'BLOCK_SIZE': 512}, num_warps=2),\n          triton.Config({'BLOCK_SIZE': 512}, num_warps=4),\n          triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n          triton.Config({'BLOCK_SIZE': 1024}, num_warps=1),\n          triton.Config({'BLOCK_SIZE': 1024}, num_warps=2),\n          triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n          triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n      ]\n\n      key = ['spatial_size']   # because the grid depends on spatial_size\n\n  We note that the grid is (batch_size, triton.cdiv(spatial_size, BLOCK_SIZE)). The batch_size is fixed per instance, but the spatial_size is fixed per problem.\n\n  We also note that the kernel does not use shared memory, so we don't set num_stages? We can leave it to the default.\n\n  We'll add the autotune decorator and change the kernel call to use the config.\n\n  Also, we must remove the fixed BLOCK_SIZE=128 in the kernel call and use the config's BLOCK_SIZE.\n\n  We change the grid to be a lambda that uses the meta parameters.\n\n  Let's write:\n\n      grid = lambda meta: (batch_size, triton.cdiv(spatial_size, meta['BLOCK_SIZE']))\n\n  And then call the kernel without passing BLOCK_SIZE explicitly? Actually, we pass it as an argument. The autotuner will set the BLOCK_SIZE in the config.\n\n  We also note that the kernel has an argument named BLOCK_SIZE, so we pass it by name.\n\n  We'll also remove the line that sets BLOCK_SIZE=128 in the forward function.\n\n  We hope that autotuning will find a better configuration.\n\n  Additionally, we can try to unroll the channel loop? Since the channel dimension is small (16), we can unroll it by 4? But we don't know if the channel dimension is always 16. It is 16 in the example, but the module is general.\n\n  We can use:\n\n        for c in range(0, channels, 4):\n            # load 4 channels for the entire spatial block? Not contiguous.\n\n  This is difficult without vectorization.\n\n  Alternatively, we can use tl.static_range to unroll the channel loop? That might help the compiler generate better code.\n\n  We change:\n\n        for c in tl.static_range(0, channels):\n\n  But note: the channel dimension is not known at compile time? We have channels as a kernel argument (tl.constexpr). So if we mark it as tl.constexpr, then the loop can be unrolled.\n\n  We already have channels as tl.constexpr. So the loop over channels is unrolled? That might be heavy for large channels? But in our example, channels=16, so unrolling 16 iterations is acceptable.\n\n  We can try that.\n\n  Therefore, we do two things:\n\n      1. Add autotuning for BLOCK_SIZE and num_warps.\n      2. Change the channel loop to use tl.static_range to force unrolling.\n\n  But note: the current loop is:\n\n        for c in range(0, channels):\n\n  We can change it to:\n\n        for c in tl.static_range(0, channels):\n\n  This will unroll the loop.\n\n  However, we note that the channel dimension is passed as a tl.constexpr, so the compiler might unroll it anyway? But using tl.static_range ensures unrolling.\n\n  Let's do both.\n\n  We hope that these changes will improve performance.\n\n  We also note that the current program does the scaling inside the kernel. We are doing:\n\n        vals = vals * scale_factor\n\n  But note: the scale_factor is a scalar. We can move the multiplication outside the channel loop? Actually, we can multiply the entire min_values by scale_factor at the end? But that would be wrong because we are doing:\n\n        min_values = min(min_values, current * scale_factor)\n\n  We cannot move it out.\n\n  Alternatively, we can do:\n\n        min_values = tl.full(..., float('inf'))\n        for c in ...:\n            load vals\n            # then do: candidate = vals * scale_factor\n            candidate = vals * scale_factor\n            min_values = tl.minimum(min_values, candidate)\n\n  That is what we are doing.\n\n  We can try to precompute the scaled value in the load? But that doesn't change the number of operations.\n\n  Given the above, we rewrite the program with autotuning and unrolled channel loop.\n\n  We'll also change the grid to be defined as a lambda using the meta parameters.\n\n  We note that the current program has:\n\n        grid = (batch_size, triton.cdiv(spatial_size, BLOCK_SIZE))\n\n  We change to:\n\n        grid = lambda meta: (batch_size, triton.cdiv(spatial_size, meta['BLOCK_SIZE']))\n\n  And we remove the fixed BLOCK_SIZE in the kernel call.\n\n  Let's write the new program.\n\n  IMPORTANT: We must not change the function signature of the kernel.\n\n  We'll add the autotune decorator and change the loop to static_range.\n\n  We'll also remove the line that sets BLOCK_SIZE=128 in the kernel call and use the one from the config.\n\n  We note that the kernel call now passes BLOCK_SIZE=meta['BLOCK_SIZE'] via the autotuner.\n\n  We hope this will improve the speed.\n\n  If not, we might need to consider other optimizations.\n\n  We also note that the current kernel initializes min_values to infinity. We could initialize to the first channel? But that would require splitting the loop. We can do:\n\n        c0 = 0\n        ptr0 = ... + c0 * x_channel_stride + ... \n        min_values = tl.load(ptr0, ...) * scale_factor\n        for c in range(1, channels):\n            ...\n\n  This would avoid the first iteration being a min with infinity? But we are doing the same number of operations. It might save one min operation per element? Not significant.\n\n  We'll stick to the current.\n\n  Let's write the code.\n</think>\nTo improve the performance of the Triton kernel for min reduction, we introduce autotuning for optimal block size and warp configuration, and unroll the channel loop for better instruction scheduling. The key optimizations are:\n\n1. Added autotuner to optimize BLOCK_SIZE (64-1024) and num_warps (1-8)\n2. Used static unrolling for the channel loop to improve instruction pipelining\n3. Made grid configuration dynamic based on autotuned parameters\n4. Maintained vectorized memory accesses and boundary checks\n\nHere's the optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 64}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 64}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n    ],\n    key=['spatial_size']\n)\n@triton.jit\ndef _min_reduction_kernel(\n    x_ptr,\n    output_ptr,\n    scale_factor,\n    spatial_size,\n    x_batch_stride,\n    x_channel_stride,\n    x_spatial_stride,\n    output_batch_stride,\n    output_spatial_stride,\n    channels: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_spatial_block = tl.program_id(1)\n    \n    spatial_start = pid_spatial_block * BLOCK_SIZE\n    spatial_offsets = spatial_start + tl.arange(0, BLOCK_SIZE)\n    spatial_mask = spatial_offsets < spatial_size\n    \n    min_values = tl.full((BLOCK_SIZE,), float('inf'), dtype=tl.float32)\n    base_batch = pid_batch * x_batch_stride\n    \n    # Unrolled channel loop for better instruction scheduling\n    for c in tl.static_range(0, channels):\n        ptr = x_ptr + base_batch + c * x_channel_stride + spatial_offsets * x_spatial_stride\n        vals = tl.load(ptr, mask=spatial_mask, other=float('inf'))\n        vals = vals * scale_factor\n        min_values = tl.minimum(min_values, vals)\n    \n    out_ptr = output_ptr + pid_batch * output_batch_stride + spatial_offsets * output_spatial_stride\n    tl.store(out_ptr, min_values, mask=spatial_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.conv(x)\n        batch_size, channels, height, width = x.shape\n        spatial_size = height * width\n        \n        x_flat = x.contiguous().view(batch_size, channels, spatial_size)\n        output = torch.empty(batch_size, 1, spatial_size, device=x.device, dtype=x.dtype)\n        \n        # Dynamic grid configuration based on autotuned BLOCK_SIZE\n        grid = lambda meta: (batch_size, triton.cdiv(spatial_size, meta['BLOCK_SIZE']))\n        \n        _min_reduction_kernel[grid](\n            x_flat, \n            output,\n            self.scale_factor,\n            spatial_size,\n            x_flat.stride(0),\n            x_flat.stride(1),\n            x_flat.stride(2),\n            output.stride(0),\n            output.stride(2),\n            channels\n        )\n        \n        return output.view(batch_size, 1, height, width)\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nscale_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "695c5c19-852e-4f8e-8798-68bcf8a56684", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64}, num_warps=2),\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=2),\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 128}, num_warps=2),\n    ],\n    key=['m', 'n', 'k'],\n)\n@triton.jit\ndef batched_matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    m, n, k,\n    stride_ab, stride_am, stride_ak,\n    stride_bb, stride_bk, stride_bn,\n    stride_cb, stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_m = tl.program_id(1)\n    pid_n = tl.program_id(2)\n    \n    a_block_ptr = tl.make_block_ptr(\n        base=a_ptr + pid_batch * stride_ab,\n        shape=(m, k),\n        strides=(stride_am, stride_ak),\n        offsets=(pid_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_K),\n        order=(1, 0)\n    )\n    \n    b_block_ptr = tl.make_block_ptr(\n        base=b_ptr + pid_batch * stride_bb,\n        shape=(k, n),\n        strides=(stride_bk, stride_bn),\n        offsets=(0, pid_n * BLOCK_N),\n        block_shape=(BLOCK_K, BLOCK_N),\n        order=(1, 0)\n    )\n    \n    c_block_ptr = tl.make_block_ptr(\n        base=c_ptr + pid_batch * stride_cb,\n        shape=(m, n),\n        strides=(stride_cm, stride_cn),\n        offsets=(pid_m * BLOCK_M, pid_n * BLOCK_N),\n        block_shape=(BLOCK_M, BLOCK_N),\n        order=(1, 0)\n    )\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k_offset in range(0, k, BLOCK_K):\n        a = tl.load(a_block_ptr, boundary_check=(0,1))\n        b = tl.load(b_block_ptr, boundary_check=(0,1))\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        \n        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_K))\n        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_K, 0))\n    \n    tl.store(c_block_ptr, accumulator, boundary_check=(0,1))\n\nclass ModelNew(nn.Module):\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        A = A.contiguous()\n        B = B.contiguous()\n        batch_size, m, k = A.shape\n        _, k_, n = B.shape\n        assert k == k_, \"Incompatible dimensions\"\n        \n        C = torch.empty((batch_size, m, n), device=A.device, dtype=A.dtype)\n        \n        grid = (batch_size, triton.cdiv(m, 64), triton.cdiv(n, 64))\n        \n        batched_matmul_kernel[grid](\n            A, B, C,\n            m, n, k,\n            A.stride(0), A.stride(1), A.stride(2),\n            B.stride(0), B.stride(1), B.stride(2),\n            C.stride(0), C.stride(1), C.stride(2),\n        )\n        return C\n\nbatch_size = 128\nm = 128\nk = 256\nn = 512\n\ndef get_inputs():\n    A = torch.randn(batch_size, m, k, device='cuda')\n    B = torch.randn(batch_size, k, n, device='cuda')\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "ca2807c4-7d86-4abd-b04b-f03bef860d1d", "generation": 1, "timestamp": 1754641412.7315824, "iteration_found": 0, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_3_Batched_matrix_multiplication", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "max_difference": ["0.070168", "122.055145", "119.930176", "122.335968", "133.542740"], "avg_difference": ["0.009816", "13.542461", "13.534920", "13.535495", "13.537962"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0257, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.105, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0257, "speed_up": 0.245, "custom_timing": 0.105}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.1050\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0257, 'speed_up': 0.245, 'custom_timing': 0.105}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.1050, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0257, 'speed_up': 0.245, 'custom_timing': 0.105}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs batched matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (batch_size, m, k).\n            B: Input tensor of shape (batch_size, k, n).\n\n        Returns:\n            C: Output tensor of shape (batch_size, m, n).\n        \"\"\"\n        return torch.bmm(A, B)\n\nbatch_size = 128\nm = 128\nk = 256\nn = 512\n\ndef get_inputs():\n    A = torch.randn(batch_size, m, k)\n    B = torch.randn(batch_size, k, n)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.1050), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0257, 'speed_up': 0.245, 'custom_timing': 0.105})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs batched matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (batch_size, m, k).\n            B: Input tensor of shape (batch_size, k, n).\n\n        Returns:\n            C: Output tensor of shape (batch_size, m, n).\n        \"\"\"\n        return torch.bmm(A, B)\n\nbatch_size = 128\nm = 128\nk = 256\nn = 512\n\ndef get_inputs():\n    A = torch.randn(batch_size, m, k)\n    B = torch.randn(batch_size, k, n)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a reference implementation of batched matrix multiplication using `torch.bmm`.\n The goal is to rewrite it using Triton to achieve a speedup.\n\n The current implementation has:\n   batch_size = 128, m = 128, k = 256, n = 512.\n\n We note that the reference timing is 0.0257 ms and the custom timing (if any) was 0.105 ms (which is slower).\n\n We are going to write a Triton kernel for batched matrix multiplication.\n\n Steps:\n 1. We will write a Triton kernel that uses tiling for the matrices.\n 2. We will use the standard tiling approach: each block computes a tile of C of size [BLOCK_M, BLOCK_N].\n 3. We will iterate over the K dimension by blocks of size BLOCK_K.\n\n Important: We must respect the constraints (do not change function signatures, grid, etc. arbitrarily).\n\n However, note that the original program does not have a Triton kernel. We are replacing the entire forward method.\n\n We are allowed to change the internal implementation as long as the inputs and outputs remain the same.\n\n We are going to create a Triton kernel for batched matrix multiplication.\n\n Let's design:\n   - We have two input tensors: A (batch_size, m, k), B (batch_size, k, n)\n   - Output: C (batch_size, m, n)\n\n We will use a 3D grid: [batch_size, triton.cdiv(m, BLOCK_M), triton.cdiv(n, BLOCK_N)]\n\n In the kernel:\n   pid_batch: program_id(0)  -> batch index\n   pid_m: program_id(1)       -> row block index\n   pid_n: program_id(2)       -> column block index\n\n We will load a block of A of size [BLOCK_M, BLOCK_K] and a block of B of size [BLOCK_K, BLOCK_N] and compute the inner product.\n\n We must handle boundaries.\n\n We note that the current hardware (H100) has Tensor Cores for FP16. However, the inputs are FP32 (from torch.randn).\n We have two options:\n   a) Convert the inputs to FP16 and use FP16 Tensor Cores. But note: the problem does not specify the dtype. We must check the original.\n   b) Use FP32.\n\n The reference uses FP32 because torch.randn is FP32. We must use FP32.\n\n However, we can also consider using TF32 if available. But by default, Triton uses FP32 for tl.dot when the inputs are FP32.\n\n Alternatively, we can let the user choose the dtype? But the problem says the inputs are from torch.randn (FP32).\n\n We will write the kernel in FP32.\n\n Steps in the kernel:\n\n   pid_batch, pid_m, pid_n = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n   # Create pointers for the first blocks of A and B.\n   # A: (batch, m, k) -> base = A + pid_batch * m * k\n   # We want to load a block of A: [BLOCK_M, BLOCK_K]\n   offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n   offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n   offs_k = tl.arange(0, BLOCK_K)\n\n   # Pointers for A and B\n   a_ptrs = A + (pid_batch * m * k) + (offs_am[:, None] * k + offs_k[None, :])\n   b_ptrs = B + (pid_batch * k * n) + (offs_k[:, None] * n + offs_bn[None, :])\n\n   # We accumulate in a tile of C\n   accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n   for k_idx in range(0, tl.cdiv(k, BLOCK_K)):\n        # Load the current block of A and B\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < m) & (offs_k[None, :] < k - k_idx * BLOCK_K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < k - k_idx * BLOCK_K) & (offs_bn[None, :] < n), other=0.0)\n\n        # We accumulate the product\n        accumulator += tl.dot(a, b, allow_tf32=True)\n\n        # Advance pointers to next block in K\n        a_ptrs += BLOCK_K\n        b_ptrs += BLOCK_K * n   # because B is (k, n) and we are moving in k by BLOCK_K\n\n   # Now write back the accumulator to C\n   c_ptrs = C + (pid_batch * m * n) + (offs_am[:, None] * n + offs_bn[None, :])\n   tl.store(c_ptrs, accumulator, mask=(offs_am[:, None] < m) & (offs_bn[None, :] < n))\n\n However, note: the above pointer arithmetic for B is not exactly standard. We have to be cautious.\n\n Alternatively, we can precompute the base pointers for the current batch and then use:\n\n   a_ptrs = base_A + (offs_am[:, None] * k + offs_k[None, :])\n   b_ptrs = base_B + (offs_k[:, None] * n + offs_bn[None, :])\n\n and then update:\n\n   base_A += BLOCK_K\n   base_B += BLOCK_K * n\n\n But note: we are iterating over k in chunks. The initial base_A and base_B for the current batch and current k chunk?\n\n Actually, we don't need to update the base_A and base_B per batch? We can precompute the base for the batch and then for the k chunk.\n\n Alternatively, we can do:\n\n   a_ptrs = base_A + (offs_am[:, None] * k + offs_k[None, :] + k_idx * BLOCK_K)\n   b_ptrs = base_B + ((offs_k[None, :] + k_idx * BLOCK_K) * n + offs_bn[:, None])\n\n But that would require recalculating the pointers every time, which might be expensive.\n\n Instead, we can do:\n\n   a_ptrs = base_A + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n   and then update a_ptrs by BLOCK_K in the k dimension.\n\n We note the strides:\n\n   For A: [m, k] -> stride_am = k, stride_ak = 1? Actually, the tensor is (m, k) so row-major: element (i,j) at i*k + j.\n\n   But wait: our tensor A is (batch, m, k). The stride for m is k, and for k is 1.\n\n   Similarly, B is (batch, k, n): the stride for k is n, and for n is 1.\n\n   So we can set:\n\n        stride_am = k\n        stride_ak = 1\n        stride_bk = n\n        stride_bn = 1\n\n   Then:\n\n        a_ptrs = base_A + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n        b_ptrs = base_B + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n   And then update for the next k chunk:\n\n        base_A += BLOCK_K * stride_ak   -> but note: we are moving along k, so we add BLOCK_K (which is 1 element per step? but we are using a block of BLOCK_K)\n\n        Actually, we want to move the entire block in k: so we add BLOCK_K to the k index. But note that the pointer is for the entire block.\n\n        We can update the pointers by:\n\n            a_ptrs += BLOCK_K * stride_ak   -> which is BLOCK_K * 1 = BLOCK_K\n            b_ptrs += BLOCK_K * stride_bk   -> which is BLOCK_K * n\n\n   So the above initial code was correct.\n\n But note: the mask for A and B in the k dimension must be for the current chunk. We can compute:\n\n        k_offs = k_idx * BLOCK_K\n        k_remaining = k - k_offs\n        k_mask = (offs_k[None, :] < k_remaining)\n\n   Then for A: we require the row index (offs_am) to be within [0, m) and the column index (current k_offs + offs_k) to be within [0, k). But we can express the mask for A as:\n\n        mask_a = (offs_am[:, None] < m) & (offs_k[None, :] < k_remaining)\n\n   Similarly for B: we require the row index (current k_offs + offs_k) to be within [0, k) and the column index (offs_bn) to be within [0, n). So:\n\n        mask_b = (offs_k[:, None] < k_remaining) & (offs_bn[None, :] < n)\n\n   However, note: the offs_k is from 0 to BLOCK_K-1. We are only loading a block of BLOCK_K at a time.\n\n   Alternatively, we can precompute the base pointer for the current k chunk:\n\n        a_ptrs = base_A + (offs_am[:, None] * k + k_offs + offs_k[None, :])\n        b_ptrs = base_B + ((k_offs + offs_k[:, None]) * n + offs_bn[None, :])\n\n   But then we don't need to update the pointers by adding BLOCK_K in the loop? Actually, we can do:\n\n        for k_offs in range(0, k, BLOCK_K):\n            # compute the mask for this chunk\n            k_remaining = k - k_offs\n            mask_k = offs_k < k_remaining   # [BLOCK_K] but we are using 1D for offs_k\n\n            # Then for A: we need to check row and the k index in this chunk\n            a = tl.load(a_ptrs, mask=(offs_am[:, None] < m) & (mask_k[None, :]), other=0.0)\n            b = tl.load(b_ptrs, mask=(mask_k[:, None]) & (offs_bn[None, :] < n), other=0.0)\n\n            accumulator += tl.dot(a, b, allow_tf32=True)\n\n            a_ptrs += BLOCK_K   # because we are moving in k by BLOCK_K (stride 1 for k in A)\n            b_ptrs += BLOCK_K * n   # because in B, moving in k by BLOCK_K (each row in B is n long)\n\n   This is efficient.\n\n We will set the grid to be (batch_size, triton.cdiv(m, BLOCK_M), triton.cdiv(n, BLOCK_N))\n\n We must choose BLOCK_M, BLOCK_N, and BLOCK_K. We will use autotune.\n\n We note that the problem says to not change the grid configuration arbitrarily? But we are writing a new kernel and the grid is determined by the problem size.\n\n However, the original program did not have a grid. We are defining the grid for the Triton kernel.\n\n We are going to write:\n\n   def forward(self, A, B):\n        batch_size, m, k = A.shape\n        batch_size_B, k_B, n = B.shape\n        assert batch_size == batch_size_B and k == k_B\n        C = torch.empty((batch_size, m, n), device=A.device, dtype=A.dtype)\n\n        # Define the grid\n        grid = (batch_size, triton.cdiv(m, BLOCK_M), triton.cdiv(n, BLOCK_N))\n\n        # Launch the kernel\n        kernel_bmm[grid](A, B, C, m, n, k, ...)\n\n        return C\n\n We are going to use autotune for BLOCK_M, BLOCK_N, BLOCK_K.\n\n We note that the current reference runs in 0.0257 ms. We must beat that.\n\n However, the previous custom timing (if any) was 0.105 ms (which is 4x slower). So we must be cautious.\n\n We will start with a standard block size. For FP32, common choices are 64 for BLOCK_M and BLOCK_N and 32 for BLOCK_K.\n\n But note: the tensor core for FP32 is not available? We are using tl.dot with allow_tf32. TF32 is supported on A100 and H100.\n\n We are on H100, so TF32 is supported. We should set:\n\n   tl.dot(..., allow_tf32=True)\n\n And we should set the environment variable for TF32? But Triton handles that.\n\n We are going to write the kernel and then autotune the block sizes.\n\n However, note: the problem says to return only the code within the EVOLVE-BLOCK. We are going to write the entire ModelNew class and the constants.\n\n But note: the original code also defined batch_size, m, k, n and get_inputs. We must keep the same get_inputs and batch_size, etc.\n\n However, the problem says: \"Make sure your rewritten program maintains the same inputs and outputs\"\n\n We are going to replace the forward method of ModelNew and then also define the kernel.\n\n We must put everything in the EVOLVE-BLOCK.\n\n We note that the original code did not have Triton. We are introducing Triton. We must import triton and tl.\n\n We are going to write:\n\n   import triton\n   import triton.language as tl\n\n   @triton.autotune(...)\n   @triton.jit\n   def kernel_bmm(...)\n\n   Then in ModelNew.forward, we call kernel_bmm.\n\n We are going to set the autotune configs for BLOCK_M, BLOCK_N, BLOCK_K.\n\n We are going to try:\n\n   configs = []\n   for BLOCK_M in [16, 32, 64]:\n        for BLOCK_N in [16, 32, 64]:\n            for BLOCK_K in [16, 32, 64]:\n                # Also consider the number of warps? We can set num_warps to 4 or 8 depending on the block size.\n                configs.append(triton.Config({'BLOCK_M': BLOCK_M, 'BLOCK_N': BLOCK_N, 'BLOCK_K': BLOCK_K}, num_warps=4))\n                configs.append(triton.Config({'BLOCK_M': BLOCK_M, 'BLOCK_N': BLOCK_N, 'BLOCK_K': BLOCK_K}, num_warps=8))\n\n   But note: the grid is 3D and the block sizes must be powers of two? Not necessarily, but it is common.\n\n However, the problem says: \"Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions\"\n\n We are going to do that.\n\n But note: the autotune key must be the same as the kernel arguments. We can use the sizes (m, n, k) and batch_size? But the problem size is fixed? Actually, the get_inputs uses fixed sizes.\n\n We can set the key to ['m', 'n', 'k'].\n\n Alternatively, we can set the key to the product of m, n, k? But Triton autotune uses a tuple of the arguments.\n\n We are going to set:\n\n   key = ['m', 'n', 'k']\n\n   and then in the kernel, we pass m, n, k.\n\n We are going to write the kernel and the ModelNew.\n\n Let's code accordingly.\n\n Important: We must handle boundaries correctly.\n\n We are going to write the kernel with the following parameters:\n\n   A, B, C: pointers to the data\n   m, n, k: int (the dimensions)\n   stride_am, stride_ak: strides for A in the m and k dimensions? Actually, we can compute the strides inside the kernel? \n        But note: we are passing contiguous tensors. We can assume they are contiguous? The original uses contiguous.\n\n   However, we can pass the strides to be generic.\n\n   We are going to assume that the tensors are contiguous. Then:\n\n        stride_A_batch = m * k, stride_A_m = k, stride_A_k = 1\n        stride_B_batch = k * n, stride_B_k = n, stride_B_n = 1\n        stride_C_batch = m * n, stride_C_m = n, stride_C_n = 1\n\n   We can precompute these in the forward method and pass to the kernel.\n\n   Alternatively, we can compute them in the kernel by using the shape of the pointers? But it's easier to pass.\n\n   We are going to pass the strides.\n\n We note: the problem says \"NEVER change @triton.jit function signatures or parameter names\". But we are creating a new kernel. So we are free.\n\n However, the kernel signature is our own.\n\n We are going to define:\n\n   kernel_bmm(\n        A_ptr, B_ptr, C_ptr,\n        m, n, k,\n        stride_Ab, stride_Am, stride_Ak,\n        stride_Bb, stride_Bk, stride_Bn,\n        stride_Cb, stride_Cm, stride_Cn,\n        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n   )\n\n But note: we are using autotune for BLOCK_M, BLOCK_N, BLOCK_K, so they must be in the config.\n\n Alternatively, we can avoid passing the strides by making the tensors contiguous. We can enforce that in the forward.\n\n   A = A.contiguous()\n   B = B.contiguous()\n\n   Then we can compute the strides by:\n\n        stride_Ab = A.stride(0)\n        stride_Am = A.stride(1)\n        stride_Ak = A.stride(2)\n\n   and similarly for B and C.\n\n   But note: the output C is created by us and is contiguous.\n\n   We are going to do:\n\n        stride_Ab = A.stride(0)\n        stride_Am = A.stride(1)\n        stride_Ak = A.stride(2)\n        stride_Bb = B.stride(0)\n        stride_Bk = B.stride(1)\n        stride_Bn = B.stride(2)\n        stride_Cb = C.stride(0)\n        stride_Cm = C.stride(1)\n        stride_Cn = C.stride(2)\n\n   Then pass these to the kernel.\n\n However, note: if the tensors are contiguous, then:\n\n        stride_Ab = m * k, stride_Am = k, stride_Ak = 1\n        stride_Bb = k * n, stride_Bk = n, stride_Bn = 1\n        stride_Cb = m * n, stride_Cm = n, stride_Cn = 1\n\n   We can use these fixed strides? But the kernel would be more flexible.\n\n   We'll do the contiguous and then pass the strides.\n\n Let's code accordingly.\n\n We are going to write the entire code block.\n\n Note: We must not change the get_inputs and the batch_size, m, k, n. We are going to keep the same.\n\n We are going to replace the ModelNew.forward and add the kernel.\n\n We note: the original ModelNew.forward was:\n\n        return torch.bmm(A, B)\n\n We are replacing it with a Triton kernel.\n\n We are going to write:\n\n        # Check contiguous and data type? We assume FP32.\n        assert A.dtype == torch.float32\n        assert B.dtype == torch.float32\n        A = A.contiguous()\n        B = B.contiguous()\n        batch_size, m, k = A.shape\n        batch_size_B, k_B, n = B.shape\n        assert batch_size == batch_size_B and k == k_B\n        C = torch.empty((batch_size, m, n), device=A.device, dtype=A.dtype)\n\n        # Precompute strides\n        stride_Ab = A.stride(0)\n        stride_Am = A.stride(1)\n        stride_Ak = A.stride(2)\n        stride_Bb = B.stride(0)\n        stride_Bk = B.stride(1)\n        stride_Bn = B.stride(2)\n        stride_Cb = C.stride(0)\n        stride_Cm = C.stride(1)\n        stride_Cn = C.stride(2)\n\n        # Launch kernel\n        grid = (batch_size, triton.cdiv(m, BLOCK_M), triton.cdiv(n, BLOCK_N))\n        # But note: BLOCK_M, BLOCK_N are set by autotune. We are going to use a function that takes the current m, n, k.\n\n        # We are going to use the autotuned kernel\n        kernel_bmm[grid](A, B, C, m, n, k, \n                         stride_Ab, stride_Am, stride_Ak,\n                         stride_Bb, stride_Bk, stride_Bn,\n                         stride_Cb, stride_Cm, stride_Cn,\n                         BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K)\n\n        return C\n\n But note: the autotune will choose the best config for the given m, n, k. Since they are fixed, we can autotune for these values.\n\n However, the problem says: the dataset has 250 kernelbench files. So we are writing a kernel that must work for any m, n, k? \n But in this specific program, the get_inputs sets fixed m, k, n.\n\n We are going to write the kernel to be generic for any m, n, k.\n\n We are going to define the autotune with key=['m','n','k'].\n\n We are going to write the kernel.\n\n Let's code accordingly.\n\n We note: the kernel is long. We are going to write it step by step.\n\n We are going to put everything in the EVOLVE-BLOCK.\n\n IMPORTANT: We must not change anything outside the EVOLVE-BLOCK. The original code outside the block is locked.\n\n But note: the original code inside the block is the entire ModelNew and the get_inputs. We are replacing the ModelNew and keeping the same get_inputs.\n\n However, we are adding the Triton kernel. That is allowed because it is inside the block.\n\n We are going to write:\n\n   import triton\n   import triton.language as tl\n\n   @triton.autotune(...)\n   @triton.jit\n   def kernel_bmm(...): ...\n\n   class ModelNew(nn.Module):\n        def forward(...): ...   # our implementation\n\n   batch_size = 128\n   m = 128\n   k = 256\n   n = 512\n\n   get_inputs and get_init_inputs remain.\n\n We are going to do that.\n\n Let's write the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64}, num_warps=2),\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=2),\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 128}, num_warps=2),\n    ],\n    key=['m', 'n', 'k'],\n)\n@triton.jit\ndef batched_matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    m, n, k,\n    stride_ab, stride_am, stride_ak,\n    stride_bb, stride_bk, stride_bn,\n    stride_cb, stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_m = tl.program_id(1)\n    pid_n = tl.program_id(2)\n    \n    a_block_ptr = tl.make_block_ptr(\n        base=a_ptr + pid_batch * stride_ab,\n        shape=(m, k),\n        strides=(stride_am, stride_ak),\n        offsets=(pid_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_K),\n        order=(1, 0)\n    )\n    \n    b_block_ptr = tl.make_block_ptr(\n        base=b_ptr + pid_batch * stride_bb,\n        shape=(k, n),\n        strides=(stride_bk, stride_bn),\n        offsets=(0, pid_n * BLOCK_N),\n        block_shape=(BLOCK_K, BLOCK_N),\n        order=(1, 0)\n    )\n    \n    c_block_ptr = tl.make_block_ptr(\n        base=c_ptr + pid_batch * stride_cb,\n        shape=(m, n),\n        strides=(stride_cm, stride_cn),\n        offsets=(pid_m * BLOCK_M, pid_n * BLOCK_N),\n        block_shape=(BLOCK_M, BLOCK_N),\n        order=(1, 0)\n    )\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k_offset in range(0, k, BLOCK_K):\n        a = tl.load(a_block_ptr, boundary_check=(0,1))\n        b = tl.load(b_block_ptr, boundary_check=(0,1))\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        \n        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_K))\n        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_K, 0))\n    \n    tl.store(c_block_ptr, accumulator, boundary_check=(0,1))\n\nclass ModelNew(nn.Module):\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        A = A.contiguous()\n        B = B.contiguous()\n        batch_size, m, k = A.shape\n        _, k_, n = B.shape\n        assert k == k_, \"Incompatible dimensions\"\n        \n        C = torch.empty((batch_size, m, n), device=A.device, dtype=A.dtype)\n        \n        grid = (batch_size, triton.cdiv(m, 64), triton.cdiv(n, 64))\n        \n        batched_matmul_kernel[grid](\n            A, B, C,\n            m, n, k,\n            A.stride(0), A.stride(1), A.stride(2),\n            B.stride(0), B.stride(1), B.stride(2),\n            C.stride(0), C.stride(1), C.stride(2),\n        )\n        return C\n\nbatch_size = 128\nm = 128\nk = 256\nn = 512\n\ndef get_inputs():\n    A = torch.randn(batch_size, m, k, device='cuda')\n    B = torch.randn(batch_size, k, n, device='cuda')\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
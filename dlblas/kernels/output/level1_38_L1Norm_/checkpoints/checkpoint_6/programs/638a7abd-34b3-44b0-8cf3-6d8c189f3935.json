{"id": "638a7abd-34b3-44b0-8cf3-6d8c189f3935", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef compute_row_sums(\n    x_ptr,\n    row_sums_ptr,\n    n_cols,\n    stride_x0, stride_x1,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # row index\n    pid1 = tl.program_id(1)  # segment index\n    \n    col_offsets = pid1 * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    base_ptr = x_ptr + pid0 * stride_x0 + col_offsets * stride_x1\n    x_vals = tl.load(base_ptr, mask=mask, other=0.0)\n    abs_x = tl.abs(x_vals)\n    segment_sum = tl.sum(abs_x, axis=0)\n    tl.atomic_add(row_sums_ptr + pid0, segment_sum)\n\n@triton.jit\ndef normalize_row(\n    x_ptr,\n    row_sums_ptr,\n    output_ptr,\n    n_cols,\n    stride_x0, stride_x1,\n    stride_out0, stride_out1,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # row index\n    pid1 = tl.program_id(1)  # segment index\n    \n    col_offsets = pid1 * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    base_ptr = x_ptr + pid0 * stride_x0 + col_offsets * stride_x1\n    x_vals = tl.load(base_ptr, mask=mask, other=0.0)\n    \n    row_sum = tl.load(row_sums_ptr + pid0)\n    normalized = x_vals / row_sum\n    \n    out_base_ptr = output_ptr + pid0 * stride_out0 + col_offsets * stride_out1\n    tl.store(out_base_ptr, normalized, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.BLOCK_SIZE = 1024\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() != 2:\n            return x / torch.sum(torch.abs(x), dim=1, keepdim=True)\n        \n        x = x.contiguous()\n        n_rows, n_cols = x.shape\n        row_sums = torch.zeros(n_rows, device=x.device, dtype=torch.float32)\n        output = torch.empty_like(x)\n        \n        grid = (n_rows, triton.cdiv(n_cols, self.BLOCK_SIZE))\n        compute_row_sums[grid](\n            x, row_sums, n_cols, \n            x.stride(0), x.stride(1),\n            BLOCK_SIZE=self.BLOCK_SIZE\n        )\n        normalize_row[grid](\n            x, row_sums, output, n_cols,\n            x.stride(0), x.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE=self.BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "0b295cb6-de1b-483f-96a4-b04e0f867dbc", "generation": 1, "timestamp": 1754639702.9356816, "iteration_found": 0, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.352, "runtime_ms": 0.0854, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0301, "speed_up": 0.352, "custom_timing": 0.0854}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.029, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0301, "speed_up": 1.038, "custom_timing": 0.029}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0290\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0301, 'speed_up': 1.038, 'custom_timing': 0.029}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0290, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0301, 'speed_up': 1.038, 'custom_timing': 0.029}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs L1 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L1 normalization layer.\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L1 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (..., dim, ...).\n\n        Returns:\n            torch.Tensor: Output tensor with L1 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.sum(torch.abs(x), dim=1, keepdim=True)\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0290), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0301, 'speed_up': 1.038, 'custom_timing': 0.029})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs L1 normalization.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the L1 normalization layer.\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies L1 normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (..., dim, ...).\n\n        Returns:\n            torch.Tensor: Output tensor with L1 normalization applied, same shape as input.\n        \"\"\"\n        return x / torch.sum(torch.abs(x), dim=1, keepdim=True)\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that performs L1 normalization: \n    output = x / torch.sum(torch.abs(x), dim=1, keepdim=True)\n\nThe current implementation uses PyTorch operations. We are to rewrite it using Triton to get a speedup.\n\nPlan:\n1. We note that the operation is: for each row, compute the sum of absolute values of the row, then divide each element in the row by that sum.\n2. We can write a Triton kernel that processes multiple rows in parallel. Each program will handle one row (or a group of rows) and compute the row-wise sum of absolute values and then do the division.\n\nConstraints:\n- We cannot change the function signature of the forward method or the model class.\n- We must maintain the same input and output.\n\nSteps for Triton kernel:\n- We will create a kernel that takes the input tensor and an output tensor.\n- The kernel will have a 1D grid where the number of blocks equals the number of rows (batch_size).\n- Each block will process one row. The row length is `dim` (which is 16384 in the example).\n\nKernel design:\n- We need to compute the sum of absolute values for the entire row. We can use a reduction (like `tl.sum` over the absolute values).\n- Then we divide each element by the sum and store the result.\n\nImportant: We must handle boundaries because the row might not be a multiple of the block size? Actually, the row length is fixed (16384). We can set the block size to be the next power of two? Or we can use a fixed block size and handle masking.\n\nLet BLOCK_SIZE be the number of elements per row we process in one block. But note: the entire row is processed by one block? Actually, we are processing one row per program. So we can set the block size to the entire row? But that might be too big (16384). Alternatively, we can break the row into multiple blocks? However, the reduction must be done across the entire row.\n\nWe can use a two-stage approach? But note: the problem is that the reduction must be done for the entire row. We can do a single block per row if the row length is within the block size limit (which is 2^13 = 8192? but 16384 is 2^14, which is too big). Therefore, we have to split the row.\n\nAlternatively, we can use multiple blocks per row? But then we have to do a reduction across blocks. That would require multiple kernels or atomic operations? We want to avoid that.\n\nAnother idea: use a two-step kernel? First kernel to compute the per-row sums (with multiple blocks per row) and then a second kernel to do the division? But that would be two passes.\n\nAlternatively, we can use one kernel that first computes the sum for the row (using a reduction in one block) and then does the division? But if the row is too long, we might not be able to fit it in one block.\n\nGiven that the row length is 16384, we can break the row into chunks and do a reduction within the block and then use shared memory to do a tree reduction? But note: Triton does not have shared memory? Actually, it does, but we can use the `static` keyword to allocate shared memory.\n\nHowever, we are constrained by the maximum block size. The maximum number of threads per block is 1024 for current GPUs? Actually, Triton can have larger block sizes? But the hardware limit is 1024. So we cannot have 16384 in one block.\n\nTherefore, we must break the row into multiple blocks. We can do:\n\n- Step 1: Kernel to compute partial sums for each row segment. We will have a 2D grid: [num_rows, num_segments]. Then we write the partial sums to a temporary buffer.\n- Step 2: Kernel to reduce the partial sums per row to get the total sum per row.\n- Step 3: Kernel to divide each element by the row sum.\n\nBut that would be three kernels. We want to minimize the number of kernels.\n\nAlternatively, we can do:\n\n- Step 1: Compute the row sums in one kernel that uses multiple blocks per row and then does a global reduction? But we don't have atomic operations for global memory? Actually, we can use atomics? But note: the row sums are independent per row, so we can have one output per row. However, multiple blocks per row would write to the same row sum? We would need an atomic add. Triton supports atomic operations? Yes, but we have to be cautious.\n\nBut note: the reduction for one row is independent of others. We can have:\n\n  Kernel 1: \n    grid = (batch_size, num_blocks_per_row)\n    Each block computes the sum of a segment of the row and then atomically adds to a global array of row sums.\n\n  Kernel 2: \n    grid = (batch_size,)\n    Each block divides the entire row by the row sum.\n\nBut that is two kernels.\n\nAlternatively, we can compute the row sums in one kernel and then do the division in the same kernel? But note: the row sum must be known for the entire row before we can do the division. We cannot do it in the same kernel because the blocks for the same row are independent and we would need to synchronize across blocks? That is not possible.\n\nTherefore, we break into two kernels:\n\n  Kernel1: compute_row_sums\n    Input: x (batch_size, dim)\n    Output: row_sums (batch_size)\n\n    How? We break each row into segments. Each block handles one segment. We compute the sum of absolute values for that segment and then atomically add to the row_sums array at the row index.\n\n  Kernel2: normalize_row\n    Input: x, row_sums\n    Output: y (same as x but normalized)\n\n    Each block handles one row? But then we have to read the entire row? We break the row into segments? Actually, we can have multiple blocks per row? But then we have to use the same row_sum for the entire row.\n\nHowever, note: the row_sum is a scalar per row. We can broadcast it to all blocks that process the same row.\n\nBut we want to do it in one kernel? Actually, we can do the normalization in the same kernel as the reduction? No, because we need the entire row sum to do the normalization.\n\nAlternatively, we can do:\n\n  Use a kernel that first computes the row sum for the entire row (using multiple blocks per row and then a reduction in shared memory? but shared memory is per block) and then does the normalization? That would require the entire row to be processed by one block? But we cannot because of the block size limit.\n\nSo we stick to two kernels.\n\nBut note: the current PyTorch implementation is one line and might be optimized. We must be cautious that two kernels might be slower.\n\nAlternatively, we can use one kernel per row? That would launch batch_size blocks, and each block would process the entire row? But the row is 16384 elements. We can set the block size to 1024? Then each block would process 1024 elements? But we need to process 16384, so we would have 16 blocks per row? Then we would have to do a two-pass reduction? That is acceptable.\n\nWe can design a kernel that does:\n\n  Phase 1: Each block in a row computes a partial sum of absolute values for its segment.\n  Phase 2: We then use a single block per row to combine the partial sums and then do the normalization? But that would require two kernels again.\n\nAlternatively, we can use a two-stage kernel:\n\n  Stage 1: Compute partial sums and store in a temporary array (of size [batch_size, num_blocks]).\n  Stage 2: Launch a kernel with batch_size blocks, each block reads the partial sums for one row, reduces them, and then reads the entire row again to do the normalization? That would be two kernels and we have to read the entire row twice.\n\nGiven the complexity and the fact that the row length is fixed and known (16384), we can precompute the number of blocks per row and then use a two-kernel approach.\n\nHowever, the problem requires that we rewrite the entire operation in Triton and we are allowed to change the internal implementation.\n\nBut note: the current model is very simple and the PyTorch implementation might already be efficient because it uses optimized operations. We must aim for a significant speedup.\n\nConsidering the constraints, we try to write a Triton kernel that does the entire operation in one block per row? But 16384 is too big. We cannot.\n\nAnother idea: use a kernel that processes the row in multiple blocks and uses shared memory? But Triton does not have explicit shared memory? Actually, we can use `tl.static` for shared memory within a block. But we cannot synchronize across blocks.\n\nTherefore, we decide to break the operation into two kernels:\n\n1. Compute the row sums (with one kernel that uses multiple blocks per row and atomics to accumulate the row sums).\n2. Normalize the rows by dividing by the row sum (with one kernel that uses multiple blocks per row, each block reads the row sum and then writes a segment of the normalized row).\n\nWe must be cautious: the atomic operations might be slow? But the row sums are independent and we are doing batch_size * num_blocks_per_row atomic operations? That might be acceptable.\n\nBut note: the atomic operations are to a global array of size batch_size. We are doing one atomic per block? So that is batch_size * (ceil(dim / BLOCK_SIZE)) atomics.\n\nAlternatively, we can avoid atomics by having a temporary array for partial sums and then a second kernel to reduce the partial sums per row? Then we do:\n\n  Kernel1: compute partial sums (without atomics) -> output a matrix of partial sums [batch_size, num_blocks]\n  Kernel2: reduce the partial sums per row to get the row sums [batch_size] (using one block per row, which can do a reduction of num_blocks elements? which is small because num_blocks = ceil(dim / BLOCK_SIZE) = ceil(16384 / BLOCK_SIZE). For BLOCK_SIZE=1024, that would be 16 elements per row? So we can do a tree reduction in one block per row.\n\nThen Kernel3: normalize the rows by reading the row sums and then dividing the row.\n\nThat would be three kernels? We want to minimize.\n\nWe can combine Kernel2 and Kernel3? But Kernel3 requires reading the entire row again? So we would have to read the entire row anyway.\n\nAlternatively, we can do:\n\n  Kernel1: compute partial sums -> [batch_size, num_blocks] (without atomics)\n  Kernel2: for each row, reduce the partial sums and then normalize the entire row? But then we have to read the entire row? That would be two kernels but with two passes over the data.\n\nBut note: the row is already in memory? So reading it again might be acceptable.\n\nHowever, the current PyTorch implementation does one pass for the absolute and sum, and then one pass for the division? Actually, it might be two passes? We are replacing two passes with three passes? That might be slower.\n\nWe need to be more efficient.\n\nIdea: can we compute the row sums in one pass without temporary arrays? We can use atomics? Let me check if Triton supports atomic add for float32? Yes.\n\nSo we can do:\n\n  Kernel1: compute_row_sums (with atomics)\n    grid = (batch_size, num_blocks_per_row)   [where num_blocks_per_row = ceil(dim / BLOCK_SIZE)]\n    Each block computes the sum of absolute values in its segment and then atomically adds to the row_sums array at the row index.\n\n  Kernel2: normalize\n    grid = (batch_size, num_blocks_per_row)\n    Each block loads the row_sum for the row, then loads its segment of the row, divides by row_sum, and stores.\n\nBut note: the row_sum must be the same for all blocks of the same row. We can read it from the row_sums array (which is of size batch_size). We can broadcast it to all blocks? But we have to load it for each block? That is acceptable because we are loading a single float per block.\n\nHowever, we must note that the row_sum computed in Kernel1 must be available for Kernel2.\n\nThis approach: two kernels, two passes over the data.\n\nBut the PyTorch version also does two passes? One for abs+sum and one for division? Actually, the current implementation:\n\n  torch.sum(torch.abs(x), dim=1, keepdim=True)   -> one pass for abs and then sum reduction (which is a pass? but the reduction is done in one pass? but the entire tensor is read once for abs and then once for the reduction? Actually, it's fused? Not necessarily. But at least two passes: one for abs and then one for the reduction? Then the division is a third pass? Or the division is done in-place? Actually, the division is a separate element-wise operation.\n\nSo the PyTorch version does:\n\n  Pass1: abs\n  Pass2: reduction (which might be a pass over the entire tensor? but the reduction dimension is dim=1, so it's a row-wise reduction? which might be efficient? but still, it's two reads: one for abs and one for reduction? and then the division is a third pass? Actually, the division is:\n\n      x / ... \n\n  which is a pass over x again.\n\nSo total: two writes? (one for abs and one for the reduction intermediate?) and then the division? Actually, no: the abs is temporary and then the reduction is done on the temporary? Then the division uses the original x and the reduction result.\n\nSo the memory access:\n\n  Read x for abs -> write abs (if not fused) -> read abs for reduction -> then read x again for division and write the output.\n\nThat is: read x twice and write an intermediate abs? That is expensive.\n\nIn our Triton version:\n\n  Kernel1: read x (for the segments) and then write row_sums (a vector of batch_size).\n  Kernel2: read x and row_sums, and write the output.\n\nSo we read x twice and write row_sums (which is small) and the output.\n\nWe avoid the intermediate abs tensor? Yes.\n\nSo we have:\n\n  Total reads: 2 * (batch_size * dim)   [for x] and 2 * (batch_size) [for row_sums? but negligible]\n  Total writes: batch_size (row_sums) and batch_size * dim (output)\n\nThe PyTorch version:\n\n  Reads: \n      read x for abs: batch_size*dim\n      write abs: batch_size*dim\n      read abs for reduction: batch_size*dim\n      read x for division: batch_size*dim\n      write output: batch_size*dim\n\n  So: 3 reads and 2 writes? (and the intermediate abs is written and then read)\n\nTherefore, our Triton version should be more efficient? Especially because we avoid the intermediate abs tensor.\n\nBut note: in the Triton kernel1, we don't store the abs? We compute the absolute value on the fly and then reduce immediately? So we do:\n\n  abs_x = tl.abs(x_segment)\n  segment_sum = tl.sum(abs_x, axis=0)\n\nThen we atomically add segment_sum to the row_sum.\n\nSo we don't write the abs.\n\nSo we are saving memory.\n\nNow, let's write the two kernels.\n\nBut note: we are constrained by the block size. We set BLOCK_SIZE for the segment? Let BLOCK_SIZE be 1024? Then for a row of 16384, we have 16 blocks per row.\n\nKernel1: compute_row_sums\n\n  Input: \n      x_ptr: pointer to the input tensor (batch_size, dim)\n      row_sums_ptr: pointer to the output row sums (batch_size), initialized to zero.\n\n  Grid: \n      batch_size (for rows) and ceil(dim / BLOCK_SIZE) for the segments? Actually, we want to cover the entire row.\n\n      So grid = (batch_size, ceil(dim / BLOCK_SIZE))\n\n  Each program: \n      row = pid0\n      segment_index = pid1\n      offset = row * dim + segment_index * BLOCK_SIZE\n      Then we load a block of BLOCK_SIZE elements? But we must mask the last block.\n\n      We compute the sum of absolute values for the segment? Then do an atomic add to row_sums_ptr[row].\n\nKernel2: normalize\n\n  Input: \n      x_ptr: same as above\n      row_sums_ptr: the row sums we computed\n      output_ptr: output tensor\n\n  Grid: same as Kernel1? (batch_size, ceil(dim / BLOCK_SIZE))\n\n  Each program:\n      row = pid0\n      segment_index = pid1\n      offset = row * dim + segment_index * BLOCK_SIZE\n\n      Load the row_sum = row_sums_ptr[row]\n      Then load the segment of x: x_segment = tl.load(x_ptr + offset, mask=...)\n      Then compute: normalized_segment = x_segment / row_sum\n      Store to output_ptr + offset.\n\nBut note: what if row_sum is zero? We must avoid division by zero? The PyTorch version does not clamp? Actually, the original PyTorch code does not clamp the sum? But the sum of absolute values is non-negative. It can be zero? Then we would get NaN? The original code: \n\n  return x / torch.sum(torch.abs(x), dim=1, keepdim=True)\n\nIf the sum is zero, then we get division by zero.\n\nWe must do the same? Or we should clamp? The original code does not clamp. So we do the same? But we can add an epsilon? Actually, the original does not. So we must not add an epsilon? But note: the input is random and the probability of zero is low? However, for correctness we must match.\n\nBut the problem says: within float32 tolerance? If the sum is zero, then we get NaN? So we must not change.\n\nWe can do:\n\n  row_sum = tl.load(row_sums_ptr + row)\n  normalized = x_segment / row_sum\n\nBut if row_sum is zero, then we get NaN? Same as PyTorch.\n\nAlternatively, we can avoid the division by zero by masking? But we don't have to because the original doesn't.\n\nNow, we must write the Triton kernels and then the model.\n\nBut note: the model class must be the same? We are going to replace the forward method? Actually, we are going to replace the entire ModelNew class? But the problem says: \"Provide the complete new program code.\"\n\nWe are allowed to change the internal implementation? Yes.\n\nWe will:\n\n  - Write two Triton kernels (compute_row_sums and normalize_row)\n  - In the forward method of ModelNew, we will:\n        row_sums = torch.zeros(x.shape[0], device=x.device, dtype=torch.float32)\n        output = torch.empty_like(x)\n        # Launch kernel1\n        grid1 = (x.shape[0], triton.cdiv(x.shape[1], BLOCK_SIZE))\n        compute_row_sums[grid1](x, row_sums, ...)\n        # Launch kernel2\n        grid2 = (x.shape[0], triton.cdiv(x.shape[1], BLOCK_SIZE))\n        normalize_row[grid2](x, row_sums, output, ...)\n\nBut note: we must handle the boundary in the last block? Yes, by masking.\n\nLet's write the kernels.\n\nWe set BLOCK_SIZE as a tunable parameter? We can set it to 1024? But 16384 is divisible by 1024? 16384 / 1024 = 16? So no masking? But we want to be generic? We can use the masking.\n\nWe pass the dimension (n_cols) to the kernel.\n\nKernel1: compute_row_sums\n\n  @triton.jit\n  def compute_row_sums(\n      x_ptr,\n      row_sums_ptr,\n      n_cols,\n      BLOCK_SIZE: tl.constexpr,\n      stride_x0, stride_x1,   # we can get strides? but we are using contiguous? We assume contiguous? The input might not be? The original model uses contiguous? We can call .contiguous() in the forward method? Or we can handle strides.\n\n  But the problem: the input tensor might be non-contiguous? The example input is created by `torch.randn(batch_size, dim)`, which is contiguous? But to be safe, we can make it contiguous? Or we can use the strides.\n\n  We are allowed to change the input? We can make it contiguous in the forward method? That is acceptable.\n\n  We will assume contiguous for simplicity? But we can also handle strides.\n\n  Alternatively, we can require the input to be contiguous? The original model does not? But we can call `x = x.contiguous()`? Then we know the strides.\n\n  We decide to make the input contiguous in the forward method? That might add a copy? But if it's already contiguous, it's free.\n\n  So we do:\n\n      x = x.contiguous()\n\n  Then the stride for the row is x.stride(0) and for the column is x.stride(1). But for a contiguous tensor, stride(0)=dim, stride(1)=1.\n\n  We can pass these strides to the kernel? But we want to be generic? We can compute the offsets with:\n\n      base = row * stride_x0 + col_offset * stride_x1\n\n  But if we assume contiguous, we can set:\n\n      base = row * n_cols + col_offset   [if we use row-major]\n\n  However, we are going to use the Triton language to compute the offsets? We can pass the strides.\n\n  Actually, the example has batch_size=16, dim=16384, so it's contiguous.\n\n  We'll pass the strides.\n\n  Steps in kernel1:\n\n      pid0 = tl.program_id(0)   # row index\n      pid1 = tl.program_id(1)   # segment index\n\n      # Compute the offsets for the segment\n      col_offsets = pid1 * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n      # The actual column indices we are going to load\n      mask = col_offsets < n_cols\n      base_ptr = x_ptr + pid0 * stride_x0 + col_offsets * stride_x1\n      x_vals = tl.load(base_ptr, mask=mask, other=0.0)\n      abs_x = tl.abs(x_vals)\n      segment_sum = tl.sum(abs_x, axis=0)   # sum over the columns in the segment\n\n      # Now we want to atomically add segment_sum to row_sums_ptr[pid0]\n      tl.atomic_add(row_sums_ptr + pid0, segment_sum)\n\n  But note: the atomic_add is for float32? Yes, Triton supports atomic_add for float32.\n\nKernel2: normalize_row\n\n  Similarly:\n\n      pid0 = tl.program_id(0)\n      pid1 = tl.program_id(1)\n\n      col_offsets = pid1 * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n      mask = col_offsets < n_cols\n      base_ptr = x_ptr + pid0 * stride_x0 + col_offsets * stride_x1\n      x_vals = tl.load(base_ptr, mask=mask, other=0.0)\n\n      # Load the row sum for this row\n      row_sum = tl.load(row_sums_ptr + pid0)   # scalar\n\n      # Compute normalized values\n      # If row_sum is zero, we get NaN? same as PyTorch.\n      normalized = x_vals / row_sum\n\n      # Store to output\n      output_ptr = ...   # we pass the output tensor\n      output_base = output_ptr + pid0 * stride_out0 + col_offsets * stride_out1\n      tl.store(output_base, normalized, mask=mask)\n\nNow, we must set the grid for both kernels:\n\n  grid1 = (batch_size, triton.cdiv(n_cols, BLOCK_SIZE))\n  grid2 = same.\n\nWe choose BLOCK_SIZE? We can set it to 1024? Then for n_cols=16384, we have 16 blocks per row.\n\nWe can also autotune? But for now, we set it to 1024.\n\nBut note: the maximum number of threads per block? We are using BLOCK_SIZE=1024? And we are launching one thread per element? That is acceptable? Actually, the block size in Triton is the number of threads? Yes.\n\nWe must set the number of warps? 1024 / 32 = 32 warps? That is the maximum? Actually, the maximum number of warps per block is 32? Yes, because 32*32=1024. So it's acceptable.\n\nNow, we write the model.\n\nWe note: the original model returns a tensor. We are returning the normalized tensor.\n\nWe must also handle the batch dimension? The input x can be (batch_size, dim). But the original model says: shape (..., dim, ...). But the example input is (batch_size, dim). We assume the input is 2D? Actually, the original model uses `dim=1` in the sum? So it sums over the first dimension? But the input might have more dimensions? \n\nWait: the original model:\n\n      return x / torch.sum(torch.abs(x), dim=1, keepdim=True)\n\nThis sums over dimension 1 (the column dimension) and then divides. The keepdim=True ensures the result has the same number of dimensions.\n\nBut the input might have more than 2 dimensions? For example, (batch_size, height, width)? Then we would sum over dim=1 (height) and then divide? But that is not the same as per-row normalization? Actually, the problem says: \"Input tensor of shape (..., dim, ...)\".\n\nBut the example input is 2D: (batch_size, dim). We are going to assume 2D? But the model must be generic? \n\nWe are constrained by the Triton kernel? Our kernel assumes 2D? We can only handle 2D? \n\nBut the problem says: \"Rewrite the program to improve its performance on the specified metrics.\" and we are allowed to change the internal implementation? But we must maintain the same inputs and outputs.\n\nWe can reshape the input to 2D? We can do:\n\n      original_shape = x.shape\n      x = x.view(-1, original_shape[-1])   # or the dimension we want? But the original model uses dim=1.\n\nWait, the original model uses dim=1? So it sums over the second dimension? But the input could be of any shape? For example, (A, B, C) and then we sum over dim=1 -> (A, 1, C) and then we divide? \n\nBut our Triton kernel is designed for 2D. We can reinterpret the input as 2D? We can do:\n\n      n_rows = 1\n      for d in x.shape[:-1]:\n          n_rows *= d\n      n_cols = x.shape[-1]\n\n      Then we reshape to (n_rows, n_cols)\n\n  Then we do the normalization on the 2D tensor? Then reshape back?\n\nBut the original model does not change the shape? So we can do:\n\n      original_shape = x.shape\n      x_2d = x.view(-1, original_shape[-1])\n      # Then do the Triton kernels on x_2d\n      output_2d = ... \n      output = output_2d.view(original_shape)\n\nBut the original model does not specify the exact dimensions? It says \"shape (..., dim, ...)\". So we assume that the last dimension is the one we are normalizing? Actually, no: the original model sums over dim=1? So if the input is (A, B, C), then dim=1 is the second dimension? Then we are normalizing along the second dimension? \n\nBut the example input is (batch_size, dim) -> then dim=1 is the second dimension? which is the entire row? \n\nBut if the input is (A, B, C), then the normalization would be:\n\n      output = x / sum_abs (where sum_abs is of shape (A, 1, C))\n\nBut if we reshape to (A*B, C) and then normalize per row? that would be:\n\n      output_2d = x_2d / sum_abs_2d   # where sum_abs_2d is of shape (A*B, 1)\n\nThen we reshape to (A, B, C) -> which is the same as the original? \n\nBut note: the original model does:\n\n      torch.sum(torch.abs(x), dim=1, keepdim=True)   # for (A, B, C) -> dim=1 -> the B dimension? then the result is (A, 1, C)\n\nBut if we reshape to (A*B, C) and then do the normalization per row? Then the sum is over the C dimension? That is different.\n\nWe see the problem: the original model sums over the second dimension (index 1) regardless of the shape. But we are changing it to sum over the last dimension? That is not the same.\n\nTherefore, we must not change the dimension? We must sum over the same dimension as the original.\n\nHow to handle arbitrary dimensions? We can flatten all the dimensions except the one we are summing over? \n\nBut the original model sums over dim=1. So we can:\n\n      # Let the input be of shape (d0, d1, d2, ..., dn)\n      # We want to sum over dimension 1? Then we can:\n\n      # Step 1: transpose the tensor so that dimension 1 becomes the last dimension? Then we can do a 2D kernel? But that would be expensive.\n\nAlternatively, we can use the strides to handle non-contiguous? But it becomes complex.\n\nGiven the problem, the example input is 2D: (batch_size, dim). And the problem says the input is of shape (..., dim, ...). But the model uses dim=1? So if the input is 3D: (batch_size, seq_len, dim), then we would be summing over seq_len? which is not the last dimension? \n\nBut the example input is 2D. And the problem says the dataset has 250 kernelbench files? We are not told the exact input shapes? But the example in the code: batch_size=16, dim=16384.\n\nSo we assume the input is 2D? \n\nBut to be safe, we can note that the original model only handles 2D? Actually, no: the original model uses `dim=1` and `keepdim=True` so it works for any shape. We want to maintain that.\n\nWe can do:\n\n      # Reshape the input: combine all dimensions except the one we are summing over (dim=1) and the dimension we are summing over? \n      # Actually, we can do:\n\n      # Let the input be x: (N, M, ...) where N = the size of the first dimension, M = the size of the second dimension, and ... is the rest?\n      # We want to sum over M? Then we can:\n\n      #   x_reshaped = x.reshape(N, M, -1)   # then we have (N, M, K) where K = product of the rest)\n      #   Then we want to normalize over M? But the normalization factor should be of shape (N, 1, K) -> then we divide x_reshaped by that factor? and then reshape back.\n\n      # But note: the original model does:\n\n      #   s = torch.sum(torch.abs(x), dim=1, keepdim=True)   # s has the same shape as x except that dimension 1 is 1? \n      #   output = x / s\n\n      # We can do:\n\n      #   original_shape = x.shape\n      #   # We want to collapse all dimensions after the second? \n      #   # Actually, we can combine the first dimension and the rest? Then we get a 2D tensor (N, M * ...) ? But that is not what we want.\n\n      # Alternatively, we can do:\n\n      #   x_flat = x.view(x.shape[0], x.shape[1], -1)   # (N, M, K)\n      #   Then we can normalize each (N, M, k) for k in range(K) independently? But we want to sum over M for each k? \n\n      #   Actually, the original model sums over M? So for each n and k, we want to compute the sum over M? \n\n      #   We can do: \n      #       s = x_flat.abs().sum(dim=1, keepdim=True)   # (N, 1, K)\n      #       output_flat = x_flat / s\n      #   Then output = output_flat.view(original_shape)\n\n      #   But this is the same as the original? \n\n      #   But we want to do it in Triton? Then we can treat the tensor as 3D? But our Triton kernel is for 2D.\n\n      #   We can reinterpret the 3D tensor as 2D? with rows = N * K, cols = M? Then we do per-row normalization? \n\n      #   But note: the row would be of length M? and we want to compute the sum over M? and then divide the entire row by that sum? \n\n      #   Then we would have:\n\n      #       n_rows = N * K\n      #       n_cols = M\n\n      #   Then we do the Triton kernels on a 2D tensor of (n_rows, n_cols). Then we reshape back to (N, M, K) and then to original_shape.\n\n      #   This is acceptable.\n\n  So the plan for the forward method:\n\n      original_shape = x.shape\n      # We want to preserve the first dimension and the second dimension? But we are summing over the second dimension? \n      # Actually, the model sums over dim=1? So we collapse the dimensions before and after the summation dimension? \n\n      # Step 1: transpose the tensor so that the summation dimension is the last? Then we can do a 2D kernel? But that is expensive.\n\n      # Alternatively, we can collapse the leading dimensions and the trailing dimensions separately? \n\n      #   Let N = product of dimensions before dim=1\n      #   Let M = dimension at dim=1\n      #   Let K = product of dimensions after dim=1\n\n      #   Then we reshape to (N, M, K) -> then we want to combine N and K? to get a 2D tensor of (N*K, M)\n\n      #   Then we do the normalization per row (each row is of length M) -> then we reshape back to (N, M, K) and then to original_shape.\n\n  So:\n\n      # We assume the input has at least two dimensions?\n      if x.dim() < 2:\n          # cannot do, return x? but the original model would sum over dim=1 -> error? \n          # we do the same as the original? \n          return x / torch.sum(torch.abs(x), dim=1, keepdim=True)   # but it will fail? \n\n      # Let N = x.shape[0]\n      # Let M = x.shape[1]\n      # Let K = torch.prod(torch.tensor(x.shape[2:])).item() if x.dim()>2 else 1\n      # But we can do:\n\n      n_batch = x.shape[0]\n      n_rows = x.shape[1]\n      # and the rest? \n      rest_dims = x.shape[2:]\n      num_rest = 1\n      for d in rest_dims:\n          num_rest *= d\n\n      total_rows = n_batch * num_rest\n      x_2d = x.reshape(total_rows, n_rows)   # (total_rows, n_rows) -> but note: we are summing over the second dimension of the original? which is now the first dimension of the 2D tensor? \n\n      Actually, we want to sum over the second dimension of the original? which is at index 1? In the reshaped tensor, the dimension we want to sum over is now the columns? So we have:\n\n          x_2d: (total_rows, n_rows)   -> we want to sum over the columns? so each row in the 2D tensor is a row we want to normalize? \n\n      But wait: in the original, for a 3D tensor (A, B, C), we want to sum over the B dimension? Then we would reshape to (A, B, C) -> (A*C, B) and then we want to compute the sum for each row of length B? and then divide that row by the sum? \n\n      Then the normalization is per row in the 2D tensor? which corresponds to a fixed A and C? and then we sum over B? \n\n      That is exactly what we want.\n\n      Then we do:\n\n          row_sums = torch.zeros(total_rows, device=x.device, dtype=torch.float32)\n          output_2d = torch.empty_like(x_2d)   # same shape as x_2d\n\n          BLOCK_SIZE = 1024\n          grid1 = (total_rows, triton.cdiv(n_rows, BLOCK_SIZE))\n          compute_row_sums[grid1](x_2d, ...)   # with n_cols = n_rows\n\n          grid2 = (total_rows, triton.cdiv(n_rows, BLOCK_SIZE))\n          normalize_row[grid2](x_2d, row_sums, output_2d, ...)\n\n          Then output = output_2d.view(original_shape)\n\n  But note: the original model used keepdim=True? Our normalization factor for each row is a scalar? then we divide the entire row by that scalar? which is the same as having a normalization factor of shape (total_rows, 1) -> which is the same as keepdim=True in the 2D tensor? and when we reshape back, it will broadcast correctly.\n\n  Therefore, we do the reshape.\n\n  But what if the input is 1D? Then we cannot reshape to (1, n)? But the original model would sum over dim=1? which would fail? because there is no dim=1? So we assume at least 2D? \n\n  We'll do:\n\n      if x.dim() == 1:\n          # then we cannot do dim=1? we return x? but the original would error? \n          # We do the same: we try to sum over dim=1 -> error? \n          # Let's avoid and use the original method for this case? \n          return x / torch.sum(torch.abs(x), dim=1, keepdim=True)\n\n      else:\n          # do the above\n\n  But to be safe, we can handle only >=2D.\n\nGiven the complexity and the fact that the example is 2D, and the dataset might be 2D, we decide to assume 2D? But the problem says \"shape (..., dim, ...)\"? so we must handle arbitrary dimensions.\n\nWe'll implement the reshaping.\n\nNow, we write the complete code.\n\nWe must also note: the Triton kernels require the input to be contiguous? We will make x_2d contiguous? \n\n  x_2d = x.reshape(total_rows, n_rows).contiguous()\n\n  Then output_2d is contiguous? then we reshape back? \n\n  But note: the output_2d is contiguous? but the original output might not be? We are returning a contiguous tensor? The original model does not require contiguous? \n\n  But the original model uses the same memory layout? We are changing it? \n\n  We are doing:\n\n      output = output_2d.view(original_shape)\n\n  This will have the same strides as the original if the original was contiguous? but if not, we are making it contiguous? \n\n  The original model does not preserve strides? Actually, the original model does element-wise operations and reduction? it might not preserve strides? \n\n  We are allowed to change the internal implementation? So we can change the memory layout? But the problem says: \"maintain the same inputs and outputs\". The output must be the same numerically, but the memory layout might be different? \n\n  The problem says: \"functionally equivalent\". So numerically the same? and same shape? but strides might be different? \n\n  But the original model uses `keepdim=True` and then divides? so the output has the same shape and the same strides as the input? \n\n  We must preserve the strides? \n\n  This is very complex? \n\n  Given the time, we decide to assume the input is 2D? and hope that the dataset is 2D? \n\n  We note that the example input is 2D: (16, 16384). \n\n  Therefore, we write the model for 2D only? \n\n  But the model class in the original code is for 2D? because the example input is 2D? \n\n  We'll do:\n\n      # Only handle 2D\n      assert x.dim() == 2, \"Input must be 2D\"\n\n  Then we do the Triton kernels.\n\nBut the problem says: \"Input tensor of shape (..., dim, ...)\" -> so it might not be 2D? \n\nWe have to handle it? \n\nGiven the complexity, we decide to handle only 2D inputs? and for non-2D, fall back to the original PyTorch? \n\nBut the problem says: rewrite to Triton? \n\nAlternatively, we can use the reshaping approach and hope that it works for any input with at least 2 dimensions? and then we do:\n\n      if x.dim() < 2:\n          # use PyTorch\n          return x / torch.sum(torch.abs(x), dim=1, keepdim=True)\n\n      else:\n          # reshape to 2D\n\n  This is acceptable.\n\nWe'll do that.\n\nNow, let's write the code.\n\nNote: the kernel launch grid might be large if total_rows is large? but that's okay.\n\nWe'll set BLOCK_SIZE=1024.\n\nWe must write the kernels with proper strides.\n\nWe assume the 2D tensor is row-major contiguous? because we did `.contiguous()`.\n\nSo for x_2d: stride0 = n_rows, stride1 = 1.\n\nFor row_sums: it is a 1D tensor, so stride0=1.\n\nFor output_2d: we also make it contiguous? we create it with `torch.empty_like(x_2d)` and then we hope it's contiguous? it should be.\n\nNow, the complete code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef compute_row_sums(\n    x_ptr,\n    row_sums_ptr,\n    n_cols,\n    stride_x0, stride_x1,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # row index\n    pid1 = tl.program_id(1)  # segment index\n    \n    col_offsets = pid1 * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    base_ptr = x_ptr + pid0 * stride_x0 + col_offsets * stride_x1\n    x_vals = tl.load(base_ptr, mask=mask, other=0.0)\n    abs_x = tl.abs(x_vals)\n    segment_sum = tl.sum(abs_x, axis=0)\n    tl.atomic_add(row_sums_ptr + pid0, segment_sum)\n\n@triton.jit\ndef normalize_row(\n    x_ptr,\n    row_sums_ptr,\n    output_ptr,\n    n_cols,\n    stride_x0, stride_x1,\n    stride_out0, stride_out1,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid0 = tl.program_id(0)  # row index\n    pid1 = tl.program_id(1)  # segment index\n    \n    col_offsets = pid1 * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    base_ptr = x_ptr + pid0 * stride_x0 + col_offsets * stride_x1\n    x_vals = tl.load(base_ptr, mask=mask, other=0.0)\n    \n    row_sum = tl.load(row_sums_ptr + pid0)\n    normalized = x_vals / row_sum\n    \n    out_base_ptr = output_ptr + pid0 * stride_out0 + col_offsets * stride_out1\n    tl.store(out_base_ptr, normalized, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.BLOCK_SIZE = 1024\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() != 2:\n            return x / torch.sum(torch.abs(x), dim=1, keepdim=True)\n        \n        x = x.contiguous()\n        n_rows, n_cols = x.shape\n        row_sums = torch.zeros(n_rows, device=x.device, dtype=torch.float32)\n        output = torch.empty_like(x)\n        \n        grid = (n_rows, triton.cdiv(n_cols, self.BLOCK_SIZE))\n        compute_row_sums[grid](\n            x, row_sums, n_cols, \n            x.stride(0), x.stride(1),\n            BLOCK_SIZE=self.BLOCK_SIZE\n        )\n        normalize_row[grid](\n            x, row_sums, output, n_cols,\n            x.stride(0), x.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE=self.BLOCK_SIZE\n        )\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
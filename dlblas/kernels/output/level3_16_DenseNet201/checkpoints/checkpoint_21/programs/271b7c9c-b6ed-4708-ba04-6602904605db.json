{"id": "271b7c9c-b6ed-4708-ba04-6602904605db", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_h,\n    input_w,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    P2: tl.constexpr,\n    OUTPUT_DTYPE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    num_elements = input_h * input_w\n    \n    arange = tl.arange(0, P2)\n    mask = arange < num_elements\n    row = arange // input_w\n    col = arange % input_w\n    offsets = base_offset + row * input_height_stride + col * input_width_stride\n    values = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    total = tl.sum(values, axis=0)\n    \n    average = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, average.to(OUTPUT_DTYPE))\n\ndef adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n    if not x.is_contiguous():\n        x = x.contiguous()\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n\n    grid = (batch, channels)\n    num_elements = height * width\n    P2 = int(2 ** math.ceil(math.log2(num_elements))) if num_elements > 0 else 1\n    \n    output_dtype = tl.float16 if x.dtype == torch.float16 else tl.float32\n    adaptive_avg_pool2d_kernel[grid](\n        x,\n        output,\n        height,\n        width,\n        x.stride(0),\n        x.stride(1),\n        x.stride(2),\n        x.stride(3),\n        output.stride(0),\n        output.stride(1),\n        P2,\n        OUTPUT_DTYPE=output_dtype\n    )\n    return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        num_features = 64\n        block_layers = [6, 12, 48, 32]\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = adaptive_avg_pool2d_triton(x).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "edca9ff8-e32f-4cbd-9131-6fd9f28d125d", "generation": 3, "timestamp": 1754592789.5988142, "iteration_found": 20, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.93, "runtime_ms": 9.81, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 9.12, "speed_up": 0.93, "custom_timing": 9.81}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.86, "runtime_ms": 10.6, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 9.12, "speed_up": 0.86, "custom_timing": 10.6}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.8600\n- runtime_ms: 10.6000\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.86, 'custom_timing': 10.6}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9120, runtime_ms: 10.0000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.912, 'custom_timing': 10.0}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.8600, runtime_ms: 10.6000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.86, 'custom_timing': 10.6}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.8370, runtime_ms: 10.9000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.837, 'custom_timing': 10.9}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_h,\n    input_w,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    BLOCK_SIZE: tl.constexpr,\n    OUTPUT_DTYPE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    total = 0.0\n    num_elements = input_h * input_w\n    \n    for row in range(input_h):\n        row_offset = base_offset + row * input_height_stride\n        for col in range(0, input_w, BLOCK_SIZE):\n            col_offsets = col + tl.arange(0, BLOCK_SIZE)\n            mask = col_offsets < input_w\n            offsets = row_offset + col_offsets * input_width_stride\n            values = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n            block_sum = tl.sum(values, axis=0)\n            total += block_sum\n\n    average = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, average.to(OUTPUT_DTYPE))\n\ndef adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n    if not x.is_contiguous():\n        x = x.contiguous()\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n\n    grid = (batch, channels)\n    BLOCK_SIZE = min(triton.next_power_of_2(width), 128)\n    \n    output_dtype = tl.float16 if x.dtype == torch.float16 else tl.float32\n    adaptive_avg_pool2d_kernel[grid](\n        x,\n        output,\n        height,\n        width,\n        x.stride(0),\n        x.stride(1),\n        x.stride(2),\n        x.stride(3),\n        output.stride(0),\n        output.stride(1),\n        BLOCK_SIZE,\n        OUTPUT_DTYPE=output_dtype\n    )\n    return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        num_features = 64\n        block_layers = [6, 12, 48, 32]\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = adaptive_avg_pool2d_triton(x).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.8370), Performs well on runtime_ms (10.9000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.837, 'custom_timing': 10.9})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_h,\n    input_w,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    BLOCK_SIZE: tl.constexpr,\n    OUTPUT_DTYPE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    total = 0.0\n    num_elements = input_h * input_w\n    \n    for row in range(input_h):\n        row_offset = base_offset + row * input_height_stride\n        for col in range(0, input_w, BLOCK_SIZE):\n            col_offsets = col + tl.arange(0, BLOCK_SIZE)\n            mask = col_offsets < input_w\n            offsets = row_offset + col_offsets * input_width_stride\n            values = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n            block_sum = tl.sum(values, axis=0)\n            total += block_sum\n\n    average = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, average.to(OUTPUT_DTYPE))\n\ndef adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n    if not x.is_contiguous():\n        x = x.contiguous()\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n\n    grid = (batch, channels)\n    BLOCK_SIZE = min(triton.next_power_of_2(width), 128)\n    \n    output_dtype = tl.float16 if x.dtype == torch.float16 else tl.float32\n    adaptive_avg_pool2d_kernel[grid](\n        x,\n        output,\n        height,\n        width,\n        x.stride(0),\n        x.stride(1),\n        x.stride(2),\n        x.stride(3),\n        output.stride(0),\n        output.stride(1),\n        BLOCK_SIZE,\n        OUTPUT_DTYPE=output_dtype\n    )\n    return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        num_features = 64\n        block_layers = [6, 12, 48, 32]\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = adaptive_avg_pool2d_triton(x).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.8600), Performs well on runtime_ms (10.6000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.86, 'custom_timing': 10.6})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    input_h, input_w,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    num_elements = input_h * input_w\n    \n    total = 0.0\n    for i in range(0, num_elements):\n        offset = base_offset + i\n        total += tl.load(input_ptr + offset)\n    \n    avg = total / num_elements\n    out_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + out_offset, avg)\n\ndef adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n    assert x.dim() == 4\n    batch, channels, height, width = x.shape\n    x = x.contiguous()\n    output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n    \n    grid = (batch, channels)\n    adaptive_avg_pool2d_kernel[grid](\n        x, output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height, width\n    )\n    return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = adaptive_avg_pool2d_triton(x)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9120), Performs well on runtime_ms (10.0000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.912, 'custom_timing': 10.0})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n# Triton-optimized 3x3 convolution kernel with fused ReLU\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 16}, num_warps=4),\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128}, num_warps=8),\n    ],\n    key=['C_in'],\n)\n@triton.jit\ndef conv2d_3x3_kernel(\n    # Pointers to matrices\n    input_ptr, weight_ptr, output_ptr,\n    # Matrix dimensions\n    B, C_in, H, W, C_out,\n    # Strides\n    stride_b, stride_cin, stride_h, stride_w,\n    stride_wb, stride_wcin, stride_wh, stride_ww,\n    # Meta-parameters\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,\n):\n    # Multiple programs in parallel\n    pid = tl.program_id(0)\n    pid_b = pid // (H * W)\n    pid_hw = pid % (H * W)\n    pid_h = pid_hw // W\n    pid_w = pid_hw % W\n    \n    # Create pointers to input and weights\n    input_offs = pid_b * stride_b + pid_h * stride_h + pid_w * stride_w\n    weight_offs = tl.arange(0, C_out)[:, None] * stride_wb\n    \n    # Block pointers with boundary checks\n    input_ptrs = input_ptr + input_offs + tl.arange(0, C_in)[None, :] * stride_cin\n    weight_ptrs = weight_ptr + weight_offs + tl.arange(0, C_in)[:, None] * stride_wcin\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    # Loop over kernel positions\n    for kh in range(3):\n        for kw in range(3):\n            # Calculate input position with padding=1\n            h = pid_h + kh - 1\n            w = pid_w + kw - 1\n            \n            # Check boundaries\n            within_bounds = (h >= 0) & (h < H) & (w >= 0) & (w < W)\n            \n            # Load input and weight tiles\n            input_val = tl.where(within_bounds, \n                                tl.load(input_ptrs + kh*stride_h + kw*stride_w, \n                                        mask=within_bounds, other=0.0), \n                                0.0)\n            weight_val = tl.load(weight_ptrs + kh*stride_wh + kw*stride_ww)\n            \n            # Compute convolution\n            acc += tl.dot(input_val, weight_val)\n    \n    # Fused ReLU activation\n    acc = tl.maximum(acc, 0.0)\n    \n    # Write back result\n    output_ptrs = output_ptr + pid_b * B * C_out + tl.arange(0, C_out)[:, None]\n    tl.store(output_ptrs, acc, mask=within_bounds)\n\nclass TritonConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, kernel_size, kernel_size))\n        nn.init.kaiming_normal_(self.weight)\n        \n    def forward(self, x):\n        B, C_in, H, W = x.shape\n        output = torch.empty(B, self.out_channels, H, W, device=x.device, dtype=x.dtype)\n        \n        grid = lambda opt: (B * H * W,)\n        conv2d_3x3_kernel[grid](\n            x, self.weight, output,\n            B, C_in, H, W, self.out_channels,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            self.weight.stride(0), self.weight.stride(1), self.weight.stride(2), self.weight.stride(3),\n            BLOCK_M=triton.next_power_of_2(C_in),\n            BLOCK_N=triton.next_power_of_2(self.out_channels)\n        )\n        return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            TritonConv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        num_features = 64\n        block_layers = [6, 12, 48, 32]\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _matmul_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    USE_BIAS: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    n_blocks_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // n_blocks_n\n    pid_n = pid % n_blocks_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_n[:, None] * stride_bn + offs_k[None, :] * stride_bk)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_remaining)\n        b_mask = (offs_n[:, None] < N) & (offs_k[None, :] < k_remaining)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        acc += tl.dot(a, b, trans_b=True, allow_tf32=True)\n        \n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    if USE_BIAS:\n        bias_ptrs = bias_ptr + offs_n\n        bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n        acc += bias[None, :]\n\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    \n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=c_mask)\n\nclass TritonConv1x1(nn.Module):\n    def __init__(self, in_channels, out_channels, bias=False, **kwargs):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_channels))\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        batch, in_channels, height, width = x.shape\n        x_reshaped = x.permute(0, 2, 3, 1).reshape(-1, in_channels)\n        output = torch.empty((x_reshaped.shape[0], self.out_channels), \n                            device=x.device, dtype=torch.float32)\n        \n        M, K = x_reshaped.shape\n        N, _ = self.weight.shape\n        \n        BLOCK_M = 64\n        BLOCK_N = 64\n        BLOCK_K = 32\n        grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n        \n        bias_ptr = self.bias.data_ptr() if self.bias is not None else 0\n        use_bias = self.bias is not None\n        \n        x_reshaped_fp16 = x_reshaped.to(torch.float16)\n        weight_fp16 = self.weight.to(torch.float16)\n        \n        _matmul_kernel[grid](\n            x_reshaped_fp16, weight_fp16, output, bias_ptr,\n            M, N, K,\n            x_reshaped_fp16.stride(0), x_reshaped_fp16.stride(1),\n            weight_fp16.stride(0), weight_fp16.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_M, BLOCK_N, BLOCK_K,\n            use_bias\n        )\n        \n        return output.reshape(batch, height, width, self.out_channels).permute(0, 3, 1, 2)\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            TritonConv1x1(num_input_features, num_output_features, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_h,\n    input_w,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    BLOCK_SIZE: tl.constexpr,\n    OUTPUT_DTYPE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    total = 0.0\n    num_elements = input_h * input_w\n    \n    for row in range(input_h):\n        row_offset = base_offset + row * input_height_stride\n        for col in range(0, input_w, BLOCK_SIZE):\n            col_offsets = col + tl.arange(0, BLOCK_SIZE)\n            mask = col_offsets < input_w\n            offsets = row_offset + col_offsets * input_width_stride\n            values = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n            block_sum = tl.sum(values, axis=0)\n            total += block_sum\n\n    average = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, average.to(OUTPUT_DTYPE))\n\ndef adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n    if not x.is_contiguous():\n        x = x.contiguous()\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n\n    grid = (batch, channels)\n    BLOCK_SIZE = min(triton.next_power_of_2(width), 128)\n    \n    output_dtype = tl.float16 if x.dtype == torch.float16 else tl.float32\n    adaptive_avg_pool2d_kernel[grid](\n        x,\n        output,\n        height,\n        width,\n        x.stride(0),\n        x.stride(1),\n        x.stride(2),\n        x.stride(3),\n        output.stride(0),\n        output.stride(1),\n        BLOCK_SIZE,\n        OUTPUT_DTYPE=output_dtype\n    )\n    return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        num_features = 64\n        block_layers = [6, 12, 48, 32]\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = adaptive_avg_pool2d_triton(x).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n# Triton-optimized 3x3 convolution kernel with fused ReLU\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 16}, num_warps=4),\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128}, num_warps=8),\n    ],\n    key=['C_in'],\n)\n@triton.jit\ndef conv2d_3x3_kernel(\n    # Pointers to matrices\n    input_ptr, weight_ptr, output_ptr,\n    # Matrix dimensions\n    B, C_in, H, W, C_out,\n    # Strides\n    stride_b, stride_cin, stride_h, stride_w,\n    stride_wb, stride_wcin, stride_wh, stride_ww,\n    # Meta-parameters\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,\n):\n    # Multiple programs in parallel\n    pid = tl.program_id(0)\n    pid_b = pid // (H * W)\n    pid_hw = pid % (H * W)\n    pid_h = pid_hw // W\n    pid_w = pid_hw % W\n    \n    # Create pointers to input and weights\n    input_offs = pid_b * stride_b + pid_h * stride_h + pid_w * stride_w\n    weight_offs = tl.arange(0, C_out)[:, None] * stride_wb\n    \n    # Block pointers with boundary checks\n    input_ptrs = input_ptr + input_offs + tl.arange(0, C_in)[None, :] * stride_cin\n    weight_ptrs = weight_ptr + weight_offs + tl.arange(0, C_in)[:, None] * stride_wcin\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    # Loop over kernel positions\n    for kh in range(3):\n        for kw in range(3):\n            # Calculate input position with padding=1\n            h = pid_h + kh - 1\n            w = pid_w + kw - 1\n            \n            # Check boundaries\n            within_bounds = (h >= 0) & (h < H) & (w >= 0) & (w < W)\n            \n            # Load input and weight tiles\n            input_val = tl.where(within_bounds, \n                                tl.load(input_ptrs + kh*stride_h + kw*stride_w, \n                                        mask=within_bounds, other=0.0), \n                                0.0)\n            weight_val = tl.load(weight_ptrs + kh*stride_wh + kw*stride_ww)\n            \n            # Compute convolution\n            acc += tl.dot(input_val, weight_val)\n    \n    # Fused ReLU activation\n    acc = tl.maximum(acc, 0.0)\n    \n    # Write back result\n    output_ptrs = output_ptr + pid_b * B * C_out + tl.arange(0, C_out)[:, None]\n    tl.store(output_ptrs, acc, mask=within_bounds)\n\nclass TritonConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, kernel_size, kernel_size))\n        nn.init.kaiming_normal_(self.weight)\n        \n    def forward(self, x):\n        B, C_in, H, W = x.shape\n        output = torch.empty(B, self.out_channels, H, W, device=x.device, dtype=x.dtype)\n        \n        grid = lambda opt: (B * H * W,)\n        conv2d_3x3_kernel[grid](\n            x, self.weight, output,\n            B, C_in, H, W, self.out_channels,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            self.weight.stride(0), self.weight.stride(1), self.weight.stride(2), self.weight.stride(3),\n            BLOCK_M=triton.next_power_of_2(C_in),\n            BLOCK_N=triton.next_power_of_2(self.out_channels)\n        )\n        return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            TritonConv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        num_features = 64\n        block_layers = [6, 12, 48, 32]\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nUnique approach: Excellent compiled (1.000), Excellent correctness (1.000), Alternative passed approach\n\n\n### Inspiration 4 (Score: -100.0000, Type: Exploratory)\n```python\n@triton.jit\n   def adaptive_avg_pool2d_kernel(\n       input_ptr,\n       output_ptr,\n       input_h,\n       input_w,\n       input_channel_stride, input_height_stride, input_width_stride,\n       output_batch_stride, output_channel_stride,\n       BLOCK_SIZE: tl.constexpr,\n   ):\n       pid_batch = tl.program_id(0)\n       pid_channel = tl.program_id(1)\n\n       # Create a range for the spatial dimensions we are going to sum over.\n       # We will use a 2D grid of blocks? But we are using one program per (batch, channel)\n       # We need to sum over [0, input_h) and [0, input_w)\n\n       # We'll use a 1D loop over the spatial dimensions. We can use a block for each row? But note that the entire channel is summed.\n\n       # Alternatively, we can use a single program to sum the entire channel by having a loop over rows and columns.\n\n       # Offsets for the input for this batch and channel\n       input_batch_offset = pid_batch * input_channel_stride * 0  # Actually, we need the correct strides.\n\n       # Wait, we are given the strides in terms of elements. But note: the input is of shape (batch, channels, height, width)\n       # So the strides are: [channels*height*width, height*width, width, 1] for contiguous.\n\n       # We assume the input is contiguous? We can make it contiguous in the function.\n\n       # The input pointer is the base, then:\n       #   input_ptr + pid_batch * (channels*height*width) + pid_channel * (height*width) + row * width + col\n\n       # We can precompute the base offset for this batch and channel:\n       base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n       total = 0.0\n       for row in range(0, input_h):\n           row_offset = row * input_height_stride\n           for col in range(0, input_w):\n               col_offset = col * input_width_stride\n               offset = base_offset + row_offset + col_offset\n               value = tl.load(input_ptr + offset)\n               total += value\n\n       average = total / (input_h * input_w)\n\n       # Write the average to the output.\n       # Output is of shape (batch, channels, 1, 1) -> so we write at [pid_batch, pid_channel]\n       output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n       tl.store(output_ptr + output_offset, average)\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _matmul_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    USE_BIAS: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    n_blocks_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // n_blocks_n\n    pid_n = pid % n_blocks_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_n[:, None] * stride_bn + offs_k[None, :] * stride_bk)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_remaining)\n        b_mask = (offs_n[:, None] < N) & (offs_k[None, :] < k_remaining)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        acc += tl.dot(a, b, trans_b=True, allow_tf32=True)\n        \n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    if USE_BIAS:\n        bias_ptrs = bias_ptr + offs_n\n        bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n        acc += bias[None, :]\n\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    \n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=c_mask)\n\nclass TritonConv1x1(nn.Module):\n    def __init__(self, in_channels, out_channels, bias=False, **kwargs):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_channels))\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        batch, in_channels, height, width = x.shape\n        x_reshaped = x.permute(0, 2, 3, 1).reshape(-1, in_channels)\n        output = torch.empty((x_reshaped.shape[0], self.out_channels), \n                            device=x.device, dtype=torch.float32)\n        \n        M, K = x_reshaped.shape\n        N, _ = self.weight.shape\n        \n        BLOCK_M = 64\n        BLOCK_N = 64\n        BLOCK_K = 32\n        grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n        \n        bias_ptr = self.bias.data_ptr() if self.bias is not None else 0\n        use_bias = self.bias is not None\n        \n        x_reshaped_fp16 = x_reshaped.to(torch.float16)\n        weight_fp16 = self.weight.to(torch.float16)\n        \n        _matmul_kernel[grid](\n            x_reshaped_fp16, weight_fp16, output, bias_ptr,\n            M, N, K,\n            x_reshaped_fp16.stride(0), x_reshaped_fp16.stride(1),\n            weight_fp16.stride(0), weight_fp16.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_M, BLOCK_N, BLOCK_K,\n            use_bias\n        )\n        \n        return output.reshape(batch, height, width, self.out_channels).permute(0, 3, 1, 2)\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            TritonConv1x1(num_input_features, num_output_features, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_h,\n    input_w,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    BLOCK_SIZE: tl.constexpr,\n    OUTPUT_DTYPE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    total = 0.0\n    num_elements = input_h * input_w\n    \n    for row in range(input_h):\n        row_offset = base_offset + row * input_height_stride\n        for col in range(0, input_w, BLOCK_SIZE):\n            col_offsets = col + tl.arange(0, BLOCK_SIZE)\n            mask = col_offsets < input_w\n            offsets = row_offset + col_offsets * input_width_stride\n            values = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n            block_sum = tl.sum(values, axis=0)\n            total += block_sum\n\n    average = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, average.to(OUTPUT_DTYPE))\n\ndef adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n    if not x.is_contiguous():\n        x = x.contiguous()\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n\n    grid = (batch, channels)\n    BLOCK_SIZE = min(triton.next_power_of_2(width), 128)\n    \n    output_dtype = tl.float16 if x.dtype == torch.float16 else tl.float32\n    adaptive_avg_pool2d_kernel[grid](\n        x,\n        output,\n        height,\n        width,\n        x.stride(0),\n        x.stride(1),\n        x.stride(2),\n        x.stride(3),\n        output.stride(0),\n        output.stride(1),\n        BLOCK_SIZE,\n        OUTPUT_DTYPE=output_dtype\n    )\n    return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        num_features = 64\n        block_layers = [6, 12, 48, 32]\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = adaptive_avg_pool2d_triton(x).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a Triton kernel for adaptive average pooling (adaptive_avg_pool2d_kernel) that is currently slower than the PyTorch reference (speed_up=0.86, meaning 14% slower). The goal is to speed it up.\n\nAnalysis of the current Triton kernel:\n- The kernel uses two nested loops: one over rows and one over columns (with a block for columns).\n- For each row, it loads a block of columns (using vectorized load) and sums them, then accumulates the sum for the entire image.\n- Finally, it computes the average and stores it.\n\nThe current implementation might be slow because:\n1. It uses a loop over rows and then a loop over column blocks. This might not be the most efficient way to traverse the image.\n2. The inner loop over column blocks might not be optimally utilizing the memory bandwidth because the block size is set to the next power of two of the width (but capped at 128). However, the width in the context of DenseNet is relatively small (e.g., 7x7 at the end). But note: the input to the adaptive_avg_pool2d_triton is the final feature map, which for DenseNet201 and input 224x224, at the end of the network might be 7x7. So the kernel is summing over 49 elements.\n\nGiven that the spatial size is small (7x7) at the end, we might consider a different strategy: having each program handle one batch and one channel, and then having a single loop over all the elements (49 iterations) is acceptable. However, the current implementation uses a block for the columns and then loops over rows. This might not be the bottleneck.\n\nBut note: the current kernel does a lot of global memory loads. Each element is loaded once. However, we are doing a reduction over the entire spatial dimension. We can try to optimize by:\n- Using shared memory? But the reduction is over 49 elements, which is small. We can do the entire reduction in registers.\n\nAlternatively, we can change the kernel to have each thread handle a fixed number of elements (the entire spatial dimension) and do a sequential reduction. Since 49 is small, we can unroll the loop or use a single loop.\n\nHowever, looking at the current kernel, it already breaks the columns into blocks and then sums each block. Then it sums the block sums. This is efficient for large images, but for small images (like 7x7) it might be overkill and the overhead of launching the kernel and the loops might be too high.\n\nLet's try a simpler approach: load all the elements in one go? But note that the maximum block size in Triton is limited. However, we are not using a block of threads per program, but rather each program is a single thread (scalar) because we are using `tl.sum` over the BLOCK_SIZE. Actually, the kernel is written to use vector loads and then a reduction per block. But note that the reduction is done in a scalar variable `total` (which is a scalar for the entire channel).\n\nAlternatively, we can try to use a 1D loop over all elements (row-major) and load them one by one? But that would be 49 loads, which is acceptable.\n\nBut wait, the current kernel already does:\n   for row in range(input_h):\n        ... loads a block of columns\n\nWe can change the inner loop to load the entire row at once? But the row might be too long for a vector load? But in our case, the row is 7 elements (if 7x7). So we can set BLOCK_SIZE to 7 and then we would have one block per row? That is what we are doing.\n\nAlternatively, we can set BLOCK_SIZE to the entire image? But that might be too large. The maximum BLOCK_SIZE in Triton is 1024? 49 is acceptable. So we can do:\n\n  col_offsets = tl.arange(0, input_w)\n  ... and then remove the inner loop.\n\nBut note: the kernel is designed to work for any size. We are not limited to 7x7.\n\nHowever, the adaptive pooling at the end of DenseNet is always 7x7? Actually, no: the input to the adaptive pooling is the final feature map, which for DenseNet201 is 7x7 for 224x224 input. But the model must be flexible.\n\nBut the kernel is called with the actual height and width. So we can adjust the block size to be the entire width? Then we would have one block per row? That would reduce the number of iterations in the inner loop to 1.\n\nBut the current BLOCK_SIZE is set to `min(triton.next_power_of_2(width), 128)`. For width=7, that would be 8. So we are doing one block per row (because 7 <=8). So the inner loop runs only once per row. So the current kernel for 7x7 does:\n   for each row (7 rows):\n        load 7 elements (masked to 7) and then sum them -> 7 loads per row.\n\nThen we do 7 rows -> 49 loads.\n\nWe can try to load the entire image in one block? We can set the BLOCK_SIZE to the entire image? But note: the kernel is designed to have a 1D block for the columns. We can change the kernel to have a 2D block? But that would require a different program grid.\n\nAlternatively, we can change the kernel to have each program handle one channel and one batch, and then do a reduction over the entire spatial domain in one loop. Since the spatial size is small, we can do:\n\n   total = 0.0\n   for i in range(input_h * input_w):\n        total += tl.load(input_ptr + base_offset + i)\n\nBut that would require the input to be contiguous in the spatial dimensions? The input is contiguous because we call `x.contiguous()`.\n\nBut note: the strides might not be 1? We are given the strides. We can compute the offset as:\n\n   offset = base_offset + i // input_w * input_height_stride + i % input_w * input_width_stride\n\nBut this might be slower because of the integer division and modulus.\n\nAlternatively, we can precompute the offsets for the entire image? But that would require storing 49 offsets, which might be too many for registers? And we cannot use shared memory because we are in a single thread.\n\nGiven the small size, we can unroll the loop? But Triton doesn't have unroll pragma.\n\nAlternatively, we can use a nested loop but flatten it in the code? But the current code is already two nested loops.\n\nAnother idea: use vectorization for the entire image? We can try to load the entire image in one vector if the size is less than 1024? But 49 is less than 1024. So we can do:\n\n   arange = tl.arange(0, input_h * input_w)\n   row_indices = arange // input_w\n   col_indices = arange % input_w\n   offsets = base_offset + row_indices * input_height_stride + col_indices * input_width_stride\n   values = tl.load(input_ptr + offsets, mask=arange < (input_h*input_w), other=0.0)\n   total = tl.sum(values)\n\nThis would do one big vector load and then one reduction. This might be faster because it reduces the number of loads (one big vector load) and the reduction is done in parallel in the hardware.\n\nBut note: the image might be too large? We are limited by the block size. We are currently using a 1D block of size BLOCK_SIZE for the columns. We can change to use a 1D block of size (input_h * input_w) for the entire image? But we are not using a block of threads, we are using one program per (batch, channel) and then within that program we are using a vector operation.\n\nBut the vector operation can have up to 1024 elements? So if the spatial size is <= 1024, we can do:\n\n   BLOCK_SIZE = input_h * input_w   # but we cannot pass this as a variable because it is not a constant?\n\nAlternatively, we can set the block size to the next power of two of the spatial size? But note: the kernel is launched with a fixed grid and the block size is a constant expression.\n\nWe can do:\n\n   P2 = triton.next_power_of_2(input_h * input_w)\n   ... and then use a vector of size P2, and mask out the extra elements.\n\nLet's rewrite the kernel accordingly:\n\n   pid_batch = tl.program_id(0)\n   pid_channel = tl.program_id(1)\n   base_offset = ... # same as before\n   num_elements = input_h * input_w\n   P2 = triton.next_power_of_2(num_elements)   # but we cannot compute this inside the kernel? Because it must be a constant expression.\n\nSo we have to pass P2 as a kernel argument (like we did in the example per_row_quantize_int8).\n\nWe can do:\n\n   @triton.jit\n   def adaptive_avg_pool2d_kernel(\n        ...,\n        P2: tl.constexpr,   # the next power of two of the spatial size\n   ):\n        ...\n        arange = tl.arange(0, P2)\n        mask = arange < num_elements\n        # Compute row and col? But we don't have the width and height? We have them as arguments.\n        # We can compute:\n        row = arange // input_w\n        col = arange % input_w\n        offsets = base_offset + row * input_height_stride + col * input_width_stride\n        values = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n        total = tl.sum(values)\n        average = total / num_elements\n\nBut note: the integer division and modulus might be expensive? But for 49 elements, it might be acceptable.\n\nAlternatively, we can precompute the offsets in the host and pass a 1D array of offsets? But that is not possible because the offsets depend on the strides.\n\nGiven the small spatial size, we can try this approach.\n\nBut note: the previous approach (looping over rows and then blocks of columns) is already doing 49 loads. The new approach does 49 loads as well. The difference is that we do one big vector load and then one reduction, which might be faster because the reduction is done in parallel in the hardware (using a tree reduction in a single instruction) and we avoid the outer loop.\n\nSo let's change the kernel to:\n\n   We remove the two nested loops and replace with one vector load and reduction.\n\nWe also need to pass P2 (the next power of two of the spatial size) from the host.\n\nSteps:\n1. In the host function, compute P2 = 2 ** (math.ceil(math.log2(height * width)))\n2. Pass P2 as an argument to the kernel.\n3. In the kernel, use a 1D vector of size P2 to load the entire image.\n\nBut note: what if the spatial size is 0? We should avoid.\n\nWe'll change the kernel accordingly.\n\nLet's rewrite the kernel and the host function.\n\nWe'll change the kernel signature to include P2.\n\nAlso, note that the current kernel uses a loop over rows and then a block for columns. We are replacing that with a single block for the entire spatial dimension.\n\nWe'll remove the BLOCK_SIZE argument and use P2 as the vector size.\n\nBut note: the grid configuration remains the same: (batch, channels).\n\nLet me write the new kernel:\n\n   pid_batch = tl.program_id(0)\n   pid_channel = tl.program_id(1)\n   base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n   num_elements = input_h * input_w\n\n   arange = tl.arange(0, P2)\n   mask = arange < num_elements\n   row = arange // input_w\n   col = arange % input_w\n   offsets = base_offset + row * input_height_stride + col * input_width_stride\n   values = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n   total = tl.sum(values)\n   average = total / num_elements\n\n   ... then store as before.\n\nBut note: the integer division and modulus might be slow? But for small spatial size, it's acceptable.\n\nAlternatively, we can precompute the offsets on the host? But we don't know the base_offset per program.\n\nWe'll try this approach.\n\nWe also need to change the host function to compute P2 and pass it.\n\nIn the host function:\n\n   P2 = int(2 ** math.ceil(math.log2(height * width)))\n   ... and then pass P2 to the kernel.\n\nBut note: if the spatial size is 0, we skip.\n\nWe'll do:\n\n   if num_elements == 0:\n        average = 0.0\n   else:\n        ... the above\n\nBut the problem says we are in a CNN, so it's unlikely.\n\nWe'll change the kernel accordingly.\n\nLet me write the code for the kernel and the host function.\n\nBut note: the current kernel uses an output of shape (batch, channels, 1, 1). The output strides are passed accordingly.\n\nWe are storing at the same location.\n\nWe'll also remove the BLOCK_SIZE argument and the loop.\n\nWe'll also remove the OUTPUT_DTYPE? We are still converting the average to the output dtype.\n\nWe'll keep that.\n\nSo the new kernel:\n\n   @triton.jit\n   def adaptive_avg_pool2d_kernel(\n        input_ptr,\n        output_ptr,\n        input_h,\n        input_w,\n        input_batch_stride,\n        input_channel_stride,\n        input_height_stride,\n        input_width_stride,\n        output_batch_stride,\n        output_channel_stride,\n        P2: tl.constexpr,   # next power of two of spatial size\n        OUTPUT_DTYPE: tl.constexpr,\n   ):\n        pid_batch = tl.program_id(0)\n        pid_channel = tl.program_id(1)\n        base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n        num_elements = input_h * input_w\n\n        # If there are no elements, set to zero and return.\n        if num_elements == 0:\n            average = 0.0\n        else:\n            arange = tl.arange(0, P2)\n            mask = arange < num_elements\n            # Compute row and column indices for each element in the flattened spatial domain.\n            row_indices = arange // input_w\n            col_indices = arange % input_w\n            offsets = base_offset + row_indices * input_height_stride + col_indices * input_width_stride\n            values = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n            total = tl.sum(values)\n            average = total / num_elements\n\n        output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n        tl.store(output_ptr + output_offset, average.to(OUTPUT_DTYPE))\n\nIn the host function:\n\n   def adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n        if not x.is_contiguous():\n            x = x.contiguous()\n        batch, channels, height, width = x.shape\n        output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n\n        grid = (batch, channels)\n        num_elements = height * width\n        if num_elements == 0:\n            P2 = 1\n        else:\n            P2 = int(2 ** math.ceil(math.log2(num_elements)))\n        \n        output_dtype = tl.float16 if x.dtype == torch.float16 else tl.float32\n        adaptive_avg_pool2d_kernel[grid](\n            x,\n            output,\n            height,\n            width,\n            x.stride(0),\n            x.stride(1),\n            x.stride(2),\n            x.stride(3),\n            output.stride(0),\n            output.stride(1),\n            P2,\n            OUTPUT_DTYPE=output_dtype\n        )\n        return output\n\nWe also need to import math.\n\nBut note: the rest of the code (the DenseNet) remains the same.\n\nLet's try this.\n\nHowever, note that the original kernel had boundary checks for each row and column block. The new kernel does a single boundary check (mask = arange < num_elements) and then computes row and column indices. This should be safe.\n\nWe are also using the same grid and the same output.\n\nWe are changing the kernel to use a single vector load and reduction. This might be faster for small spatial sizes (like 7x7) because it avoids the outer loop and the inner block reduction.\n\nBut note: the vector load of 49 elements is done in one go? Actually, the hardware might break it into multiple loads, but the reduction will be done in parallel.\n\nWe'll hope for a speedup.\n\nWe are only changing the Triton kernel and the host function for the adaptive pooling.\n\nWe must not change the rest of the model.\n\nLet me write the entire EVOLVE-BLOCK with the changes.\n\nNote: We are replacing the kernel and the host function, and we are adding an import for math.\n\nWe'll add:\n   import math\n\nat the top.\n\nBut note: the current program already has:\n   import triton\n   import triton.language as tl\n\nWe just add math.\n\nLet me write the code.\n\nImportant: We are only allowed to change the Triton kernel and the host function. The rest of the model (DenseBlock, TransitionLayer, ModelNew) remains the same.\n\nWe'll output the entire block as required.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_h,\n    input_w,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    P2: tl.constexpr,\n    OUTPUT_DTYPE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    num_elements = input_h * input_w\n    \n    arange = tl.arange(0, P2)\n    mask = arange < num_elements\n    row = arange // input_w\n    col = arange % input_w\n    offsets = base_offset + row * input_height_stride + col * input_width_stride\n    values = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    total = tl.sum(values, axis=0)\n    \n    average = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, average.to(OUTPUT_DTYPE))\n\ndef adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n    if not x.is_contiguous():\n        x = x.contiguous()\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n\n    grid = (batch, channels)\n    num_elements = height * width\n    P2 = int(2 ** math.ceil(math.log2(num_elements))) if num_elements > 0 else 1\n    \n    output_dtype = tl.float16 if x.dtype == torch.float16 else tl.float32\n    adaptive_avg_pool2d_kernel[grid](\n        x,\n        output,\n        height,\n        width,\n        x.stride(0),\n        x.stride(1),\n        x.stride(2),\n        x.stride(3),\n        output.stride(0),\n        output.stride(1),\n        P2,\n        OUTPUT_DTYPE=output_dtype\n    )\n    return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        num_features = 64\n        block_layers = [6, 12, 48, 32]\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = adaptive_avg_pool2d_triton(x).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "03e9e25d-572f-4ca4-97e6-a4adbc12e3dd", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_h,\n    input_w,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Calculate base offset for this batch and channel\n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n    # Initialize accumulator\n    total = 0.0\n    num_elements = 0\n    \n    # Process rows in blocks\n    for row_start in range(0, input_h, BLOCK_SIZE):\n        row_idx = row_start + tl.arange(0, BLOCK_SIZE)\n        row_mask = row_idx < input_h\n        row_offset = row_idx * input_height_stride\n        \n        # Process columns in blocks\n        for col_start in range(0, input_w, BLOCK_SIZE):\n            col_idx = col_start + tl.arange(0, BLOCK_SIZE)\n            col_mask = col_idx < input_w\n            col_offset = col_idx * input_width_stride\n            \n            # Combine masks and compute offsets\n            full_mask = row_mask[:, None] & col_mask[None, :]\n            offsets = base_offset + row_offset[:, None] + col_offset[None, :]\n            \n            # Load input values with masking\n            values = tl.load(input_ptr + offsets, mask=full_mask, other=0.0)\n            total += tl.sum(values)\n            num_elements += tl.sum(full_mask)\n    \n    # Compute average and store result\n    average = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, average)\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "4ba46ad1-7b8f-4dfe-8fe7-95a98d0803b0", "generation": 4, "timestamp": 1754593893.1991541, "iteration_found": 28, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.885, "runtime_ms": 10.3, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 9.12, "speed_up": 0.885, "custom_timing": 10.3}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.885, "runtime_ms": 10.3, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 9.12, "speed_up": 0.885, "custom_timing": 10.3}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.8850\n- runtime_ms: 10.3000\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.885, 'custom_timing': 10.3}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.8770, runtime_ms: 10.4000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.877, 'custom_timing': 10.4}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9030, runtime_ms: 10.1000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.903, 'custom_timing': 10.1}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.8850, runtime_ms: 10.3000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.885, 'custom_timing': 10.3}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_h,\n    input_w,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Calculate base offset for this batch and channel\n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n    # Initialize accumulators\n    total = 0.0\n    num_elements = 0\n    total_elements = input_h * input_w\n    \n    # Process flattened image as 1D contiguous blocks\n    for start in range(0, total_elements, BLOCK_SIZE):\n        idx = start + tl.arange(0, BLOCK_SIZE)\n        mask = idx < total_elements\n        \n        # Calculate row and column indices\n        row = idx // input_w\n        col = idx % input_w\n        \n        # Compute memory offsets\n        offsets = base_offset + row * input_height_stride + col * input_width_stride\n        \n        # Load input values with masking\n        values = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n        total += tl.sum(values)\n        num_elements += tl.sum(mask)\n    \n    # Compute average and store result\n    average = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, average)\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.8850), Performs well on runtime_ms (10.3000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.885, 'custom_timing': 10.3})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_h,\n    input_w,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Calculate base offset for this batch and channel\n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n    # Initialize accumulator\n    total = 0.0\n    num_elements = 0\n    \n    # Process rows in blocks\n    for row_start in range(0, input_h, BLOCK_SIZE):\n        row_idx = row_start + tl.arange(0, BLOCK_SIZE)\n        row_mask = row_idx < input_h\n        row_offset = row_idx * input_height_stride\n        \n        # Process columns in blocks\n        for col_start in range(0, input_w, BLOCK_SIZE):\n            col_idx = col_start + tl.arange(0, BLOCK_SIZE)\n            col_mask = col_idx < input_w\n            col_offset = col_idx * input_width_stride\n            \n            # Combine masks and compute offsets\n            full_mask = row_mask[:, None] & col_mask[None, :]\n            offsets = base_offset + row_offset[:, None] + col_offset[None, :]\n            \n            # Load input values with masking\n            values = tl.load(input_ptr + offsets, mask=full_mask, other=0.0)\n            total += tl.sum(values)\n            num_elements += tl.sum(full_mask)\n    \n    # Compute average and store result\n    average = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, average)\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9030), Performs well on runtime_ms (10.1000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.903, 'custom_timing': 10.1})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    input_h, input_w,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    num_elements = input_h * input_w\n    \n    total = 0.0\n    for i in range(0, num_elements):\n        offset = base_offset + i\n        total += tl.load(input_ptr + offset)\n    \n    avg = total / num_elements\n    out_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + out_offset, avg)\n\ndef adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n    assert x.dim() == 4\n    batch, channels, height, width = x.shape\n    x = x.contiguous()\n    output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n    \n    grid = (batch, channels)\n    adaptive_avg_pool2d_kernel[grid](\n        x, output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height, width\n    )\n    return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = adaptive_avg_pool2d_triton(x)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.8770), Performs well on runtime_ms (10.4000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.877, 'custom_timing': 10.4})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 4}, num_warps=4),\n    ],\n    key=['input_h', 'input_w'],\n)\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    input_h, input_w,\n    BLOCK_SIZE: tl.constexpr,\n    VEC_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    total_elements = input_h * input_w\n    \n    total = 0.0\n    count = 0\n    BLOCK_ELEMS = BLOCK_SIZE * VEC_SIZE\n    \n    for start in range(0, total_elements, BLOCK_ELEMS):\n        offsets = base_offset + start + tl.arange(0, BLOCK_ELEMS)\n        linear_idx = start + tl.arange(0, BLOCK_ELEMS)\n        mask = linear_idx < total_elements\n        \n        data = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n        total += tl.sum(data)\n        count += tl.sum(mask)\n    \n    avg = total / tl.maximum(count, 1)\n    out_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + out_offset, avg)\n\ndef adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n    assert x.dim() == 4\n    batch, channels, height, width = x.shape\n    x = x.contiguous()\n    output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n    \n    grid = (batch, channels)\n    adaptive_avg_pool2d_kernel[grid](\n        x, output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height, width\n    )\n    return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = adaptive_avg_pool2d_triton(x)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    input_h, input_w,\n    BLOCK_SIZE: tl.constexpr = 64,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n    total = 0.0\n    count = 0\n    \n    for row_start in range(0, input_h, BLOCK_SIZE):\n        row_idx = row_start + tl.arange(0, BLOCK_SIZE)\n        row_mask = row_idx < input_h\n        row_offset = row_idx * input_height_stride\n        \n        for col_start in range(0, input_w, BLOCK_SIZE):\n            col_idx = col_start + tl.arange(0, BLOCK_SIZE)\n            col_mask = col_idx < input_w\n            col_offset = col_idx * input_width_stride\n            \n            full_mask = row_mask[:, None] & col_mask[None, :]\n            offsets = base_offset + row_offset[:, None] + col_offset[None, :]\n            \n            block = tl.load(input_ptr + offsets, mask=full_mask, other=0.0)\n            total += tl.sum(block)\n            count += tl.sum(full_mask)\n    \n    avg = total / count if count > 0 else 0.0\n    out_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + out_offset, avg)\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_h,\n    input_w,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Calculate base offset for this batch and channel\n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n    # Initialize accumulator\n    total = 0.0\n    num_elements = 0\n    \n    # Process rows in blocks\n    for row_start in range(0, input_h, BLOCK_SIZE):\n        row_idx = row_start + tl.arange(0, BLOCK_SIZE)\n        row_mask = row_idx < input_h\n        row_offset = row_idx * input_height_stride\n        \n        # Process columns in blocks\n        for col_start in range(0, input_w, BLOCK_SIZE):\n            col_idx = col_start + tl.arange(0, BLOCK_SIZE)\n            col_mask = col_idx < input_w\n            col_offset = col_idx * input_width_stride\n            \n            # Combine masks and compute offsets\n            full_mask = row_mask[:, None] & col_mask[None, :]\n            offsets = base_offset + row_offset[:, None] + col_offset[None, :]\n            \n            # Load input values with masking\n            values = tl.load(input_ptr + offsets, mask=full_mask, other=0.0)\n            total += tl.sum(values)\n            num_elements += tl.sum(full_mask)\n    \n    # Compute average and store result\n    average = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, average)\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    input_h, input_w,\n    BLOCK_SIZE: tl.constexpr = 64,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n    total = 0.0\n    count = 0\n    \n    for row_start in range(0, input_h, BLOCK_SIZE):\n        row_idx = row_start + tl.arange(0, BLOCK_SIZE)\n        row_mask = row_idx < input_h\n        row_offset = row_idx * input_height_stride\n        \n        for col_start in range(0, input_w, BLOCK_SIZE):\n            col_idx = col_start + tl.arange(0, BLOCK_SIZE)\n            col_mask = col_idx < input_w\n            col_offset = col_idx * input_width_stride\n            \n            full_mask = row_mask[:, None] & col_mask[None, :]\n            offsets = base_offset + row_offset[:, None] + col_offset[None, :]\n            \n            block = tl.load(input_ptr + offsets, mask=full_mask, other=0.0)\n            total += tl.sum(block)\n            count += tl.sum(full_mask)\n    \n    avg = total / count if count > 0 else 0.0\n    out_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + out_offset, avg)\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 8}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512, 'VEC_SIZE': 4}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024, 'VEC_SIZE': 4}, num_warps=4),\n    ],\n    key=['input_h', 'input_w'],\n)\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    input_h, input_w,\n    BLOCK_SIZE: tl.constexpr,\n    VEC_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    total_elements = input_h * input_w\n    \n    total = 0.0\n    count = 0\n    BLOCK_ELEMS = BLOCK_SIZE * VEC_SIZE\n    \n    for start in range(0, total_elements, BLOCK_ELEMS):\n        offsets = base_offset + start + tl.arange(0, BLOCK_ELEMS)\n        linear_idx = start + tl.arange(0, BLOCK_ELEMS)\n        mask = linear_idx < total_elements\n        \n        data = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n        total += tl.sum(data)\n        count += tl.sum(mask)\n    \n    avg = total / tl.maximum(count, 1)\n    out_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + out_offset, avg)\n\ndef adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n    assert x.dim() == 4\n    batch, channels, height, width = x.shape\n    x = x.contiguous()\n    output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n    \n    grid = (batch, channels)\n    adaptive_avg_pool2d_kernel[grid](\n        x, output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height, width\n    )\n    return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = adaptive_avg_pool2d_triton(x)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: -100.0000, Type: Exploratory)\n```python\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    input_h, input_w,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    num_elements = input_h * input_w\n    \n    total = 0.0\n    for i in range(0, num_elements):\n        offset = base_offset + i\n        total += tl.load(input_ptr + offset)\n    \n    avg = total / num_elements\n    out_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + out_offset, avg)\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 5 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    input_h, input_w,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    num_elements = input_h * input_w\n    \n    total = 0.0\n    for i in range(0, num_elements):\n        offset = base_offset + i\n        total += tl.load(input_ptr + offset)\n    \n    avg = total / num_elements\n    out_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + out_offset, avg)\n\ndef adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n    assert x.dim() == 4\n    batch, channels, height, width = x.shape\n    x = x.contiguous()\n    output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n    \n    grid = (batch, channels)\n    adaptive_avg_pool2d_kernel[grid](\n        x, output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height, width\n    )\n    return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = adaptive_avg_pool2d_triton(x)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_h,\n    input_w,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Calculate base offset for this batch and channel\n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n    # Initialize accumulators\n    total = 0.0\n    num_elements = 0\n    total_elements = input_h * input_w\n    \n    # Process flattened image as 1D contiguous blocks\n    for start in range(0, total_elements, BLOCK_SIZE):\n        idx = start + tl.arange(0, BLOCK_SIZE)\n        mask = idx < total_elements\n        \n        # Calculate row and column indices\n        row = idx // input_w\n        col = idx % input_w\n        \n        # Compute memory offsets\n        offsets = base_offset + row * input_height_stride + col * input_width_stride\n        \n        # Load input values with masking\n        values = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n        total += tl.sum(values)\n        num_elements += tl.sum(mask)\n    \n    # Compute average and store result\n    average = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, average)\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current Triton kernel for adaptive average pooling 2D. The current implementation flattens the 2D image and processes it in 1D contiguous blocks. However, the performance (speed_up) is currently 0.885 (slower than the reference PyTorch implementation). We need to improve the speed.\n\n Observations:\n - The current kernel uses a 1D block approach, which might not be optimal for memory access patterns on a GPU.\n - We note that the input is a 4D tensor (batch, channels, height, width). The kernel is launched with grid (batch, channels) and each program computes the average over height*width elements.\n\n Areas for improvement:\n 1. Memory Access Pattern: The current 1D flattening might lead to non-coalesced memory accesses because adjacent threads in the block are accessing rows and columns that might not be contiguous in memory. We should consider a 2D block structure to improve memory coalescing.\n 2. Block Processing: Instead of flattening, we can process the image in 2D blocks. This allows us to load contiguous blocks of the image, which are stored in row-major order, and thus we can get coalesced memory accesses.\n\n Proposed Approach:\n - We can take inspiration from the top-performing program (Program 2) and the Inspiration 1 program, which use a 2D block structure. They process the image by iterating over blocks of rows and columns, and within each block, they load a 2D tile with coalesced access.\n\n Steps for the new kernel:\n 1. We will use two nested loops: one for rows (in steps of BLOCK_SIZE_H) and one for columns (in steps of BLOCK_SIZE_W).\n 2. We will use two-dimensional indices within the block: one for the row and one for the column.\n 3. We will create masks for rows and columns to handle boundaries.\n 4. We will compute the offsets for the 2D block and load the values with the combined mask.\n\n Note: We are not changing the function signature. We will replace the current loop structure with the 2D block approach.\n\n We also note that the current program uses a single BLOCK_SIZE for the 1D block. We will change this to two block sizes (for rows and columns) but to keep the signature we can use a single BLOCK_SIZE and then set BLOCK_SIZE_H and BLOCK_SIZE_W to be the same (square block) or we can adjust. However, the signature constraint says we cannot change the parameters. Therefore, we can use the same BLOCK_SIZE for both dimensions? Or we can use the same BLOCK_SIZE and then split it appropriately? \n\n Alternatively, we can change the kernel to use two block sizes, but that would change the signature. Therefore, we must stick to the same parameters.\n\n Let's reinterpret BLOCK_SIZE as the block size for both dimensions? But note that the current BLOCK_SIZE is for the 1D block. We want to use a 2D block of size (BLOCK_SIZE, BLOCK_SIZE). However, the total number of threads in a 2D block would be BLOCK_SIZE * BLOCK_SIZE, which might be too large. Instead, we can keep the same BLOCK_SIZE as the linear dimension and then use a 1D block of BLOCK_SIZE threads, but process 2D by having each thread handle multiple elements? \n\n However, the top performing program (Program 2) uses two loops and a 2D block without changing the signature. They used BLOCK_SIZE for both row and column blocks. We can do the same.\n\n How the top program (Program 2) works:\n   for row_start in range(0, input_h, BLOCK_SIZE):\n        row_idx = row_start + tl.arange(0, BLOCK_SIZE)\n        row_mask = row_idx < input_h\n        row_offset = row_idx * input_height_stride\n\n        for col_start in range(0, input_w, BLOCK_SIZE):\n            col_idx = col_start + tl.arange(0, BLOCK_SIZE)\n            col_mask = col_idx < input_w\n            col_offset = col_idx * input_width_stride\n\n            full_mask = row_mask[:, None] & col_mask[None, :]\n            offsets = base_offset + row_offset[:, None] + col_offset[None, :]\n\n            values = tl.load(input_ptr + offsets, mask=full_mask, other=0.0)\n            total += tl.sum(values)\n            num_elements += tl.sum(full_mask)\n\n We can adopt this approach.\n\n Steps in the new kernel:\n  - Remove the 1D loop and replace with two nested loops (over rows and columns in steps of BLOCK_SIZE).\n  - Use the same BLOCK_SIZE for both row and column blocks.\n\n However, note that the top program (Program 2) had a speed_up of 0.903, which is better than our current 0.885 but still below 1. We need to get at least 1.05 (5% improvement). Therefore, we must do better.\n\n Why is the current approach slow?\n  - The 1D approach might not be coalesced because adjacent threads in the 1D block might be accessing non-contiguous memory (if they are in the same row but different columns, that's contiguous, but if they are in different rows, that's not). The 2D block approach can load contiguous chunks of memory.\n\n But note: the 2D block approach in Program 2 had a speed_up of 0.903. We must try to optimize further.\n\n What else can we do?\n  - Vectorization: We can try to load multiple contiguous elements in a row at once. However, the top program (D1) uses vectorization. We can combine the 2D block with vectorization? But note that we are constrained by the signature.\n\n Since we cannot change the signature, we cannot add a VEC_SIZE parameter. Therefore, we can try to use the existing BLOCK_SIZE and then internally use vector loads? We can try to load a vector of consecutive elements in the row.\n\n However, the Triton kernel does not have an explicit vectorization in the loop. We might need to change the inner loop to process multiple columns at once. But without changing the signature, we have to be cautious.\n\n Alternatively, we can use the autotuning feature to find the best block size and also use vectorization by having a fixed vector size? But without adding a parameter, we can hardcode a vector size? Let's see:\n\n We can do:\n   for col_start in range(0, input_w, BLOCK_SIZE * VECTOR_SIZE):\n        ... and then load VECTOR_SIZE elements per thread?\n\n But note: we cannot change the parameters. Therefore, we cannot add VECTOR_SIZE. However, we can use a fixed vector size? But the optimal vector size might depend on the width.\n\n Another idea: We can use the fact that the input is contiguous in the last dimension (width). Therefore, we can load multiple consecutive elements in the same row using a vector load. We can do:\n\n   col_idx = col_start + tl.arange(0, BLOCK_SIZE) * VECTOR_SIZE\n   ... but then we have to adjust the mask and the offset.\n\n However, without changing the signature, we cannot add a vector size. We can hardcode a vector size? But that might not be optimal for all input sizes.\n\n Alternatively, we can use the existing BLOCK_SIZE to represent the number of rows we process at a time, and then within each row, we can use a vectorized load for the entire row? But that might not be possible because the row might be too long.\n\n Considering the constraints, let's stick to the 2D block approach without vectorization for now and hope that it is faster than the current 1D approach. We note that Program 2 (with 2D blocks) had a speed_up of 0.903, which is better than our current 0.885.\n\n We will rewrite the kernel to use the 2D block approach.\n\n Steps:\n  1. We'll iterate over the rows in steps of BLOCK_SIZE (which we now interpret as the block height).\n  2. For each row block, we iterate over the columns in steps of BLOCK_SIZE (which we now interpret as the block width).\n  3. We load a 2D tile of size [BLOCK_SIZE, BLOCK_SIZE] with appropriate masking.\n\n However, note: the current kernel uses a single BLOCK_SIZE for the 1D block. We are going to use the same BLOCK_SIZE for both dimensions. This means that the number of elements processed per iteration is BLOCK_SIZE * BLOCK_SIZE. We must be cautious because if BLOCK_SIZE is large, we might run into resource limitations. But the autotuner in the future can adjust this.\n\n Implementation:\n\n  total = 0.0\n  num_elements = 0\n\n  for row_start in range(0, input_h, BLOCK_SIZE):\n      row_idx = row_start + tl.arange(0, BLOCK_SIZE)\n      row_mask = row_idx < input_h\n      row_offset = row_idx * input_height_stride\n\n      for col_start in range(0, input_w, BLOCK_SIZE):\n          col_idx = col_start + tl.arange(0, BLOCK_SIZE)\n          col_mask = col_idx < input_w\n          col_offset = col_idx * input_width_stride\n\n          # Create a 2D mask [BLOCK_SIZE, BLOCK_SIZE]\n          full_mask = row_mask[:, None] & col_mask[None, :]\n\n          # Compute the base offset for the tile: [BLOCK_SIZE, BLOCK_SIZE]\n          offsets = base_offset + row_offset[:, None] + col_offset[None, :]\n\n          # Load the tile\n          values = tl.load(input_ptr + offsets, mask=full_mask, other=0.0)\n\n          total += tl.sum(values)\n          num_elements += tl.sum(full_mask)\n\n  average = total / num_elements\n\n We must note: the original kernel used a 1D block and did a single loop. We are now doing two nested loops. This might be more efficient because of better memory coalescing.\n\n We are going to replace the current kernel body with this 2D approach.\n\n Since we cannot change the signature, we keep the same BLOCK_SIZE parameter. We note that the BLOCK_SIZE in the current kernel was used as the 1D block size. Now we are using it as the 2D block size (for both height and width). This is acceptable because the autotuner can choose a different value for BLOCK_SIZE.\n\n Let's write the new kernel accordingly.\n\n Important: We must not change the function signature and the grid configuration. The grid is still (batch, channels) and each program processes one channel of one batch.\n\n We hope that this 2D block approach will improve memory access and thus performance.\n\n Let's write the code accordingly.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_h,\n    input_w,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Calculate base offset for this batch and channel\n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n    # Initialize accumulator\n    total = 0.0\n    num_elements = 0\n    \n    # Process rows in blocks\n    for row_start in range(0, input_h, BLOCK_SIZE):\n        row_idx = row_start + tl.arange(0, BLOCK_SIZE)\n        row_mask = row_idx < input_h\n        row_offset = row_idx * input_height_stride\n        \n        # Process columns in blocks\n        for col_start in range(0, input_w, BLOCK_SIZE):\n            col_idx = col_start + tl.arange(0, BLOCK_SIZE)\n            col_mask = col_idx < input_w\n            col_offset = col_idx * input_width_stride\n            \n            # Combine masks and compute offsets\n            full_mask = row_mask[:, None] & col_mask[None, :]\n            offsets = base_offset + row_offset[:, None] + col_offset[None, :]\n            \n            # Load input values with masking\n            values = tl.load(input_ptr + offsets, mask=full_mask, other=0.0)\n            total += tl.sum(values)\n            num_elements += tl.sum(full_mask)\n    \n    # Compute average and store result\n    average = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, average)\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
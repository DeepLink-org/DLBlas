{"id": "f09174f7-a54b-4ec3-835d-693fb95e06c3", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    input_h, input_w,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    num_elements = input_h * input_w\n    \n    total = 0.0\n    for i in range(0, num_elements):\n        offset = base_offset + i\n        total += tl.load(input_ptr + offset)\n    \n    avg = total / num_elements\n    out_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + out_offset, avg)\n\ndef adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n    assert x.dim() == 4\n    batch, channels, height, width = x.shape\n    x = x.contiguous()\n    output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n    \n    grid = (batch, channels)\n    adaptive_avg_pool2d_kernel[grid](\n        x, output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height, width\n    )\n    return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = adaptive_avg_pool2d_triton(x)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "f379b24d-79b1-4807-93c7-2a3349a8c457", "generation": 1, "timestamp": 1754589096.6569922, "iteration_found": 4, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.877, "runtime_ms": 10.4, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 9.12, "speed_up": 0.877, "custom_timing": 10.4}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 10.2, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 9.12, "speed_up": 0.894, "custom_timing": 10.2}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 10.2000\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.894, 'custom_timing': 10.2}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 0.0000, correctness: 0.0000, score: -100.0000, stage: no_valid_triton_code, feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 10.2000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.894, 'custom_timing': 10.2}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (10.2000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 9.12, 'speed_up': 0.894, 'custom_timing': 10.2})\n\n\n### Program 2 (Score: -100.0000)\n```python\n@triton.jit\n   def adaptive_avg_pool2d_kernel(\n       input_ptr,\n       output_ptr,\n       input_h,\n       input_w,\n       input_channel_stride, input_height_stride, input_width_stride,\n       output_batch_stride, output_channel_stride,\n       BLOCK_SIZE: tl.constexpr,\n   ):\n       pid_batch = tl.program_id(0)\n       pid_channel = tl.program_id(1)\n\n       # Create a range for the spatial dimensions we are going to sum over.\n       # We will use a 2D grid of blocks? But we are using one program per (batch, channel)\n       # We need to sum over [0, input_h) and [0, input_w)\n\n       # We'll use a 1D loop over the spatial dimensions. We can use a block for each row? But note that the entire channel is summed.\n\n       # Alternatively, we can use a single program to sum the entire channel by having a loop over rows and columns.\n\n       # Offsets for the input for this batch and channel\n       input_batch_offset = pid_batch * input_channel_stride * 0  # Actually, we need the correct strides.\n\n       # Wait, we are given the strides in terms of elements. But note: the input is of shape (batch, channels, height, width)\n       # So the strides are: [channels*height*width, height*width, width, 1] for contiguous.\n\n       # We assume the input is contiguous? We can make it contiguous in the function.\n\n       # The input pointer is the base, then:\n       #   input_ptr + pid_batch * (channels*height*width) + pid_channel * (height*width) + row * width + col\n\n       # We can precompute the base offset for this batch and channel:\n       base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n       total = 0.0\n       for row in range(0, input_h):\n           row_offset = row * input_height_stride\n           for col in range(0, input_w):\n               col_offset = col * input_width_stride\n               offset = base_offset + row_offset + col_offset\n               value = tl.load(input_ptr + offset)\n               total += value\n\n       average = total / (input_h * input_w)\n\n       # Write the average to the output.\n       # Output is of shape (batch, channels, 1, 1) -> so we write at [pid_batch, pid_channel]\n       output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n       tl.store(output_ptr + output_offset, average)\n```\nKey features: Performs well on compiled (0.0000), Performs well on correctness (0.0000), Performs well on score (-100.0000), Performs well on stage (no_valid_triton_code), Performs well on feedback (You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.)\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a DenseNet201 model implemented in PyTorch. The goal is to rewrite the model to use Triton for one of the key operations to improve performance.\n\nHowever, note that the current program is entirely in PyTorch and does not use Triton at all. The task is to replace a part of the model with a Triton kernel to speed up the computation.\n\nLooking at the model, one of the most time-consuming operations is the convolution. But note that the convolutions are 3x3 and 1x1, and we have a dense block structure.\n\nAlternatively, we might consider the adaptive average pooling at the end. However, adaptive average pooling is already very efficient in PyTorch.\n\nBut note the performance metrics: the reference timing is 9.12 ms and our custom timing is 10.2 ms (which is slower). We are currently at speed_up=0.894 (which is less than 1). We need to get at least 5% speedup.\n\nGiven the constraints of the problem, we are allowed to change only the Triton kernel within the EVOLVE-BLOCK. However, the current code does not have any Triton kernel.\n\nTherefore, we must identify an operation that can be accelerated by Triton. One candidate is the convolution operation. However, writing a convolution kernel in Triton is complex and we must be cautious.\n\nAlternatively, note that the dense block involves many concatenations and the convolution operations on the growing feature maps. The convolution in the dense block might be a bottleneck.\n\nBut note: the problem states that we are to generate Triton implementations for the kernelbench files. However, the current program is a full model. We must focus on a specific kernel.\n\nLooking at the provided example, they replaced a per-row quantization with a Triton kernel. Similarly, we can replace a specific operation in the model with a Triton kernel.\n\nGiven the structure of the model, one operation that is repeated many times and might benefit from optimization is the convolution. However, writing an efficient convolution kernel is non-trivial.\n\nAlternatively, we might consider the batch normalization? But batch normalization is already highly optimized.\n\nAnother idea: the forward pass of the DenseBlock involves a loop over layers and concatenation. The concatenation itself is a memory-bound operation and might not be accelerated by Triton. However, the convolution operations within the dense block are the main compute.\n\nBut note: the problem says to generate Triton code for the core function. We are allowed to change only the Triton kernel.\n\nGiven the complexity, let's consider replacing the convolution operation in the dense block with a Triton kernel. However, we must be cautious because the convolution has multiple input and output channels and spatial dimensions.\n\nAlternatively, we might start with a simpler operation: the adaptive average pooling at the end. The adaptive average pooling in the forward pass is:\n\n    x = F.adaptive_avg_pool2d(x, (1, 1))\n\nThis operation reduces the spatial dimensions to 1x1 by taking the average over each channel. We can write a Triton kernel for this.\n\nThe adaptive average pooling operation for a tensor of shape [batch, channels, height, width] reduces to [batch, channels, 1, 1]. We can write a Triton kernel that:\n\n  1. Iterates over batches and channels.\n  2. For each (batch, channel), sums all the values in the [height, width] and divides by (height*width).\n\nThis is an embarrassingly parallel problem over batches and channels.\n\nLet's design the kernel:\n\n  - We will have two program IDs: one for the batch and one for the channel.\n  - Each program will load the entire [height, width] for its (batch, channel) and compute the average.\n\nHowever, note that the height and width can be large (e.g., 7x7 in the last layer). We must use a block-wise reduction.\n\nAlternatively, we can use a loop over rows and columns. Since the last feature map is small (for DenseNet201, the last spatial size is 7x7 for 224x224 input) because of the multiple downsamplings, we can safely do a sequential reduction.\n\nBut note: the problem says we are on H100 with high memory bandwidth. We want to coalesce memory accesses.\n\nWe can do:\n\n  - Each program (for a batch and channel) will load the entire 2D plane. We can use a loop over rows and columns.\n\nBut the number of elements per channel is 7*7=49, which is small. So we can do:\n\n  total = 0.0\n  for h in range(height):\n      for w in range(width):\n          total += input[b, c, h, w]\n\n  output[b, c] = total / (height*width)\n\nWe can write a Triton kernel for this.\n\nHowever, note that the adaptive average pooling in PyTorch is already very efficient. But we are going to try to write a Triton kernel that might be faster by avoiding some overhead.\n\nAlternatively, we might consider that the last feature map is small, so the overhead of launching a kernel per (batch, channel) might be acceptable.\n\nBut note: we are going to have a kernel that has grid size (batch, channels). For batch=10 and channels=1920 (for DenseNet201), that's 19200 blocks. This might be acceptable.\n\nHowever, we must be cautious: the kernel must be efficient for the small reduction.\n\nAlternatively, we can have a 2D grid of blocks that each reduce a portion of the channel and then do a second kernel? But that might be overkill.\n\nGiven the small size (49 elements), we can do:\n\n  - Each program (for a batch and channel) will load the entire 49 elements in a loop and sum.\n\nBut note: we want to use vectorization and avoid uncoalesced memory accesses.\n\nWe can load contiguous elements. The input is in NCHW format, so the last dimension is contiguous. We can have:\n\n  base = batch * (channels * height * width) + channel * (height * width)\n  Then we can load [base : base + height*width] in a loop.\n\nWe can also use vectorized loads? But the number of elements might not be a multiple of the vector size.\n\nAlternatively, we can use a block of threads per (batch, channel) and do a parallel reduction? But that would require multiple threads per block and we would have to synchronize.\n\nHowever, note that the problem says we are to write a Triton kernel. We can write a kernel that uses a single thread per (batch, channel) because the reduction is small.\n\nLet's write the kernel:\n\n  @triton.jit\n  def adaptive_avg_pool2d_kernel(\n      input_ptr,\n      output_ptr,\n      input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n      output_batch_stride, output_channel_stride,\n      input_h, input_w,\n      BLOCK_SIZE: tl.constexpr,  # Not used? Or we can use for the reduction?\n  ):\n      pid_batch = tl.program_id(0)\n      pid_channel = tl.program_id(1)\n\n      # Compute the base pointer for this batch and channel\n      base = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n      total = 0.0\n      for h in range(0, input_h):\n          for w in range(0, input_w):\n              offset = base + h * input_height_stride + w * input_width_stride\n              total += tl.load(input_ptr + offset)\n\n      avg = total / (input_h * input_w)\n\n      # Write to output: output[pid_batch, pid_channel, 0, 0] -> but the output is 4D? Actually, we are going to write to a 2D output? Or the output is [batch, channels]?\n      # Actually, the adaptive_avg_pool2d in PyTorch returns [batch, channels, 1, 1]. Then we do a view to flatten to [batch, channels].\n      # We can write the output as a 2D tensor? But the model expects the output of the adaptive_avg_pool2d to be [batch, channels, 1, 1]. Then we do .view(x.size(0), -1).\n\n      # We are going to write the average to a 2D output tensor? Actually, we can create an output tensor of shape [batch, channels] and then the view is not needed.\n\n      # But the model does: F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n\n      # We can replace both by a kernel that writes a 2D tensor [batch, channels].\n\n      # So we write:\n      output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n      tl.store(output_ptr + output_offset, avg)\n\nBut note: the output of the adaptive_avg_pool2d in the model is immediately flattened. So we can skip the 1x1 and just have a 2D tensor.\n\nWe can change the model to use our custom kernel and then skip the view.\n\nSo in the forward pass of ModelNew:\n\n    # Instead of:\n    #   x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n    # we do:\n    x = adaptive_avg_pool2d_triton(x)\n\nWhere adaptive_avg_pool2d_triton is a function that uses our kernel and returns a 2D tensor [batch, channels].\n\nBut note: the input x is of shape [batch, channels, height, width]. We assume it is contiguous? We can make it contiguous.\n\nWe must also compute the strides. We can get the strides from the tensor.\n\nHowever, note: the input tensor might not be contiguous. We can do:\n\n    x = x.contiguous()\n\nThen we can compute the strides.\n\nBut note: the input tensor after the final batch normalization and ReLU is contiguous? We can ensure that.\n\nLet's write the function:\n\n  def adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n      assert x.dim() == 4\n      batch, channels, height, width = x.shape\n      # We want output of shape [batch, channels]\n      output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n\n      # Strides for the input tensor (contiguous in NCHW format)\n      # strides: [channels*height*width, height*width, width, 1] for contiguous?\n      # But we can get the strides by:\n      input_batch_stride = x.stride(0)\n      input_channel_stride = x.stride(1)\n      input_height_stride = x.stride(2)\n      input_width_stride = x.stride(3)\n\n      # Output strides: [channels, 1] for contiguous output? Actually, we are creating a contiguous output.\n      output_batch_stride = output.stride(0)\n      output_channel_stride = output.stride(1)\n\n      grid = (batch, channels)\n      adaptive_avg_pool2d_kernel[grid](x, output, \n          input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n          output_batch_stride, output_channel_stride,\n          height, width)\n\n      return output\n\nBut note: the kernel is launched with a grid of (batch, channels). For each (b, c) we have one program.\n\nWe must check if the kernel compiles and is correct.\n\nHowever, note that the current adaptive average pooling in PyTorch is very efficient and the tensor is small. We might not get a speedup. But we are going to try.\n\nAlternatively, we could try to optimize the convolution in the dense block. But that is more complex.\n\nGiven the instructions, we are to make small iterative changes. We start by replacing the adaptive average pooling.\n\nBut note: the problem says that we must not change the function signatures of the model. We are changing the internal implementation of the adaptive average pooling to a 2D output. The model then uses that 2D tensor for the linear layer. This is equivalent to the original which did:\n\n    x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n\nSo it should be the same.\n\nLet's write the Triton kernel and the helper function. We must put the Triton kernel inside the EVOLVE-BLOCK.\n\nWe note that the current program does not import Triton. We must add:\n\n    import triton\n    import triton.language as tl\n\nBut note: the problem says to only change the Triton kernel source within the block. We are allowed to add imports and helper functions as long as they are within the block.\n\nWe are going to:\n\n  1. Add the Triton imports at the top of the block.\n  2. Define the Triton kernel.\n  3. Define the helper function `adaptive_avg_pool2d_triton`.\n  4. Change the forward pass of ModelNew to use the helper function.\n\nBut note: the original program does not use Triton, so we are adding a new dependency. The evaluation environment has Triton installed.\n\nLet's write the code.\n\nHowever, note the constraints:\n\n  \u274c NEVER change @triton.jit function signatures or parameter names -> We are defining a new one.\n  \u274c NEVER modify grid configuration, output tensor shapes, or PID logic -> We are defining the grid as (batch, channels) and the output shape is (batch, channels). This is acceptable because we are replacing the adaptive pooling and the view.\n\nBut note: the output tensor shape of the adaptive_avg_pool2d_triton is (batch, channels) which is exactly the same as the original after the view.\n\nWe are not changing the output of the model.\n\nNow, we must be cautious about the performance. The original adaptive average pooling and view might be very fast. But we hope that the Triton kernel will be faster by avoiding the intermediate 4D tensor and the view.\n\nAlternatively, the Triton kernel might be slower because of the overhead of launching a kernel with many blocks and the sequential reduction. But we are going to try.\n\nWe are going to replace:\n\n    x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n\nwith:\n\n    x = adaptive_avg_pool2d_triton(x)\n\nWe must define the helper function and the kernel.\n\nLet's do it.\n\nNote: We are to return the entire code block with the EVOLVE-BLOCK markers.\n\nWe are going to insert the Triton kernel and the helper function inside the ModelNew class? No, we will put them outside, but inside the EVOLVE-BLOCK.\n\nWe must be cautious about the indentation.\n\nAlternatively, we can put the kernel and the helper function in the global scope of the block.\n\nWe are going to:\n\n  - Add the imports for triton and tl at the top of the block (after the existing imports).\n  - Define the kernel and the helper function after the imports and before the class definitions.\n\nBut note: the class definitions are the same as before. We are only changing the forward pass of ModelNew.\n\nLet's write the code accordingly.\n\nHowever, note that the kernel must be compiled only once. We can define it as a top-level function.\n\nWe are going to restructure the code:\n\n  import ...\n  import triton\n  import triton.language as tl\n\n  # Define the Triton kernel\n  @triton.jit\n  def ... \n\n  # Define the helper function\n  def adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n      ...\n\n  # Then the existing classes: DenseBlock, TransitionLayer, ModelNew.\n\n  Then in ModelNew.forward, replace the line.\n\nBut note: the current ModelNew.forward does:\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n        x = self.classifier(x)\n\nWe change to:\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = adaptive_avg_pool2d_triton(x)   # This returns [batch, channels]\n        x = self.classifier(x)\n\nWe are removing the view because it's not needed.\n\nLet's code accordingly.\n\nWe must also note that the kernel must handle non-contiguous tensors? We are making the input contiguous in the helper function.\n\nWe will do:\n\n    x = x.contiguous()\n\nin the helper function to ensure contiguous memory.\n\nNow, we write the kernel and the helper function.\n\nBut note: the kernel uses the strides. We are using the strides from the tensor, which for a contiguous tensor are:\n\n    input_batch_stride = channels * height * width\n    input_channel_stride = height * width\n    input_height_stride = width\n    input_width_stride = 1\n\nBut we get them from the tensor.\n\nWe assume the tensor is contiguous after calling `contiguous()`.\n\nWe are going to write the kernel and the helper function.\n\nWe must be cautious: the kernel might not be faster for small tensors. But we are going to try.\n\nAlternatively, we can tune the kernel by using a different number of warps? But the reduction is small, so we can use one thread per (batch, channel). We set num_warps=1.\n\nWe can also use the autotune? Not necessary for such a simple kernel.\n\nWe write:\n\n  @triton.jit\n  def adaptive_avg_pool2d_kernel(\n      input_ptr,\n      output_ptr,\n      input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n      output_batch_stride, output_channel_stride,\n      input_h, input_w,\n      BLOCK_SIZE: tl.constexpr,   # We are not using block for reduction? We can remove?\n  ):\n      pid_batch = tl.program_id(0)\n      pid_channel = tl.program_id(1)\n\n      base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n      total = 0.0\n      for h in range(0, input_h):\n          h_offset = h * input_height_stride\n          for w in range(0, input_w):\n              w_offset = w * input_width_stride\n              offset = base_offset + h_offset + w_offset\n              total += tl.load(input_ptr + offset)\n\n      avg = total / (input_h * input_w)\n      out_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n      tl.store(output_ptr + out_offset, avg)\n\nBut note: the kernel does not use BLOCK_SIZE. We can remove it? But the decorator requires a constexpr. We can leave it and ignore.\n\nAlternatively, we can remove the BLOCK_SIZE parameter? But the problem says not to change the function signatures? Actually, we are defining a new kernel. We can define it as we wish.\n\nWe remove BLOCK_SIZE because we are not using it.\n\nBut note: the problem says \"NEVER change @triton.jit function signatures or parameter names\" for the existing ones. We are creating a new one.\n\nWe'll remove it.\n\nWe write:\n\n  @triton.jit\n  def adaptive_avg_pool2d_kernel(\n      input_ptr,\n      output_ptr,\n      input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n      output_batch_stride, output_channel_stride,\n      input_h, input_w,\n  ):\n\nBut wait, Triton requires that we have at least one tl.constexpr. We can mark input_h and input_w as constexpr? Because they are known at compile time? But they are not necessarily constants.\n\nAlternatively, we can mark them as regular arguments. But note: the kernel will be compiled for each different input_h and input_w? That might lead to long compilation times.\n\nWe can mark them as constexpr? But they are not compile-time constants? Actually, they are known at launch time.\n\nWe can pass them as regular integers and hope that Triton can handle them.\n\nAlternatively, we can use tl.constexpr for input_h and input_w? Then we must have them as constants at compile time. But they are not.\n\nSo we leave them as regular arguments.\n\nNow, we write the helper function:\n\n  def adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n      assert x.dim() == 4\n      batch, channels, height, width = x.shape\n      # Make sure the input is contiguous\n      x = x.contiguous()\n      output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n\n      # Strides for the input tensor (after contiguous, it is in NCHW format)\n      input_batch_stride = x.stride(0)\n      input_channel_stride = x.stride(1)\n      input_height_stride = x.stride(2)\n      input_width_stride = x.stride(3)\n\n      # Strides for the output tensor: it's 2D contiguous\n      output_batch_stride = output.stride(0)\n      output_channel_stride = output.stride(1)\n\n      grid = (batch, channels)\n      adaptive_avg_pool2d_kernel[grid](\n          x, output,\n          input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n          output_batch_stride, output_channel_stride,\n          height, width\n      )\n      return output\n\nWe then replace the line in ModelNew.forward.\n\nLet's code accordingly.\n\nBut note: the kernel might be slow because of the double loop. We can try to use a single loop? We can compute the total number of elements per channel and then do one loop.\n\nWe change:\n\n      total = 0.0\n      num_elements = input_h * input_w\n      for i in range(0, num_elements):\n          # How to compute the offset? We have to account for the layout.\n          # We know that the tensor is contiguous, so the elements are in row-major order.\n          # We can compute the offset as base_offset + i, but note: the strides might not be 1 for the channel? \n          # Actually, the contiguous tensor has the last dimension (width) as stride 1, then the next is width, then height*width, then channels*height*width.\n          # But we have the base_offset which is the start of the channel, then we add i (because the elements in the channel are contiguous?).\n\n          offset = base_offset + i\n          total += tl.load(input_ptr + offset)\n\nBut wait: the input might not be contiguous in the channel? We made it contiguous. And the contiguous format for NCHW is:\n\n  [batch0, channel0, ... , batch0, channel0, last element], then batch0, channel1, ...]\n\nSo the entire channel is contiguous? Actually, no: the tensor is 4D contiguous, meaning that the memory layout is:\n\n  batch0, channel0: all elements in row0, then row1, ... then batch0, channel1: ...\n\nSo the entire channel is contiguous? Yes, because the stride for channel is height*width.\n\nTherefore, we can do:\n\n      total = 0.0\n      num_elements = input_h * input_w\n      for i in range(0, num_elements):\n          offset = base_offset + i\n          total += tl.load(input_ptr + offset)\n\nBut note: the offset i runs from 0 to num_elements-1. This is one loop instead of two. It might be more efficient? Or the same.\n\nWe can do that to simplify.\n\nWe change the kernel accordingly.\n\nBut note: we must be cautious because the base_offset is computed as:\n\n      base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\nand then we add i. This is correct because the entire channel is a contiguous block of size input_h * input_w.\n\nSo the kernel becomes:\n\n  @triton.jit\n  def adaptive_avg_pool2d_kernel(\n      input_ptr,\n      output_ptr,\n      input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n      output_batch_stride, output_channel_stride,\n      input_h, input_w,\n  ):\n      pid_batch = tl.program_id(0)\n      pid_channel = tl.program_id(1)\n\n      base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n      num_elements = input_h * input_w\n      total = 0.0\n      for i in range(0, num_elements):\n          offset = base_offset + i\n          total += tl.load(input_ptr + offset)\n\n      avg = total / num_elements\n      out_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n      tl.store(output_ptr + out_offset, avg)\n\nThis is simpler.\n\nWe are ready to write the code.\n\nBut note: the problem says to only change the Triton kernel source within the block. We are changing multiple parts: adding the kernel, the helper function, and the forward pass.\n\nWe are going to output the entire block with the changes.\n\nLet's write the code.\n\nIMPORTANT: We must not change anything outside the EVOLVE-BLOCK. We are only replacing the code inside the block.\n\nWe are going to:\n\n  1. Add the Triton imports.\n  2. Define the kernel and the helper function.\n  3. Change the ModelNew.forward to use the helper function.\n\nWe output the entire block.\n\nNote: The original block had the classes and the testing code. We are only changing the ModelNew.forward and adding the Triton code.\n\nLet's do it.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool2d_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    input_h, input_w,\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    num_elements = input_h * input_w\n    \n    total = 0.0\n    for i in range(0, num_elements):\n        offset = base_offset + i\n        total += tl.load(input_ptr + offset)\n    \n    avg = total / num_elements\n    out_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + out_offset, avg)\n\ndef adaptive_avg_pool2d_triton(x: torch.Tensor) -> torch.Tensor:\n    assert x.dim() == 4\n    batch, channels, height, width = x.shape\n    x = x.contiguous()\n    output = torch.empty((batch, channels), device=x.device, dtype=x.dtype)\n    \n    grid = (batch, channels)\n    adaptive_avg_pool2d_kernel[grid](\n        x, output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height, width\n    )\n    return output\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):\n        \"\"\"\n        :param num_layers: The number of layers in the dense block\n        :param num_input_features: The number of input feature maps\n        :param growth_rate: The growth rate for the dense block (new features added per layer)\n        \"\"\"\n        super(DenseBlock, self).__init__()\n        layers = []\n        for i in range(num_layers):\n            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))\n        self.layers = nn.ModuleList(layers)\n\n    def _make_layer(self, in_features: int, growth_rate: int):\n        \"\"\"\n        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.\n        \"\"\"\n        return nn.Sequential(\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),\n            nn.Dropout(0.0)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)\n        \"\"\"\n        features = [x]\n        for layer in self.layers:\n            new_feature = layer(x)\n            features.append(new_feature)\n            x = torch.cat(features, 1)  # Concatenate along channel axis\n        return x\n\nclass TransitionLayer(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        \"\"\"\n        :param num_input_features: The number of input feature maps\n        :param num_output_features: The number of output feature maps\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.transition = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),\n            nn.AvgPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, num_input_features, height, width)\n        :return: Downsampled tensor with reduced number of feature maps\n        \"\"\"\n        return self.transition(x)\n\nclass ModelNew(nn.Module):\n    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):\n        \"\"\"\n        :param growth_rate: The growth rate of the DenseNet (new features added per layer)\n        :param num_classes: The number of output classes for classification\n        \"\"\"\n        super(ModelNew, self).__init__()\n\n        # Initial convolution and pooling\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n        # Each dense block is followed by a transition layer, except the last one\n        num_features = 64\n        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201\n\n        self.dense_blocks = nn.ModuleList()\n        self.transition_layers = nn.ModuleList()\n\n        for i, num_layers in enumerate(block_layers):\n            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)\n            self.dense_blocks.append(block)\n            num_features = num_features + num_layers * growth_rate\n\n            if i != len(block_layers) - 1:\n                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)\n                self.transition_layers.append(transition)\n                num_features = num_features // 2\n\n        # Final batch norm and classifier\n        self.final_bn = nn.BatchNorm2d(num_features)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        :param x: Input tensor of shape (batch_size, 3, height, width)\n        :return: Output tensor of shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n\n        for i, block in enumerate(self.dense_blocks):\n            x = block(x)\n            if i != len(self.dense_blocks) - 1:\n                x = self.transition_layers[i](x)\n\n        x = self.final_bn(x)\n        x = F.relu(x, inplace=True)\n        x = adaptive_avg_pool2d_triton(x)\n        x = self.classifier(x)\n        return x\n\n# Testing the DenseNet201 model\nbatch_size = 10\nnum_classes = 10\nheight, width = 224, 224  # Standard input size for DenseNet\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [32, num_classes]\n\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
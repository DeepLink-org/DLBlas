{"id": "565f3f2b-136d-4bf7-bc63-5512135bd9c7", "code": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 32}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 64}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 128}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 32}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 64}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 128}, num_warps=8),\n    ],\n    key=['input_size', 'hidden_size'],\n)\n@triton.jit\ndef fused_rnn_kernel(\n    x_ptr,\n    h_prev_ptr,\n    W_x_ptr,\n    W_h_ptr,\n    b_ptr,\n    h_ptr,\n    input_size,\n    hidden_size,\n    BLOCK_HIDDEN: tl.constexpr,\n    BLOCK_INPUT: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    batch_id = pid // tl.cdiv(hidden_size, BLOCK_HIDDEN)\n    block_id = pid % tl.cdiv(hidden_size, BLOCK_HIDDEN)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_HIDDEN,), dtype=tl.float32)\n    \n    # Compute input part: x @ W_x\n    for i in range(0, input_size, BLOCK_INPUT):\n        i_offs = i + tl.arange(0, BLOCK_INPUT)\n        mask_i = i_offs < input_size\n        \n        # Load input block\n        x_val = tl.load(x_ptr + batch_id * input_size + i_offs, mask=mask_i, other=0.0)\n        \n        # Load W_x block\n        w_offs = i_offs[:, None] * hidden_size + block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)[None, :]\n        mask_wx = mask_i[:, None] & (tl.arange(0, BLOCK_HIDDEN)[None, :] < (hidden_size - block_id * BLOCK_HIDDEN))\n        W_x_val = tl.load(W_x_ptr + w_offs, mask=mask_wx, other=0.0)\n        \n        # Accumulate\n        acc += tl.sum(x_val[:, None] * W_x_val, axis=0)\n    \n    # Compute hidden part: h_prev @ W_h\n    for j in range(0, hidden_size, BLOCK_INPUT):\n        j_offs = j + tl.arange(0, BLOCK_INPUT)\n        mask_j = j_offs < hidden_size\n        \n        # Load hidden state block\n        h_prev_val = tl.load(h_prev_ptr + batch_id * hidden_size + j_offs, mask=mask_j, other=0.0)\n        \n        # Load W_h block\n        w_offs = j_offs[:, None] * hidden_size + block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)[None, :]\n        mask_wh = mask_j[:, None] & (tl.arange(0, BLOCK_HIDDEN)[None, :] < (hidden_size - block_id * BLOCK_HIDDEN))\n        W_h_val = tl.load(W_h_ptr + w_offs, mask=mask_wh, other=0.0)\n        \n        # Accumulate\n        acc += tl.sum(h_prev_val[:, None] * W_h_val, axis=0)\n    \n    # Add bias\n    bias_offs = block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)\n    mask_bias = bias_offs < hidden_size\n    bias = tl.load(b_ptr + bias_offs, mask=mask_bias, other=0.0)\n    acc += bias\n    \n    # Apply activation (tanh)\n    output = tl.tanh(acc)\n    \n    # Store result\n    output_offs = batch_id * hidden_size + block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)\n    tl.store(h_ptr + output_offs, output, mask=mask_bias)\n\ndef fused_rnn(x, h_prev, W_x, W_h, b):\n    batch_size, input_size = x.shape\n    _, hidden_size = h_prev.shape\n    \n    # Allocate output tensor\n    h = torch.empty_like(h_prev)\n    \n    # Grid = (batch_size * num_hidden_blocks,)\n    grid = (batch_size * triton.cdiv(hidden_size, 128),)\n    \n    # Launch kernel\n    fused_rnn_kernel[grid](\n        x, h_prev, W_x, W_h, b, h,\n        input_size, hidden_size,\n        BLOCK_HIDDEN=128,  # Default value for autotune\n        BLOCK_INPUT=32     # Default value for autotune\n    )\n    return h", "language": "python", "parent_id": "a4ee8b7c-1fe1-429f-89c1-2929ee44449f", "generation": 2, "timestamp": 1754595125.9612038, "iteration_found": 10, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.826, "runtime_ms": 0.102, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0843, "speed_up": 0.826, "custom_timing": 0.102}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 0.0000\n- correctness: 0.0000\n- score: -100.0000\n- stage: no_valid_triton_code\n- feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 0.0000, correctness: 0.0000, score: -100.0000, stage: no_valid_triton_code, feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: -100.0000)\n```python\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 32}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 64}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 128}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 32}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 64}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 128}, num_warps=8),\n    ],\n    key=['input_size', 'hidden_size'],\n)\n@triton.jit\ndef fused_rnn_kernel(...):\n    ...\n```\nKey features: Performs well on compiled (0.0000), Performs well on correctness (0.0000), Performs well on score (-100.0000), Performs well on stage (no_valid_triton_code), Performs well on feedback (You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.)\n\n\n\n\n# Current Program\n```python\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 32}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 64}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 128}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 32}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 64}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 128}, num_warps=8),\n    ],\n    key=['input_size', 'hidden_size'],\n)\n@triton.jit\ndef fused_rnn_kernel(...):\n    ...\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a task to rewrite a Triton kernel for a fused RNN operation. However, the provided current program is incomplete (it only shows the autotune decorator and the function signature). We must have a complete kernel to fix the compilation and correctness.\n\nGiven the feedback: \"You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\"\n\nThis indicates that the previous code was missing the necessary imports and likely the body of the kernel.\n\nWe are also told that the current stage is \"no_valid_triton_code\", so our first goal is to write a Triton kernel that compiles and passes the correctness test.\n\nWe don't have the original PyTorch reference code, but we can infer from the kernel function name and parameters that it is a fused RNN kernel.\n\nLet's assume the kernel is for a simple RNN cell: \n    h = tanh(x @ W_x + h_prev @ W_h + b)\n\nWe are going to write a basic Triton kernel for this operation. We will focus on getting a correct and compiling version first.\n\nSteps:\n1. We must import triton and triton.language.\n2. We must define the kernel function with the appropriate parameters. Since the signature is not fully provided, we have to infer.\n\nLooking at the autotune config, we see two block dimensions: BLOCK_HIDDEN and BLOCK_INPUT. This suggests that we are tiling over the hidden and input dimensions.\n\nWe assume the kernel has the following parameters:\n    x_ptr: input tensor pointer (shape: [batch_size, input_size])\n    h_prev_ptr: previous hidden state pointer (shape: [batch_size, hidden_size])\n    W_x_ptr: weight matrix for input (shape: [input_size, hidden_size])\n    W_h_ptr: weight matrix for hidden (shape: [hidden_size, hidden_size])\n    b_ptr: bias vector (shape: [hidden_size])\n    h_ptr: output hidden state (shape: [batch_size, hidden_size])\n    input_size: int\n    hidden_size: int\n\nWe'll use two program IDs: one for the batch and one for the hidden dimension? But note: the autotune key is ['input_size', 'hidden_size'], and we have two block sizes.\n\nAlternatively, we can have a 2D grid:\n    grid = (triton.cdiv(hidden_size, BLOCK_HIDDEN), batch_size)\n\nBut note: the kernel might be designed to compute an entire row of the hidden state for a batch element.\n\nAlternatively, we can have a 1D grid over batch and then within the kernel loop over hidden in blocks? But the autotune has two block sizes.\n\nLet me propose:\n\nWe'll have the grid be (batch_size,). Each program will compute one row of the output hidden state.\n\nBut the hidden state has dimension hidden_size, which we will compute in blocks of BLOCK_HIDDEN.\n\nHowever, note that the input is of size input_size, and we have BLOCK_INPUT. This suggests we might also be tiling the input dimension.\n\nSo we are going to do:\n\nFor each batch element (pid_batch), we compute the hidden state vector. We break the hidden vector into blocks of size BLOCK_HIDDEN. For each block in the hidden dimension, we accumulate the dot product of the input with the corresponding block of W_x and the previous hidden state with the corresponding block of W_h.\n\nWe can use two nested loops: one over input blocks for W_x and one over hidden blocks for W_h? Actually, note that the operation is:\n\n    h_i = tanh( (x @ W_x)_i + (h_prev @ W_h)_i + b_i )\n\nWe can compute each element of h independently? But that would be inefficient because the dot products require reduction.\n\nAlternatively, we break the hidden dimension into blocks of BLOCK_HIDDEN, and for each block we compute the entire dot product for that block.\n\nSo:\n\n    pid = tl.program_id(0)  # we'll have batch_size * (hidden_size // BLOCK_HIDDEN) programs\n    pid_batch = pid // (hidden_size // BLOCK_HIDDEN)\n    pid_hidden_block = pid % (hidden_size // BLOCK_HIDDEN)\n\nBut note: we also have to break the input dimension? Because the input vector is of size input_size, and we have BLOCK_INPUT.\n\nWe can break the input dimension into blocks of BLOCK_INPUT and then do a reduction.\n\nAlternatively, we can have a 2D grid: one dimension for the batch and one for the hidden blocks.\n\nHowever, the autotune config does not specify a grid, so we are free to design.\n\nGiven the constraints, let me design a 1D grid over the batch and hidden blocks. The grid will be (batch_size * (ceil(hidden_size / BLOCK_HIDDEN)), ).\n\nWe will compute a block of BLOCK_HIDDEN elements of the hidden state for one batch.\n\nWe must also break the input dimension into blocks of BLOCK_INPUT to compute the dot product with W_x.\n\nSimilarly, for the hidden state, we break the previous hidden state into blocks of BLOCK_HIDDEN (or the same as the current block?) for W_h.\n\nBut note: W_h is [hidden_size, hidden_size]. So for the current hidden block (of size BLOCK_HIDDEN in the output), we need to multiply the entire previous hidden state (length hidden_size) with a block of W_h of size [hidden_size, BLOCK_HIDDEN]. We can break that into blocks too.\n\nThis becomes complex. For the sake of getting a compiling and correct version, we can do:\n\n    For the current hidden block (of size BLOCK_HIDDEN) for a given batch, we compute:\n        x @ W_x_block: where W_x_block is [input_size, BLOCK_HIDDEN] (the block corresponding to the current hidden block)\n        h_prev @ W_h_block: where W_h_block is [hidden_size, BLOCK_HIDDEN] (the block corresponding to the current hidden block)\n        and then add bias_block.\n\nBut note: the entire x vector is used for the current hidden block, and the entire h_prev vector is used. So we have to do two separate reductions.\n\nWe can do:\n\n    Initialize accumulator for x part and h part to zero.\n\n    Loop over input blocks (each of size BLOCK_INPUT) for the x part:\n        x_block = load x[pid_batch, i:i+BLOCK_INPUT]\n        W_x_block = load W_x[i:i+BLOCK_INPUT, pid_hidden_block * BLOCK_HIDDEN : (pid_hidden_block+1)*BLOCK_HIDDEN]\n        accumulator_x += dot(x_block, W_x_block)\n\n    Similarly, loop over hidden blocks (each of size BLOCK_HIDDEN) for the h part? But note: the previous hidden state is of length hidden_size, and we are multiplying with a block of W_h that is [hidden_size, BLOCK_HIDDEN]. So we can break the hidden_size dimension of the previous hidden state into blocks of, say, BLOCK_HIDDEN? Actually, we can use the same BLOCK_HIDDEN for the reduction dimension? But note: the previous hidden state is contiguous and we can use any block size. We'll use the same BLOCK_INPUT for consistency? Actually, we don't have a block size for that. Let me call it BLOCK_REDUCE for the reduction dimension? But we don't have that in the autotune.\n\nAlternatively, we can use the same BLOCK_INPUT for the reduction in both? Or we can use a different one? Since we are autotuning over BLOCK_INPUT and BLOCK_HIDDEN, we can use BLOCK_INPUT for the input reduction and BLOCK_HIDDEN for the hidden reduction? Actually, that might be confusing.\n\nGiven the complexity, and since we are in the first phase (compilation pass), we can do a simpler version: without tiling the reduction dimensions. But that would require the entire input and hidden state to fit in the local memory, which is not practical for large sizes.\n\nTherefore, we must tile. We'll use the same BLOCK_INPUT for tiling the input dimension and for tiling the hidden state dimension? Actually, note: the hidden state dimension for the previous state is of length hidden_size, which might be large. We can break it into blocks of BLOCK_INPUT? Or we can use BLOCK_HIDDEN for the hidden state reduction? But that might not be efficient.\n\nAlternatively, we can break the reduction dimensions arbitrarily? We can use a fixed block size for the reduction, but we have two block sizes: one for the input reduction and one for the hidden reduction? We don't have two different block sizes for reduction. So we can use BLOCK_INPUT for both? That is, we break the input dimension and the hidden dimension (for the previous state) into blocks of BLOCK_INPUT.\n\nBut note: the hidden dimension might be very large and BLOCK_INPUT is tuned for the input. This might not be optimal.\n\nGiven the time, and since we are just trying to get a compiling version, we will do:\n\n    For the x part: we break the input dimension into blocks of BLOCK_INPUT.\n    For the h part: we break the hidden dimension (the previous state) into blocks of BLOCK_HIDDEN? But note: the weight matrix W_h is [hidden_size, hidden_size]. The block we are loading for the current output hidden block is of size [hidden_size, BLOCK_HIDDEN]. So we break the hidden_size dimension (the reduction dimension) into blocks of BLOCK_HIDDEN? That would be consistent.\n\nHowever, we have two different reduction block sizes? We can do:\n\n    For the x part: reduction over input_size in blocks of BLOCK_INPUT.\n    For the h part: reduction over hidden_size in blocks of BLOCK_HIDDEN.\n\nThis is acceptable.\n\nWe'll write:\n\n    pid = tl.program_id(0)\n    pid_batch = pid // (hidden_blocks)\n    pid_hidden_block = pid % hidden_blocks\n\n    hidden_blocks = tl.cdiv(hidden_size, BLOCK_HIDDEN)\n\n    # Initialize the accumulator for the linear layer (x part and h part) and bias.\n    acc_x = tl.zeros((BLOCK_HIDDEN,), dtype=tl.float32)\n    acc_h = tl.zeros((BLOCK_HIDDEN,), dtype=tl.float32)\n\n    # Compute the x part: x[pid_batch, :] @ W_x[:, pid_hidden_block*BLOCK_HIDDEN : ...]\n    # We break the input dimension into blocks.\n    input_blocks = tl.cdiv(input_size, BLOCK_INPUT)\n    for i in range(input_blocks):\n        # Load a block of the input\n        offs_input = i * BLOCK_INPUT + tl.arange(0, BLOCK_INPUT)\n        input_mask = offs_input < input_size\n        x = tl.load(x_ptr + pid_batch * input_size + offs_input, mask=input_mask, other=0.0)\n\n        # Load a block of W_x: [BLOCK_INPUT, BLOCK_HIDDEN]\n        offs_W_x_row = offs_input\n        offs_W_x_col = pid_hidden_block * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)\n        W_x_block = tl.load(W_x_ptr + offs_W_x_row[:, None] * hidden_size + offs_W_x_col[None, :],\n                           mask=input_mask[:, None] & (offs_W_x_col[None, :] < hidden_size),\n                           other=0.0)\n\n        # Compute the dot product for this block\n        acc_x += tl.sum(x[:, None] * W_x_block, axis=0)\n\n    # Similarly, for the h part: h_prev[pid_batch, :] @ W_h[:, pid_hidden_block*BLOCK_HIDDEN : ...]\n    # We break the previous hidden state dimension (hidden_size) into blocks of BLOCK_HIDDEN.\n    hidden_reduce_blocks = tl.cdiv(hidden_size, BLOCK_HIDDEN)\n    for i in range(hidden_reduce_blocks):\n        offs_hidden = i * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)\n        hidden_mask = offs_hidden < hidden_size\n        h_prev = tl.load(h_prev_ptr + pid_batch * hidden_size + offs_hidden, mask=hidden_mask, other=0.0)\n\n        # Load a block of W_h: [BLOCK_HIDDEN, BLOCK_HIDDEN] for the current output block\n        offs_W_h_row = offs_hidden\n        offs_W_h_col = pid_hidden_block * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)\n        W_h_block = tl.load(W_h_ptr + offs_W_h_row[:, None] * hidden_size + offs_W_h_col[None, :],\n                           mask=hidden_mask[:, None] & (offs_W_h_col[None, :] < hidden_size),\n                           other=0.0)\n\n        acc_h += tl.sum(h_prev[:, None] * W_h_block, axis=0)\n\n    # Load bias for this block\n    offs_bias = pid_hidden_block * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)\n    bias_mask = offs_bias < hidden_size\n    bias = tl.load(b_ptr + offs_bias, mask=bias_mask, other=0.0)\n\n    # Total\n    total = acc_x + acc_h + bias\n    h_block = tl.tanh(total)\n\n    # Write back\n    tl.store(h_ptr + pid_batch * hidden_size + offs_bias, h_block, mask=bias_mask)\n\nHowever, note: the bias is of size hidden_size, so we are loading a block of it.\n\nBut wait: the kernel parameters must match the call. We don't know how the kernel is called. We assume the above.\n\nBut note: the kernel function name is fused_rnn_kernel and we are using autotune. We must also define the grid in the launcher function.\n\nWe are not asked to write the launcher, only the kernel? Actually, the problem says to return the complete updated code containing the EVOLVE-BLOCK. But note the context: we are only allowed to change the Triton kernel source within the block. However, the current code does not show the kernel body. So we are writing it from scratch.\n\nBut note: the previous code had the autotune decorator and the function signature with ... which we must replace.\n\nWe must also include the necessary imports.\n\nLet me write the complete kernel code including imports and the launcher? But the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" and we are to return the complete updated code containing the EVOLVE-BLOCK.\n\nLooking at the example, the example included the entire function including the launcher. But note: the problem says the other code is locked. So we are only to change the kernel? Actually, the example showed a complete program.\n\nGiven the ambiguity, and since the feedback says the keyword 'import triton' is not found, we must include the imports.\n\nWe are going to write:\n\n    import triton\n    import triton.language as tl\n\n    @triton.autotune(...)\n    @triton.jit\n    def fused_rnn_kernel(...):\n        ... # the kernel body\n\n    def fused_rnn(...):\n        ... # the launcher\n\nBut note: the problem says \"Name the output structure class as ModelNew\". That is from the example? Actually, in the example they defined a function. But in our case, we are writing a kernel and a launcher.\n\nAlternatively, the problem might be that the previous code did not have the imports. So we must add them.\n\nWe are going to write the complete code that would be placed in the file. Since the problem says \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker\", we must include the entire code block that we are allowed to change.\n\nHowever, note that the previous code only showed the autotune and the kernel function without body. We are going to write the body and also the necessary imports.\n\nLet me write the code accordingly.\n\nImportant: We are in the first phase, so we focus on compilation and correctness. Performance comes later.\n\nWe'll write a simple version that might not be optimal but compiles and is correct.\n\nWe assume the following parameters for the kernel:\n\n    x_ptr, h_prev_ptr, W_x_ptr, W_h_ptr, b_ptr, h_ptr, input_size, hidden_size, BLOCK_HIDDEN: tl.constexpr, BLOCK_INPUT: tl.constexpr\n\nWe'll use the grid as (batch_size * (ceil(hidden_size / BLOCK_HIDDEN),).\n\nWe must be cautious of the memory layout: row-major.\n\nBut note: the weight matrices are stored in row-major? \n    W_x: [input_size, hidden_size] -> row i is the weights for input dimension i for all hidden units.\n    W_h: [hidden_size, hidden_size] -> row i is the weights for the i-th hidden unit of the previous state for all current hidden units.\n\nSo we are accessing:\n    W_x[input_index, hidden_index] -> at offset input_index * hidden_size + hidden_index\n    W_h[prev_hidden_index, current_hidden_index] -> at offset prev_hidden_index * hidden_size + current_hidden_index\n\nNow, let's code accordingly.\n\nWe must also note: the input_size and hidden_size might not be multiples of the block sizes, so we use masks.\n\nWe are going to write the kernel and the launcher function.\n\nBut note: the problem says \"the other code is locked\", meaning we cannot change the launcher? Actually, we are generating the entire Triton implementation. We have to assume that the launcher is part of the locked code? But the previous code only showed the kernel. \n\nGiven the feedback, I think we must provide the entire Triton implementation including the launcher.\n\nLet me write the complete code.\n\nNote: We are not given the exact signature of the launcher, so we have to design it.\n\nWe assume the launcher function is called like the example:\n\n    def fused_rnn(x: torch.Tensor, h_prev: torch.Tensor, W_x: torch.Tensor, W_h: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n        ...\n\nWe'll write accordingly.\n\nWe'll do:\n\n    import math\n    import torch\n    import triton\n    import triton.language as tl\n\n    @triton.autotune(...)\n    @triton.jit\n    def fused_rnn_kernel(...):\n        ... # body\n\n    def fused_rnn(x, h_prev, W_x, W_h, b):\n        # Check shapes\n        batch_size, input_size = x.shape\n        _, hidden_size = h_prev.shape\n        assert W_x.shape == (input_size, hidden_size)\n        assert W_h.shape == (hidden_size, hidden_size)\n        assert b.shape == (hidden_size,)\n        h = torch.empty((batch_size, hidden_size), device=x.device, dtype=x.dtype)\n\n        # We'll use 1D grid: batch_size * (ceil(hidden_size/BLOCK_HIDDEN))\n        hidden_blocks = (hidden_size + BLOCK_HIDDEN - 1) // BLOCK_HIDDEN\n        grid = (batch_size * hidden_blocks,)\n\n        # Launch kernel\n        fused_rnn_kernel[grid](x, h_prev, W_x, W_h, b, h, input_size, hidden_size, ...)\n\n        return h\n\nBut note: the kernel parameters must include the block sizes as constexpr. We pass them as kernel arguments? Actually, we can pass them as triton.constexpr. But the autotune is over BLOCK_HIDDEN and BLOCK_INPUT, so they are provided by the autotune config.\n\nIn the kernel, we define BLOCK_HIDDEN and BLOCK_INPUT as tl.constexpr.\n\nWe'll write the kernel accordingly.\n\nLet's code.\n\nWe are going to write a version that compiles and is correct (within tolerance). We'll use float32 for accumulation.\n\nWe note: the tanh activation.\n\nWe'll do:\n\n    total = acc_x + acc_h + bias\n    h_block = tl.tanh(total)\n\nNow, the complete code:\n</think>\n```python\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 32}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 64}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 128}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 32}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 64}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 128}, num_warps=8),\n    ],\n    key=['input_size', 'hidden_size'],\n)\n@triton.jit\ndef fused_rnn_kernel(\n    x_ptr,\n    h_prev_ptr,\n    W_x_ptr,\n    W_h_ptr,\n    b_ptr,\n    h_ptr,\n    input_size,\n    hidden_size,\n    BLOCK_HIDDEN: tl.constexpr,\n    BLOCK_INPUT: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    batch_id = pid // tl.cdiv(hidden_size, BLOCK_HIDDEN)\n    block_id = pid % tl.cdiv(hidden_size, BLOCK_HIDDEN)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_HIDDEN,), dtype=tl.float32)\n    \n    # Compute input part: x @ W_x\n    for i in range(0, input_size, BLOCK_INPUT):\n        i_offs = i + tl.arange(0, BLOCK_INPUT)\n        mask_i = i_offs < input_size\n        \n        # Load input block\n        x_val = tl.load(x_ptr + batch_id * input_size + i_offs, mask=mask_i, other=0.0)\n        \n        # Load W_x block\n        w_offs = i_offs[:, None] * hidden_size + block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)[None, :]\n        mask_wx = mask_i[:, None] & (tl.arange(0, BLOCK_HIDDEN)[None, :] < (hidden_size - block_id * BLOCK_HIDDEN))\n        W_x_val = tl.load(W_x_ptr + w_offs, mask=mask_wx, other=0.0)\n        \n        # Accumulate\n        acc += tl.sum(x_val[:, None] * W_x_val, axis=0)\n    \n    # Compute hidden part: h_prev @ W_h\n    for j in range(0, hidden_size, BLOCK_INPUT):\n        j_offs = j + tl.arange(0, BLOCK_INPUT)\n        mask_j = j_offs < hidden_size\n        \n        # Load hidden state block\n        h_prev_val = tl.load(h_prev_ptr + batch_id * hidden_size + j_offs, mask=mask_j, other=0.0)\n        \n        # Load W_h block\n        w_offs = j_offs[:, None] * hidden_size + block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)[None, :]\n        mask_wh = mask_j[:, None] & (tl.arange(0, BLOCK_HIDDEN)[None, :] < (hidden_size - block_id * BLOCK_HIDDEN))\n        W_h_val = tl.load(W_h_ptr + w_offs, mask=mask_wh, other=0.0)\n        \n        # Accumulate\n        acc += tl.sum(h_prev_val[:, None] * W_h_val, axis=0)\n    \n    # Add bias\n    bias_offs = block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)\n    mask_bias = bias_offs < hidden_size\n    bias = tl.load(b_ptr + bias_offs, mask=mask_bias, other=0.0)\n    acc += bias\n    \n    # Apply activation (tanh)\n    output = tl.tanh(acc)\n    \n    # Store result\n    output_offs = batch_id * hidden_size + block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)\n    tl.store(h_ptr + output_offs, output, mask=mask_bias)\n\ndef fused_rnn(x, h_prev, W_x, W_h, b):\n    batch_size, input_size = x.shape\n    _, hidden_size = h_prev.shape\n    \n    # Allocate output tensor\n    h = torch.empty_like(h_prev)\n    \n    # Grid = (batch_size * num_hidden_blocks,)\n    grid = (batch_size * triton.cdiv(hidden_size, 128),)\n    \n    # Launch kernel\n    fused_rnn_kernel[grid](\n        x, h_prev, W_x, W_h, b, h,\n        input_size, hidden_size,\n        BLOCK_HIDDEN=128,  # Default value for autotune\n        BLOCK_INPUT=32     # Default value for autotune\n    )\n    return h\n```"]}}}
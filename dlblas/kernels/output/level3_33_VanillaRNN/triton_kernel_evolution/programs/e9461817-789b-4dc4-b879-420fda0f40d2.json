{"id": "e9461817-789b-4dc4-b879-420fda0f40d2", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 64, 'VEC': 4}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 64, 'VEC': 4}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 128, 'VEC': 4}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 128, 'VEC': 4}, num_warps=8),\n    ],\n    key=['input_size', 'hidden_size'],\n)\n@triton.jit\ndef fused_rnn_kernel(\n    x_ptr, hidden_ptr, w1_ptr, w2_ptr, bias_ptr, output_hidden_ptr,\n    input_size, hidden_size, batch_size,\n    BLOCK_HIDDEN: tl.constexpr, BLOCK_INPUT: tl.constexpr, VEC: tl.constexpr\n):\n    pid = tl.program_id(0)\n    batch_id = pid // tl.cdiv(hidden_size, BLOCK_HIDDEN)\n    block_id = pid % tl.cdiv(hidden_size, BLOCK_HIDDEN)\n    \n    # Initialize accumulator in FP16 for Tensor Core efficiency\n    acc = tl.zeros((BLOCK_HIDDEN,), dtype=tl.float16)\n    \n    # Precompute row offsets for weight matrices\n    row_offsets = block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)\n    row_mask = row_offsets < hidden_size\n\n    # Process input dimension (x part) with vectorized loads\n    for i in range(0, input_size, BLOCK_INPUT):\n        i_offs = i + tl.arange(0, BLOCK_INPUT, VEC)\n        mask_i = i_offs < input_size\n        \n        # Load input vector\n        x_val = tl.load(\n            x_ptr + batch_id * input_size + i_offs, \n            mask=mask_i, \n            other=0.0,\n            cache_modifier=\".cg\"\n        )\n        \n        # Compute weight offsets and load with vectorization\n        w1_offs = row_offsets[:, None] * input_size + i_offs[None, :]\n        mask_w1 = row_mask[:, None] & mask_i[None, :]\n        w1_val = tl.load(\n            w1_ptr + w1_offs, \n            mask=mask_w1, \n            other=0.0, \n            cache_modifier=\".cg\"\n        )\n        \n        # FP16 accumulation\n        acc += tl.sum(w1_val * x_val[None, :], axis=1)\n    \n    # Process hidden dimension with vectorized loads\n    for j in range(0, hidden_size, BLOCK_INPUT):\n        j_offs = j + tl.arange(0, BLOCK_INPUT, VEC)\n        mask_j = j_offs < hidden_size\n        \n        # Load hidden state vector\n        h_val = tl.load(\n            hidden_ptr + batch_id * hidden_size + j_offs, \n            mask=mask_j, \n            other=0.0,\n            cache_modifier=\".cg\"\n        )\n        \n        # Compute weight offsets and load with vectorization\n        w2_offs = row_offsets[:, None] * hidden_size + j_offs[None, :]\n        mask_w2 = row_mask[:, None] & mask_j[None, :]\n        w2_val = tl.load(\n            w2_ptr + w2_offs, \n            mask=mask_w2, \n            other=0.0, \n            cache_modifier=\".cg\"\n        )\n        \n        # FP16 accumulation\n        acc += tl.sum(w2_val * h_val[None, :], axis=1)\n    \n    # Convert to FP32 for precision-critical operations\n    bias_offs = block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)\n    mask_bias = bias_offs < hidden_size\n    bias_val = tl.load(bias_ptr + bias_offs, mask=mask_bias, other=0.0)\n    total = acc.to(tl.float32) + bias_val\n    new_hidden = libdevice.tanh(total)\n    \n    # Store result with vectorization\n    out_offs = batch_id * hidden_size + bias_offs\n    tl.store(output_hidden_ptr + out_offs, new_hidden, mask=mask_bias)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.hidden = torch.randn((batch_size, hidden_size))\n        \n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n        self.h2o = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        self.hidden = self.hidden.to(x.device)\n        \n        # Split weights for fused kernel\n        W1 = self.i2h.weight[:, :self.input_size].contiguous()\n        W2 = self.i2h.weight[:, self.input_size:].contiguous()\n        bias = self.i2h.bias.contiguous()\n        \n        # Prepare new hidden tensor\n        new_hidden = torch.empty_like(self.hidden)\n        \n        # Launch kernel with optimized configuration\n        grid = (batch_size * triton.cdiv(self.hidden_size, 128),)\n        fused_rnn_kernel[grid](\n            x, self.hidden, W1, W2, bias, new_hidden,\n            self.input_size, self.hidden_size, batch_size,\n            BLOCK_HIDDEN=128, BLOCK_INPUT=64, VEC=4\n        )\n        \n        self.hidden = new_hidden\n        return self.h2o(self.hidden)\n\nbatch_size = 8\ninput_size = 1024\nhidden_size = 256\noutput_size = 128\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, output_size]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "7fb80cad-c7f7-4b57-9178-4e250ad91d2c", "generation": 3, "timestamp": 1754597977.7558408, "iteration_found": 28, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_33_VanillaRNN", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp9dum7p50.py\", line 121, in forward\n    fused_rnn_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 135, in _bench\n    raise ValueError(f\"Conflicting meta-parameters: {', '.join(conflicts)}.\"\nValueError: Conflicting meta-parameters: BLOCK_HIDDEN, BLOCK_INPUT, VEC. Make sure that you don't re-define auto-tuned symbols.\n", "runtime_error_name": "builtins.ValueError", "reference_timing_ms": 0.0843, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.746, "runtime_ms": 0.113, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0843, "speed_up": 0.746, "custom_timing": 0.113}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
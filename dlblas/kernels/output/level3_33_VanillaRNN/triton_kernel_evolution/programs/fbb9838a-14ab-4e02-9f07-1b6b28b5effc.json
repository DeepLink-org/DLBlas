{"id": "fbb9838a-14ab-4e02-9f07-1b6b28b5effc", "code": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 32}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 64}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 128}, num_warps=4),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 32}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 64}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 128}, num_warps=8),\n    ],\n    key=['input_size', 'hidden_size'],\n)\n@triton.jit\ndef fused_rnn_kernel(\n    x_ptr,\n    h_prev_ptr,\n    W_x_ptr,\n    W_h_ptr,\n    b_ptr,\n    h_ptr,\n    input_size,\n    hidden_size,\n    BLOCK_HIDDEN: tl.constexpr,\n    BLOCK_INPUT: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    batch_id = pid // tl.cdiv(hidden_size, BLOCK_HIDDEN)\n    block_id = pid % tl.cdiv(hidden_size, BLOCK_HIDDEN)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_HIDDEN,), dtype=tl.float32)\n    \n    # Compute input part: x @ W_x\n    for i in range(0, input_size, BLOCK_INPUT):\n        i_offs = i + tl.arange(0, BLOCK_INPUT)\n        mask_i = i_offs < input_size\n        \n        # Load input block\n        x_val = tl.load(x_ptr + batch_id * input_size + i_offs, mask=mask_i, other=0.0)\n        \n        # Load W_x block\n        w_offs = i_offs[:, None] * hidden_size + block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)[None, :]\n        mask_wx = mask_i[:, None] & (tl.arange(0, BLOCK_HIDDEN)[None, :] < (hidden_size - block_id * BLOCK_HIDDEN))\n        W_x_val = tl.load(W_x_ptr + w_offs, mask=mask_wx, other=0.0, cache_modifier=\".cg\")\n        \n        # Vectorized accumulation using dot product\n        x_val_2d = tl.reshape(x_val, (1, BLOCK_INPUT))\n        acc += tl.reshape(tl.dot(x_val_2d, W_x_val, allow_tf32=True), (BLOCK_HIDDEN,))\n    \n    # Compute hidden part: h_prev @ W_h\n    for j in range(0, hidden_size, BLOCK_INPUT):\n        j_offs = j + tl.arange(0, BLOCK_INPUT)\n        mask_j = j_offs < hidden_size\n        \n        # Load hidden state block\n        h_prev_val = tl.load(h_prev_ptr + batch_id * hidden_size + j_offs, mask=mask_j, other=0.0)\n        \n        # Load W_h block\n        w_offs = j_offs[:, None] * hidden_size + block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)[None, :]\n        mask_wh = mask_j[:, None] & (tl.arange(0, BLOCK_HIDDEN)[None, :] < (hidden_size - block_id * BLOCK_HIDDEN))\n        W_h_val = tl.load(W_h_ptr + w_offs, mask=mask_wh, other=0.0, cache_modifier=\".cg\")\n        \n        # Vectorized accumulation using dot product\n        h_prev_2d = tl.reshape(h_prev_val, (1, BLOCK_INPUT))\n        acc += tl.reshape(tl.dot(h_prev_2d, W_h_val, allow_tf32=True), (BLOCK_HIDDEN,))\n    \n    # Add bias\n    bias_offs = block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)\n    mask_bias = bias_offs < hidden_size\n    bias = tl.load(b_ptr + bias_offs, mask=mask_bias, other=0.0)\n    acc += bias\n    \n    # Apply activation (tanh)\n    output = tl.tanh(acc)\n    \n    # Store result\n    output_offs = batch_id * hidden_size + block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)\n    tl.store(h_ptr + output_offs, output, mask=mask_bias)\n\ndef fused_rnn(x, h_prev, W_x, W_h, b):\n    batch_size, input_size = x.shape\n    _, hidden_size = h_prev.shape\n    \n    # Allocate output tensor\n    h = torch.empty_like(h_prev)\n    \n    # Grid = (batch_size * num_hidden_blocks,)\n    grid = (batch_size * triton.cdiv(hidden_size, 128),)\n    \n    # Launch kernel\n    fused_rnn_kernel[grid](\n        x, h_prev, W_x, W_h, b, h,\n        input_size, hidden_size,\n        BLOCK_HIDDEN=128,  # Default value for autotune\n        BLOCK_INPUT=32     # Default value for autotune\n    )\n    return h", "language": "python", "parent_id": "1fc600cc-409e-447f-bca1-5c254fde387f", "generation": 4, "timestamp": 1754597909.366497, "iteration_found": 29, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.986, "runtime_ms": 0.0855, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0843, "speed_up": 0.986, "custom_timing": 0.0855}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.852, "runtime_ms": 0.0989, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0843, "speed_up": 0.852, "custom_timing": 0.0989}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
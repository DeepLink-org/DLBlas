{"id": "16247e68-e449-4100-b814-2290f43cac89", "code": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 64, 'VEC': 4}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 64, 'VEC': 2}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 128, 'BLOCK_INPUT': 128, 'VEC': 4}, num_warps=8),\n        triton.Config({'BLOCK_HIDDEN': 256, 'BLOCK_INPUT': 64, 'VEC': 4}, num_warps=8),\n    ],\n    key=['input_size', 'hidden_size'],\n)\n@triton.jit\ndef fused_rnn_kernel(\n    x_ptr,\n    h_prev_ptr,\n    W_x_ptr,\n    W_h_ptr,\n    b_ptr,\n    h_ptr,\n    input_size,\n    hidden_size,\n    BLOCK_HIDDEN: tl.constexpr,\n    BLOCK_INPUT: tl.constexpr,\n    VEC: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    batch_id = pid // tl.cdiv(hidden_size, BLOCK_HIDDEN)\n    block_id = pid % tl.cdiv(hidden_size, BLOCK_HIDDEN)\n    \n    # Initialize accumulator in FP16 for tensor cores\n    acc = tl.zeros((BLOCK_HIDDEN,), dtype=tl.float16)\n    \n    # Precompute base pointers\n    x_base = x_ptr + batch_id * input_size\n    h_prev_base = h_prev_ptr + batch_id * hidden_size\n    W_x_base = W_x_ptr + block_id * BLOCK_HIDDEN\n    W_h_base = W_h_ptr + block_id * BLOCK_HIDDEN\n    \n    # Compute input part: x @ W_x with vectorization\n    for i in range(0, input_size, BLOCK_INPUT):\n        i_offs = i + tl.arange(0, BLOCK_INPUT, VEC)\n        mask_i = i_offs < input_size\n        \n        # Vectorized load for input\n        x_val = tl.load(x_base + i_offs, mask=mask_i, other=0.0)\n        x_val = x_val.to(tl.float16)\n        \n        # Compute weight matrix offset\n        w_offs = i_offs * hidden_size + tl.arange(0, BLOCK_HIDDEN)[:, None]\n        mask_wx = mask_i & (tl.arange(0, BLOCK_HIDDEN)[:, None] < (hidden_size - block_id * BLOCK_HIDDEN))\n        \n        # Vectorized load for weights\n        W_x_val = tl.load(W_x_base + w_offs, mask=mask_wx, other=0.0, cache_modifier=\".cg\")\n        W_x_val = W_x_val.to(tl.float16)\n        \n        # Accumulate with tensor core friendly operation\n        acc += tl.sum(x_val[None, :] * W_x_val, axis=1)\n    \n    # Compute hidden part: h_prev @ W_h with vectorization\n    for j in range(0, hidden_size, BLOCK_INPUT):\n        j_offs = j + tl.arange(0, BLOCK_INPUT, VEC)\n        mask_j = j_offs < hidden_size\n        \n        # Vectorized load for hidden state\n        h_prev_val = tl.load(h_prev_base + j_offs, mask=mask_j, other=0.0)\n        h_prev_val = h_prev_val.to(tl.float16)\n        \n        # Compute weight matrix offset\n        w_offs = j_offs * hidden_size + tl.arange(0, BLOCK_HIDDEN)[:, None]\n        mask_wh = mask_j & (tl.arange(0, BLOCK_HIDDEN)[:, None] < (hidden_size - block_id * BLOCK_HIDDEN))\n        \n        # Vectorized load for weights\n        W_h_val = tl.load(W_h_base + w_offs, mask=mask_wh, other=0.0, cache_modifier=\".cg\")\n        W_h_val = W_h_val.to(tl.float16)\n        \n        # Accumulate with tensor core friendly operation\n        acc += tl.sum(h_prev_val[None, :] * W_h_val, axis=1)\n    \n    # Add bias (convert to FP32 for precision)\n    bias_offs = block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)\n    mask_bias = bias_offs < hidden_size\n    bias = tl.load(b_ptr + bias_offs, mask=mask_bias, other=0.0).to(tl.float32)\n    acc = acc.to(tl.float32) + bias\n    \n    # Apply activation (tanh) in FP32\n    output = tl.tanh(acc)\n    \n    # Store result with vectorization\n    output_offs = batch_id * hidden_size + block_id * BLOCK_HIDDEN + tl.arange(0, BLOCK_HIDDEN)\n    tl.store(h_ptr + output_offs, output, mask=mask_bias)\n\ndef fused_rnn(x, h_prev, W_x, W_h, b):\n    batch_size, input_size = x.shape\n    _, hidden_size = h_prev.shape\n    \n    # Allocate output tensor\n    h = torch.empty_like(h_prev)\n    \n    # Grid = (batch_size * num_hidden_blocks,)\n    grid = (batch_size * triton.cdiv(hidden_size, 128),)\n    \n    # Launch kernel with tensor core optimized parameters\n    fused_rnn_kernel[grid](\n        x, h_prev, W_x, W_h, b, h,\n        input_size, hidden_size,\n        BLOCK_HIDDEN=128,  # Tensor core friendly size\n        BLOCK_INPUT=64,    # Tensor core friendly size\n        VEC=4              # Vectorization factor\n    )\n    return h", "language": "python", "parent_id": "565f3f2b-136d-4bf7-bc63-5512135bd9c7", "generation": 3, "timestamp": 1754596699.813854, "iteration_found": 20, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.753, "runtime_ms": 0.112, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0843, "speed_up": 0.753, "custom_timing": 0.112}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.826, "runtime_ms": 0.102, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0843, "speed_up": 0.826, "custom_timing": 0.102}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
{"id": "12eaab6f-7eb5-459a-a4f4-8ccf07191f69", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef linear_triton_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    BLOCK_SIZE_K: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_feature = tl.program_id(1)\n    \n    accumulator = 0.0\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        mask = k_offsets < in_features\n        \n        x_offset = pid_batch * in_features + k_offsets\n        w_offset = pid_feature * in_features + k_offsets\n        \n        x_vals = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n        w_vals = tl.load(w_ptr + w_offset, mask=mask, other=0.0)\n        accumulator += tl.sum(x_vals * w_vals)\n    \n    bias = tl.load(b_ptr + pid_feature)\n    accumulator += bias\n    output_offset = pid_batch * out_features + pid_feature\n    tl.store(output_ptr + output_offset, accumulator)\n\n@triton.jit\ndef max_pool_sum_scale_kernel(\n    x_ptr,\n    output_ptr,\n    out_features,\n    kernel_size: tl.constexpr,\n    scale_factor,\n    n_windows: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    total_sum = 0.0\n    \n    for window_idx in range(n_windows):\n        start = window_idx * kernel_size\n        window_offsets = pid_batch * out_features + start + tl.arange(0, kernel_size)\n        x_vals = tl.load(x_ptr + window_offsets)\n        max_val = tl.max(x_vals, axis=0)\n        total_sum += max_val\n    \n    total_sum = total_sum * scale_factor\n    tl.store(output_ptr + pid_batch, total_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.kernel_size = kernel_size\n        self.scale_factor = scale_factor\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        x = x.contiguous()\n        batch_size, _ = x.shape\n        \n        output_linear = torch.empty(batch_size, self.out_features, device=x.device, dtype=x.dtype)\n        grid_linear = (batch_size, self.out_features)\n        BLOCK_SIZE_K = 64\n        linear_triton_kernel[grid_linear](\n            x, self.weight, self.bias, output_linear,\n            self.in_features, self.out_features, BLOCK_SIZE_K\n        )\n        \n        n_windows = self.out_features // self.kernel_size\n        output_final = torch.empty(batch_size, device=x.device, dtype=x.dtype)\n        grid_pool = (batch_size,)\n        max_pool_sum_scale_kernel[grid_pool](\n            output_linear, output_final, self.out_features,\n            self.kernel_size, self.scale_factor, n_windows\n        )\n        \n        return output_final\n\nbatch_size = 128\nin_features = 10\nout_features = 5\nkernel_size = 2\nscale_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "24bd79ac-3df0-476a-95de-dba4530e09da", "generation": 2, "timestamp": 1754575137.9369595, "iteration_found": 4, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 56.9, "speed_up": 1.369, "runtime_ms": 0.0767, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.105, "speed_up": 1.369, "custom_timing": 0.0767}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 0.0000\n- correctness: 0.0000\n- score: -100.0000\n- stage: no_valid_triton_code\n- feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Areas identified for improvement: - Focus on optimizing the code for better performance on the target metrics\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 0.0000, correctness: 0.0000, score: -100.0000, stage: no_valid_triton_code, feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0759, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.105, 'speed_up': 1.383, 'custom_timing': 0.0759}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs matrix multiplication, max pooling, sum, and scaling.\n    \"\"\"\n    def __init__(self, in_features, out_features, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.max_pool = nn.MaxPool1d(kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.max_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.sum(x, dim=1)\n        x = x * self.scale_factor\n        return x\n\nbatch_size = 128\nin_features = 10\nout_features = 5\nkernel_size = 2\nscale_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0759), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.105, 'speed_up': 1.383, 'custom_timing': 0.0759})\n\n\n### Program 2 (Score: -100.0000)\n```python\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.max_pool = nn.MaxPool1d(kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = self.max_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.sum(x, dim=1)\n        x = x * self.scale_factor\n        return x\n```\nKey features: Performs well on compiled (0.0000), Performs well on correctness (0.0000), Performs well on score (-100.0000), Performs well on stage (no_valid_triton_code), Performs well on feedback (You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.0000, Type: Exploratory)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs matrix multiplication, max pooling, sum, and scaling.\n    \"\"\"\n    def __init__(self, in_features, out_features, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.max_pool = nn.MaxPool1d(kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.max_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.sum(x, dim=1)\n        x = x * self.scale_factor\n        return x\n\nbatch_size = 128\nin_features = 10\nout_features = 5\nkernel_size = 2\nscale_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nUnique approach: Excellent compiled (1.000), Excellent correctness (1.000), Alternative passed approach\n\n\n\n# Current Program\n```python\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.max_pool = nn.MaxPool1d(kernel_size)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        x = self.matmul(x)\n        x = self.max_pool(x.unsqueeze(1)).squeeze(1)\n        x = torch.sum(x, dim=1)\n        x = x * self.scale_factor\n        return x\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear transformation (matmul)\n  2. MaxPool1d\n  3. Sum over the last dimension\n  4. Scaling by a factor\n\nThe current implementation is in PyTorch and we are to rewrite it using Triton for performance.\n\nHowever, note that the model has multiple operations. We are allowed to rewrite the entire forward function with Triton kernels.\n\nBut note: The problem says we are to generate Triton implementations. The previous attempts did not have any Triton code.\n\nWe must write a Triton kernel that does the entire forward pass? Or we can break it down?\n\nConstraints:\n  - We cannot change the function signature of the forward method or the __init__.\n  - We must return the same output as the original.\n\nPlan:\n  We are going to fuse the operations in one Triton kernel? Or use multiple kernels?\n\nBut note: The operations are sequential and the output of one is input to the next.\n\nHowever, the operations are:\n  1. Linear: This is a matrix multiplication. We can write a Triton kernel for matmul.\n  2. MaxPool1d: This is a reduction over a sliding window. We can write a Triton kernel for max pooling.\n  3. Sum: This is a reduction over the last dimension.\n  4. Scaling: element-wise multiplication.\n\nBut note: The input to the model is of shape (batch_size, in_features). The linear layer outputs (batch_size, out_features). Then we do max pool on a 1D signal? But note the max pool is applied after unsqueezing to (batch_size, 1, out_features). Then max pool with kernel_size, which reduces the last dimension. Then we squeeze and then sum over the last dimension? Actually, after max pool we get (batch_size, 1, out_features//kernel_size) (assuming stride=kernel_size). Then we squeeze to (batch_size, out_features//kernel_size). Then we sum over the last dimension? That would give (batch_size,). Then we multiply by scale.\n\nBut note: The original code:\n\n  x = self.matmul(x)  # (batch_size, out_features)\n  x = self.max_pool(x.unsqueeze(1)).squeeze(1)  # (batch_size, out_features//kernel_size) [if kernel_size divides out_features]\n  x = torch.sum(x, dim=1)  # (batch_size,)\n  x = x * self.scale_factor\n\nSo the output is a tensor of shape (batch_size,).\n\nWe cannot change the input/output.\n\nWe are going to write a Triton kernel that does the entire forward pass? That might be complex.\n\nAlternatively, we can break it down into multiple kernels. However, the problem says we are to generate Triton implementations. We are allowed to use multiple kernels.\n\nBut note: The problem says \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" and we are to return the code within the EVOLVE-BLOCK.\n\nHowever, the current program is a PyTorch model without Triton. We must replace the forward method with a Triton-based implementation.\n\nWe have to write a Triton kernel for the entire forward pass? Or we can use a sequence of Triton kernels? The problem does not specify.\n\nBut note: The example given (per_row_quantize_int8) used one Triton kernel. However, that was a single operation.\n\nWe have multiple operations. We have two options:\n\nOption 1: Fuse the entire computation in one kernel.\n\nOption 2: Write separate kernels for each operation and call them in sequence.\n\nHowever, the problem says: \"Generate functionally equivalent but faster Triton implementations\".\n\nWe are free to do either. But fusing might be faster.\n\nBut the operations are:\n\n  Step1: Linear (matrix multiplication) with weights and bias? Note: nn.Linear has weight and bias. We must account for bias.\n\n  Step2: MaxPool1d: This is non-trivial because it requires sliding window.\n\n  Step3: Sum over the last dimension.\n\n  Step4: Scaling.\n\nWe can break the operations:\n\n  Operation1: Linear -> we can write a Triton matmul kernel.\n\n  Operation2: MaxPool1d -> we can write a Triton max pool kernel.\n\n  Operation3: Sum reduction -> we can write a Triton reduction kernel.\n\nBut note: The output of the max pool is 1D and then we do a sum over the last dimension. We could fuse the max pool and the sum? But the max pool is a 1D window and then we do a global sum? Actually, the max pool reduces the length of the feature vector, and then we do a sum over that reduced dimension.\n\nAlternatively, we can do:\n\n  After the linear layer, we have a matrix of (batch_size, out_features). Then we do max pool on windows of size kernel_size? Then we get (batch_size, out_features//kernel_size). Then we do a sum over that last dimension.\n\nWe can fuse:\n\n  Step 2 and 3: After the linear, we do a max pool and then a sum per row? That would be a two-step reduction? But note: the max pool is a local reduction and then the sum is a global reduction over the entire row? Actually, the max pool reduces the row into a shorter row, and then the sum reduces that row to a scalar.\n\nWe can do:\n\n  kernel1: linear (matmul) for the entire batch -> output (batch_size, out_features)\n  kernel2: max pool and then sum and then scale -> output (batch_size,)\n\nBut note: the max pool and the sum can be fused? We could do:\n\n  For each row in the output of the linear layer:\n      For each window in the row (of size kernel_size):\n          compute the max in the window\n      Then take the sum of the max values and multiply by the scale.\n\nBut note: the max pool step reduces the row of length out_features to a row of length out_features//kernel_size. Then we do the sum over that. So we can do:\n\n  For each row, we break the row into segments of kernel_size, compute the max for each segment, then sum the maxes and multiply by the scale.\n\nSo we can fuse step2, step3, and step4.\n\nTherefore, we can have two kernels:\n\n  Kernel1: Matrix multiplication (with weights and bias) for the linear layer.\n\n  Kernel2: For each row, do:\n        - max pool (with window size kernel_size) to reduce the row to a vector of length (out_features // kernel_size) [if divisible]\n        - then sum that vector to a scalar\n        - multiply by scale_factor\n\nBut note: the problem says we are to return the entire model. We are going to replace the forward method.\n\nHowever, the problem says we are to write Triton kernels. We are going to write two kernels? And then call them in the forward method.\n\nBut note: the example had a function that called the Triton kernel. We can do the same.\n\nBut note: the problem says to return the entire code block that is within the EVOLVE-BLOCK.\n\nWe are going to write:\n\n  - Two Triton kernels? Or one kernel that does everything? One kernel for the entire forward might be too complex.\n\n  Alternatively, we can do:\n\n      We precompute the weights and bias for the linear layer. Then we do:\n\n        x = linear(x, weight, bias)   [with a Triton kernel]\n        x = max_pool_sum_scale(x, kernel_size, scale_factor)   [with another Triton kernel]\n\n  This is acceptable.\n\nBut note: the problem says \"NO explanations, NO markdown, NO additional text\" and only the code.\n\nWe must also note that the input to the model is (batch_size, in_features). The model parameters (weight and bias) are stored in the model. We have to load them and pass to the kernel.\n\nHowever, note that the __init__ method is fixed? We are only replacing the forward method? Actually, the __init__ is also in the EVOLVE-BLOCK? The previous code had the __init__.\n\nBut note: the problem says \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" meaning we cannot change the __init__? Actually, the __init__ is part of the model and we are allowed to change the entire model? But the problem says to return the complete updated code containing the EVOLVE-BLOCK.\n\nLooking at the previous attempts, the EVOLVE-BLOCK contained the entire model definition and the input functions.\n\nWe are going to rewrite the entire model? But note the problem says: \"Name the output structure class as ModelNew.\"\n\nSo we are to write:\n\n  class ModelNew(nn.Module):\n      ... \n\n  and the get_inputs, get_init_inputs.\n\nBut the problem says: \"Make sure your rewritten program maintains the same inputs and outputs as the original program\"\n\nSo the __init__ must take the same arguments: in_features, out_features, kernel_size, scale_factor.\n\nWe are going to:\n\n  In __init__:\n      We store the parameters and also create the weight and bias for the linear layer? But note: the original used nn.Linear. We cannot use nn.Linear if we are writing a Triton kernel? We will have to handle the weight and bias ourselves.\n\n  We are going to:\n\n      self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n      self.bias = nn.Parameter(torch.Tensor(out_features))\n\n      and initialize them? The original nn.Linear initializes the weight and bias.\n\n  But note: the original model had:\n\n        self.matmul = nn.Linear(in_features, out_features)\n\n  We are replacing that. So we must initialize the parameters similarly.\n\n  We can do:\n\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        # initialize\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n  But note: the original nn.Linear does exactly that.\n\n  Then in the forward, we will call our Triton kernel for linear.\n\n  Then we call our Triton kernel for max_pool_sum_scale.\n\nBut note: the max_pool_sum_scale kernel must also take the kernel_size and scale_factor.\n\nHowever, the kernel_size and scale_factor are fixed and known at initialization? So we can pass them as constants to the kernel.\n\nBut note: the kernel_size must divide the out_features? We assume it does? The original code does not check. We must assume the out_features is divisible by kernel_size? Otherwise, the original max pool would have padding issues? Actually, by default, MaxPool1d will use ceil_mode=False and if the size is not divisible, it will ignore the remainder? We must mimic that.\n\nWe are going to write:\n\n  In the forward:\n\n      # Step 1: Linear\n      x = linear_triton(x, self.weight, self.bias)\n\n      # Step 2: MaxPool, Sum, and Scale\n      x = max_pool_sum_scale_triton(x, self.kernel_size, self.scale_factor)\n\n  But note: the original had unsqueeze and squeeze? We don't need that because our kernel can work on 2D tensors? We can design the max pool kernel to work on the last dimension.\n\n  The input to the max_pool_sum_scale kernel is (batch_size, out_features). We break the last dimension into windows of size kernel_size. The number of windows is (out_features + kernel_size - 1) // kernel_size? Actually, we are going to do the same as PyTorch: by default, it uses ceil_mode=False, so it only takes full windows.\n\n  So the output of the max pool would be (batch_size, out_features // kernel_size). Then we sum that to (batch_size,) and scale.\n\n  We can do:\n\n      For each row i in batch:\n          For each window j from 0 to (out_features // kernel_size) - 1:\n              start = j * kernel_size\n              end = start + kernel_size\n              max_val = max(x[i, start:end])\n              Then we add max_val to a running sum.\n          Then multiply the sum by scale_factor.\n\n  But note: the original did:\n\n        x = self.max_pool(x.unsqueeze(1)).squeeze(1)   # becomes (batch_size, out_features//kernel_size)\n        x = torch.sum(x, dim=1)   # becomes (batch_size,)\n        x = x * self.scale_factor\n\n  So the original did: sum_i ( max_pool_value_i ) * scale_factor.\n\n  We are doing: for each row, we compute the max in each window and then sum those maxes and then scale.\n\n  So it is the same.\n\n  But note: the original max pool does not include the last partial window? We must do the same: only full windows.\n\n  Therefore, we require that the kernel_size divides the out_features? Otherwise, we ignore the last few elements.\n\n  We can compute: n_windows = out_features // kernel_size\n\n  Then we only consider the first n_windows * kernel_size elements.\n\n  Then the output of the max pool step would be n_windows values per row.\n\n  Then we sum those n_windows values and multiply by scale.\n\n  So the kernel for max_pool_sum_scale:\n\n      Input: x (batch_size, out_features)\n      kernel_size: int\n      scale: float\n\n      Output: (batch_size,)\n\n      For each row, we reduce the row to a scalar.\n\n  This kernel can be written as a 1D grid of size batch_size.\n\n  We will have:\n\n      pid = tl.program_id(0)   # row index\n\n  Then we load the row? But the row length is out_features. We break it into chunks of kernel_size. We can use tl.arange to get the base indices.\n\n  Steps:\n\n      row = tl.load(x_ptr + pid * out_features + offsets, mask=offsets < out_features, ...)   # but we are going to load the entire row? That might be too long.\n\n  Alternatively, we can use a loop over the windows? And for each window, we load the elements in the window and compute the max.\n\n  We can do:\n\n      total_sum = 0.0\n      for window_idx in range(0, n_windows):\n          start = window_idx * kernel_size\n          block = tl.arange(0, kernel_size)\n          offsets = pid * out_features + start + block\n          vals = tl.load(x_ptr + offsets, mask=block < kernel_size, ...)   # mask is not needed because kernel_size is the same? But if the row is not divisible, we might have the last window? We are only doing n_windows = out_features//kernel_size, so we don't go beyond.\n\n          max_val = tl.max(vals, axis=0)\n          total_sum += max_val\n\n      total_sum = total_sum * scale_factor\n      tl.store(output_ptr + pid, total_sum)\n\n  But note: we cannot have a loop with a variable number of iterations? We can, because n_windows is known at compile time? Actually, we can pass it as a constant.\n\n  Alternatively, we can compute n_windows = out_features // kernel_size and pass it as a constant.\n\n  But note: the out_features and kernel_size are known at initialization. So we can set them as constants in the kernel.\n\n  However, the kernel must be compiled for specific values? We can pass them as arguments and use tl.constexpr.\n\n  We can define:\n\n      @triton.jit\n      def max_pool_sum_scale_kernel(x_ptr, output_ptr, out_features, kernel_size, scale_factor, BLOCK_SIZE: tl.constexpr, ...):\n          pid = tl.program_id(0)\n          n_windows = out_features // kernel_size   # integer division, we ignore remainder\n\n          total_sum = 0.0\n          for window_idx in range(0, n_windows):\n              start = window_idx * kernel_size\n              # We create a block of indices for the window\n              window_offsets = pid * out_features + start + tl.arange(0, kernel_size)\n              # Load the window\n              vals = tl.load(x_ptr + window_offsets, mask=None)   # because we know the window is within [0, out_features) since n_windows*kernel_size <= out_features?\n              max_val = tl.max(vals, axis=0)\n              total_sum += max_val\n\n          total_sum = total_sum * scale_factor\n          tl.store(output_ptr + pid, total_sum)\n\n  But note: what if the row is not contiguous? We assume the input is contiguous.\n\n  We can require the input to be contiguous.\n\n  Also, we must note: the linear kernel also requires contiguous inputs.\n\n  We can do:\n\n      x = x.contiguous()\n\n  before passing to the linear kernel.\n\n  Similarly, the output of the linear kernel is contiguous? We will write it as contiguous.\n\n  Now, the linear kernel:\n\n      We have to do: y = x @ weight.T + bias\n\n      We can write a Triton kernel for matrix multiplication. We can use the standard matmul kernel from Triton examples.\n\n      We note that the input x is (batch_size, in_features) and weight is (out_features, in_features). So we do:\n\n          y[i, j] = sum_k x[i,k] * weight[j,k] + bias[j]\n\n      We can write a kernel that uses block tiling.\n\n      We can use:\n\n          pid_batch = tl.program_id(0)   # over batch\n          pid_feature = tl.program_id(1)  # over out_features\n\n      Then we load a block of x and a block of weight and compute the dot product.\n\n      But note: the bias is added per feature.\n\n      We can write:\n\n          accumulator = 0.0\n          for k in range(0, in_features, BLOCK_SIZE_K):\n              x_offset = pid_batch * in_features + k + tl.arange(0, BLOCK_SIZE_K)\n              w_offset = pid_feature * in_features + k + tl.arange(0, BLOCK_SIZE_K)\n\n              x_block = tl.load(x_ptr + x_offset, mask=(tl.arange(0, BLOCK_SIZE_K) < in_features - k), ...)\n              w_block = tl.load(weight_ptr + w_offset, mask=(tl.arange(0, BLOCK_SIZE_K) < in_features - k), ...)\n\n              accumulator += tl.sum(x_block * w_block, axis=0)\n\n          accumulator += tl.load(bias_ptr + pid_feature)\n          tl.store(output_ptr + pid_batch * out_features + pid_feature, accumulator)\n\n      The grid would be (batch_size, out_features).\n\n      But note: the in_features might not be divisible by BLOCK_SIZE_K? We use masking.\n\n  We are going to write two kernels.\n\n  We must also note: the data types. We assume float32.\n\n  Steps for the forward method:\n\n      x = x.contiguous()\n      # Allocate output for linear: (batch_size, out_features)\n      output_linear = torch.empty(x.shape[0], self.out_features, device=x.device, dtype=x.dtype)\n\n      # Launch linear kernel\n      grid_linear = (x.shape[0], self.out_features)\n      linear_kernel[grid_linear](x, self.weight, self.bias, output_linear, ...)\n\n      # Now, we have output_linear of shape (batch_size, out_features)\n      # Now, for the max_pool_sum_scale kernel, we allocate output: (batch_size,)\n      output_final = torch.empty(x.shape[0], device=x.device, dtype=x.dtype)\n\n      # Compute n_windows = self.out_features // self.kernel_size\n      n_windows = self.out_features // self.kernel_size   # integer division\n\n      # Launch the second kernel: grid of (batch_size,)\n      grid_pool = (x.shape[0],)\n      max_pool_sum_scale_kernel[grid_pool](output_linear, output_final, self.out_features, self.kernel_size, self.scale_factor, ...)\n\n      return output_final\n\n  We must write the two kernels.\n\n  However, note: the problem says to focus on performance. We are using two kernels, which might not be the fastest because we are writing intermediate results to global memory.\n\n  Alternatively, we can fuse the entire forward pass? That would avoid the intermediate storage.\n\n  We can do:\n\n      For each row i (batch index) and for each output feature j (from 0 to out_features-1) we compute the linear part? But then we have to do the max pool and sum? That is complex.\n\n  Due to time, we will do two kernels.\n\n  But note: the problem says to make small iterative steps. We are making a big step? However, the previous version had no Triton code.\n\n  We are going to write:\n\n      linear_triton_kernel and max_pool_sum_scale_triton_kernel.\n\n  We must also note: the problem says to not change the grid configuration? Actually, we are allowed to change the grid because the problem says: \"\u274c NEVER modify grid configuration\" for the existing Triton kernel? But we are writing new kernels. We are free to set the grid as needed.\n\n  However, the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" meaning we are only to change the Triton kernel source? But the entire model is in the block. We are replacing the forward method and also adding the kernels.\n\n  The problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\n  We are going to write the entire model with the two kernels and the new forward method.\n\n  We must also include the necessary imports (triton, tl, etc.)\n\n  We note: the previous feedback said: \"The key word 'import triton' is not found.\"\n\n  So we must import triton.\n\n  We write:\n\n      import triton\n      import triton.language as tl\n\n  We also note: we are using math, so we import math.\n\n  We are going to write:\n\n      class ModelNew(nn.Module):\n          def __init__(self, in_features, out_features, kernel_size, scale_factor):\n              super().__init__()\n              self.in_features = in_features\n              self.out_features = out_features\n              self.kernel_size = kernel_size\n              self.scale_factor = scale_factor\n\n              # Initialize weight and bias\n              self.weight = nn.Parameter(torch.empty(out_features, in_features))\n              self.bias = nn.Parameter(torch.empty(out_features))\n              # Initialize as in nn.Linear\n              nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n              fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n              bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n              nn.init.uniform_(self.bias, -bound, bound)\n\n          def forward(self, x):\n              ...   # as above\n\n      And then define the two kernels.\n\n  We also need to define the input functions: get_inputs and get_init_inputs.\n\n  The get_init_inputs returns the arguments for the model constructor: [in_features, out_features, kernel_size, scale_factor]\n\n  The get_inputs returns a list of one tensor: [torch.randn(batch_size, in_features)]\n\n  We set batch_size, in_features, out_features, kernel_size, scale_factor as in the previous code.\n\n  But note: the previous code had:\n\n        batch_size = 128\n        in_features = 10\n        out_features = 5\n        kernel_size = 2\n        scale_factor = 0.5\n\n  We keep that.\n\n  Now, we write the kernels. We are going to write them as top-level functions in the EVOLVE-BLOCK.\n\n  We must also note: we are to not remove boundary checks. We use masks appropriately.\n\n  We write the linear kernel:\n\n      @triton.jit\n      def linear_triton_kernel(x_ptr, w_ptr, b_ptr, output_ptr,\n                               in_features, out_features,\n                               BLOCK_SIZE_K: tl.constexpr):\n          pid_batch = tl.program_id(0)\n          pid_feature = tl.program_id(1)\n\n          # We are going to accumulate\n          accumulator = 0.0\n          for k in range(0, in_features, BLOCK_SIZE_K):\n              # Create offsets for the block\n              k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n              # Create masks for the block\n              mask = k_offsets < in_features\n              x_offset = pid_batch * in_features + k_offsets\n              w_offset = pid_feature * in_features + k_offsets   # because w is (out_features, in_features)\n\n              x_vals = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n              w_vals = tl.load(w_ptr + w_offset, mask=mask, other=0.0)\n              accumulator += tl.sum(x_vals * w_vals)\n\n          # Add bias\n          bias = tl.load(b_ptr + pid_feature)\n          accumulator += bias\n\n          # Store\n          output_offset = pid_batch * out_features + pid_feature\n          tl.store(output_ptr + output_offset, accumulator)\n\n  And the max_pool_sum_scale kernel:\n\n      @triton.jit\n      def max_pool_sum_scale_kernel(x_ptr, output_ptr, out_features, kernel_size, scale_factor,\n                                   BLOCK_SIZE_W: tl.constexpr):\n          pid_batch = tl.program_id(0)\n\n          n_windows = out_features // kernel_size   # integer division\n\n          total_sum = 0.0\n          for window_idx in range(n_windows):\n              start = window_idx * kernel_size\n              # We create offsets for the window in the row\n              window_offsets = start + tl.arange(0, BLOCK_SIZE_W)\n              # We know the window size is kernel_size, so we set mask to avoid going beyond the window? Actually, we are using the entire window? But if kernel_size is not a power of two? We set BLOCK_SIZE_W = kernel_size? But kernel_size might be dynamic? We pass it as a constant.\n\n          We can set BLOCK_SIZE_W = kernel_size? But kernel_size is passed as an argument? We can use tl.constexpr for kernel_size? Actually, we can set:\n\n          total_sum = 0.0\n          for window_idx in range(n_windows):\n              start = window_idx * kernel_size\n              # We create a block of size kernel_size\n              window_offsets = pid_batch * out_features + start + tl.arange(0, kernel_size)\n              # We don't need mask because we know we are within the row? Because n_windows * kernel_size <= out_features? Actually, we set n_windows = out_features//kernel_size, so we are safe.\n              x_vals = tl.load(x_ptr + window_offsets)\n              max_val = tl.max(x_vals, axis=0)\n              total_sum += max_val\n\n          total_sum = total_sum * scale_factor\n          tl.store(output_ptr + pid_batch, total_sum)\n\n  But note: the kernel_size must be a power of two? Not necessarily. But we are using tl.arange(0, kernel_size). We can set BLOCK_SIZE_W = kernel_size? But we are using it as a constant? We can pass it as tl.constexpr.\n\n  However, we cannot pass kernel_size as a tl.constexpr because it is an argument? We can mark it as tl.constexpr in the kernel? Actually, we can. We do:\n\n      kernel_size: tl.constexpr\n\n  Then we can use it to set the block size.\n\n  We change the kernel to:\n\n      @triton.jit\n      def max_pool_sum_scale_kernel(x_ptr, output_ptr, out_features, kernel_size: tl.constexpr, scale_factor,\n                                   n_windows: tl.constexpr):\n          pid_batch = tl.program_id(0)\n\n          total_sum = 0.0\n          for window_idx in range(n_windows):\n              start = window_idx * kernel_size\n              window_offsets = pid_batch * out_features + start + tl.arange(0, kernel_size)\n              x_vals = tl.load(x_ptr + window_offsets)\n              max_val = tl.max(x_vals, axis=0)\n              total_sum += max_val\n\n          total_sum = total_sum * scale_factor\n          tl.store(output_ptr + pid_batch, total_sum)\n\n  And we compute n_windows = out_features // kernel_size and pass it as a constant.\n\n  But note: we cannot have a for loop over a dynamic range? We must pass n_windows as a constant? We can set it as a tl.constexpr.\n\n  We do:\n\n      n_windows: tl.constexpr\n\n  Then we can unroll the loop? Or Triton will handle it.\n\n  Alternatively, we can avoid the loop by using a 2D grid? But that would be more complex.\n\n  We decide to pass n_windows and kernel_size as tl.constexpr.\n\n  How to call the kernel:\n\n      n_windows = out_features // kernel_size   # integer\n      max_pool_sum_scale_kernel[grid_pool](output_linear, output_final, out_features, kernel_size, scale_factor, n_windows)\n\n  But note: we must set the block size? The kernel uses a 1D grid? We can set the number of threads per block? Triton handles that.\n\n  Alternatively, we can use autotune? But for simplicity, we skip.\n\n  We must also note: the linear kernel might be slow if we don't tile properly. We use a simple tiling in the K dimension.\n\n  We set BLOCK_SIZE_K to 64? Or 128? We can set it to 64.\n\n  We call the linear kernel with:\n\n      BLOCK_SIZE_K = 64\n\n      grid_linear = (batch_size, out_features)\n      linear_triton_kernel[grid_linear](x, self.weight, self.bias, output_linear, in_features, out_features, BLOCK_SIZE_K)\n\n  We assume the input is contiguous.\n\n  Now, we write the entire code.\n\n  IMPORTANT: We are writing the entire EVOLVE-BLOCK.\n\n  We note: the previous code had the class and the input functions. We are replacing the entire block.\n\n  We write:\n\n      import math\n      import torch\n      import torch.nn as nn\n      import triton\n      import triton.language as tl\n\n      # Triton kernels\n\n      @triton.jit\n      def linear_triton_kernel(x_ptr, w_ptr, b_ptr, output_ptr,\n                               in_features, out_features,\n                               BLOCK_SIZE_K: tl.constexpr):\n          pid_batch = tl.program_id(0)\n          pid_feature = tl.program_id(1)\n\n          accumulator = 0.0\n          for k in range(0, in_features, BLOCK_SIZE_K):\n              k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n              mask = k_offsets < in_features\n              x_offset = pid_batch * in_features + k_offsets\n              w_offset = pid_feature * in_features + k_offsets\n\n              x_vals = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n              w_vals = tl.load(w_ptr + w_offset, mask=mask, other=0.0)\n              accumulator += tl.sum(x_vals * w_vals)\n\n          bias = tl.load(b_ptr + pid_feature)\n          accumulator += bias\n\n          output_offset = pid_batch * out_features + pid_feature\n          tl.store(output_ptr + output_offset, accumulator)\n\n      @triton.jit\n      def max_pool_sum_scale_kernel(x_ptr, output_ptr, out_features, kernel_size: tl.constexpr, scale_factor, n_windows: tl.constexpr):\n          pid_batch = tl.program_id(0)\n\n          total_sum = 0.0\n          for window_idx in range(n_windows):\n              start = window_idx * kernel_size\n              window_offsets = pid_batch * out_features + start + tl.arange(0, kernel_size)\n              x_vals = tl.load(x_ptr + window_offsets)\n              max_val = tl.max(x_vals, axis=0)\n              total_sum += max_val\n\n          total_sum = total_sum * scale_factor\n          tl.store(output_ptr + pid_batch, total_sum)\n\n      class ModelNew(nn.Module):\n          def __init__(self, in_features, out_features, kernel_size, scale_factor):\n              super(ModelNew, self).__init__()\n              self.in_features = in_features\n              self.out_features = out_features\n              self.kernel_size = kernel_size\n              self.scale_factor = scale_factor\n\n              self.weight = nn.Parameter(torch.empty(out_features, in_features))\n              self.bias = nn.Parameter(torch.empty(out_features))\n              # Initialize as in nn.Linear\n              nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n              fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n              bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n              nn.init.uniform_(self.bias, -bound, bound)\n\n          def forward(self, x):\n              # Ensure contiguous\n              x = x.contiguous()\n              batch_size, _ = x.shape\n\n              # Allocate output for linear\n              output_linear = torch.empty(batch_size, self.out_features, device=x.device, dtype=x.dtype)\n\n              # Grid for linear: (batch_size, out_features)\n              grid_linear = (batch_size, self.out_features)\n\n              # Constants for linear kernel\n              BLOCK_SIZE_K = 64   # tunable?\n\n              # Launch linear kernel\n              linear_triton_kernel[grid_linear](x, self.weight, self.bias, output_linear, \n                                                self.in_features, self.out_features, BLOCK_SIZE_K)\n\n              # Now, for the second kernel\n              # We require that kernel_size divides out_features? We use integer division\n              n_windows = self.out_features // self.kernel_size   # integer division\n\n              # Allocate output for the final result: (batch_size,)\n              output_final = torch.empty(batch_size, device=x.device, dtype=x.dtype)\n\n              # Grid for the second kernel: (batch_size,)\n              grid_pool = (batch_size,)\n\n              # Launch the max_pool_sum_scale kernel\n              # We pass kernel_size and n_windows as constants (tl.constexpr) so they must be known at compile time? But they are fixed per model.\n              max_pool_sum_scale_kernel[grid_pool](output_linear, output_final, self.out_features, \n                                                   self.kernel_size, self.scale_factor, n_windows)\n\n              return output_final\n\n      # Then the input functions\n      batch_size = 128\n      in_features = 10\n      out_features = 5\n      kernel_size = 2\n      scale_factor = 0.5\n\n      def get_inputs():\n          return [torch.randn(batch_size, in_features)]\n\n      def get_init_inputs():\n          return [in_features, out_features, kernel_size, scale_factor]\n\n  This is the complete code.\n\n  However, note: the max_pool_sum_scale_kernel uses a for loop over n_windows. We passed n_windows as a tl.constexpr. But n_windows is computed at runtime? Actually, it is computed from the model parameters (out_features and kernel_size) which are fixed for the model. So for a given model, it is constant. But if we change the model, we recompile.\n\n  This should work.\n\n  We must also note: the linear kernel might be inefficient because it does not use shared memory and does not tile in the batch and feature dimensions? But for small matrices, it might be acceptable.\n\n  We are making an initial version that compiles and runs correctly. Then we can optimize.\n\n  We hope this will be faster than the PyTorch version.\n\n  We output the entire code.\n\n  IMPORTANT: We are to return the code within the EVOLVE-BLOCK.\n\n  We mark:\n\n      # ================== EVOLVE-BLOCK-START ==================\n      ... code ...\n      # =================== EVOLVE-BLOCK-END ===================\n\n  We do not include any other text.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef linear_triton_kernel(\n    x_ptr,\n    w_ptr,\n    b_ptr,\n    output_ptr,\n    in_features,\n    out_features,\n    BLOCK_SIZE_K: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_feature = tl.program_id(1)\n    \n    accumulator = 0.0\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        mask = k_offsets < in_features\n        \n        x_offset = pid_batch * in_features + k_offsets\n        w_offset = pid_feature * in_features + k_offsets\n        \n        x_vals = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n        w_vals = tl.load(w_ptr + w_offset, mask=mask, other=0.0)\n        accumulator += tl.sum(x_vals * w_vals)\n    \n    bias = tl.load(b_ptr + pid_feature)\n    accumulator += bias\n    output_offset = pid_batch * out_features + pid_feature\n    tl.store(output_ptr + output_offset, accumulator)\n\n@triton.jit\ndef max_pool_sum_scale_kernel(\n    x_ptr,\n    output_ptr,\n    out_features,\n    kernel_size: tl.constexpr,\n    scale_factor,\n    n_windows: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    total_sum = 0.0\n    \n    for window_idx in range(n_windows):\n        start = window_idx * kernel_size\n        window_offsets = pid_batch * out_features + start + tl.arange(0, kernel_size)\n        x_vals = tl.load(x_ptr + window_offsets)\n        max_val = tl.max(x_vals, axis=0)\n        total_sum += max_val\n    \n    total_sum = total_sum * scale_factor\n    tl.store(output_ptr + pid_batch, total_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, scale_factor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.kernel_size = kernel_size\n        self.scale_factor = scale_factor\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        x = x.contiguous()\n        batch_size, _ = x.shape\n        \n        output_linear = torch.empty(batch_size, self.out_features, device=x.device, dtype=x.dtype)\n        grid_linear = (batch_size, self.out_features)\n        BLOCK_SIZE_K = 64\n        linear_triton_kernel[grid_linear](\n            x, self.weight, self.bias, output_linear,\n            self.in_features, self.out_features, BLOCK_SIZE_K\n        )\n        \n        n_windows = self.out_features // self.kernel_size\n        output_final = torch.empty(batch_size, device=x.device, dtype=x.dtype)\n        grid_pool = (batch_size,)\n        max_pool_sum_scale_kernel[grid_pool](\n            output_linear, output_final, self.out_features,\n            self.kernel_size, self.scale_factor, n_windows\n        )\n        \n        return output_final\n\nbatch_size = 128\nin_features = 10\nout_features = 5\nkernel_size = 2\nscale_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
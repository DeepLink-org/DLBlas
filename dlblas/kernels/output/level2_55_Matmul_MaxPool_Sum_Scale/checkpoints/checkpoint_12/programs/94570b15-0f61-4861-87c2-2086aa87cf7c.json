{"id": "94570b15-0f61-4861-87c2-2086aa87cf7c", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, \n    weight_ptr, \n    bias_ptr, \n    output_ptr, \n    in_features, \n    out_features, \n    scale_factor, \n    stride_x, \n    stride_out,\n    BLOCK_SIZE_IN: tl.constexpr, \n    BLOCK_SIZE_OUT: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Compute starting position for the current batch\n    x_offset = pid * stride_x\n    total_sum = 0.0\n\n    # Calculate number of complete windows (kernel_size=2, stride=2)\n    num_windows = (out_features - 2) // 2 + 1\n    \n    # Process each complete window\n    for window_idx in range(0, num_windows):\n        j = window_idx * 2\n        # Load biases for both features in window\n        bias0 = tl.load(bias_ptr + j)\n        bias1 = tl.load(bias_ptr + j + 1)\n            \n        acc0 = bias0\n        acc1 = bias1\n        \n        # Process input features in blocks\n        for k_block in range(0, tl.cdiv(in_features, BLOCK_SIZE_IN)):\n            k_start = k_block * BLOCK_SIZE_IN\n            k_offsets = k_start + tl.arange(0, BLOCK_SIZE_IN)\n            mask = k_offsets < in_features\n            \n            # Load input block\n            x_vals = tl.load(x_ptr + x_offset + k_offsets, mask=mask, other=0.0)\n            \n            # Load weight blocks for both features\n            w0_offsets = j * in_features + k_offsets\n            w0_vals = tl.load(weight_ptr + w0_offsets, mask=mask, other=0.0)\n            w1_offsets = (j + 1) * in_features + k_offsets\n            w1_vals = tl.load(weight_ptr + w1_offsets, mask=mask, other=0.0)\n            \n            # Accumulate dot products\n            acc0 += tl.sum(x_vals * w0_vals)\n            acc1 += tl.sum(x_vals * w1_vals)\n        \n        # Apply max pooling to window\n        max_val = tl.maximum(acc0, acc1)\n        total_sum += max_val\n    \n    # Apply scaling and store result\n    total_sum = total_sum * scale_factor\n    tl.store(output_ptr + pid * stride_out, total_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, scale_factor):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.kernel_size = kernel_size\n        self.scale_factor = scale_factor\n        \n        # Initialize parameters\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        \n        # Use optimized kernel for kernel_size=2\n        if self.kernel_size == 2:\n            grid = (batch_size,)\n            BLOCK_SIZE_IN = 128\n            BLOCK_SIZE_OUT = 2\n            \n            _forward_kernel[grid](\n                x, self.weight, self.bias, output,\n                self.in_features, self.out_features, self.scale_factor,\n                x.stride(0), output.stride(0),\n                BLOCK_SIZE_IN=BLOCK_SIZE_IN,\n                BLOCK_SIZE_OUT=BLOCK_SIZE_OUT\n            )\n        else:\n            # Fallback for other kernel sizes\n            x = torch.nn.functional.linear(x, self.weight, self.bias)\n            x = x.unsqueeze(1)\n            x = torch.nn.functional.max_pool1d(x, kernel_size=self.kernel_size)\n            x = x.squeeze(1)\n            x = torch.sum(x, dim=1)\n            x = x * self.scale_factor\n            output = x\n        \n        return output\n\nbatch_size = 128\nin_features = 10\nout_features = 5\nkernel_size = 2\nscale_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "759d8268-ea77-4779-94b2-25f64e3fdafb", "generation": 2, "timestamp": 1754576450.3454611, "iteration_found": 8, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 127.90000000000002, "speed_up": 2.079, "runtime_ms": 0.0505, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.105, "speed_up": 2.079, "custom_timing": 0.0505}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_55_Matmul_MaxPool_Sum_Scale", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "max_difference": ["0.704939", "0.586457", "0.753231", "0.838420", "0.644629"], "avg_difference": ["0.227003", "0.186672", "0.200441", "0.230876", "0.198333"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.105, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 0.0000\n- score: 10.0000\n- stage: wrong_answer\n- verify_payload: {'id': 'level2_55_Matmul_MaxPool_Sum_Scale', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'max_difference': ['0.704939', '0.586457', '0.753231', '0.838420', '0.644629'], 'avg_difference': ['0.227003', '0.186672', '0.200441', '0.230876', '0.198333'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.105, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_55_Matmul_MaxPool_Sum_Scale', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'max_difference': ['0.704939', '0.586457', '0.753231', '0.838420', '0.644629'], 'avg_difference': ['0.227003', '0.186672', '0.200441', '0.230876', '0.198333'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.105, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, \n    weight_ptr, \n    bias_ptr, \n    output_ptr, \n    in_features, \n    out_features, \n    scale_factor, \n    stride_x, \n    stride_out,\n    BLOCK_SIZE_IN: tl.constexpr, \n    BLOCK_SIZE_OUT: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Compute starting position for the current batch\n    x_offset = pid * stride_x\n    total_sum = 0.0\n\n    # Process output features in pairs\n    for j in range(0, out_features, 2):\n        # Load biases\n        bias0 = tl.load(bias_ptr + j)\n        if j + 1 < out_features:\n            bias1 = tl.load(bias_ptr + j + 1)\n        else:\n            bias1 = 0.0\n            \n        acc0 = bias0\n        acc1 = bias1\n        \n        # Process input features in blocks\n        for k_block in range(0, tl.cdiv(in_features, BLOCK_SIZE_IN)):\n            k_start = k_block * BLOCK_SIZE_IN\n            k_offsets = k_start + tl.arange(0, BLOCK_SIZE_IN)\n            mask = k_offsets < in_features\n            \n            # Load input block\n            x_vals = tl.load(x_ptr + x_offset + k_offsets, mask=mask, other=0.0)\n            \n            # Load weight block for first feature\n            w0_offsets = j * in_features + k_offsets\n            w0_vals = tl.load(weight_ptr + w0_offsets, mask=mask, other=0.0)\n            acc0 += tl.sum(x_vals * w0_vals)\n            \n            # Load weight block for second feature if exists\n            if j + 1 < out_features:\n                w1_offsets = (j + 1) * in_features + k_offsets\n                w1_vals = tl.load(weight_ptr + w1_offsets, mask=mask, other=0.0)\n                acc1 += tl.sum(x_vals * w1_vals)\n        \n        # Max pooling for the pair\n        if j + 1 < out_features:\n            max_val = tl.maximum(acc0, acc1)\n        else:\n            max_val = acc0\n        total_sum += max_val\n    \n    # Apply scaling and store result\n    total_sum = total_sum * scale_factor\n    tl.store(output_ptr + pid * stride_out, total_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, scale_factor):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.kernel_size = kernel_size\n        self.scale_factor = scale_factor\n        \n        # Initialize parameters\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        \n        # Use optimized kernel for kernel_size=2\n        if self.kernel_size == 2:\n            grid = (batch_size,)\n            BLOCK_SIZE_IN = 128\n            BLOCK_SIZE_OUT = 2\n            \n            _forward_kernel[grid](\n                x, self.weight, self.bias, output,\n                self.in_features, self.out_features, self.scale_factor,\n                x.stride(0), output.stride(0),\n                BLOCK_SIZE_IN=BLOCK_SIZE_IN,\n                BLOCK_SIZE_OUT=BLOCK_SIZE_OUT\n            )\n        else:\n            # Fallback for other kernel sizes\n            x = torch.nn.functional.linear(x, self.weight, self.bias)\n            x = x.unsqueeze(1)\n            x = torch.nn.functional.max_pool1d(x, kernel_size=self.kernel_size)\n            x = x.squeeze(1)\n            x = torch.sum(x, dim=1)\n            x = x * self.scale_factor\n            output = x\n        \n        return output\n\nbatch_size = 128\nin_features = 10\nout_features = 5\nkernel_size = 2\nscale_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_55_Matmul_MaxPool_Sum_Scale', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'max_difference': ['0.704939', '0.586457', '0.753231', '0.838420', '0.644629'], 'avg_difference': ['0.227003', '0.186672', '0.200441', '0.230876', '0.198333'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.105, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, \n    weight_ptr, \n    bias_ptr, \n    output_ptr, \n    in_features, \n    out_features, \n    scale_factor, \n    stride_x, \n    stride_out,\n    BLOCK_SIZE_IN: tl.constexpr, \n    BLOCK_SIZE_OUT: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Compute starting position for the current batch\n    x_offset = pid * stride_x\n    total_sum = 0.0\n\n    # Process output features in pairs\n    for j in range(0, out_features, 2):\n        # Load biases\n        bias0 = tl.load(bias_ptr + j)\n        if j + 1 < out_features:\n            bias1 = tl.load(bias_ptr + j + 1)\n        else:\n            bias1 = 0.0\n            \n        acc0 = bias0\n        acc1 = bias1\n        \n        # Process input features in blocks\n        for k_block in range(0, tl.cdiv(in_features, BLOCK_SIZE_IN)):\n            k_start = k_block * BLOCK_SIZE_IN\n            k_offsets = k_start + tl.arange(0, BLOCK_SIZE_IN)\n            mask = k_offsets < in_features\n            \n            # Load input block\n            x_vals = tl.load(x_ptr + x_offset + k_offsets, mask=mask, other=0.0)\n            \n            # Load weight block for first feature\n            w0_offsets = j * in_features + k_offsets\n            w0_vals = tl.load(weight_ptr + w0_offsets, mask=mask, other=0.0)\n            acc0 += tl.sum(x_vals * w0_vals)\n            \n            # Load weight block for second feature if exists\n            if j + 1 < out_features:\n                w1_offsets = (j + 1) * in_features + k_offsets\n                w1_vals = tl.load(weight_ptr + w1_offsets, mask=mask, other=0.0)\n                acc1 += tl.sum(x_vals * w1_vals)\n        \n        # Max pooling for the pair\n        if j + 1 < out_features:\n            max_val = tl.maximum(acc0, acc1)\n        else:\n            max_val = acc0\n        total_sum += max_val\n    \n    # Apply scaling and store result\n    total_sum = total_sum * scale_factor\n    tl.store(output_ptr + pid * stride_out, total_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, scale_factor):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.kernel_size = kernel_size\n        self.scale_factor = scale_factor\n        \n        # Initialize parameters\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        \n        # Use optimized kernel for kernel_size=2\n        if self.kernel_size == 2:\n            grid = (batch_size,)\n            BLOCK_SIZE_IN = 128\n            BLOCK_SIZE_OUT = 2\n            \n            _forward_kernel[grid](\n                x, self.weight, self.bias, output,\n                self.in_features, self.out_features, self.scale_factor,\n                x.stride(0), output.stride(0),\n                BLOCK_SIZE_IN=BLOCK_SIZE_IN,\n                BLOCK_SIZE_OUT=BLOCK_SIZE_OUT\n            )\n        else:\n            # Fallback for other kernel sizes\n            x = torch.nn.functional.linear(x, self.weight, self.bias)\n            x = x.unsqueeze(1)\n            x = torch.nn.functional.max_pool1d(x, kernel_size=self.kernel_size)\n            x = x.squeeze(1)\n            x = torch.sum(x, dim=1)\n            x = x * self.scale_factor\n            output = x\n        \n        return output\n\nbatch_size = 128\nin_features = 10\nout_features = 5\nkernel_size = 2\nscale_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given that the current program compiles but fails correctness (output mismatch).\n The current kernel is designed for kernel_size=2 and uses a custom kernel that:\n   - For each sample in the batch (pid):\n        total_sum = 0.0\n        For each pair of output features (j, j+1):\n            Load bias for j and j+1 (if exists)\n            For blocks of input features:\n                Load a block of input features (x_vals)\n                Load corresponding weights for feature j and j+1\n                Compute acc0 = bias0 + sum(x_vals * w0_vals) and similarly for acc1 (if exists)\n            Then, for the pair, take max(acc0, acc1) (if j+1 exists) and add to total_sum.\n        Then scale total_sum by scale_factor and store.\n\n However, the reference PyTorch code (fallback) does:\n        x = linear(x, weight, bias)  # [batch_size, out_features]\n        x = x.unsqueeze(1)  # [batch_size, 1, out_features]\n        x = max_pool1d(x, kernel_size=kernel_size)  # [batch_size, 1, out_features // kernel_size] (if kernel_size=2, then out_features must be even? but here out_features=5 -> odd)\n        x = x.squeeze(1)  # [batch_size, out_features//kernel_size]\n        x = torch.sum(x, dim=1)  # [batch_size]\n        x = x * scale_factor\n\n The issue: \n   - The kernel does max_pool over adjacent features? But note: the reference does max_pool1d with kernel_size, which in the case of kernel_size=2, it reduces the out_features dimension by taking max over every 2 consecutive features? Actually, the output of max_pool1d with kernel_size=2 and stride=2 would reduce the dimension to (out_features+1)//2? But note: the max_pool1d by default uses stride=kernel_size? Actually, the documentation says: if stride is not provided, it defaults to kernel_size.\n\n   However, our kernel does:\n        for j in [0,2,4,...,out_features-1]:\n            take two features j and j+1 -> compute max(acc0, acc1) and then add to total_sum.\n\n        But then the reference does: \n            max_pool1d -> then sum over the reduced features.\n\n        So in the reference, for kernel_size=2 and out_features=5:\n            linear output: [batch_size, 5]\n            unsqueeze: [batch_size, 1, 5]\n            max_pool1d: with kernel_size=2, stride=2 -> output: [batch_size, 1, 3]  (because (5-2)//2+1 = 3? Actually, no, the formula is floor((n + 2*padding - dilation*(kernel_size-1) -1)/stride + 1). Without padding, it would be (5-2)//2+1 = 2? Actually, let me check: \n                input: [1,5] -> kernel_size=2, stride=2 -> [1, (5-2)//2+1] = [1,2]? \n            Then it sums over the last dimension? Actually, the code does: \n                torch.sum(x, dim=1)  # after squeeze(1) we have [batch_size, out_features//kernel_size]? but then we sum over dim=1 -> [batch_size]\n\n        But wait: the kernel in the reference does:\n            x = torch.sum(x, dim=1)   # so if after max_pool1d we have [batch_size, 1, 3], then squeeze(1) gives [batch_size, 3] and then sum(dim=1) gives [batch_size].\n\n        Our kernel does:\n            total_sum = max(acc0,acc1) for each pair and then adds all these maxes? \n            For out_features=5, we have pairs: (0,1) -> max0, (2,3) -> max1, (4) -> max2. Then total_sum = max0+max1+max2.\n\n        But the reference does: \n            max_pool1d: \n                [ [x0, x1, x2, x3, x4] ] -> becomes [ max(x0,x1), max(x2,x3), x4 ]? Actually, no: because with kernel_size=2 and stride=2, it will take windows: [0:2] -> max, [2:4] -> max, [4:5] -> but the window is only one element? So it becomes [max(x0,x1), max(x2,x3), x4] and then we sum these three -> max0+max1+x4.\n\n        Our kernel for j=4: we take max(acc0, ...) but there is no j+1, so we take acc0 (which is the linear result for feature 4). So we are effectively doing: max0+max1+max2 (where max2 = feature4) -> same as reference.\n\n        Then why the mismatch?\n\n Let me check the kernel again:\n\n   The kernel does:\n        total_sum = 0.0\n        for j in range(0, out_features, 2):\n            ... \n            for k_block ...:   # this does the dot product for the entire row? for feature j and j+1?\n            then max_val = max(acc0, acc1) if exists, else acc0\n            total_sum += max_val\n\n        Then total_sum *= scale_factor.\n\n   This matches the reference.\n\n But wait: the reference does max_pool1d on the entire tensor? It does max over adjacent features? But note: the kernel in the reference does max_pool1d with kernel_size and then the entire window? It doesn't skip by 2? It does windows of consecutive features? But in our kernel, we are grouping features 0 and 1, then 2 and 3, then 4. That is consecutive.\n\n So the algorithm seems correct.\n\n Why the correctness fails? \n\n Possible issues:\n\n  1. The kernel does not handle odd out_features? It does: for j=4, it loads bias4 and then only one weight row? That seems correct.\n\n  2. The indices for weights: \n        w0_offsets = j * in_features + k_offsets\n        w1_offsets = (j+1)*in_features + k_offsets\n\n  3. The accumulation: \n        acc0 += tl.sum(x_vals * w0_vals)   -> this is a scalar per feature? \n\n  4. The max_val: for a pair (j, j+1) we take the max of the two scalars? Then total_sum adds these maxes.\n\n  5. The scaling: total_sum * scale_factor.\n\n  But the reference: \n        linear -> [batch, out_features] \n        then max_pool1d -> [batch, 1, (out_features+1)//2]? Actually, the dimension of the max_pool1d output is (out_features - kernel_size) // stride + 1? With stride=kernel_size=2, and no padding, then for 5: (5-2)//2+1 = 2? That doesn't match 3.\n\n  Actually, the formula for max_pool1d with kernel_size=2, stride=2, padding=0, dilation=1:\n        output_size = (input_size - kernel_size) // stride + 1 = (5-2)//2+1 = (3)//2+1 = 1+1 = 2? \n\n  But wait: the documentation says: \n        When ceil_mode=False (default), floor((5+2*0-1*(2-1)-1)/2+1) = floor((5-1-1)/2+1) = floor(3/2+1)=floor(1.5+1)=2.5 -> 2? \n\n  So the output after max_pool1d would be of size [batch, 1, 2]? Then we squeeze to [batch,2] and then sum over dim=1 -> [batch].\n\n  But our kernel: \n        We process 5 features: we have 3 groups: (0,1) -> max0, (2,3) -> max1, (4) -> max2? \n        Then we add max0+max1+max2.\n\n  The reference: \n        max_pool1d: \n            window0: [0,1] -> max0\n            window1: [2,3] -> max1\n            window2: [4] is not taken? because the window starts at index4: [4,5] -> but 5 is out of bounds? so the last window is only [4]? \n        Actually, the max_pool1d with kernel_size=2 and stride=2 on a dimension of size 5 would take:\n            window0: [0,1] -> max0\n            window1: [2,3] -> max1\n            window2: [4] -> but the kernel_size=2 requires two elements? so if we don't have padding, then the last window is skipped? \n\n        The default behavior of max_pool1d without padding is to not include the last window if it doesn't fit. So the output would be 2? \n\n  Therefore, the reference output for out_features=5 and kernel_size=2 would be:\n        linear: [batch,5]\n        max_pool1d: [batch,2]   (taking windows [0:2] and [2:4])\n        then sum over the two -> max0+max1.\n\n  But our kernel does: max0+max1+max2 (where max2 is the value of the 5th feature).\n\n  So the kernel is incorrect for odd out_features? \n\n  How to fix?\n\n  We must mimic the max_pool1d behavior: it uses sliding windows of size kernel_size, and moves by stride (default=kernel_size). The output length is (out_features - kernel_size) // stride + 1? But with floor division? \n\n  Actually, the formula is: floor((n + 2*pad - dilation*(kernel_size-1) - 1)/stride + 1)\n\n  Without padding and dilation=1: floor((n - kernel_size)/stride + 1)\n\n  For n=5, kernel_size=2, stride=2: (5-2)/2+1 = 3/2+1 = 1.5+1=2.5 -> floor=2.\n\n  So the reference only does 2 windows.\n\n  Therefore, the kernel should only process two windows? But we are processing 3 groups? \n\n  We must change the kernel to do:\n\n        total_windows = (out_features - kernel_size) // kernel_size + 1   -> 2 for out_features=5, kernel_size=2.\n\n        But note: the kernel_size is passed as an argument. In the current kernel, we are only using the kernel for kernel_size=2? and the fallback for others.\n\n        However, the kernel currently does not use the kernel_size parameter? It does pairs (which is kernel_size=2) but for the entire out_features? \n\n        The reference max_pool1d with kernel_size=2 and stride=2 does not cover the last feature if the number of features is odd? \n\n  How to fix:\n\n        We must change the loop to run for only (out_features // kernel_size) windows? but wait: the number of windows is (out_features - kernel_size) // stride + 1? \n\n        Actually, in our case, the kernel_size is 2 and we are using stride=2? \n\n        So the kernel should do:\n\n            for window_idx in range(0, num_windows):\n                j = window_idx * kernel_size   # kernel_size=2\n                # then load two consecutive features? \n\n            Then the last feature (if out_features is odd) is not included? \n\n        But the reference does not include the last feature? \n\n  However, the problem: the reference code in the fallback does:\n\n        x = torch.nn.functional.max_pool1d(x, kernel_size=self.kernel_size)\n\n        and for kernel_size=2 and input of size [batch,1,5], the output is [batch,1,2].\n\n        Therefore, the kernel must not include the last feature when out_features is odd? \n\n  So we change:\n\n        We compute: num_windows = (out_features) // kernel_size   ??? -> for 5//2=2? then we do two windows.\n\n        But then the last feature (index 4) is not included? \n\n  Alternatively, we can compute the number of windows as: (out_features + kernel_size - 1) // kernel_size? That would be ceil(out_features/kernel_size). But that is 3 for 5 and 2.\n\n        But the reference uses floor((n - kernel_size)/stride + 1) which is 2.\n\n  So we must use the same formula as max_pool1d: \n\n        num_windows = (out_features - kernel_size) // kernel_size + 1   -> for 5: (5-2)//2+1 = 3//2+1 = 1+1=2.\n\n  Therefore, we change the loop to:\n\n        num_windows = (out_features - kernel_size) // kernel_size + 1\n        for window_idx in range(0, num_windows):\n            j = window_idx * kernel_size\n\n        But note: the kernel_size is passed to the model. In the kernel, we don't have kernel_size as a kernel argument? Currently, we are only using the kernel for kernel_size=2. But the kernel is written for kernel_size=2? \n\n        However, the kernel currently does:\n\n            for j in range(0, out_features, 2)   -> step=2.\n\n        This does 3 iterations for out_features=5.\n\n        We want to do 2 iterations? \n\n  How about we change the loop to:\n\n        j = 0\n        while j < out_features:\n            # if we have at least kernel_size (2) features left, then take two\n            if j + kernel_size <= out_features:\n                # take two\n                ... \n                j += kernel_size\n            else:\n                # take the remaining? but the reference doesn't take the last one? \n                # because the window would be incomplete? \n                break\n\n        But the reference doesn't take the last one? \n\n  Actually, the reference max_pool1d without padding and without ceil_mode does not include the last window if it is incomplete? \n\n  Therefore, we break when j + kernel_size > out_features.\n\n  So we change the loop to:\n\n        j = 0\n        while j < out_features:\n            if j + kernel_size > out_features:\n                break\n            # process features j and j+1\n            ... \n            j += kernel_size\n\n        Then we do only two windows for out_features=5.\n\n  But note: the kernel currently uses a step of 2? and we are changing to step by kernel_size (which is 2) but break when the next window doesn't fit? \n\n  Alternatively, we can precompute the number of windows:\n\n        num_windows = (out_features - kernel_size) // kernel_size + 1   # for kernel_size=2, stride=2, padding=0\n\n        Then:\n\n        for window_idx in range(0, num_windows):\n            j = window_idx * kernel_size\n\n        Then we do exactly num_windows windows.\n\n  However, note: the kernel currently does not have kernel_size as an argument? \n\n  Looking at the kernel signature:\n\n        _forward_kernel( ... in_features, out_features, scale_factor, ... BLOCK_SIZE_IN, BLOCK_SIZE_OUT)\n\n  We don't have kernel_size in the kernel.\n\n  But the kernel is called only when self.kernel_size==2? So we can hardcode kernel_size=2? \n\n  Alternatively, we can pass kernel_size as an argument? \n\n  Since the kernel is only called for kernel_size=2, we can hardcode:\n\n        num_windows = (out_features - 2) // 2 + 1   # if out_features is even: (out_features-2)//2+1 = (out_features)//2 -1+1 = out_features//2\n        for window_idx in range(0, num_windows):\n            j = window_idx * 2\n\n  But wait: what if out_features is 5? then (5-2)//2+1 = 3//2+1 = 1+1=2 -> correct.\n\n  But note: the formula (out_features - kernel_size) // stride + 1, and we use stride=kernel_size=2.\n\n  So we can compute:\n\n        num_windows = (out_features - 2) // 2 + 1\n\n  But integer division in Triton: // is floor division? \n\n  Example: \n        out_features=5: (5-2)=3, 3//2=1, 1+1=2 -> correct.\n\n        out_features=4: (4-2)//2+1 = 2//2+1=1+1=2 -> which is 4//2=2 windows? correct.\n\n        out_features=3: (3-2)//2+1 = 1//2+1=0+1=1 -> correct: one window.\n\n        out_features=2: (2-2)//2+1=0+1=1 -> correct.\n\n        out_features=1: (1-2) is negative? then (1-2)//2+1 -> negative? \n\n  We must avoid negative. Also, note: if out_features < kernel_size, then we don't do any window? and the output would be 0? \n\n  But the reference max_pool1d on a tensor of size [batch,1,1] with kernel_size=2: \n        (1-2)//2+1 = negative? -> but the formula: floor((1 + 2*0 - 1*(2-1) -1)/2+1) = floor((1-1-1)/2+1)=floor(-1/2+1)=floor(0.5)=0? \n        So the output would be 0? \n\n  Therefore, we must do:\n\n        num_windows = 0\n        if out_features >= kernel_size:\n            num_windows = (out_features - kernel_size) // stride + 1   # with stride=2\n\n  But in our kernel, we are hardcoding kernel_size=2 and stride=2? \n\n  Since we are in the kernel for kernel_size=2, we can do:\n\n        num_windows = (out_features - 2) // 2 + 1\n        if num_windows < 0:\n            num_windows = 0\n\n  But note: Triton uses C-style integer division: for nonnegative numbers, it's floor. For negative, we don't have to worry because if out_features<2, then (out_features-2) is negative -> then we set to 0.\n\n  Alternatively, we can compute:\n\n        num_windows = tl.cdiv(out_features - 2, 2) + 1   # but wait: we want floor division? \n\n  Actually, we want:\n\n        num_windows = (out_features - 2) // 2 + 1   if (out_features>=2) else 0.\n\n  We can do:\n\n        num_windows = 0\n        if out_features >= 2:\n            num_windows = (out_features - 2) // 2 + 1\n\n  But in Triton, we can write:\n\n        num_windows = (out_features - 2 + 1) // 2   # ??? not exactly.\n\n  Alternatively, we can compute:\n\n        num_windows = (out_features - 1) // 2   # because for out_features=5: (5-1)//2=2, for 4: (4-1)//2=1 -> that's not 2.\n\n  Let me not overcomplicate: we can write:\n\n        num_windows = 0\n        if out_features >= 2:\n            num_windows = (out_features - 2) // 2 + 1\n        else:\n            num_windows = 0\n\n  But note: Triton doesn't have if at the top level? We can do:\n\n        num_windows = tl.where(out_features>=2, (out_features-2)//2+1, 0)\n\n  But we are using this in a for loop? and the range must be nonnegative? \n\n  Alternatively, we can do:\n\n        num_windows = (out_features - 2 + 1) // 2   # this gives ceil((out_features-2+1)/2) ??? \n\n  Actually, we want the number of windows: which is (out_features-2)//2+1 for nonnegative and >=2.\n\n  Let me compute for out_features=2: (2-2)//2+1 = 0+1=1 -> correct.\n  out_features=3: (3-2)//2+1 = 1//2+1=0+1=1 -> correct.\n  out_features=4: (4-2)//2+1=1+1=2 -> correct.\n  out_features=5: (5-2)//2+1=1+1=2 -> correct.\n\n  So we can compute:\n\n        num_windows = (out_features - 2) // 2 + 1\n\n  But if out_features<2, then (out_features-2) is negative -> then we get negative? \n\n  So we clamp:\n\n        num_windows = tl.maximum(0, (out_features - 2) // 2 + 1)\n\n  But note: integer division of negative numbers: in Triton, // for negative? we don't want negative.\n\n  Alternatively, we can do:\n\n        num_windows = 0\n        if out_features > 1:\n            num_windows = (out_features - 2) // 2 + 1\n\n  But we are in the kernel and we cannot break control flow? \n\n  We can write:\n\n        num_windows = tl.maximum(0, (out_features - 2) // 2 + 1)\n\n  However, when out_features=1: (1-2)//2+1 = (-1)//2+1. In Python, (-1)//2 = -1? then -1+1=0 -> correct.\n\n        out_features=0: (0-2)//2+1 = (-2)//2+1 = -1+1=0 -> correct.\n\n  So we can compute num_windows = (out_features - 2) // 2 + 1, and it will be 0 when out_features<2.\n\n  Therefore, we change the loop:\n\n        num_windows = (out_features - 2) // 2 + 1\n        for window_idx in range(0, num_windows):\n            j = window_idx * 2\n\n        And then inside the window, we process two consecutive features.\n\n  But note: the kernel currently does:\n\n        for j in range(0, out_features, 2):\n\n        and then checks if j+1 < out_features? \n\n        Now we are iterating only over complete windows? so we don't need to check j+1? because j+1 is always present? \n\n        Because we are only iterating when j+1 is within [0, out_features-1]? \n\n        Actually, we are iterating for j = 0,2,4,..., up to (num_windows-1)*2, and we know that j+1 is always < out_features? \n\n        But wait: for out_features=5, we do two windows: j=0 and j=2. For j=0: we have j+1=1 (which is <5). For j=2: we have j+1=3 (which is <5). Then we break? \n\n        But note: the window for j=4 is not included? because (4+2=6>5 -> not included). \n\n        So we don't do the last feature? \n\n        But the reference doesn't do the last feature? \n\n        Therefore, we remove the part that handles the last odd feature? \n\n  So the kernel becomes:\n\n        total_sum = 0.0\n        num_windows = (out_features - 2) // 2 + 1   # number of complete windows of size 2\n        for window_idx in range(0, num_windows):\n            j = window_idx * 2\n            # Load two biases\n            bias0 = tl.load(bias_ptr + j)\n            bias1 = tl.load(bias_ptr + j+1)\n            acc0 = bias0\n            acc1 = bias1\n            for k_block in ...:\n                ... # same as before for the two features\n            max_val = tl.maximum(acc0, acc1)\n            total_sum += max_val\n\n        Then scale and store.\n\n  But what about the case when out_features==0 or 1? then num_windows=0 and total_sum=0? \n\n  For out_features=1: the reference would do?\n        linear: [batch,1]\n        max_pool1d: kernel_size=2 -> requires at least 2? so it would output 0 elements? then the output after max_pool1d is [batch,1,0]? then squeeze to [batch,0]? then sum over dim=1 -> zeros? \n\n        So the kernel returns 0? which matches.\n\n  Therefore, we change the kernel to use num_windows.\n\n  Let's update the kernel accordingly.\n\n  Also note: the kernel currently uses BLOCK_SIZE_OUT? but we are not using it? We can remove it? but the signature must remain? \n\n  We are not allowed to change the signature? \n\n  So we leave it.\n\n  Steps:\n\n        Replace the loop:\n            for j in range(0, out_features, 2):\n        with:\n\n            num_windows = (out_features - 2) // 2 + 1\n            for window_idx in range(0, num_windows):\n                j = window_idx * 2\n\n        And inside, we don't need the if for j+1 because we know j+1 exists? \n\n        But what if out_features is exactly 1? then num_windows=0 -> we skip the loop? \n\n        And if out_features==0, same.\n\n        But what if out_features==2? then num_windows= (2-2)//2+1 = 0+1=1 -> we do one window.\n\n        So we remove the condition for j+1? \n\n        However, note: we are loading bias for j+1: \n            bias1 = tl.load(bias_ptr + j+1)\n\n        This is safe because j+1 is at most (num_windows-1)*2+1 = ( (out_features-2)//2 )*2 + 1 = (out_features-2) [if even] + 1 = out_features-1 -> which is valid.\n\n        So we can remove the condition.\n\n  But note: the old code had a fallback for the last odd feature? now we are removing that.\n\n  This matches the reference.\n\n  Let me check the example out_features=5: \n        We do two windows: j=0 and j=2 -> we process features [0,1] and [2,3] -> then we skip feature 4? \n\n        The reference does the same: it only processes [0,1] and [2,3] -> output of max_pool1d is two elements? then sum them.\n\n  So we change the kernel accordingly.\n\n  Also, note: the old code had an else branch for the last feature? we remove that.\n\n  We also remove the condition inside the k_block loop for the second feature? because we are only processing two features? \n\n  But wait: we are now always processing two features? so we can remove the condition:\n\n        if j + 1 < out_features: \n            ... \n\n        Now we don't need that condition.\n\n  So we change:\n\n        Inside the k_block loop, we always load two weight rows? \n\n  But what if out_features is 1? then we don't enter the window loop? so no problem.\n\n  Therefore, we remove the condition for the second feature.\n\n  The new kernel code for the window loop:\n\n        num_windows = (out_features - 2) // 2 + 1\n        for window_idx in range(0, num_windows):\n            j = window_idx * 2\n            bias0 = tl.load(bias_ptr + j)\n            bias1 = tl.load(bias_ptr + j+1)\n            acc0 = bias0\n            acc1 = bias1\n            for k_block in ...:\n                ... \n                # Load input block\n                ...\n                # Load weight block for j\n                w0_offsets = j * in_features + k_offsets\n                w0_vals = tl.load(weight_ptr + w0_offsets, mask=mask, other=0.0)\n                # Load weight block for j+1\n                w1_offsets = (j+1) * in_features + k_offsets\n                w1_vals = tl.load(weight_ptr + w1_offsets, mask=mask, other=0.0)\n                acc0 += tl.sum(x_vals * w0_vals)\n                acc1 += tl.sum(x_vals * w1_vals)\n            max_val = tl.maximum(acc0, acc1)\n            total_sum += max_val\n\n  And then scale.\n\n  Let's update the kernel accordingly.\n\n  Note: We are not allowed to change the function signature? so we leave BLOCK_SIZE_OUT even though we don't use it.\n\n  Also, note: the kernel is called with BLOCK_SIZE_OUT=2? we can remove that? but we cannot change the call? \n\n  We leave it.\n\n  We also note: the kernel must be correct for any kernel_size? but we are only calling it for kernel_size=2? \n\n  But wait: what if the model is called with kernel_size=2, but we are processing the max_pool in a way that is only valid for kernel_size=2? \n\n  The reference max_pool1d for kernel_size=2 is fixed. \n\n  Therefore, we update the kernel.\n\n  Let me write the updated kernel code.\n\n  However, note: the model has a fallback for kernel_size !=2? so we are safe.\n\n  We change the kernel loop and remove the condition for the second feature.\n\n  Also, we remove the handling for the last odd feature.\n\n  We hope this will fix the correctness.\n\n  We also note: the reference timing might be faster? but we are not changing the performance characteristics? \n\n  Let's update.\n\n  IMPORTANT: We are only allowed to change the Triton kernel source? and we are not allowed to change the grid or the class? \n\n  But note: the kernel code is inside the EVOLVE-BLOCK? so we change the kernel.\n\n  We also note: the model class and the rest of the code remains.\n\n  We output the entire block.\n\n  But note: the current program has:\n\n        class ModelNew ... \n\n        and then batch_size, in_features, ... and get_inputs, get_init_inputs.\n\n  We leave that.\n\n  We change the kernel.\n\n  Code:\n</think>\nThe previous implementation incorrectly handled the max-pooling operation by processing all output features, including incomplete windows. This caused output mismatches with the PyTorch reference implementation. The fix involves:\n\n1. Calculating the exact number of complete windows (num_windows) using the max-pooling formula: (out_features - kernel_size) // stride + 1\n2. Processing only complete windows to match PyTorch's max_pool1d behavior\n3. Removing special handling for incomplete windows since they're excluded by design\n4. Always processing two features per window since the kernel is only used for kernel_size=2\n\nThese changes ensure numerical equivalence with the reference implementation while maintaining the same computation pattern.\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _forward_kernel(\n    x_ptr, \n    weight_ptr, \n    bias_ptr, \n    output_ptr, \n    in_features, \n    out_features, \n    scale_factor, \n    stride_x, \n    stride_out,\n    BLOCK_SIZE_IN: tl.constexpr, \n    BLOCK_SIZE_OUT: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    # Compute starting position for the current batch\n    x_offset = pid * stride_x\n    total_sum = 0.0\n\n    # Calculate number of complete windows (kernel_size=2, stride=2)\n    num_windows = (out_features - 2) // 2 + 1\n    \n    # Process each complete window\n    for window_idx in range(0, num_windows):\n        j = window_idx * 2\n        # Load biases for both features in window\n        bias0 = tl.load(bias_ptr + j)\n        bias1 = tl.load(bias_ptr + j + 1)\n            \n        acc0 = bias0\n        acc1 = bias1\n        \n        # Process input features in blocks\n        for k_block in range(0, tl.cdiv(in_features, BLOCK_SIZE_IN)):\n            k_start = k_block * BLOCK_SIZE_IN\n            k_offsets = k_start + tl.arange(0, BLOCK_SIZE_IN)\n            mask = k_offsets < in_features\n            \n            # Load input block\n            x_vals = tl.load(x_ptr + x_offset + k_offsets, mask=mask, other=0.0)\n            \n            # Load weight blocks for both features\n            w0_offsets = j * in_features + k_offsets\n            w0_vals = tl.load(weight_ptr + w0_offsets, mask=mask, other=0.0)\n            w1_offsets = (j + 1) * in_features + k_offsets\n            w1_vals = tl.load(weight_ptr + w1_offsets, mask=mask, other=0.0)\n            \n            # Accumulate dot products\n            acc0 += tl.sum(x_vals * w0_vals)\n            acc1 += tl.sum(x_vals * w1_vals)\n        \n        # Apply max pooling to window\n        max_val = tl.maximum(acc0, acc1)\n        total_sum += max_val\n    \n    # Apply scaling and store result\n    total_sum = total_sum * scale_factor\n    tl.store(output_ptr + pid * stride_out, total_sum)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, scale_factor):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.kernel_size = kernel_size\n        self.scale_factor = scale_factor\n        \n        # Initialize parameters\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, device=x.device, dtype=torch.float32)\n        \n        # Use optimized kernel for kernel_size=2\n        if self.kernel_size == 2:\n            grid = (batch_size,)\n            BLOCK_SIZE_IN = 128\n            BLOCK_SIZE_OUT = 2\n            \n            _forward_kernel[grid](\n                x, self.weight, self.bias, output,\n                self.in_features, self.out_features, self.scale_factor,\n                x.stride(0), output.stride(0),\n                BLOCK_SIZE_IN=BLOCK_SIZE_IN,\n                BLOCK_SIZE_OUT=BLOCK_SIZE_OUT\n            )\n        else:\n            # Fallback for other kernel sizes\n            x = torch.nn.functional.linear(x, self.weight, self.bias)\n            x = x.unsqueeze(1)\n            x = torch.nn.functional.max_pool1d(x, kernel_size=self.kernel_size)\n            x = x.squeeze(1)\n            x = torch.sum(x, dim=1)\n            x = x * self.scale_factor\n            output = x\n        \n        return output\n\nbatch_size = 128\nin_features = 10\nout_features = 5\nkernel_size = 2\nscale_factor = 0.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, kernel_size, scale_factor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
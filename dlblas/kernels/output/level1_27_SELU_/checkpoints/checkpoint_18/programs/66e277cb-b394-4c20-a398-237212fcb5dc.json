{"id": "66e277cb-b394-4c20-a398-237212fcb5dc", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 2048}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 4096}, num_warps=8),\n    ],\n    key=['n_elements']\n)\n@triton.jit\ndef _selu_kernel(\n    x_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Load input\n    x = tl.load(x_ptr + offsets, mask=mask)\n    \n    # SELU constants\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n    \n    # Compute SELU activation\n    zero = 0.0\n    result = tl.where(x > zero, \n                     scale * x, \n                     scale * alpha * (tl.exp(x) - 1.0))\n    \n    # Store result\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        n_elements = x.numel()\n        output = torch.empty_like(x)\n        \n        if n_elements == 0:\n            return output\n            \n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _selu_kernel[grid](x, output, n_elements)\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "2e4a9aa1-6d93-4bac-9d89-374d56fbed3f", "generation": 1, "timestamp": 1754634242.3555458, "iteration_found": 2, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.4, "runtime_ms": 0.0507, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0203, "speed_up": 0.4, "custom_timing": 0.0507}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0157, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0203, "speed_up": 1.293, "custom_timing": 0.0157}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0157\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0203, 'speed_up': 1.293, 'custom_timing': 0.0157}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0157, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0203, 'speed_up': 1.293, 'custom_timing': 0.0157}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a SELU activation.\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies SELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with SELU applied, same shape as input.\n        \"\"\"\n        return torch.selu(x)\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0157), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0203, 'speed_up': 1.293, 'custom_timing': 0.0157})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a SELU activation.\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies SELU activation to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of any shape.\n\n        Returns:\n            torch.Tensor: Output tensor with SELU applied, same shape as input.\n        \"\"\"\n        return torch.selu(x)\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a simple model that applies the SELU activation function. The current implementation uses `torch.selu` which is a built-in PyTorch function. However, we are to rewrite it using Triton to potentially get a speedup.\n\nThe SELU activation function is defined as:\n  selu(x) = scale * (max(0, x) + min(0, alpha * (exp(x) - 1)))\nwhere scale = 1.0507009873554804934193349852946 and alpha = 1.6732632423543772848170429916717.\n\nWe note that the current implementation (using `torch.selu`) is already efficient, but we are to write a Triton kernel to replace it and hopefully get a speedup.\n\nPlan:\n1. We will write a Triton kernel that computes the SELU activation element-wise.\n2. We will launch one program per element in the input tensor. However, note that the input tensor can be of any shape. We will flatten the tensor for the kernel and then reshape back.\n3. We must be cautious about numerical precision and boundary conditions.\n\nSteps in the kernel:\n  - Each element is processed independently.\n  - For each element, we compute:\n        if x > 0:\n            result = scale * x\n        else:\n            result = scale * alpha * (exp(x) - 1)\n  - We then write the result back.\n\nWe note that the current model has an input of shape (batch_size, dim) = (16, 16384). We can set the grid to be (batch_size * dim // BLOCK_SIZE, ) and use a 1D kernel.\n\nImportant: We must use the same constants for scale and alpha as the PyTorch implementation.\n\nLet's write the kernel:\n\nWe'll define:\n    scale = 1.0507009873554804934193349852946\n    alpha = 1.6732632423543772848170429916717\n\nWe note that the input and output are of the same shape and type (float32, as the input is torch.randn which is float32 by default).\n\nWe must also note that the input tensor is contiguous (as per the example) but we can make sure in the function.\n\nWe'll design the kernel with a BLOCK_SIZE that is a power of two and at least 1024.\n\nWe'll use:\n    pid = tl.program_id(0)\n    off = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = off < n_elements\n\nThen we load the data at off with mask, compute the SELU, and store.\n\nWe must be cautious with the exponential function: we use `tl.exp` from Triton.\n\nWe'll also use autotuning for the BLOCK_SIZE and number of warps.\n\nHowever, note that the problem size is 16*16384 = 262144 elements. We can set the grid accordingly.\n\nLet's write the code accordingly.\n\nWe'll create a new class `ModelNew` that uses the Triton kernel in the forward pass.\n\nWe must also adjust the `get_inputs` and `get_init_inputs` to remain the same.\n\nWe note that the original program did not have any parameters, so we don't need to change the __init__.\n\nWe will replace the forward function to use the Triton kernel.\n\nWe'll write the kernel in the same file.\n\nImportant: We must not change the function signatures of the ModelNew class, and the get_inputs and get_init_inputs functions.\n\nWe'll write the kernel as a jit function and then call it in the forward.\n\nLet's code accordingly.\n\nNote: We are only allowed to change the code within the EVOLVE-BLOCK. We must not change the class name or the function names.\n\nWe are going to do:\n    class ModelNew(nn.Module):\n        def forward(self, x):\n            # Flatten x to 1D\n            n_elements = x.numel()\n            output = torch.empty_like(x)\n            # If the tensor is empty, return immediately\n            if n_elements == 0:\n                return output\n            # Launch kernel\n            grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n            # We pass the flattened view\n            _selu_kernel[grid](x.view(-1), output.view(-1), n_elements)\n            return output\n\nWe must write the kernel with proper masking.\n\nWe'll define the kernel with:\n    @triton.jit\n    def _selu_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n        pid = tl.program_id(0)\n        off = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        mask = off < n_elements\n        x = tl.load(x_ptr + off, mask=mask)\n        # Compute SELU\n        scale = 1.0507009873554804934193349852946\n        alpha = 1.6732632423543772848170429916717\n        zero = 0.0\n        # Condition: x > 0\n        positive_part = tl.where(x > zero, x, zero)\n        negative_part = tl.where(x <= zero, x, zero)\n        # For negative part: alpha * (exp(negative_part) - 1)\n        negative_val = alpha * (tl.exp(negative_part) - 1.0)\n        # Then combine: scale * (positive_part + negative_val)\n        result = scale * (positive_part + negative_val)\n        tl.store(output_ptr + off, result, mask=mask)\n\nBut note: we can do without splitting into two parts? We can do:\n        result = tl.where(x > 0, scale * x, scale * alpha * (tl.exp(x) - 1))\n\nBut note: when x<=0, we use the exponential form. However, note that the exponential form is only defined for the negative part? Actually, for x<=0 we use the exponential, and for x>0 we use the linear part.\n\nBut the expression for the negative part uses the entire x? Actually, yes: for x<=0, we compute: scale * alpha * (exp(x)-1). And for x>0, we compute: scale * x.\n\nSo we can write:\n        result = tl.where(x > 0, scale * x, scale * alpha * (tl.exp(x) - 1))\n\nBut note: the original definition is:\n        scale * (max(0,x) + min(0, alpha*(exp(x)-1)))\nBut note: min(0, alpha*(exp(x)-1)) is negative when x<=0? Actually, when x<=0, (exp(x)-1) is negative? Actually, for x<=0, exp(x) is in (0,1] so (exp(x)-1) is in (-1,0]. Then multiplied by alpha (positive) it is negative. Then we add the two? But the above expression by conditionals is equivalent.\n\nAlternatively, we can write without conditionals? But conditionals are fine.\n\nBut note: we can use the Triton where.\n\nWe'll use the conditional expression.\n\nWe'll also autotune the block size.\n\nWe note that the kernel is memory bound, so we should use a large block size to improve memory coalescing.\n\nWe'll set several configs for BLOCK_SIZE and number of warps.\n\nWe'll use:\n    configs = [\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=1),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=2),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n    ]\n\nBut note: the autotune key is the n_elements? Actually, we can use the same config for any n_elements? We'll set the key to ['n_elements'].\n\nAlternatively, we can set the key to []. Because the kernel is simple and the problem size is fixed? But note the problem size can vary. We'll use the n_elements as the key.\n\nBut note: the autotuner in the example above used key=[\"n_elements\"].\n\nWe'll do the same.\n\nWe'll write the kernel with autotune.\n\nHowever, note that the kernel is very simple and might be memory bound. We want to maximize memory throughput.\n\nWe'll try to use vectorized loads and stores? But the data is contiguous and we are processing in a 1D fashion. The Triton compiler might automatically vectorize. We can also try to use the `tl.make_block_ptr` for better memory access? But for element-wise, the current approach is standard.\n\nWe'll proceed with the above.\n\nLet's code accordingly.\n\nNote: We must import triton and triton.language.\n\nWe'll add the necessary imports inside the EVOLVE-BLOCK.\n\nBut note: the original code did not import triton. We must import them.\n\nWe'll do:\n    import triton\n    import triton.language as tl\n\nBut note: the entire code block must be self-contained.\n\nWe'll write the entire code within the EVOLVE-BLOCK.\n\nWe note that the original program had:\n    import torch\n    import torch.nn as nn\n\nWe'll keep those and add the triton imports.\n\nWe'll also note that the kernel must be defined at the top level? Actually, it can be inside the class? But we'll define it outside the class.\n\nWe'll define the kernel as a top-level function.\n\nWe'll write:\n\n    import math\n    import triton\n    import triton.language as tl\n\n    @triton.autotune(...)\n    @triton.jit\n    def _selu_kernel(...):\n        ...\n\nThen in the forward of ModelNew, we call this kernel.\n\nBut note: the kernel must be compiled for the current device. We are launching it in the forward.\n\nWe'll write the code accordingly.\n\nWe must be cautious: the kernel uses constants (scale and alpha) that are float32. We are using them directly in the kernel, which is fine.\n\nWe'll now write the complete code within the EVOLVE-BLOCK.\n\nWe note that the current program has:\n    batch_size = 16\n    dim = 16384\n\nWe must not change these.\n\nWe also note that the get_inputs function returns a tensor of shape (batch_size, dim). Our kernel will work on any contiguous tensor.\n\nWe'll do:\n\n    class ModelNew(nn.Module):\n        def forward(self, x):\n            # We'll flatten the tensor and then unflatten after.\n            n_elements = x.numel()\n            output = torch.empty_like(x)\n            if n_elements == 0:\n                return output\n            # We use a 1D grid\n            grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n            # Launch kernel\n            _selu_kernel[grid](x.view(-1), output.view(-1), n_elements)\n            return output\n\nWe must define the kernel with the same device as x.\n\nBut note: the kernel launch will automatically use the current device of the pointers.\n\nWe are ready.\n\nLet's write the complete code.\n\nNote: We are to return the entire code block (with the EVOLVE-BLOCK markers) and nothing else.\n\nWe must be cautious about the performance. We are hoping that the Triton kernel can be faster than the PyTorch built-in because of optimized memory access and autotuning.\n\nBut note: PyTorch's built-in selu is already highly optimized. However, we are to try.\n\nWe'll run the kernel with the autotuner to pick the best configuration.\n\nWe'll set the autotune configs as above.\n\nWe note that the kernel is very simple and might be memory bound. The main benefit might come from autotuning the block size and warps.\n\nWe'll write the kernel.\n\nWe'll also note that we are using the same constants as PyTorch. We can verify by checking the PyTorch source? But we'll use the standard ones.\n\nWe'll use:\n    scale = 1.0507009873554804934193349852946\n    alpha = 1.6732632423543772848170429916717\n\nBut note: the PyTorch implementation of selu uses:\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n\nSo we are safe.\n\nWe are ready to write the code.\n\nNote: We are not allowed to change the class name (ModelNew) or the functions get_inputs and get_init_inputs.\n\nWe'll now write the complete code block.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 2048}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 4096}, num_warps=8),\n    ],\n    key=['n_elements']\n)\n@triton.jit\ndef _selu_kernel(\n    x_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    # Load input\n    x = tl.load(x_ptr + offsets, mask=mask)\n    \n    # SELU constants\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n    \n    # Compute SELU activation\n    zero = 0.0\n    result = tl.where(x > zero, \n                     scale * x, \n                     scale * alpha * (tl.exp(x) - 1.0))\n    \n    # Store result\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nclass ModelNew(nn.Module):\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        n_elements = x.numel()\n        output = torch.empty_like(x)\n        \n        if n_elements == 0:\n            return output\n            \n        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n        _selu_kernel[grid](x, output, n_elements)\n        return output\n\nbatch_size = 16\ndim = 16384\n\ndef get_inputs():\n    x = torch.randn(batch_size, dim)\n    return [x]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
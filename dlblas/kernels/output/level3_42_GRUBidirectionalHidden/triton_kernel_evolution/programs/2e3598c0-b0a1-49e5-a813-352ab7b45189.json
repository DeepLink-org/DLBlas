{"id": "2e3598c0-b0a1-49e5-a813-352ab7b45189", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gru_forward_kernel(\n    # Input tensors\n    x_ptr,\n    h_prev_ptr,\n    w_ih_ptr,\n    w_hh_ptr,\n    b_ih_ptr,\n    b_hh_ptr,\n    output_ptr,\n    # Dimensions\n    seq_len,\n    batch_size,\n    input_size,\n    hidden_size,\n    # Strides\n    stride_x_seq,\n    stride_x_batch,\n    stride_x_size,\n    stride_h_batch,\n    stride_h_size,\n    stride_w_ih_row,\n    stride_w_ih_col,\n    stride_w_hh_row,\n    stride_w_hh_col,\n    # Block configuration\n    BLOCK_SIZE: tl.constexpr,\n    USE_BIAS: tl.constexpr\n):\n    # Parallelize over batches and hidden features\n    pid = tl.program_id(0)\n    pid_batch = pid // hidden_size\n    pid_feature = pid % hidden_size\n    \n    # Skip if out of bounds\n    if pid_batch >= batch_size or pid_feature >= hidden_size:\n        return\n\n    # Initialize accumulators for gates\n    i_u = tl.zeros((1,), dtype=tl.float32)\n    i_r = tl.zeros((1,), dtype=tl.float32)\n    i_n = tl.zeros((1,), dtype=tl.float32)\n    h_u = tl.zeros((1,), dtype=tl.float32)\n    h_r = tl.zeros((1,), dtype=tl.float32)\n    h_n = tl.zeros((1,), dtype=tl.float32)\n\n    # Compute input contributions\n    for block in range(0, input_size, BLOCK_SIZE):\n        cols = block + tl.arange(0, BLOCK_SIZE)\n        mask = cols < input_size\n        \n        # Load input block\n        x_offset = pid_batch * stride_x_batch + cols * stride_x_size\n        x_val = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n        \n        # Load weights and compute\n        w_ih_u = tl.load(w_ih_ptr + pid_feature * stride_w_ih_row + cols * stride_w_ih_col, mask=mask, other=0.0)\n        w_ih_r = tl.load(w_ih_ptr + (pid_feature + hidden_size) * stride_w_ih_row + cols * stride_w_ih_col, mask=mask, other=0.0)\n        w_ih_n = tl.load(w_ih_ptr + (pid_feature + 2*hidden_size) * stride_w_ih_row + cols * stride_w_ih_col, mask=mask, other=0.0)\n        \n        i_u += tl.sum(x_val * w_ih_u)\n        i_r += tl.sum(x_val * w_ih_r)\n        i_n += tl.sum(x_val * w_ih_n)\n\n    # Compute hidden contributions\n    for block in range(0, hidden_size, BLOCK_SIZE):\n        cols = block + tl.arange(0, BLOCK_SIZE)\n        mask = cols < hidden_size\n        \n        # Load hidden state\n        h_offset = pid_batch * stride_h_batch + cols\n        h_val = tl.load(h_prev_ptr + h_offset, mask=mask, other=0.0)\n        \n        # Load weights and compute\n        w_hh_u = tl.load(w_hh_ptr + pid_feature * stride_w_hh_row + cols * stride_w_hh_col, mask=mask, other=0.0)\n        w_hh_r = tl.load(w_hh_ptr + (pid_feature + hidden_size) * stride_w_hh_row + cols * stride_w_hh_col, mask=mask, other=0.0)\n        w_hh_n = tl.load(w_hh_ptr + (pid_feature + 2*hidden_size) * stride_w_hh_row + cols * stride_w_hh_col, mask=mask, other=0.0)\n        \n        h_u += tl.sum(h_val * w_hh_u)\n        h_r += tl.sum(h_val * w_hh_r)\n        h_n += tl.sum(h_val * w_hh_n)\n\n    # Add biases if available\n    if USE_BIAS:\n        b_ih_u = tl.load(b_ih_ptr + pid_feature)\n        b_ih_r = tl.load(b_ih_ptr + pid_feature + hidden_size)\n        b_ih_n = tl.load(b_ih_ptr + pid_feature + 2*hidden_size)\n        b_hh_u = tl.load(b_hh_ptr + pid_feature)\n        b_hh_r = tl.load(b_hh_ptr + pid_feature + hidden_size)\n        b_hh_n = tl.load(b_hh_ptr + pid_feature + 2*hidden_size)\n        \n        i_u += b_ih_u\n        i_r += b_ih_r\n        i_n += b_ih_n\n        h_u += b_hh_u\n        h_r += b_hh_r\n        h_n += b_hh_n\n\n    # Compute gates\n    update_gate = tl.sigmoid(i_u + h_u)\n    reset_gate = tl.sigmoid(i_r + h_r)\n    new_gate = tl.tanh(i_n + reset_gate * h_n)\n    \n    # Load previous hidden state for this feature\n    h_prev_val = tl.load(h_prev_ptr + pid_batch * stride_h_batch + pid_feature)\n    \n    # Compute new hidden state\n    new_h = (1 - update_gate) * new_gate + update_gate * h_prev_val\n    \n    # Store result\n    tl.store(output_ptr + pid_batch * stride_h_batch + pid_feature, new_h)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        super(ModelNew, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n        self.bias = bias\n        \n        # Create GRU parameters\n        self.weight_ih = nn.ParameterList()\n        self.weight_hh = nn.ParameterList()\n        self.bias_ih = nn.ParameterList() if bias else []\n        self.bias_hh = nn.ParameterList() if bias else []\n        \n        for _ in range(num_layers):\n            # Input-hidden weights (3*hidden_size x input_size)\n            self.weight_ih.append(nn.Parameter(torch.empty(3 * hidden_size, input_size)))\n            # Hidden-hidden weights (3*hidden_size x hidden_size)\n            self.weight_hh.append(nn.Parameter(torch.empty(3 * hidden_size, hidden_size)))\n            \n            if bias:\n                self.bias_ih.append(nn.Parameter(torch.empty(3 * hidden_size)))\n                self.bias_hh.append(nn.Parameter(torch.empty(3 * hidden_size)))\n        \n        # Initialize parameters\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            nn.init.uniform_(weight, -stdv, stdv)\n    \n    def forward(self, x):\n        if self.batch_first:\n            x = x.transpose(0, 1)\n        \n        seq_len, batch_size, input_size = x.shape\n        device = x.device\n        \n        # Initialize hidden states for each layer and direction\n        hx = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size, \n                         device=device, dtype=x.dtype)\n        \n        # Process each layer\n        for layer in range(self.num_layers):\n            # Forward direction processing\n            h_forward = self.process_direction(\n                x, \n                hx[layer * 2], \n                self.weight_ih[layer], \n                self.weight_hh[layer],\n                self.bias_ih[layer] if self.bias else None,\n                self.bias_hh[layer] if self.bias else None,\n                direction=1\n            )\n            \n            # Backward direction processing\n            h_backward = self.process_direction(\n                x.flip(0), \n                hx[layer * 2 + 1], \n                self.weight_ih[layer], \n                self.weight_hh[layer],\n                self.bias_ih[layer] if self.bias else None,\n                self.bias_hh[layer] if self.bias else None,\n                direction=-1\n            )\n            \n            # Store results\n            hx[layer * 2] = h_forward\n            hx[layer * 2 + 1] = h_backward\n        \n        return hx\n    \n    def process_direction(self, x, h_prev, w_ih, w_hh, b_ih, b_hh, direction):\n        seq_len, batch_size, input_size = x.shape\n        hidden_size = self.hidden_size\n        h_curr = h_prev.clone()\n        \n        # Process each time step\n        for t in range(seq_len):\n            # Prepare output tensor\n            h_next = torch.empty_like(h_curr)\n            \n            # Configure kernel launch\n            grid = (batch_size * hidden_size,)\n            \n            # Launch kernel\n            gru_forward_kernel[grid](\n                x[t],  # Current input\n                h_curr,  # Previous hidden state\n                w_ih, w_hh, \n                b_ih, b_hh,\n                h_next,  # Output\n                seq_len, \n                batch_size, \n                input_size, \n                hidden_size,\n                x.stride(0), x.stride(1), x.stride(2),  # x_strides\n                h_curr.stride(0), h_curr.stride(1),  # h_strides\n                w_ih.stride(0), w_ih.stride(1),  # w_ih_strides\n                w_hh.stride(0), w_hh.stride(1),  # w_hh_strides\n                BLOCK_SIZE=128,\n                USE_BIAS=b_ih is not None\n            )\n            h_curr = h_next\n        \n        return h_curr\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.randn(seq_len, batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "95bd96a6-4132-480d-bfc2-b9b7bb477b32", "generation": 2, "timestamp": 1754599789.2387424, "iteration_found": 22, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_42_GRUBidirectionalHidden", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmphg_lowkg.py\", line 166, in forward\n    h_forward = self.process_direction(\n  File \"/tmp/tmphg_lowkg.py\", line 207, in process_direction\n    gru_forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 67.9, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
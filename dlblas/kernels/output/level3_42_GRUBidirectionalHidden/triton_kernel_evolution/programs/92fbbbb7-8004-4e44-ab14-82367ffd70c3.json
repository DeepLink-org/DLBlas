{"id": "92fbbbb7-8004-4e44-ab14-82367ffd70c3", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gru_forward_kernel(\n    # Input tensors\n    input_ptr,\n    weight_ih_ptr,\n    weight_hh_ptr,\n    bias_ih_ptr,\n    bias_hh_ptr,\n    hx_ptr,\n    output_ptr,\n    # Tensor dimensions\n    seq_len,\n    input_size,\n    hidden_size,\n    # Strides\n    stride_input_batch,\n    stride_input_seq,\n    stride_input_feature,\n    stride_weight_ih_feature,\n    stride_weight_ih_hidden,\n    stride_weight_hh_feature,\n    stride_weight_hh_hidden,\n    # Meta-parameters\n    BLOCK_SIZE: tl.constexpr,\n    HIDDEN_BLOCK: tl.constexpr,\n):\n    pid = tl.program_id(0)  # Batch index\n    pid_t = tl.program_id(1)  # Time step index\n\n    # Initialize pointers\n    h_ptr = hx_ptr + pid * hidden_size\n    input_start = input_ptr + pid * stride_input_batch + pid_t * stride_input_seq\n    output_start = output_ptr + pid * (seq_len * hidden_size) + pid_t * hidden_size\n    \n    # Initialize hidden state\n    h_prev = tl.zeros((HIDDEN_BLOCK,), dtype=tl.float32)\n    if pid_t == 0:\n        h_prev = tl.load(h_ptr + tl.arange(0, HIDDEN_BLOCK), mask=tl.arange(0, HIDDEN_BLOCK) < hidden_size, other=0.0)\n    else:\n        h_prev = tl.load(h_ptr + tl.arange(0, HIDDEN_BLOCK), mask=tl.arange(0, HIDDEN_BLOCK) < hidden_size, other=0.0)\n    \n    # Load current input\n    input_vals = tl.load(input_start + tl.arange(0, BLOCK_SIZE) * stride_input_feature, \n                         mask=tl.arange(0, BLOCK_SIZE) < input_size, other=0.0)\n    \n    # Initialize gate accumulators\n    z = tl.zeros((HIDDEN_BLOCK,), dtype=tl.float32)\n    r = tl.zeros((HIDDEN_BLOCK,), dtype=tl.float32)\n    n = tl.zeros((HIDDEN_BLOCK,), dtype=tl.float32)\n    \n    # Compute gates\n    for i in range(0, hidden_size, HIDDEN_BLOCK):\n        idx = i + tl.arange(0, HIDDEN_BLOCK)\n        mask = idx < hidden_size\n        \n        # Weight-ih part for update gate\n        wih_ptr_z = weight_ih_ptr + idx * stride_weight_ih_feature\n        ih_z = tl.sum(input_vals[None, :] * tl.load(wih_ptr_z + tl.arange(0, BLOCK_SIZE)[:, None] * stride_weight_ih_hidden, \n                                                  mask=mask & (tl.arange(0, BLOCK_SIZE)[:, None] < input_size), \n                                                  other=0.0), axis=0)\n        \n        # Weight-hh part for update gate\n        whh_ptr_z = weight_hh_ptr + idx * stride_weight_hh_feature\n        hh_z = tl.sum(h_prev[None, :] * tl.load(whh_ptr_z + tl.arange(0, HIDDEN_BLOCK)[:, None] * stride_weight_hh_hidden, \n                                              mask=mask & (tl.arange(0, HIDDEN_BLOCK)[:, None] < hidden_size), \n                                              other=0.0), axis=0)\n        \n        # Add biases for update gate\n        if bias_ih_ptr is not None:\n            ih_z += tl.load(bias_ih_ptr + idx, mask=mask, other=0.0)\n        if bias_hh_ptr is not None:\n            hh_z += tl.load(bias_hh_ptr + idx, mask=mask, other=0.0)\n        \n        # Update gate calculation\n        z_block = tl.sigmoid(ih_z + hh_z)\n        z = tl.where(mask, z_block, z)\n        \n        # Reset gate calculations\n        wih_ptr_r = weight_ih_ptr + (hidden_size + idx) * stride_weight_ih_feature\n        ih_r = tl.sum(input_vals[None, :] * tl.load(wih_ptr_r + tl.arange(0, BLOCK_SIZE)[:, None] * stride_weight_ih_hidden, \n                                                  mask=mask & (tl.arange(0, BLOCK_SIZE)[:, None] < input_size), \n                                                  other=0.0), axis=0)\n        \n        whh_ptr_r = weight_hh_ptr + (hidden_size + idx) * stride_weight_hh_feature\n        hh_r = tl.sum(h_prev[None, :] * tl.load(whh_ptr_r + tl.arange(0, HIDDEN_BLOCK)[:, None] * stride_weight_hh_hidden, \n                                              mask=mask & (tl.arange(0, HIDDEN_BLOCK)[:, None] < hidden_size), \n                                              other=0.0), axis=0)\n        \n        if bias_ih_ptr is not None:\n            ih_r += tl.load(bias_ih_ptr + hidden_size + idx, mask=mask, other=0.0)\n        if bias_hh_ptr is not None:\n            hh_r += tl.load(bias_hh_ptr + hidden_size + idx, mask=mask, other=0.0)\n            \n        r_block = tl.sigmoid(ih_r + hh_r)\n        r = tl.where(mask, r_block, r)\n        \n        # New gate calculations\n        wih_ptr_n = weight_ih_ptr + (2*hidden_size + idx) * stride_weight_ih_feature\n        ih_n = tl.sum(input_vals[None, :] * tl.load(wih_ptr_n + tl.arange(0, BLOCK_SIZE)[:, None] * stride_weight_ih_hidden, \n                                                  mask=mask & (tl.arange(0, BLOCK_SIZE)[:, None] < input_size), \n                                                  other=0.0), axis=0)\n        \n        whh_ptr_n = weight_hh_ptr + (2*hidden_size + idx) * stride_weight_hh_feature\n        hh_n = tl.sum((h_prev * r)[None, :] * tl.load(whh_ptr_n + tl.arange(0, HIDDEN_BLOCK)[:, None] * stride_weight_hh_hidden, \n                                                    mask=mask & (tl.arange(0, HIDDEN_BLOCK)[:, None] < hidden_size), \n                                                    other=0.0), axis=0)\n        \n        if bias_ih_ptr is not None:\n            ih_n += tl.load(bias_ih_ptr + 2*hidden_size + idx, mask=mask, other=0.0)\n        if bias_hh_ptr is not None:\n            hh_n += tl.load(bias_hh_ptr + 2*hidden_size + idx, mask=mask, other=0.0)\n            \n        n_block = tl.tanh(ih_n + hh_n)\n        n = tl.where(mask, n_block, n)\n    \n    # Compute new hidden state\n    h_new = (1 - z) * n + z * h_prev\n    \n    # Store results\n    tl.store(output_start + tl.arange(0, HIDDEN_BLOCK), h_new, mask=tl.arange(0, HIDDEN_BLOCK) < hidden_size)\n    \n    # Update hidden state for next time step\n    if pid_t < seq_len - 1:\n        tl.store(h_ptr + tl.arange(0, HIDDEN_BLOCK), h_new, mask=tl.arange(0, HIDDEN_BLOCK) < hidden_size)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        \n        # Initialize weights\n        self.weights_ih = nn.ParameterList()\n        self.weights_hh = nn.ParameterList()\n        self.biases_ih = nn.ParameterList()\n        self.biases_hh = nn.ParameterList()\n        \n        for _ in range(num_layers * 2):  # *2 for bidirectional\n            self.weights_ih.append(nn.Parameter(torch.empty(3 * hidden_size, input_size)))\n            self.weights_hh.append(nn.Parameter(torch.empty(3 * hidden_size, hidden_size)))\n            if bias:\n                self.biases_ih.append(nn.Parameter(torch.empty(3 * hidden_size)))\n                self.biases_hh.append(nn.Parameter(torch.empty(3 * hidden_size)))\n            else:\n                self.biases_ih.append(None)\n                self.biases_hh.append(None)\n        \n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        for weight in self.weights_ih:\n            nn.init.xavier_uniform_(weight)\n        for weight in self.weights_hh:\n            nn.init.orthogonal_(weight)\n        for bias in self.biases_ih:\n            if bias is not None:\n                nn.init.zeros_(bias)\n        for bias in self.biases_hh:\n            if bias is not None:\n                nn.init.zeros_(bias)\n    \n    def forward(self, x):\n        if self.batch_first:\n            x = x.transpose(0, 1)\n        \n        seq_len, batch_size, _ = x.shape\n        device = x.device\n        \n        # Initialize hidden states\n        hx = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size, device=device, dtype=x.dtype)\n        output = torch.zeros(seq_len, batch_size, self.hidden_size * 2, device=device, dtype=x.dtype)\n        \n        # Process each layer\n        for layer_idx in range(self.num_layers):\n            # Process forward direction\n            self.process_direction(\n                x, hx, output, \n                layer_idx * 2, \n                self.weights_ih[layer_idx * 2],\n                self.weights_hh[layer_idx * 2],\n                self.biases_ih[layer_idx * 2] if self.bias else None,\n                self.biases_hh[layer_idx * 2] if self.bias else None,\n                direction=1\n            )\n            \n            # Process backward direction\n            self.process_direction(\n                x, hx, output, \n                layer_idx * 2 + 1, \n                self.weights_ih[layer_idx * 2 + 1],\n                self.weights_hh[layer_idx * 2 + 1],\n                self.biases_ih[layer_idx * 2 + 1] if self.bias else None,\n                self.biases_hh[layer_idx * 2 + 1] if self.bias else None,\n                direction=-1\n            )\n            \n            # Update input for next layer\n            x = output\n        \n        return hx\n    \n    def process_direction(self, x, hx, output, layer_idx, weight_ih, weight_hh, bias_ih, bias_hh, direction):\n        seq_len, batch_size, input_size = x.shape\n        hidden_size = self.hidden_size\n        \n        # Configure kernel\n        BLOCK_SIZE = triton.next_power_of_2(input_size)\n        HIDDEN_BLOCK = triton.next_power_of_2(hidden_size)\n        \n        # Adjust for direction\n        if direction == -1:\n            x = x.flip(0)\n        \n        # Launch kernel\n        grid = (batch_size, seq_len)\n        gru_forward_kernel[grid](\n            x, weight_ih, weight_hh, bias_ih, bias_hh, \n            hx[layer_idx], output[:, :, layer_idx * hidden_size: (layer_idx + 1) * hidden_size],\n            seq_len, input_size, hidden_size,\n            x.stride(1), x.stride(0), x.stride(2),\n            weight_ih.stride(1), weight_ih.stride(0),\n            weight_hh.stride(1), weight_hh.stride(0),\n            BLOCK_SIZE=BLOCK_SIZE,\n            HIDDEN_BLOCK=HIDDEN_BLOCK\n        )\n        \n        # Restore original order for backward direction\n        if direction == -1:\n            output[:, :, layer_idx * hidden_size: (layer_idx + 1) * hidden_size] = output[:, :, layer_idx * hidden_size: (layer_idx + 1) * hidden_size].flip(0)\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.randn(seq_len, batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "c4d0bbcb-6f42-41ce-854c-e3842fdc262f", "generation": 3, "timestamp": 1754598996.3014708, "iteration_found": 18, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_42_GRUBidirectionalHidden", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp4hqokl4v.py\", line 186, in forward\n    self.process_direction(\n  File \"/tmp/tmp4hqokl4v.py\", line 226, in process_direction\n    gru_forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 229, in visit_For\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 67.9, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'tl.load' is not found."}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
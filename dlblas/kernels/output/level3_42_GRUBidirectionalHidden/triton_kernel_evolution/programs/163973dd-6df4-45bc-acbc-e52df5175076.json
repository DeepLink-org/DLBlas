{"id": "163973dd-6df4-45bc-acbc-e52df5175076", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n@triton.jit\ndef gru_cell_kernel(\n    x_ptr, hx_ptr, w_ih_ptr, w_hh_ptr, b_ih_ptr, b_hh_ptr, output_ptr,\n    input_size, hidden_size, batch_size, \n    x_stride, hx_stride, output_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_batch = pid // hidden_size\n    pid_feature = pid % hidden_size\n\n    # Offsets\n    off_x = pid_batch * x_stride\n    off_hx = pid_batch * hx_stride\n    off_w_ih = pid_feature * input_size\n    off_w_hh = pid_feature * hidden_size\n\n    # Initialize accumulators\n    i_acc_z = 0.0\n    i_acc_r = 0.0\n    i_acc_n = 0.0\n    h_acc_z = 0.0\n    h_acc_r = 0.0\n    h_acc_n = 0.0\n\n    # Compute input part (for all gates)\n    for i in range(0, input_size, BLOCK_SIZE):\n        cols = i + tl.arange(0, BLOCK_SIZE)\n        mask = cols < input_size\n        \n        # Load input vector\n        x = tl.load(x_ptr + off_x + cols, mask=mask, other=0.0)\n        # Load weights for each gate\n        w_ih_z = tl.load(w_ih_ptr + off_w_ih + cols, mask=mask, other=0.0)\n        w_ih_r = tl.load(w_ih_ptr + (hidden_size * input_size) + off_w_ih + cols, mask=mask, other=0.0)\n        w_ih_n = tl.load(w_ih_ptr + (2 * hidden_size * input_size) + off_w_ih + cols, mask=mask, other=0.0)\n        \n        # Update accumulators\n        i_acc_z += x * w_ih_z\n        i_acc_r += x * w_ih_r\n        i_acc_n += x * w_ih_n\n\n    # Compute hidden part (for all gates)\n    for k in range(0, hidden_size, BLOCK_SIZE):\n        cols = k + tl.arange(0, BLOCK_SIZE)\n        mask = cols < hidden_size\n        \n        # Load hidden state\n        h = tl.load(hx_ptr + off_hx + cols, mask=mask, other=0.0)\n        # Load weights for each gate\n        w_hh_z = tl.load(w_hh_ptr + off_w_hh + cols, mask=mask, other=0.0)\n        w_hh_r = tl.load(w_hh_ptr + (hidden_size * hidden_size) + off_w_hh + cols, mask=mask, other=0.0)\n        w_hh_n = tl.load(w_hh_ptr + (2 * hidden_size * hidden_size) + off_w_hh + cols, mask=mask, other=0.0)\n        \n        # Update accumulators\n        h_acc_z += h * w_hh_z\n        h_acc_r += h * w_hh_r\n        h_acc_n += h * w_hh_n\n\n    # Add biases if present\n    if b_ih_ptr is not None:\n        b_ih_z = tl.load(b_ih_ptr + pid_feature)\n        b_ih_r = tl.load(b_ih_ptr + hidden_size + pid_feature)\n        b_ih_n = tl.load(b_ih_ptr + 2 * hidden_size + pid_feature)\n        i_acc_z += b_ih_z\n        i_acc_r += b_ih_r\n        i_acc_n += b_ih_n\n        \n    if b_hh_ptr is not None:\n        b_hh_z = tl.load(b_hh_ptr + pid_feature)\n        b_hh_r = tl.load(b_hh_ptr + hidden_size + pid_feature)\n        b_hh_n = tl.load(b_hh_ptr + 2 * hidden_size + pid_feature)\n        h_acc_z += b_hh_z\n        h_acc_r += b_hh_r\n        h_acc_n += b_hh_n\n\n    # Compute gates\n    z = tl.sigmoid(i_acc_z + h_acc_z)\n    r = tl.sigmoid(i_acc_r + h_acc_r)\n    n = libdevice.tanh(i_acc_n + r * h_acc_n)\n    \n    # Compute new hidden state\n    h_prev = tl.load(hx_ptr + off_hx + pid_feature)\n    new_h = (1 - z) * n + z * h_prev\n    \n    # Store result\n    tl.store(output_ptr + pid_batch * output_stride + pid_feature, new_h)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        super(ModelNew, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n        self.bias = bias\n        \n        # Create GRU parameters for each layer and direction\n        self.weight_ih = nn.ParameterList()\n        self.weight_hh = nn.ParameterList()\n        self.bias_ih = nn.ParameterList() if bias else [None] * num_layers * 2\n        self.bias_hh = nn.ParameterList() if bias else [None] * num_layers * 2\n        \n        for i in range(num_layers * 2):  # *2 for bidirectional\n            layer_input_size = input_size if i == 0 else hidden_size * 2\n            self.weight_ih.append(nn.Parameter(torch.empty(3 * hidden_size, layer_input_size)))\n            self.weight_hh.append(nn.Parameter(torch.empty(3 * hidden_size, hidden_size)))\n            if bias:\n                self.bias_ih.append(nn.Parameter(torch.empty(3 * hidden_size)))\n                self.bias_hh.append(nn.Parameter(torch.empty(3 * hidden_size)))\n        \n        # Initialize parameters\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        for weight in self.weight_ih:\n            nn.init.xavier_uniform_(weight)\n        for weight in self.weight_hh:\n            nn.init.orthogonal_(weight)\n        for bias in self.bias_ih:\n            if bias is not None:\n                nn.init.zeros_(bias)\n        for bias in self.bias_hh:\n            if bias is not None:\n                nn.init.zeros_(bias)\n    \n    def forward(self, x):\n        if self.batch_first:\n            x = x.transpose(0, 1)\n        \n        seq_len, batch_size, _ = x.shape\n        device = x.device\n        \n        # Initialize hidden state (num_layers * num_directions, batch, hidden_size)\n        hx = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size, \n                         device=device, dtype=x.dtype)\n        \n        # Process each layer\n        for layer in range(self.num_layers):\n            hidden_forward = []\n            hidden_backward = []\n            \n            # Process forward direction (layer index even)\n            h_prev_forward = hx[layer * 2]\n            for t in range(seq_len):\n                x_t = x[t]\n                grid = (batch_size * self.hidden_size,)\n                BLOCK_SIZE = 128\n                h_next_forward = torch.empty_like(h_prev_forward)\n                gru_cell_kernel[grid](\n                    x_t, h_prev_forward,\n                    self.weight_ih[layer * 2], self.weight_hh[layer * 2],\n                    self.bias_ih[layer * 2] if self.bias else None,\n                    self.bias_hh[layer * 2] if self.bias else None,\n                    h_next_forward,\n                    x_t.size(-1), self.hidden_size, batch_size,\n                    x_t.stride(0), h_prev_forward.stride(0), h_next_forward.stride(0),\n                    BLOCK_SIZE\n                )\n                hidden_forward.append(h_next_forward)\n                h_prev_forward = h_next_forward\n            \n            # Process backward direction (layer index odd)\n            h_prev_backward = hx[layer * 2 + 1]\n            for t in range(seq_len-1, -1, -1):\n                x_t = x[t]\n                grid = (batch_size * self.hidden_size,)\n                BLOCK_SIZE = 128\n                h_next_backward = torch.empty_like(h_prev_backward)\n                gru_cell_kernel[grid](\n                    x_t, h_prev_backward,\n                    self.weight_ih[layer * 2 + 1], self.weight_hh[layer * 2 + 1],\n                    self.bias_ih[layer * 2 + 1] if self.bias else None,\n                    self.bias_hh[layer * 2 + 1] if self.bias else None,\n                    h_next_backward,\n                    x_t.size(-1), self.hidden_size, batch_size,\n                    x_t.stride(0), h_prev_backward.stride(0), h_next_backward.stride(0),\n                    BLOCK_SIZE\n                )\n                hidden_backward.append(h_next_backward)\n                h_prev_backward = h_next_backward\n            \n            # Reverse backward results to align with sequence order\n            hidden_backward.reverse()\n            \n            # Update input for next layer (concatenate forward/backward outputs)\n            x = torch.stack([\n                torch.cat((hf, hb), dim=1) \n                for hf, hb in zip(hidden_forward, hidden_backward)\n            ])\n            \n            # Store final hidden states for this layer\n            hx[layer * 2] = h_prev_forward\n            hx[layer * 2 + 1] = h_prev_backward\n        \n        return hx\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.randn(seq_len, batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "8c2cf0cf-37da-4330-8769-5359b8704c4f", "generation": 3, "timestamp": 1754599301.9828768, "iteration_found": 20, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_42_GRUBidirectionalHidden", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpd5x9oheb.py\", line 157, in forward\n    gru_cell_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 26:4:\n    off_w_hh = pid_feature * hidden_size\n\n    # Initialize accumulators\n    i_acc_z = 0.0\n    i_acc_r = 0.0\n    i_acc_n = 0.0\n    h_acc_z = 0.0\n    h_acc_r = 0.0\n    h_acc_n = 0.0\n\n    # Compute input part (for all gates)\n    for i in range(0, input_size, BLOCK_SIZE):\n    ^\nAssertionError(\"Loop-carried variable i_acc_z has initial type fp32 but is re-assigned to <['128'], fp32> in loop! Please make sure that the type stays consistent.\")\n\n\nThe above exception was the direct cause of the following exception:\n\nAssertionError: Loop-carried variable i_acc_z has initial type fp32 but is re-assigned to <['128'], fp32> in loop! Please make sure that the type stays consistent.\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 67.9, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_42_GRUBidirectionalHidden", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_issue": "Output shape mismatch: Expected torch.Size([12, 10, 256]), got torch.Size([1, 10, 256])", "correctness_issue_name": "correctness_issue", "reference_timing_ms": 67.9, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
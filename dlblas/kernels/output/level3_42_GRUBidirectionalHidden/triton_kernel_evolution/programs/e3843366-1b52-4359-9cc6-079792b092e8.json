{"id": "e3843366-1b52-4359-9cc6-079792b092e8", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE\": 128}, num_warps=8),\n        triton.Config({\"BLOCK_SIZE\": 256}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 64}, num_warps=8),\n        triton.Config({\"BLOCK_SIZE\": 512}, num_warps=8),\n    ],\n    key=[\"input_size\", \"hidden_size\"],\n)\n@triton.jit\ndef gru_forward_kernel(\n    # Input tensors\n    x_ptr,\n    h_prev_ptr,\n    w_ih_ptr,\n    w_hh_ptr,\n    b_ih_ptr,\n    b_hh_ptr,\n    output_ptr,\n    # Tensor dimensions\n    input_size,\n    hidden_size,\n    batch_size,\n    # Strides\n    x_batch_stride,\n    x_feature_stride,\n    h_batch_stride,\n    w_ih_row_stride,\n    w_ih_col_stride,\n    w_hh_row_stride,\n    w_hh_col_stride,\n    # Meta-parameters\n    BLOCK_SIZE: tl.constexpr,\n    USE_BIAS: tl.constexpr,\n):\n    # Parallelize over batches and hidden features\n    pid = tl.program_id(0)\n    pid_batch = pid // hidden_size\n    pid_feature = pid % hidden_size\n    \n    # Skip if out of bounds\n    if pid_batch >= batch_size or pid_feature >= hidden_size:\n        return\n\n    # Pointers to weights for this feature\n    w_ih_update = w_ih_ptr + pid_feature * w_ih_row_stride\n    w_ih_reset = w_ih_ptr + (pid_feature + hidden_size) * w_ih_row_stride\n    w_ih_new = w_ih_ptr + (pid_feature + 2 * hidden_size) * w_ih_row_stride\n    w_hh_update = w_hh_ptr + pid_feature * w_hh_row_stride\n    w_hh_reset = w_hh_ptr + (pid_feature + hidden_size) * w_hh_row_stride\n    w_hh_new = w_hh_ptr + (pid_feature + 2 * hidden_size) * w_hh_row_stride\n\n    # Accumulators\n    i_u = tl.zeros((1,), dtype=tl.float32)\n    i_r = tl.zeros((1,), dtype=tl.float32)\n    i_n = tl.zeros((1,), dtype=tl.float32)\n    h_u = tl.zeros((1,), dtype=tl.float32)\n    h_r = tl.zeros((1,), dtype=tl.float32)\n    h_n = tl.zeros((1,), dtype=tl.float32)\n\n    # Compute input contributions\n    for i in range(0, input_size, BLOCK_SIZE):\n        cols = i + tl.arange(0, BLOCK_SIZE)\n        mask = cols < input_size\n        \n        # Load input block\n        x_offset = pid_batch * x_batch_stride + cols * x_feature_stride\n        x = tl.load(x_ptr + x_offset, mask=mask, other=0.0)\n        \n        # Load weights and compute\n        w_ih_u = tl.load(w_ih_update + cols * w_ih_col_stride, mask=mask, other=0.0)\n        w_ih_r = tl.load(w_ih_reset + cols * w_ih_col_stride, mask=mask, other=0.0)\n        w_ih_n = tl.load(w_ih_new + cols * w_ih_col_stride, mask=mask, other=0.0)\n        \n        i_u += tl.sum(x * w_ih_u)\n        i_r += tl.sum(x * w_ih_r)\n        i_n += tl.sum(x * w_ih_n)\n\n    # Compute hidden contributions\n    for i in range(0, hidden_size, BLOCK_SIZE):\n        cols = i + tl.arange(0, BLOCK_SIZE)\n        mask = cols < hidden_size\n        \n        # Load hidden state\n        h_offset = pid_batch * h_batch_stride + cols\n        h = tl.load(h_prev_ptr + h_offset, mask=mask, other=0.0)\n        \n        # Load weights and compute\n        w_hh_u = tl.load(w_hh_update + cols * w_hh_col_stride, mask=mask, other=0.0)\n        w_hh_r = tl.load(w_hh_reset + cols * w_hh_col_stride, mask=mask, other=0.0)\n        w_hh_n = tl.load(w_hh_new + cols * w_hh_col_stride, mask=mask, other=0.0)\n        \n        h_u += tl.sum(h * w_hh_u)\n        h_r += tl.sum(h * w_hh_r)\n        h_n += tl.sum(h * w_hh_n)\n\n    # Add biases if available\n    if USE_BIAS:\n        b_ih_offset = tl.arange(0, 3) * hidden_size + pid_feature\n        b_hh_offset = tl.arange(0, 3) * hidden_size + pid_feature\n        \n        b_ih = tl.load(b_ih_ptr + b_ih_offset, mask=[True, True, True], other=0.0)\n        b_hh = tl.load(b_hh_ptr + b_hh_offset, mask=[True, True, True], other=0.0)\n        \n        i_u += b_ih[0]\n        i_r += b_ih[1]\n        i_n += b_ih[2]\n        h_u += b_hh[0]\n        h_r += b_hh[1]\n        h_n += b_hh[2]\n\n    # Compute gates\n    update_gate = tl.sigmoid(i_u + h_u)\n    reset_gate = tl.sigmoid(i_r + h_r)\n    new_gate = tl.tanh(i_n + reset_gate * h_n)\n    \n    # Load previous hidden state for this feature\n    h_prev_val = tl.load(h_prev_ptr + pid_batch * h_batch_stride + pid_feature)\n    \n    # Compute new hidden state\n    new_h = (1 - update_gate) * new_gate + update_gate * h_prev_val\n    \n    # Store result\n    tl.store(output_ptr + pid_batch * h_batch_stride + pid_feature, new_h)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        super(ModelNew, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n        self.bias = bias\n        \n        # Create GRU parameters\n        self.weight_ih = nn.ParameterList()\n        self.weight_hh = nn.ParameterList()\n        self.bias_ih = nn.ParameterList() if bias else [None] * num_layers\n        self.bias_hh = nn.ParameterList() if bias else [None] * num_layers\n        \n        for i in range(num_layers):\n            # Input-hidden weights (3*hidden_size x input_size)\n            self.weight_ih.append(nn.Parameter(torch.empty(3 * hidden_size, input_size)))\n            # Hidden-hidden weights (3*hidden_size x hidden_size)\n            self.weight_hh.append(nn.Parameter(torch.empty(3 * hidden_size, hidden_size)))\n            \n            if bias:\n                self.bias_ih.append(nn.Parameter(torch.empty(3 * hidden_size)))\n                self.bias_hh.append(nn.Parameter(torch.empty(3 * hidden_size)))\n        \n        # Initialize parameters\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        for weight in self.weight_ih:\n            nn.init.xavier_uniform_(weight)\n        for weight in self.weight_hh:\n            nn.init.orthogonal_(weight)\n        for bias in self.bias_ih:\n            if bias is not None:\n                nn.init.zeros_(bias)\n        for bias in self.bias_hh:\n            if bias is not None:\n                nn.init.zeros_(bias)\n    \n    def forward(self, x):\n        if self.batch_first:\n            x = x.transpose(0, 1)\n        \n        seq_len, batch_size, input_size = x.shape\n        device = x.device\n        \n        # Initialize hidden states for each layer and direction\n        hx = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size, \n                         device=device, dtype=x.dtype)\n        \n        # Process each layer\n        for layer in range(self.num_layers):\n            # Forward direction processing\n            h_forward = self.process_direction(\n                x, \n                hx[layer * 2], \n                self.weight_ih[layer], \n                self.weight_hh[layer],\n                self.bias_ih[layer] if self.bias else None,\n                self.bias_hh[layer] if self.bias else None,\n                direction=1\n            )\n            \n            # Backward direction processing\n            h_backward = self.process_direction(\n                x.flip(0), \n                hx[layer * 2 + 1], \n                self.weight_ih[layer], \n                self.weight_hh[layer],\n                self.bias_ih[layer] if self.bias else None,\n                self.bias_hh[layer] if self.bias else None,\n                direction=-1\n            )\n            \n            # Store results\n            hx[layer * 2] = h_forward\n            hx[layer * 2 + 1] = h_backward.flip(0) if self.batch_first else h_backward\n        \n        return hx\n    \n    def process_direction(self, x, h_prev, w_ih, w_hh, b_ih, b_hh, direction):\n        seq_len, batch_size, input_size = x.shape\n        hidden_size = self.hidden_size\n        \n        # Process each time step\n        for t in range(seq_len):\n            # Configure kernel launch\n            grid = (batch_size * hidden_size,)\n            \n            # Prepare output tensor\n            h_next = torch.empty_like(h_prev)\n            \n            # Launch kernel\n            gru_forward_kernel[grid](\n                x[t],  # Current input\n                h_prev,  # Previous hidden state\n                w_ih, w_hh, \n                b_ih, b_hh,\n                h_next,  # Output\n                input_size, \n                hidden_size, \n                batch_size,\n                x.stride(1), x.stride(2),  # x_strides\n                h_prev.stride(0),  # h_batch_stride\n                w_ih.stride(0), w_ih.stride(1),  # w_ih_strides\n                w_hh.stride(0), w_hh.stride(1),  # w_hh_strides\n                BLOCK_SIZE=128,\n                USE_BIAS=b_ih is not None\n            )\n            h_prev = h_next\n        \n        return h_prev\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.randn(seq_len, batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "f40cb602-1c68-43be-b2a4-83e720c50fff", "generation": 2, "timestamp": 1754598185.715059, "iteration_found": 14, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_42_GRUBidirectionalHidden", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpik16_jbr.py\", line 186, in forward\n    h_forward = self.process_direction(\n  File \"/tmp/tmpik16_jbr.py\", line 226, in process_direction\n    gru_forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 135, in _bench\n    raise ValueError(f\"Conflicting meta-parameters: {', '.join(conflicts)}.\"\nValueError: Conflicting meta-parameters: BLOCK_SIZE. Make sure that you don't re-define auto-tuned symbols.\n", "runtime_error_name": "builtins.ValueError", "reference_timing_ms": 67.9, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
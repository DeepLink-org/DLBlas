{"id": "fd243930-2694-4073-8a72-29611165aa0d", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_F': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_F': 128, 'BLOCK_SIZE_K': 32}, num_warps=8),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef linear_relu_div_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    batch_size, in_features, out_features, divisor,\n    stride_input_batch, stride_input_feat,\n    stride_weight_out, stride_weight_in,\n    stride_output_batch, stride_output_feat,\n    BLOCK_SIZE_B: tl.constexpr,\n    BLOCK_SIZE_F: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_f = tl.program_id(1)\n    \n    # Create block offsets\n    b_offset = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n    f_offset = pid_f * BLOCK_SIZE_F + tl.arange(0, BLOCK_SIZE_F)\n    \n    # Create masks for boundary checks\n    batch_mask = b_offset < batch_size\n    feat_mask = f_offset < out_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_F), dtype=tl.float32)\n    \n    # Blocked matrix multiplication\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        # Load input block\n        input_ptrs = input_ptr + b_offset[:, None] * stride_input_batch + k_offsets[None, :]\n        input_mask = batch_mask[:, None] & k_mask[None, :]\n        input_block = tl.load(input_ptrs, mask=input_mask, other=0.0)\n        \n        # Load weight block\n        weight_ptrs = weight_ptr + f_offset[:, None] * stride_weight_out + k_offsets[None, :]\n        weight_mask = feat_mask[:, None] & k_mask[None, :]\n        weight_block = tl.load(weight_ptrs, mask=weight_mask, other=0.0)\n        \n        # Matrix multiplication with tensor cores\n        weight_block_t = tl.trans(weight_block)\n        acc += tl.dot(input_block, weight_block_t, allow_tf32=True)\n\n    # Load bias and fuse operations\n    bias_ptrs = bias_ptr + f_offset\n    bias_block = tl.load(bias_ptrs, mask=feat_mask, other=0.0)\n    acc += bias_block[None, :]\n    acc = tl.maximum(acc, 0.0)  # ReLU\n    acc = acc / divisor  # Division\n    \n    # Store results\n    output_ptrs = output_ptr + b_offset[:, None] * stride_output_batch + f_offset[None, :]\n    output_mask = batch_mask[:, None] & feat_mask[None, :]\n    tl.store(output_ptrs, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.divisor = divisor\n        \n        # Initialize weights and biases\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        # Original nn.Linear initialization\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty((batch_size, self.out_features), \n                             device=x.device, dtype=x.dtype)\n        \n        # Grid configuration with autotuning\n        grid = lambda meta: (\n            triton.cdiv(batch_size, meta['BLOCK_SIZE_B']),\n            triton.cdiv(self.out_features, meta['BLOCK_SIZE_F'])\n        )\n        \n        # Launch kernel with tensor core optimization\n        linear_relu_div_kernel[grid](\n            x, self.weight, self.bias, output,\n            batch_size, self.in_features, self.out_features, self.divisor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1)\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "f7da870f-038b-43f7-8972-ff8a35e59301", "generation": 3, "timestamp": 1754578409.6774228, "iteration_found": 23, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.744, "runtime_ms": 0.0737, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0548, "speed_up": 0.744, "custom_timing": 0.0737}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.652, "runtime_ms": 0.0841, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0548, "speed_up": 0.652, "custom_timing": 0.0841}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.6520\n- runtime_ms: 0.0841\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0548, 'speed_up': 0.652, 'custom_timing': 0.0841}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7920, runtime_ms: 0.0692, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0548, 'speed_up': 0.792, 'custom_timing': 0.0692}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.6520, runtime_ms: 0.0841, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0548, 'speed_up': 0.652, 'custom_timing': 0.0841}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9270, runtime_ms: 0.0591, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0548, 'speed_up': 0.927, 'custom_timing': 0.0591}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef linear_relu_div_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    batch_size, in_features, out_features, divisor,\n    stride_input_batch, stride_input_feat,\n    stride_weight_out, stride_weight_in,\n    stride_output_batch, stride_output_feat,\n    BLOCK_SIZE_B: tl.constexpr, BLOCK_SIZE_F: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_f = tl.program_id(1)\n    \n    b_offs = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n    f_offs = pid_f * BLOCK_SIZE_F + tl.arange(0, BLOCK_SIZE_F)\n    \n    acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_F), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offs = k + tl.arange(0, BLOCK_SIZE_K)\n        \n        # Load input block: [BLOCK_SIZE_B, BLOCK_SIZE_K]\n        input_mask = (b_offs[:, None] < batch_size) & (k_offs[None, :] < in_features)\n        input_block = tl.load(\n            input_ptr + b_offs[:, None] * stride_input_batch + k_offs[None, :] * stride_input_feat,\n            mask=input_mask,\n            other=0.0\n        )\n        \n        # Load weight block: [BLOCK_SIZE_F, BLOCK_SIZE_K]\n        weight_mask = (f_offs[:, None] < out_features) & (k_offs[None, :] < in_features)\n        weight_block = tl.load(\n            weight_ptr + f_offs[:, None] * stride_weight_out + k_offs[None, :] * stride_weight_in,\n            mask=weight_mask,\n            other=0.0\n        )\n        \n        # Transpose weight and compute dot product\n        weight_block_t = tl.trans(weight_block)\n        acc += tl.dot(input_block, weight_block_t)\n    \n    # Load bias and apply\n    bias_vals = tl.load(bias_ptr + f_offs, mask=f_offs < out_features, other=0.0)\n    acc += bias_vals[None, :]\n    \n    # Apply ReLU and division\n    acc = tl.maximum(acc, 0.0)\n    acc = acc / divisor\n    \n    # Store output\n    output_offs = b_offs[:, None] * stride_output_batch + f_offs[None, :] * stride_output_feat\n    output_mask = (b_offs[:, None] < batch_size) & (f_offs[None, :] < out_features)\n    tl.store(output_ptr + output_offs, acc, mask=output_mask)\n\nclass TritonLinearReLUDiv(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.divisor = divisor\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(\n            batch_size, self.out_features, \n            device=x.device, dtype=x.dtype\n        )\n        \n        grid = (\n            triton.cdiv(batch_size, 64),\n            triton.cdiv(self.out_features, 64)\n        )\n        \n        linear_relu_div_kernel[grid](\n            x, self.weight, self.bias, output,\n            batch_size, self.in_features, self.out_features, self.divisor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_B=64, BLOCK_SIZE_F=64, BLOCK_SIZE_K=64\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super(ModelNew, self).__init__()\n        self.linear = TritonLinearReLUDiv(in_features, out_features, divisor)\n    \n    def forward(self, x):\n        return self.linear(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9270), Performs well on runtime_ms (0.0591), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0548, 'speed_up': 0.927, 'custom_timing': 0.0591})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Triton kernel for fused linear + ReLU + division\n@triton.jit\ndef linear_relu_div_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    in_features, out_features, divisor,\n    stride_input_batch, stride_input_feat,\n    stride_weight_out, stride_weight_in,\n    stride_bias,\n    stride_output_batch, stride_output_feat,\n    BLOCK_SIZE_FEAT: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    # Extract program IDs\n    pid_batch = tl.program_id(0)\n    pid_feat_block = tl.program_id(1)\n    \n    # Create feature indices for this block\n    feat_idx = pid_feat_block * BLOCK_SIZE_FEAT + tl.arange(0, BLOCK_SIZE_FEAT)\n    mask_feat = feat_idx < out_features\n    \n    # Accumulator initialization\n    acc = tl.zeros((BLOCK_SIZE_FEAT,), dtype=tl.float32)\n    \n    # Loop over input features in blocks\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offs = k + tl.arange(0, BLOCK_SIZE_K)\n        mask_k = k_offs < in_features\n        \n        # Load input block\n        input_offs = pid_batch * stride_input_batch + k_offs * stride_input_feat\n        input_chunk = tl.load(input_ptr + input_offs, mask=mask_k, other=0.0)\n        \n        # Load weight block\n        weight_offs = feat_idx[:, None] * stride_weight_out + k_offs[None, :] * stride_weight_in\n        weight_chunk = tl.load(weight_ptr + weight_offs, \n                               mask=mask_k[None, :] & mask_feat[:, None], \n                               other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(weight_chunk * input_chunk[None, :], axis=1)\n    \n    # Add bias (fixed stride handling)\n    bias = tl.load(bias_ptr + feat_idx, mask=mask_feat, other=0.0)\n    acc += bias\n    \n    # Apply ReLU and division\n    acc = tl.where(acc < 0, 0, acc)  # ReLU\n    acc = acc / divisor\n    \n    # Store results\n    output_offs = pid_batch * stride_output_batch + feat_idx * stride_output_feat\n    tl.store(output_ptr + output_offs, acc, mask=mask_feat)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.divisor = divisor\n        \n        # Initialize weights and biases\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        # Original nn.Linear initialization\n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)\n        bound = 1 / (out_features**0.5) if out_features > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        x = x.contiguous()\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, self.out_features, \n                             device=x.device, dtype=x.dtype)\n        \n        # Kernel configuration\n        BLOCK_SIZE_FEAT = 128\n        BLOCK_SIZE_K = 64\n        grid = (batch_size, triton.cdiv(self.out_features, BLOCK_SIZE_FEAT))\n        \n        # Launch kernel (fixed bias stride)\n        linear_relu_div_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features, self.divisor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            1,  # Fixed bias stride to 1\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_FEAT=BLOCK_SIZE_FEAT,\n            BLOCK_SIZE_K=BLOCK_SIZE_K\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.6520), Performs well on runtime_ms (0.0841), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0548, 'speed_up': 0.652, 'custom_timing': 0.0841})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_B': 32, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_F': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 128}, num_warps=4),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef fused_linear_relu_div(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    in_features, out_features, divisor, batch_size,\n    stride_input, stride_weight, stride_output,\n    BLOCK_SIZE_B: tl.constexpr, BLOCK_SIZE_F: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_f = tl.program_id(1)\n    \n    # Create block offsets\n    b_offset = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n    f_offset = pid_f * BLOCK_SIZE_F + tl.arange(0, BLOCK_SIZE_F)\n    \n    # Create masks for boundary checks\n    batch_mask = b_offset < batch_size\n    feat_mask = f_offset < out_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_F), dtype=tl.float32)\n    \n    # Blocked matrix multiplication\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        # Load input block with boundary masking\n        input_ptrs = input_ptr + b_offset[:, None] * stride_input + k_offsets[None, :]\n        input_mask = batch_mask[:, None] & k_mask[None, :]\n        input_block = tl.load(input_ptrs, mask=input_mask, other=0.0)\n        \n        # Load weight block with boundary masking\n        weight_ptrs = weight_ptr + f_offset[:, None] * stride_weight + k_offsets[None, :]\n        weight_mask = feat_mask[:, None] & k_mask[None, :]\n        weight_block = tl.load(weight_ptrs, mask=weight_mask, other=0.0)\n        \n        # Efficient matrix multiplication with Tensor Core optimization\n        weight_block_t = tl.trans(weight_block)\n        acc += tl.dot(input_block, weight_block_t, allow_tf32=True)\n\n    # Load bias and fuse operations\n    bias_ptrs = bias_ptr + f_offset\n    bias_block = tl.load(bias_ptrs, mask=feat_mask, other=0.0)\n    acc += bias_block[None, :]\n    acc = tl.maximum(acc, 0.0)  # ReLU\n    acc = acc / divisor  # Division\n    \n    # Store results with boundary masking\n    output_ptrs = output_ptr + b_offset[:, None] * stride_output + f_offset[None, :]\n    output_mask = batch_mask[:, None] & feat_mask[None, :]\n    tl.store(output_ptrs, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.divisor = divisor\n        \n        # Initialize weights and biases\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        # Original nn.Linear initialization\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty((batch_size, self.out_features), \n                             device=x.device, dtype=x.dtype)\n        \n        # Grid configuration with autotuned block sizes\n        grid = lambda meta: (\n            triton.cdiv(batch_size, meta['BLOCK_SIZE_B']),\n            triton.cdiv(self.out_features, meta['BLOCK_SIZE_F'])\n        )\n        \n        # Launch fused kernel\n        fused_linear_relu_div[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features, self.divisor, batch_size,\n            x.stride(0), self.weight.stride(0), output.stride(0)\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7920), Performs well on runtime_ms (0.0692), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0548, 'speed_up': 0.792, 'custom_timing': 0.0692})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_B': 32, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_F': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 32, 'BLOCK_SIZE_F': 128, 'BLOCK_SIZE_K': 128}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef linear_relu_div_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, K, divisor,\n    stride_ib, stride_ik,\n    stride_wk, stride_wn,\n    stride_ob, stride_on,\n    BLOCK_SIZE_B: tl.constexpr,\n    BLOCK_SIZE_F: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_f = tl.program_id(1)\n    \n    # Create offsets for the batch dimension\n    b_offset = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n    # Create offsets for the feature dimension\n    f_offset = pid_f * BLOCK_SIZE_F + tl.arange(0, BLOCK_SIZE_F)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_F), dtype=tl.float32)\n    \n    # Loop over K dimension\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_offsets = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        \n        # Create masks for input and weight\n        input_mask = (b_offset[:, None] < M) & (k_offsets[None, :] < K)\n        weight_mask = (f_offset[:, None] < N) & (k_offsets[None, :] < K)\n        \n        # Load input block\n        input_block = tl.load(\n            input_ptr + b_offset[:, None] * stride_ib + k_offsets[None, :] * stride_ik,\n            mask=input_mask,\n            other=0.0\n        )\n        \n        # Load weight block (transposed access)\n        weight_block = tl.load(\n            weight_ptr + k_offsets[None, :] * stride_wk + f_offset[:, None] * stride_wn,\n            mask=weight_mask,\n            other=0.0\n        )\n        \n        # Compute matrix multiplication\n        acc += tl.dot(input_block, weight_block, allow_tf32=True)\n    \n    # Load bias\n    bias_block = tl.load(\n        bias_ptr + f_offset, \n        mask=f_offset < N, \n        other=0.0\n    )\n    \n    # Apply bias, ReLU and division\n    acc += bias_block[None, :]\n    acc = tl.maximum(acc, 0.0)\n    acc = acc / divisor\n    \n    # Create output mask and store\n    output_mask = (b_offset[:, None] < M) & (f_offset[None, :] < N)\n    output_offsets = b_offset[:, None] * stride_ob + f_offset[None, :] * stride_on\n    tl.store(output_ptr + output_offsets, acc, mask=output_mask)\n\nclass TritonLinearReLUDiv(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.divisor = divisor\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(\n            batch_size, self.out_features, \n            device=x.device, dtype=x.dtype\n        )\n        \n        grid = lambda meta: (\n            triton.cdiv(batch_size, meta['BLOCK_SIZE_B']),\n            triton.cdiv(self.out_features, meta['BLOCK_SIZE_F'])\n        )\n        \n        linear_relu_div_kernel[grid](\n            x, self.weight, self.bias, output,\n            batch_size, self.out_features, self.in_features, self.divisor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_B=32, BLOCK_SIZE_F=64, BLOCK_SIZE_K=64\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super(ModelNew, self).__init__()\n        self.linear = TritonLinearReLUDiv(in_features, out_features, divisor)\n    \n    def forward(self, x):\n        return self.linear(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_K': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_K': 32}, num_warps=8),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef fused_linear_relu_div(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    divisor,\n    stride_input0, stride_weight0, stride_weight1, stride_output0,\n    in_features, out_features,\n    BLOCK_SIZE_K: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_col_block = tl.program_id(1)\n    \n    col_offsets = pid_col_block * 128 + tl.arange(0, 128)\n    col_mask = col_offsets < out_features\n    \n    acc = tl.zeros((128,), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_offsets = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        input_ptr_row = input_ptr + pid_batch * stride_input0 + k_offsets\n        input_chunk = tl.load(input_ptr_row, mask=k_mask, other=0.0)\n        \n        weight_ptr_block = weight_ptr + col_offsets[:, None] * stride_weight0 + k_offsets[None, :] * stride_weight1\n        weight_chunk = tl.load(weight_ptr_block, mask=col_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        acc += tl.dot(weight_chunk, input_chunk, allow_tf32=True)\n    \n    if bias_ptr is not None:\n        bias_vals = tl.load(bias_ptr + col_offsets, mask=col_mask, other=0.0)\n        acc += bias_vals\n    \n    acc = tl.maximum(acc, 0.0)\n    acc = acc / divisor\n    \n    output_ptr_row = output_ptr + pid_batch * stride_output0 + col_offsets\n    tl.store(output_ptr_row, acc, mask=col_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.divisor = divisor\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        output = torch.empty((x.size(0), self.out_features), device=x.device, dtype=x.dtype)\n        grid = (x.size(0), triton.cdiv(self.out_features, 128))\n        \n        fused_linear_relu_div[grid](\n            x, self.weight, self.bias, output,\n            self.divisor,\n            x.stride(0), self.weight.stride(0), self.weight.stride(1), output.stride(0),\n            self.in_features, self.out_features\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef linear_relu_div_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    batch_size, in_features, out_features, divisor,\n    stride_input_batch, stride_input_feat,\n    stride_weight_out, stride_weight_in,\n    stride_output_batch, stride_output_feat,\n    BLOCK_SIZE_B: tl.constexpr, BLOCK_SIZE_F: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_f = tl.program_id(1)\n    \n    b_offs = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n    f_offs = pid_f * BLOCK_SIZE_F + tl.arange(0, BLOCK_SIZE_F)\n    \n    acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_F), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offs = k + tl.arange(0, BLOCK_SIZE_K)\n        \n        # Load input block: [BLOCK_SIZE_B, BLOCK_SIZE_K]\n        input_mask = (b_offs[:, None] < batch_size) & (k_offs[None, :] < in_features)\n        input_block = tl.load(\n            input_ptr + b_offs[:, None] * stride_input_batch + k_offs[None, :] * stride_input_feat,\n            mask=input_mask,\n            other=0.0\n        )\n        \n        # Load weight block: [BLOCK_SIZE_F, BLOCK_SIZE_K]\n        weight_mask = (f_offs[:, None] < out_features) & (k_offs[None, :] < in_features)\n        weight_block = tl.load(\n            weight_ptr + f_offs[:, None] * stride_weight_out + k_offs[None, :] * stride_weight_in,\n            mask=weight_mask,\n            other=0.0\n        )\n        \n        # Transpose weight and compute dot product\n        weight_block_t = tl.trans(weight_block)\n        acc += tl.dot(input_block, weight_block_t)\n    \n    # Load bias and apply\n    bias_vals = tl.load(bias_ptr + f_offs, mask=f_offs < out_features, other=0.0)\n    acc += bias_vals[None, :]\n    \n    # Apply ReLU and division\n    acc = tl.maximum(acc, 0.0)\n    acc = acc / divisor\n    \n    # Store output\n    output_offs = b_offs[:, None] * stride_output_batch + f_offs[None, :] * stride_output_feat\n    output_mask = (b_offs[:, None] < batch_size) & (f_offs[None, :] < out_features)\n    tl.store(output_ptr + output_offs, acc, mask=output_mask)\n\nclass TritonLinearReLUDiv(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.divisor = divisor\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(\n            batch_size, self.out_features, \n            device=x.device, dtype=x.dtype\n        )\n        \n        grid = (\n            triton.cdiv(batch_size, 64),\n            triton.cdiv(self.out_features, 64)\n        )\n        \n        linear_relu_div_kernel[grid](\n            x, self.weight, self.bias, output,\n            batch_size, self.in_features, self.out_features, self.divisor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_B=64, BLOCK_SIZE_F=64, BLOCK_SIZE_K=64\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super(ModelNew, self).__init__()\n        self.linear = TritonLinearReLUDiv(in_features, out_features, divisor)\n    \n    def forward(self, x):\n        return self.linear(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_B': 32, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_F': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 128}, num_warps=4),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef fused_linear_relu_div(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    in_features, out_features, divisor, batch_size,\n    stride_input, stride_weight, stride_output,\n    BLOCK_SIZE_B: tl.constexpr, BLOCK_SIZE_F: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_f = tl.program_id(1)\n    \n    # Create block offsets\n    b_offset = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n    f_offset = pid_f * BLOCK_SIZE_F + tl.arange(0, BLOCK_SIZE_F)\n    \n    # Create masks for boundary checks\n    batch_mask = b_offset < batch_size\n    feat_mask = f_offset < out_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_F), dtype=tl.float32)\n    \n    # Blocked matrix multiplication\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        # Load input block with boundary masking\n        input_ptrs = input_ptr + b_offset[:, None] * stride_input + k_offsets[None, :]\n        input_mask = batch_mask[:, None] & k_mask[None, :]\n        input_block = tl.load(input_ptrs, mask=input_mask, other=0.0)\n        \n        # Load weight block with boundary masking\n        weight_ptrs = weight_ptr + f_offset[:, None] * stride_weight + k_offsets[None, :]\n        weight_mask = feat_mask[:, None] & k_mask[None, :]\n        weight_block = tl.load(weight_ptrs, mask=weight_mask, other=0.0)\n        \n        # Efficient matrix multiplication with Tensor Core optimization\n        weight_block_t = tl.trans(weight_block)\n        acc += tl.dot(input_block, weight_block_t, allow_tf32=True)\n\n    # Load bias and fuse operations\n    bias_ptrs = bias_ptr + f_offset\n    bias_block = tl.load(bias_ptrs, mask=feat_mask, other=0.0)\n    acc += bias_block[None, :]\n    acc = tl.maximum(acc, 0.0)  # ReLU\n    acc = acc / divisor  # Division\n    \n    # Store results with boundary masking\n    output_ptrs = output_ptr + b_offset[:, None] * stride_output + f_offset[None, :]\n    output_mask = batch_mask[:, None] & feat_mask[None, :]\n    tl.store(output_ptrs, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.divisor = divisor\n        \n        # Initialize weights and biases\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        # Original nn.Linear initialization\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty((batch_size, self.out_features), \n                             device=x.device, dtype=x.dtype)\n        \n        # Grid configuration with autotuned block sizes\n        grid = lambda meta: (\n            triton.cdiv(batch_size, meta['BLOCK_SIZE_B']),\n            triton.cdiv(self.out_features, meta['BLOCK_SIZE_F'])\n        )\n        \n        # Launch fused kernel\n        fused_linear_relu_div[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features, self.divisor, batch_size,\n            x.stride(0), self.weight.stride(0), output.stride(0)\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128}, num_warps=8),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef fused_linear_relu_div(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    divisor,\n    stride_input0, stride_weight0, stride_weight1, stride_output0,\n    in_features, out_features,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_col_block = tl.program_id(1)\n    \n    col_offsets = pid_col_block * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    col_mask = col_offsets < out_features\n    \n    acc = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_offsets = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        input_ptr_row = input_ptr + pid_batch * stride_input0 + k_offsets\n        input_chunk = tl.load(input_ptr_row, mask=k_mask, other=0.0)\n        \n        weight_ptr_block = weight_ptr + col_offsets[:, None] * stride_weight0 + k_offsets[None, :] * stride_weight1\n        weight_chunk = tl.load(weight_ptr_block, mask=col_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        acc += tl.sum(input_chunk[None, :] * weight_chunk, axis=1)\n    \n    if bias_ptr is not None:\n        bias_vals = tl.load(bias_ptr + col_offsets, mask=col_mask, other=0.0)\n        acc += bias_vals\n    \n    acc = tl.maximum(acc, 0.0)\n    acc = acc / divisor\n    \n    output_ptr_row = output_ptr + pid_batch * stride_output0 + col_offsets\n    tl.store(output_ptr_row, acc, mask=col_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.divisor = divisor\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        output = torch.empty((x.size(0), self.out_features), device=x.device, dtype=x.dtype)\n        grid = (x.size(0), triton.cdiv(self.out_features, 256))\n        \n        fused_linear_relu_div[grid](\n            x, self.weight, self.bias, output,\n            self.divisor,\n            x.stride(0), self.weight.stride(0), self.weight.stride(1), output.stride(0),\n            self.in_features, self.out_features\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_K': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_K': 32}, num_warps=8),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef fused_linear_relu_div(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    divisor,\n    stride_input0, stride_weight0, stride_weight1, stride_output0,\n    in_features, out_features,\n    BLOCK_SIZE_K: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_col_block = tl.program_id(1)\n    \n    col_offsets = pid_col_block * 128 + tl.arange(0, 128)\n    col_mask = col_offsets < out_features\n    \n    acc = tl.zeros((128,), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_offsets = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        input_ptr_row = input_ptr + pid_batch * stride_input0 + k_offsets\n        input_chunk = tl.load(input_ptr_row, mask=k_mask, other=0.0)\n        \n        weight_ptr_block = weight_ptr + col_offsets[:, None] * stride_weight0 + k_offsets[None, :] * stride_weight1\n        weight_chunk = tl.load(weight_ptr_block, mask=col_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        acc += tl.dot(weight_chunk, input_chunk, allow_tf32=True)\n    \n    if bias_ptr is not None:\n        bias_vals = tl.load(bias_ptr + col_offsets, mask=col_mask, other=0.0)\n        acc += bias_vals\n    \n    acc = tl.maximum(acc, 0.0)\n    acc = acc / divisor\n    \n    output_ptr_row = output_ptr + pid_batch * stride_output0 + col_offsets\n    tl.store(output_ptr_row, acc, mask=col_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.divisor = divisor\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        output = torch.empty((x.size(0), self.out_features), device=x.device, dtype=x.dtype)\n        grid = (x.size(0), triton.cdiv(self.out_features, 128))\n        \n        fused_linear_relu_div[grid](\n            x, self.weight, self.bias, output,\n            self.divisor,\n            x.stride(0), self.weight.stride(0), self.weight.stride(1), output.stride(0),\n            self.in_features, self.out_features\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_B': 32, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_F': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 32, 'BLOCK_SIZE_F': 128, 'BLOCK_SIZE_K': 128}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef linear_relu_div_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, K, divisor,\n    stride_ib, stride_ik,\n    stride_wk, stride_wn,\n    stride_ob, stride_on,\n    BLOCK_SIZE_B: tl.constexpr,\n    BLOCK_SIZE_F: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_f = tl.program_id(1)\n    \n    # Create offsets for the batch dimension\n    b_offset = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n    # Create offsets for the feature dimension\n    f_offset = pid_f * BLOCK_SIZE_F + tl.arange(0, BLOCK_SIZE_F)\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_F), dtype=tl.float32)\n    \n    # Loop over K dimension\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_offsets = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        \n        # Create masks for input and weight\n        input_mask = (b_offset[:, None] < M) & (k_offsets[None, :] < K)\n        weight_mask = (f_offset[:, None] < N) & (k_offsets[None, :] < K)\n        \n        # Load input block\n        input_block = tl.load(\n            input_ptr + b_offset[:, None] * stride_ib + k_offsets[None, :] * stride_ik,\n            mask=input_mask,\n            other=0.0\n        )\n        \n        # Load weight block (transposed access)\n        weight_block = tl.load(\n            weight_ptr + k_offsets[None, :] * stride_wk + f_offset[:, None] * stride_wn,\n            mask=weight_mask,\n            other=0.0\n        )\n        \n        # Compute matrix multiplication\n        acc += tl.dot(input_block, weight_block, allow_tf32=True)\n    \n    # Load bias\n    bias_block = tl.load(\n        bias_ptr + f_offset, \n        mask=f_offset < N, \n        other=0.0\n    )\n    \n    # Apply bias, ReLU and division\n    acc += bias_block[None, :]\n    acc = tl.maximum(acc, 0.0)\n    acc = acc / divisor\n    \n    # Create output mask and store\n    output_mask = (b_offset[:, None] < M) & (f_offset[None, :] < N)\n    output_offsets = b_offset[:, None] * stride_ob + f_offset[None, :] * stride_on\n    tl.store(output_ptr + output_offsets, acc, mask=output_mask)\n\nclass TritonLinearReLUDiv(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.divisor = divisor\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty(\n            batch_size, self.out_features, \n            device=x.device, dtype=x.dtype\n        )\n        \n        grid = lambda meta: (\n            triton.cdiv(batch_size, meta['BLOCK_SIZE_B']),\n            triton.cdiv(self.out_features, meta['BLOCK_SIZE_F'])\n        )\n        \n        linear_relu_div_kernel[grid](\n            x, self.weight, self.bias, output,\n            batch_size, self.out_features, self.in_features, self.divisor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_B=32, BLOCK_SIZE_F=64, BLOCK_SIZE_K=64\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super(ModelNew, self).__init__()\n        self.linear = TritonLinearReLUDiv(in_features, out_features, divisor)\n    \n    def forward(self, x):\n        return self.linear(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Triton kernel for fused linear + ReLU + division\n@triton.jit\ndef linear_relu_div_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    in_features, out_features, divisor,\n    stride_input_batch, stride_input_feat,\n    stride_weight_out, stride_weight_in,\n    stride_bias,\n    stride_output_batch, stride_output_feat,\n    BLOCK_SIZE_FEAT: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    # Extract program IDs\n    pid_batch = tl.program_id(0)\n    pid_feat_block = tl.program_id(1)\n    \n    # Create feature indices for this block\n    feat_idx = pid_feat_block * BLOCK_SIZE_FEAT + tl.arange(0, BLOCK_SIZE_FEAT)\n    mask_feat = feat_idx < out_features\n    \n    # Accumulator initialization\n    acc = tl.zeros((BLOCK_SIZE_FEAT,), dtype=tl.float32)\n    \n    # Loop over input features in blocks\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offs = k + tl.arange(0, BLOCK_SIZE_K)\n        mask_k = k_offs < in_features\n        \n        # Load input block\n        input_offs = pid_batch * stride_input_batch + k_offs * stride_input_feat\n        input_chunk = tl.load(input_ptr + input_offs, mask=mask_k, other=0.0)\n        \n        # Load weight block\n        weight_offs = feat_idx[:, None] * stride_weight_out + k_offs[None, :] * stride_weight_in\n        weight_chunk = tl.load(weight_ptr + weight_offs, \n                               mask=mask_k[None, :] & mask_feat[:, None], \n                               other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(weight_chunk * input_chunk[None, :], axis=1)\n    \n    # Add bias (fixed stride handling)\n    bias = tl.load(bias_ptr + feat_idx, mask=mask_feat, other=0.0)\n    acc += bias\n    \n    # Apply ReLU and division\n    acc = tl.where(acc < 0, 0, acc)  # ReLU\n    acc = acc / divisor\n    \n    # Store results\n    output_offs = pid_batch * stride_output_batch + feat_idx * stride_output_feat\n    tl.store(output_ptr + output_offs, acc, mask=mask_feat)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.divisor = divisor\n        \n        # Initialize weights and biases\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        # Original nn.Linear initialization\n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)\n        bound = 1 / (out_features**0.5) if out_features > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        x = x.contiguous()\n        batch_size = x.shape[0]\n        output = torch.empty(batch_size, self.out_features, \n                             device=x.device, dtype=x.dtype)\n        \n        # Kernel configuration\n        BLOCK_SIZE_FEAT = 128\n        BLOCK_SIZE_K = 64\n        grid = (batch_size, triton.cdiv(self.out_features, BLOCK_SIZE_FEAT))\n        \n        # Launch kernel (fixed bias stride)\n        linear_relu_div_kernel[grid](\n            x, self.weight, self.bias, output,\n            self.in_features, self.out_features, self.divisor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            1,  # Fixed bias stride to 1\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_FEAT=BLOCK_SIZE_FEAT,\n            BLOCK_SIZE_K=BLOCK_SIZE_K\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program and asked to improve its performance.\n The current program has a speed_up of 0.652 (which is less than 1, meaning it's slower than the reference).\n We are to rewrite the Triton kernel to make it faster.\n\n Analysis of the current kernel:\n   - It uses two program IDs: pid_batch (over batches) and pid_feat_block (over output features in blocks).\n   - For each block, it iterates over the input features (in_features) in chunks of BLOCK_SIZE_K.\n   - It loads a row of input (for the current batch) and a block of weights (for a block of output features and the current input feature block).\n   - Then it does a dot product by summing the product of the weight chunk and the input chunk along the feature axis.\n\n The problem is that the current kernel does not use the tensor cores because it uses a loop and then a reduction (tl.sum) which is not a matrix multiplication.\n\n We note that the Triton `tl.dot` function can use tensor cores when the dimensions are appropriate (and with allow_tf32=True for FP32 accumulation on tensor cores).\n\n Therefore, we should restructure the kernel to use `tl.dot` for the matrix multiplication.\n\n How to do it?\n   Instead of having one dimension for the batch and one for the output features, we can have a 2D grid:\n        pid_batch: for the batch dimension, and\n        pid_feat: for the output feature dimension (in blocks).\n\n   But note: the current kernel already has a 2D grid. However, the inner loop is over the input features and then we do a reduction.\n\n   We can change the kernel to load a block of input (multiple batches and multiple input features) and a block of weights (multiple output features and multiple input features) and then use `tl.dot` to compute a block of the output.\n\n   However, note that the current kernel only processes one batch per program in the batch dimension and a block of output features. This limits the amount of work and parallelism.\n\n   Alternatively, we can use a blocked matrix multiplication approach that is standard in Triton matrix multiplication examples.\n\n   Steps for a standard matmul kernel:\n      - We have two program IDs: over the batch and over the output feature blocks (or a 2D grid for the output matrix: rows and columns).\n      - Each block computes a tile of the output of size [BLOCK_SIZE_B, BLOCK_SIZE_F].\n      - We loop over the input features in blocks of size BLOCK_SIZE_K, and for each block we load:\n            a tile of the input: [BLOCK_SIZE_B, BLOCK_SIZE_K]\n            a tile of the weight: [BLOCK_SIZE_F, BLOCK_SIZE_K] (which we then transpose to [BLOCK_SIZE_K, BLOCK_SIZE_F] or use with the dot product appropriately)\n      - Then we use `tl.dot` to compute the product of the input tile and the weight tile (transposed) and accumulate.\n\n   We can adapt the current kernel to this standard blocked matmul.\n\n   We note that the current kernel only handles one batch at a time in the batch dimension. We can change it to handle a block of batches.\n\n   Given that the batch dimension is 128, we can set a block size for the batch dimension (say 32 or 64) and then have the grid be (ceil(batch_size / BLOCK_SIZE_B), ceil(out_features / BLOCK_SIZE_F)).\n\n   We will change the kernel to:\n\n        pid_b = tl.program_id(0)   # over batches in blocks\n        pid_f = tl.program_id(1)   # over output features in blocks\n\n        b_offset = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n        f_offset = pid_f * BLOCK_SIZE_F + tl.arange(0, BLOCK_SIZE_F)\n\n        acc = ... (of size [BLOCK_SIZE_B, BLOCK_SIZE_F])\n\n        for k in range(0, in_features, BLOCK_SIZE_K):\n            k_offs = k + tl.arange(0, BLOCK_SIZE_K)\n\n            # Load input block: [BLOCK_SIZE_B, BLOCK_SIZE_K]\n            input_block = tl.load( ... )\n\n            # Load weight block: [BLOCK_SIZE_F, BLOCK_SIZE_K] (because weight is [out_features, in_features])\n            weight_block = tl.load( ... )\n\n            # We want to compute: input_block (BLOCK_SIZE_B, BLOCK_SIZE_K) * weight_block (BLOCK_SIZE_F, BLOCK_SIZE_K)^T\n            # But note: tl.dot requires the second matrix to be transposed? Actually, we can do:\n            #   acc += tl.dot(input_block, weight_block, allow_tf32=True)   -> but this would be (BLOCK_SIZE_B, BLOCK_SIZE_F) if we set the weight_block as the second matrix without transposing?\n            # However, the dot product in Triton expects the second matrix to be in column-major? Actually, the dot product does: \n            #   input_block (BLOCK_SIZE_B, BLOCK_SIZE_K) * weight_block (BLOCK_SIZE_K, BLOCK_SIZE_F) -> (BLOCK_SIZE_B, BLOCK_SIZE_F)\n            # So we need to transpose the weight_block to be (BLOCK_SIZE_K, BLOCK_SIZE_F). Alternatively, we can load the weight block in a transposed manner? \n            # But note: we are storing the weight in row-major: [out_features, in_features]. So we are loading a block of [BLOCK_SIZE_F, BLOCK_SIZE_K] from the weight matrix.\n\n            # We can transpose the weight_block to get (BLOCK_SIZE_K, BLOCK_SIZE_F) and then do:\n            #   weight_block_t = tl.trans(weight_block)\n            #   acc += tl.dot(input_block, weight_block_t, allow_tf32=True)\n\n            # Or we can change the way we load the weight block? Actually, we can leave it as is and then use tl.dot with the transposed weight.\n\n        Then we add the bias (which we broadcast to the batch dimension), apply ReLU and division.\n\n   This approach should be much faster because it uses tensor cores.\n\n   We will also add autotuning for the block sizes.\n\n   Given that the current program uses fixed BLOCK_SIZE_FEAT=128 and BLOCK_SIZE_K=64, but note that the autotuner in the example programs tried several configurations.\n\n   We will restructure the kernel to use the standard matmul approach and then add autotuning.\n\n   Steps for the rewrite:\n\n   1. Change the kernel signature to include block sizes for batch, feature, and K.\n   2. Change the program IDs to be two-dimensional (over batch blocks and feature blocks).\n   3. Create offsets for the batch and feature dimensions.\n   4. Initialize the accumulator.\n   5. Loop over K in blocks and load input and weight blocks.\n   6. Use tl.dot to update the accumulator.\n   7. Then add bias, apply ReLU and division, and store.\n\n   8. In the model class, we will set up autotuning configurations and use a grid that is based on the block sizes.\n\n   We note that the current program does not have autotuning. We will add it.\n\n   We are going to base our rewrite on the top-performing program that had a speed_up of 0.927 (which is the best so far) and the inspiration programs that used the dot product.\n\n   Let's look at the top-performing program (Program 1) in the history:\n\n        @triton.jit\n        def linear_relu_div_kernel(...):\n            ... # uses tl.dot\n\n   We will write a kernel similar to that.\n\n   Important: We must keep the same function signature for the model and the same input/output behavior.\n\n   We are going to write:\n\n        @triton.autotune(...)\n        @triton.jit\n        def linear_relu_div_kernel(...):\n            ...\n\n   We'll choose a set of configurations for autotuning.\n\n   We note that the hardware is H100 (SM8.9) and we are using tensor cores.\n\n   We'll set:\n\n        configs = [\n            triton.Config({'BLOCK_SIZE_B': 32, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_F': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n            triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 128}, num_warps=4),\n        ]\n\n   And then set the grid as:\n\n        grid = lambda meta: (\n            triton.cdiv(batch_size, meta['BLOCK_SIZE_B']),\n            triton.cdiv(out_features, meta['BLOCK_SIZE_F'])\n        )\n\n   We'll change the kernel to handle blocks of batches and blocks of features.\n\n   We also note that the current kernel uses a 1D bias. We'll load the bias for the current feature block and then add it to every row in the batch block.\n\n   We'll be careful with the masking at the boundaries.\n\n   Let's write the kernel accordingly.\n\n   We are going to replace the current kernel with the autotuned blocked matmul kernel.\n\n   We note that the current program has:\n        in_features = 1024, out_features = 512, batch_size = 128.\n\n   We'll also change the ModelNew to use the autotuned kernel.\n\n   We'll keep the same class structure.\n\n   We note that the current program uses:\n        output = torch.empty(...)\n\n   We'll change the kernel call to use the autotuning grid.\n\n   We are going to write the kernel with the following parameters:\n\n        input_ptr, weight_ptr, bias_ptr, output_ptr,\n        batch_size, in_features, out_features, divisor,\n        stride_input_batch, stride_input_feat,\n        stride_weight_out, stride_weight_in,\n        stride_output_batch, stride_output_feat,\n        BLOCK_SIZE_B: tl.constexpr, BLOCK_SIZE_F: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n\n   And then in the kernel, we'll have the standard matmul loop.\n\n   We'll also use the same initialization for the weight and bias.\n\n   Let's write the code accordingly.\n\n   IMPORTANT: We are only allowed to change the Triton kernel and the forward method of ModelNew (and add autotuning). The rest of the class (__init__) remains.\n\n   We are going to output the entire block.\n\n   We note that the current program has:\n        batch_size, in_features, out_features, divisor, and get_inputs/get_init_inputs at the bottom.\n\n   We'll keep those.\n\n   We are going to write a new kernel and change the ModelNew.forward to use autotuning.\n\n   Steps for the kernel:\n\n        pid_b = tl.program_id(0)\n        pid_f = tl.program_id(1)\n\n        # Offsets for the batch dimension\n        batch_offs = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n        # Offsets for the feature dimension\n        feat_offs = pid_f * BLOCK_SIZE_F + tl.arange(0, BLOCK_SIZE_F)\n\n        # Accumulator\n        acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_F), dtype=tl.float32)\n\n        for k in range(0, in_features, BLOCK_SIZE_K):\n            k_offs = k + tl.arange(0, BLOCK_SIZE_K)\n\n            # Load input block: [BLOCK_SIZE_B, BLOCK_SIZE_K]\n            input_mask = (batch_offs[:, None] < batch_size) & (k_offs[None, :] < in_features)\n            input_ptr_block = input_ptr + batch_offs[:, None] * stride_input_batch + k_offs[None, :] * stride_input_feat\n            input_block = tl.load(input_ptr_block, mask=input_mask, other=0.0)\n\n            # Load weight block: [BLOCK_SIZE_F, BLOCK_SIZE_K] (because weight is stored as [out_features, in_features])\n            weight_mask = (feat_offs[:, None] < out_features) & (k_offs[None, :] < in_features)\n            weight_ptr_block = weight_ptr + feat_offs[:, None] * stride_weight_out + k_offs[None, :] * stride_weight_in\n            weight_block = tl.load(weight_ptr_block, mask=weight_mask, other=0.0)\n\n            # We want to compute: input_block (BLOCK_SIZE_B, BLOCK_SIZE_K) * weight_block (BLOCK_SIZE_F, BLOCK_SIZE_K)^T\n            # But note: the dot product in Triton expects the second matrix to be in column-major? Actually, we can do:\n            #   We can transpose weight_block to get [BLOCK_SIZE_K, BLOCK_SIZE_F] and then do:\n            #   acc += tl.dot(input_block, weight_block, trans_b=True, allow_tf32=True)\n            #   OR\n            #   We can note that tl.dot(A, B) where A is [M, K] and B is [K, N] produces [M, N]. But here we have:\n            #       input_block: [BLOCK_SIZE_B, BLOCK_SIZE_K]\n            #       weight_block: [BLOCK_SIZE_F, BLOCK_SIZE_K] -> we want to use it as [BLOCK_SIZE_K, BLOCK_SIZE_F]? \n            #   So we can transpose weight_block to get [BLOCK_SIZE_K, BLOCK_SIZE_F] and then do:\n            #       tl.dot(input_block, tl.trans(weight_block), allow_tf32=True)\n            #   But note: the dot product expects the second matrix to be of size [K, N] and we have [BLOCK_SIZE_K, BLOCK_SIZE_F] for the transposed weight_block.\n\n            weight_block_t = tl.trans(weight_block)\n            acc += tl.dot(input_block, weight_block_t, allow_tf32=True)\n\n        # After the loop, add bias\n        bias_ptr_block = bias_ptr + feat_offs\n        bias_block = tl.load(bias_ptr_block, mask=feat_offs < out_features, other=0.0)   # [BLOCK_SIZE_F]\n        # Add bias to every row in the batch block\n        acc += bias_block[None, :]\n\n        # ReLU and division\n        acc = tl.maximum(acc, 0.0)\n        acc = acc / divisor\n\n        # Store\n        output_ptr_block = output_ptr + batch_offs[:, None] * stride_output_batch + feat_offs[None, :] * stride_output_feat\n        output_mask = (batch_offs[:, None] < batch_size) & (feat_offs[None, :] < out_features)\n        tl.store(output_ptr_block, acc, mask=output_mask)\n\n   We'll then set up autotuning in the kernel.\n\n   We note: the current kernel in the top-performing program had a similar structure.\n\n   We'll add autotuning configurations.\n\n   We also note that the current program uses a fixed grid in the top-performing program, but we are going to use a lambda that depends on the autotuning parameters.\n\n   In the ModelNew.forward:\n\n        grid = lambda meta: (\n            triton.cdiv(batch_size, meta['BLOCK_SIZE_B']),\n            triton.cdiv(out_features, meta['BLOCK_SIZE_F'])\n        )\n\n        linear_relu_div_kernel[grid](\n            ...,\n            BLOCK_SIZE_B=...   # we don't specify fixed values because we are autotuning, so we pass the meta parameters\n            ... \n        )\n\n   We must pass the block sizes as kernel arguments and mark them as tl.constexpr.\n\n   We are going to write the kernel accordingly.\n\n   We note: the current program had a speed_up of 0.652 and we aim for at least 1.05.\n\n   We are hopeful that by using the tensor cores we can achieve that.\n\n   Let's write the entire block.\n\n   Note: We are going to remove the old kernel and replace it with the new one.\n\n   We also note that the current program had a fixed bias stride of 1. In the new kernel, we are loading the bias with a stride of 1 (because we are using feat_offs as the index). So we don't need to pass the bias stride.\n\n   We'll adjust the kernel arguments accordingly.\n\n   We'll remove the stride_bias because we are not using it.\n\n   We'll change the kernel arguments to:\n\n        input_ptr, weight_ptr, bias_ptr, output_ptr,\n        batch_size, in_features, out_features, divisor,\n        stride_input_batch, stride_input_feat,\n        stride_weight_out, stride_weight_in,\n        stride_output_batch, stride_output_feat,\n        BLOCK_SIZE_B: tl.constexpr, BLOCK_SIZE_F: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n\n   And then in the call from the model, we pass:\n\n        x, self.weight, self.bias, output,\n        batch_size, self.in_features, self.out_features, self.divisor,\n        x.stride(0), x.stride(1),\n        self.weight.stride(0), self.weight.stride(1),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE_B=config['BLOCK_SIZE_B'], ...   # but we don't specify, because autotuning will set the meta\n\n   We'll rely on the autotuner to set the block sizes.\n\n   We'll now write the code.\n\n   IMPORTANT: We must not change the class name (ModelNew) and the functions get_inputs and get_init_inputs.\n\n   We are going to output the entire block with the new kernel and the autotuning.\n\n   Let's do it.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_F': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_F': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_F': 128, 'BLOCK_SIZE_K': 32}, num_warps=8),\n    ],\n    key=['in_features', 'out_features'],\n)\n@triton.jit\ndef linear_relu_div_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    batch_size, in_features, out_features, divisor,\n    stride_input_batch, stride_input_feat,\n    stride_weight_out, stride_weight_in,\n    stride_output_batch, stride_output_feat,\n    BLOCK_SIZE_B: tl.constexpr,\n    BLOCK_SIZE_F: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_f = tl.program_id(1)\n    \n    # Create block offsets\n    b_offset = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n    f_offset = pid_f * BLOCK_SIZE_F + tl.arange(0, BLOCK_SIZE_F)\n    \n    # Create masks for boundary checks\n    batch_mask = b_offset < batch_size\n    feat_mask = f_offset < out_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_F), dtype=tl.float32)\n    \n    # Blocked matrix multiplication\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        # Load input block\n        input_ptrs = input_ptr + b_offset[:, None] * stride_input_batch + k_offsets[None, :]\n        input_mask = batch_mask[:, None] & k_mask[None, :]\n        input_block = tl.load(input_ptrs, mask=input_mask, other=0.0)\n        \n        # Load weight block\n        weight_ptrs = weight_ptr + f_offset[:, None] * stride_weight_out + k_offsets[None, :]\n        weight_mask = feat_mask[:, None] & k_mask[None, :]\n        weight_block = tl.load(weight_ptrs, mask=weight_mask, other=0.0)\n        \n        # Matrix multiplication with tensor cores\n        weight_block_t = tl.trans(weight_block)\n        acc += tl.dot(input_block, weight_block_t, allow_tf32=True)\n\n    # Load bias and fuse operations\n    bias_ptrs = bias_ptr + f_offset\n    bias_block = tl.load(bias_ptrs, mask=feat_mask, other=0.0)\n    acc += bias_block[None, :]\n    acc = tl.maximum(acc, 0.0)  # ReLU\n    acc = acc / divisor  # Division\n    \n    # Store results\n    output_ptrs = output_ptr + b_offset[:, None] * stride_output_batch + f_offset[None, :]\n    output_mask = batch_mask[:, None] & feat_mask[None, :]\n    tl.store(output_ptrs, acc, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, divisor):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.divisor = divisor\n        \n        # Initialize weights and biases\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        # Original nn.Linear initialization\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        output = torch.empty((batch_size, self.out_features), \n                             device=x.device, dtype=x.dtype)\n        \n        # Grid configuration with autotuning\n        grid = lambda meta: (\n            triton.cdiv(batch_size, meta['BLOCK_SIZE_B']),\n            triton.cdiv(self.out_features, meta['BLOCK_SIZE_F'])\n        )\n        \n        # Launch kernel with tensor core optimization\n        linear_relu_div_kernel[grid](\n            x, self.weight, self.bias, output,\n            batch_size, self.in_features, self.out_features, self.divisor,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1)\n        )\n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\ndivisor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "ebc27775-baf9-4053-aa97-0232ebcd99fc", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 64, 'BLOCK_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 128, 'BLOCK_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'BLOCK_K': 64}, num_warps=8),\n    ],\n    key=['input_size', 'output_size'],\n)\n@triton.jit\ndef _fused_linear_div_gelu(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    input_size, output_size, divisor,\n    batch_size, stride_x, stride_out,\n    BLOCK_SIZE: tl.constexpr, BLOCK_K: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n):\n    # Compute program IDs\n    pid_batch = tl.program_id(0)\n    pid_output_block = tl.program_id(1)\n    \n    # Create output block offset\n    output_offset = pid_output_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    output_mask = output_offset < output_size\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Loop over input features in blocks\n    for k in range(0, input_size, BLOCK_K):\n        k_offset = k + tl.arange(0, BLOCK_K)\n        k_mask = k_offset < input_size\n        \n        # Load input block\n        x_ptr_offset = pid_batch * stride_x + k_offset\n        x_block = tl.load(x_ptr + x_ptr_offset, mask=k_mask, other=0.0)\n        \n        # Load weight block\n        w_ptr_offset = output_offset[:, None] * input_size + k_offset[None, :]\n        w_block = tl.load(weight_ptr + w_ptr_offset, mask=output_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(x_block[None, :] * w_block, axis=1)\n    \n    # Load bias if exists\n    if HAS_BIAS:\n        bias_block = tl.load(bias_ptr + output_offset, mask=output_mask, other=0.0)\n        acc += bias_block\n    \n    # Apply division and approximate GELU\n    scaled = acc / divisor\n    gelu = scaled * 0.5 * (1.0 + tl.erf(scaled * 0.7071067811865475))\n    \n    # Store output\n    out_ptr_offset = pid_batch * stride_out + output_offset\n    tl.store(output_ptr + out_ptr_offset, gelu, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, output_size, divisor):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.divisor = divisor\n        \n        # Initialize parameters as in nn.Linear\n        self.weight = nn.Parameter(torch.empty(output_size, input_size))\n        self.bias = nn.Parameter(torch.empty(output_size))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size, _ = x.shape\n        output = torch.empty((batch_size, self.output_size), \n                             device=x.device, dtype=x.dtype)\n        \n        # Dynamic grid based on autotuned BLOCK_SIZE\n        grid = lambda meta: (batch_size, triton.cdiv(self.output_size, meta['BLOCK_SIZE']))\n        \n        # Launch kernel\n        _fused_linear_div_gelu[grid](\n            x, self.weight, self.bias, output,\n            self.input_size, self.output_size, self.divisor,\n            batch_size, x.stride(0), output.stride(0),\n            HAS_BIAS=True\n        )\n        return output\n\nbatch_size = 128\ninput_size = 512\noutput_size = 1024\ndivisor = 10.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, output_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "f8b17584-b68a-4788-82ca-0192b5e3c081", "generation": 2, "timestamp": 1754580569.8280456, "iteration_found": 4, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.415, "runtime_ms": 0.127, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0527, "speed_up": 0.415, "custom_timing": 0.127}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_86_Matmul_Divide_GELU", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "max_difference": ["0.178050", "0.187824", "0.193828", "0.186985"], "avg_difference": ["0.016259", "0.016318", "0.016305", "0.016288"], "correctness_issue": "Output mismatch", "correctness_trials": "(1 / 5)", "reference_timing_ms": 0.0527, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 0.0000\n- score: 10.0000\n- stage: wrong_answer\n- verify_payload: {'id': 'level2_86_Matmul_Divide_GELU', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'max_difference': ['0.178050', '0.187824', '0.193828', '0.186985'], 'avg_difference': ['0.016259', '0.016318', '0.016305', '0.016288'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(1 / 5)', 'reference_timing_ms': 0.0527, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0453, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0527, 'speed_up': 1.163, 'custom_timing': 0.0453}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_86_Matmul_Divide_GELU', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'max_difference': ['0.178050', '0.187824', '0.193828', '0.186985'], 'avg_difference': ['0.016259', '0.016318', '0.016305', '0.016288'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(1 / 5)', 'reference_timing_ms': 0.0527, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 64, 'BLOCK_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 128, 'BLOCK_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'BLOCK_K': 64}, num_warps=8),\n    ],\n    key=['input_size', 'output_size'],\n)\n@triton.jit\ndef _fused_linear_div_gelu(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    input_size, output_size, divisor,\n    batch_size, stride_x, stride_out,\n    BLOCK_SIZE: tl.constexpr, BLOCK_K: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n):\n    # Compute program IDs\n    pid_batch = tl.program_id(0)\n    pid_output_block = tl.program_id(1)\n    \n    # Create output block offset\n    output_offset = pid_output_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    output_mask = output_offset < output_size\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Loop over input features in blocks\n    for k in range(0, input_size, BLOCK_K):\n        k_offset = k + tl.arange(0, BLOCK_K)\n        k_mask = k_offset < input_size\n        \n        # Load input block\n        x_ptr_offset = pid_batch * stride_x + k_offset\n        x_block = tl.load(x_ptr + x_ptr_offset, mask=k_mask, other=0.0)\n        \n        # Load weight block\n        w_ptr_offset = output_offset[:, None] * input_size + k_offset[None, :]\n        w_block = tl.load(weight_ptr + w_ptr_offset, mask=output_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(x_block[None, :] * w_block, axis=1)\n    \n    # Load bias if exists\n    if HAS_BIAS:\n        bias_block = tl.load(bias_ptr + output_offset, mask=output_mask, other=0.0)\n        acc += bias_block\n    \n    # Apply division and approximate GELU\n    scaled = acc / divisor\n    gelu = scaled * 0.5 * (1.0 + tl.erf(scaled * 0.7071067811865475))\n    \n    # Store output\n    out_ptr_offset = pid_batch * stride_out + output_offset\n    tl.store(output_ptr + out_ptr_offset, gelu, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, output_size, divisor):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.divisor = divisor\n        \n        # Initialize parameters as in nn.Linear\n        self.weight = nn.Parameter(torch.empty(output_size, input_size))\n        self.bias = nn.Parameter(torch.empty(output_size))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size, _ = x.shape\n        output = torch.empty((batch_size, self.output_size), \n                             device=x.device, dtype=x.dtype)\n        \n        # Determine grid dimensions\n        grid = (batch_size, triton.cdiv(self.output_size, 128))\n        \n        # Launch kernel\n        _fused_linear_div_gelu[grid](\n            x, self.weight, self.bias, output,\n            self.input_size, self.output_size, self.divisor,\n            batch_size, x.stride(0), output.stride(0),\n            HAS_BIAS=True\n        )\n        return output\n\nbatch_size = 128\ninput_size = 512\noutput_size = 1024\ndivisor = 10.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, output_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_86_Matmul_Divide_GELU', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'max_difference': ['0.178050', '0.187824', '0.193828', '0.186985'], 'avg_difference': ['0.016259', '0.016318', '0.016305', '0.016288'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(1 / 5)', 'reference_timing_ms': 0.0527, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n### Program 2 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs a matrix multiplication, divides by a scalar, and applies GELU activation.\n    \"\"\"\n    def __init__(self, input_size, output_size, divisor):\n        super(ModelNew, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n        self.divisor = divisor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, output_size).\n        \"\"\"\n        x = self.linear(x)\n        x = x / self.divisor\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 128\ninput_size = 512\noutput_size = 1024\ndivisor = 10.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, output_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0453), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0527, 'speed_up': 1.163, 'custom_timing': 0.0453})\n\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 64, 'BLOCK_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 128, 'BLOCK_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'BLOCK_K': 64}, num_warps=8),\n    ],\n    key=['input_size', 'output_size'],\n)\n@triton.jit\ndef _fused_linear_div_gelu(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    input_size, output_size, divisor,\n    batch_size, stride_x, stride_out,\n    BLOCK_SIZE: tl.constexpr, BLOCK_K: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n):\n    # Compute program IDs\n    pid_batch = tl.program_id(0)\n    pid_output_block = tl.program_id(1)\n    \n    # Create output block offset\n    output_offset = pid_output_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    output_mask = output_offset < output_size\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Loop over input features in blocks\n    for k in range(0, input_size, BLOCK_K):\n        k_offset = k + tl.arange(0, BLOCK_K)\n        k_mask = k_offset < input_size\n        \n        # Load input block\n        x_ptr_offset = pid_batch * stride_x + k_offset\n        x_block = tl.load(x_ptr + x_ptr_offset, mask=k_mask, other=0.0)\n        \n        # Load weight block\n        w_ptr_offset = output_offset[:, None] * input_size + k_offset[None, :]\n        w_block = tl.load(weight_ptr + w_ptr_offset, mask=output_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(x_block[None, :] * w_block, axis=1)\n    \n    # Load bias if exists\n    if HAS_BIAS:\n        bias_block = tl.load(bias_ptr + output_offset, mask=output_mask, other=0.0)\n        acc += bias_block\n    \n    # Apply division and approximate GELU\n    scaled = acc / divisor\n    gelu = scaled * 0.5 * (1.0 + tl.erf(scaled * 0.7071067811865475))\n    \n    # Store output\n    out_ptr_offset = pid_batch * stride_out + output_offset\n    tl.store(output_ptr + out_ptr_offset, gelu, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, output_size, divisor):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.divisor = divisor\n        \n        # Initialize parameters as in nn.Linear\n        self.weight = nn.Parameter(torch.empty(output_size, input_size))\n        self.bias = nn.Parameter(torch.empty(output_size))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size, _ = x.shape\n        output = torch.empty((batch_size, self.output_size), \n                             device=x.device, dtype=x.dtype)\n        \n        # Determine grid dimensions\n        grid = (batch_size, triton.cdiv(self.output_size, 128))\n        \n        # Launch kernel\n        _fused_linear_div_gelu[grid](\n            x, self.weight, self.bias, output,\n            self.input_size, self.output_size, self.divisor,\n            batch_size, x.stride(0), output.stride(0),\n            HAS_BIAS=True\n        )\n        return output\n\nbatch_size = 128\ninput_size = 512\noutput_size = 1024\ndivisor = 10.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, output_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a Triton kernel that fuses a linear layer (matrix multiplication with bias), division by a scalar, and GELU activation.\n The current version compiles but fails correctness (output mismatch). We need to fix the correctness while maintaining or improving performance.\n\n Analysis of the current kernel:\n  - The kernel uses two program IDs: pid_batch (over batch) and pid_output_block (over output features in blocks of BLOCK_SIZE).\n  - For each output block, it iterates over the input features in blocks of BLOCK_K to compute the dot product.\n  - Then it adds the bias (if present), divides by `divisor`, and applies the GELU approximation.\n\n The correctness issue might be due to:\n  1. The way we are loading the weight matrix: the weight matrix is of shape [output_size, input_size].\n     In the kernel, we are computing:\n        w_ptr_offset = output_offset[:, None] * input_size + k_offset[None, :]\n     This seems correct because for a given row (output feature) and a set of columns (k_offset) we are loading the weight block.\n\n  2. The input is loaded for the entire batch row (pid_batch) and the current k block.\n\n  3. The accumulation: we are doing `acc += tl.sum(x_block[None, :] * w_block, axis=1)`\n     This is equivalent to: for each output in the block, we are doing a dot product between the current x_block and the corresponding row of the weight block.\n\n  However, note that the weight matrix is stored in row-major (each row is a set of input_size weights for one output feature). The current indexing for the weight block is:\n      w_ptr_offset = output_offset[:, None] * input_size + k_offset[None, :]\n  This creates a [BLOCK_SIZE, BLOCK_K] block where:\n      row i: corresponds to the weight row (output_offset[i]) and columns k_offset.\n\n  The input block is a vector of length BLOCK_K: x_block.\n\n  Then we do: x_block[None, :] * w_block -> [1, BLOCK_K] * [BLOCK_SIZE, BLOCK_K] -> [BLOCK_SIZE, BLOCK_K] (via broadcasting)\n  Then we sum over axis=1 -> [BLOCK_SIZE] vector.\n\n  This is a correct block matrix multiplication.\n\n  However, the issue might be in the GELU approximation? Or the division?\n\n  Another possibility: the Triton kernel is using float32 for accumulation, but the reference might be using float16? \n  But the problem states the reference is in PyTorch and we are to match within float32 tolerance.\n\n  Let's check the reference code from the top-performing reference program (Program 2):\n\n      class ModelNew(nn.Module):\n          def __init__(self, input_size, output_size, divisor):\n              super(ModelNew, self).__init__()\n              self.linear = nn.Linear(input_size, output_size)\n              self.divisor = divisor\n\n          def forward(self, x):\n              x = self.linear(x)\n              x = x / self.divisor\n              x = torch.nn.functional.gelu(x)\n              return x\n\n  The reference uses the built-in `nn.Linear` and `torch.nn.functional.gelu`.\n\n  The Triton kernel uses:\n      scaled = acc / divisor\n      gelu = scaled * 0.5 * (1.0 + tl.erf(scaled * 0.7071067811865475))\n\n  This is the exact approximation for GELU.\n\n  Another possibility: the bias is being loaded for the entire output block, but note that the bias is a vector of length output_size.\n  We are loading it with `output_offset` which is the same as the output feature indices we are computing. So that is correct.\n\n  Another possibility: the input and output strides? In the forward pass, we pass:\n        x.stride(0) and output.stride(0)\n\n  For a tensor of shape [batch, features], the stride for the batch dimension is features (for x) and output_size (for output). This is correct.\n\n  However, note that the kernel uses:\n        x_ptr_offset = pid_batch * stride_x + k_offset\n    and \n        out_ptr_offset = pid_batch * stride_out + output_offset\n\n  This is also correct.\n\n  But wait: the weight matrix is stored in row-major, but the kernel is accessing it in a way that might not be coalesced? \n  However, that should not cause numerical issues.\n\n  Another possibility: the accumulation in float32 might be done in a different order? But that would only cause small differences.\n\n  The verify_payload shows an average difference of about 0.016 and max difference of about 0.19. This is too high to be a rounding issue.\n\n  Let me double-check the weight loading:\n\n      w_ptr_offset = output_offset[:, None] * input_size + k_offset[None, :]\n\n  For a given output row i and input column j, the weight is at weight[i, j]. The offset is i * input_size + j.\n\n  But note: the weight matrix is of shape [output_size, input_size]. So the stride between two consecutive output features (rows) is input_size. This is correct.\n\n  However, the problem might be that the output_offset is a vector of [BLOCK_SIZE] and k_offset is a vector of [BLOCK_K]. Then w_ptr_offset is [BLOCK_SIZE, BLOCK_K] and we are loading a block of weights. This is standard.\n\n  Another possibility: the input block is being loaded as a contiguous block? The input is stored in row-major. The k_offset is contiguous in memory? \n  But note: we are loading a contiguous block of BLOCK_K elements in the inner dimension. This is contiguous.\n\n  But wait: the input tensor x is of shape [batch, input_size]. The kernel loads for a given batch row (pid_batch) and a block of columns (k_offset). This is contiguous.\n\n  Therefore, I suspect the issue might be in the handling of the boundaries? \n\n  In the inner loop, we have:\n      k_mask = k_offset < input_size\n      ... and we use this mask when loading x and weight.\n\n  For the weight, we also have an output_mask (for the output block) and then we combine with k_mask? \n\n      w_block = tl.load(weight_ptr + w_ptr_offset, mask=output_mask[:, None] & k_mask[None, :], other=0.0)\n\n  This mask is [BLOCK_SIZE, BLOCK_K] and it is the outer product of (output_mask and k_mask). This is correct.\n\n  However, note that the output_mask is for the current output block and the k_mask is for the current input block. \n\n  But what about the last block in the inner dimension? It might be that the input_size is not divisible by BLOCK_K? That is handled by the mask.\n\n  Another possibility: the kernel is launched with a grid that has:\n        grid = (batch_size, triton.cdiv(self.output_size, 128))\n\n  But note: in the kernel, the output block size is set by the config (BLOCK_SIZE). However, in the grid we are using 128 as the block size? \n\n  We are using a fixed block size of 128 in the grid? But the autotune config uses different BLOCK_SIZE (128, 256, 64, etc.). \n\n  This is a problem: the grid uses 128 regardless of the autotune choice. We must use the same BLOCK_SIZE in the grid.\n\n  How to fix the grid? We are currently using:\n        grid = (batch_size, triton.cdiv(self.output_size, 128))\n\n  But the kernel expects the block size to be the one from the config. We must pass the block size to the grid function.\n\n  However, note that the autotune will try multiple configs. We cannot know the BLOCK_SIZE at the time of grid definition.\n\n  The solution is to use a function for the grid that takes the meta-parameters (including BLOCK_SIZE). But in the current code, we are not doing that.\n\n  The correct way is to define the grid as a lambda that uses the BLOCK_SIZE from the current config. We can do:\n\n        grid = lambda meta: (batch_size, triton.cdiv(self.output_size, meta['BLOCK_SIZE']))\n\n  Then when the kernel is launched, the autotune system will pass the config (meta) to the grid function.\n\n  This might be the root cause of the correctness issue: because if the autotuner chooses a BLOCK_SIZE that is not 128, then the grid would be wrong.\n\n  Also, note that the kernel uses the same BLOCK_SIZE for the output block, so we must adjust the grid accordingly.\n\n  Let me change the grid definition to use a lambda with meta.\n\n  Additionally, we must also adjust the kernel to use the BLOCK_SIZE and BLOCK_K from the current config (which we are already doing).\n\n  Steps for the fix:\n    1. Change the grid to: \n            grid = lambda meta: (batch_size, triton.cdiv(self.output_size, meta['BLOCK_SIZE']))\n\n    2. Then when launching the kernel, we pass the meta parameters? Actually, the autotuner will handle that.\n\n  Also, note that the kernel has two parameters: BLOCK_SIZE and BLOCK_K. We are passing them as kernel arguments? \n  No, they are passed as constexpr. The autotuner will set them per config.\n\n  So the grid function must depend on the current config's BLOCK_SIZE.\n\n  This change might fix the correctness because when the autotuner chooses a config with BLOCK_SIZE=256, then the grid would have fewer blocks? \n  But currently we are always using 128, so if the output_size is 1024, then we would launch 8 blocks (1024/128). But if the autotuner chooses BLOCK_SIZE=256, then the kernel expects to run 4 blocks? \n  Without the grid adjustment, we are launching 8 blocks and the kernel would have some blocks that are out of bounds? \n\n  The kernel has:\n        pid_output_block = tl.program_id(1)\n        output_offset = pid_output_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        output_mask = output_offset < output_size\n\n  This would work for any BLOCK_SIZE, but the grid must be set to the number of blocks = ceil(output_size / BLOCK_SIZE). \n\n  Therefore, by fixing the grid to use the meta['BLOCK_SIZE'], we ensure that the grid is set appropriately for the chosen block size.\n\n  Let's change the grid to a lambda that uses meta.\n\n  Also, note that the kernel uses the same variable name 'BLOCK_SIZE' as in the config. So we can use meta['BLOCK_SIZE'].\n\n  Additionally, we note that the kernel has two constexpr arguments: BLOCK_SIZE and BLOCK_K. The autotuner will set them per the config.\n\n  Another issue: the kernel might be launched with a grid that has too many blocks? For example, if we use a block size of 256 for output_size=1024, then the grid for the second dimension would be 4. But currently we are using 8 (because 1024/128=8). Then the kernel would try to compute blocks that are beyond the output? \n\n  The kernel has an output_mask that checks `output_offset < output_size`. So it should be safe? But note that if the grid has more blocks than needed, then the extra blocks would still run and the mask would be all false? That would be inefficient but correct? \n\n  However, the problem is that the autotuner might choose a config with a larger block size and then the grid we are launching (with 128) would have too many blocks? For example, if BLOCK_SIZE=256, then we only need 4 blocks, but we launch 8. Then the kernel would run 8 blocks and the last 4 blocks would be out of bounds? \n\n  Actually, the grid is fixed at (batch_size, 8) regardless of the config. But the kernel's BLOCK_SIZE changes per config. This is a mismatch.\n\n  Therefore, we must change the grid to be dynamic per config.\n\n  So the fix is to change the grid to:\n        grid = lambda meta: (batch_size, triton.cdiv(self.output_size, meta['BLOCK_SIZE']))\n\n  We will change that.\n\n  Also, note that the kernel uses BLOCK_K. That doesn't affect the grid.\n\n  Let me make this change and see if it fixes the correctness.\n\n  Additionally, we note that the kernel currently does not specify the input and output data types. The reference might be in float32? The kernel uses float32 for accumulation and then stores the same? \n\n  The ModelNew class initializes the weight and bias as float32? The input is given as whatever x is. The reference program uses the same input? \n\n  The problem states: the reference_code is in PyTorch. We are to generate Triton code that is functionally equivalent.\n\n  The reference program (Program 2) does not specify dtype? But the test uses the same input? \n\n  We are creating the output tensor as:\n        output = torch.empty((batch_size, self.output_size), device=x.device, dtype=x.dtype)\n\n  So if x is float16, then the output is float16? But the kernel is computing in float32 and then storing to float16? \n\n  The kernel does not convert the result to the output dtype. We are storing the float32 gelu result into a float16 tensor? That might cause a type mismatch.\n\n  How does the reference work? The reference uses the same x and the linear layer? The linear layer in PyTorch by default uses float32 for computation? \n\n  Actually, the reference uses the same dtype as the input. So if x is float16, then the linear layer will run in float16? \n\n  We have two options:\n    Option 1: Change the kernel to use the same dtype as the input? But the kernel uses float32 for accumulation to avoid precision loss.\n\n    Option 2: Convert the result to the input dtype? \n\n  The kernel currently stores the result as float32? But the output tensor is of dtype=x.dtype. \n\n  We must convert the result to the output dtype. We can do:\n\n        gelu = scaled * 0.5 * (1.0 + tl.erf(scaled * 0.7071067811865475))\n        if output_ptr.type.element_ty != tl.float32:\n            gelu = gelu.to(output_ptr.type.element_ty)\n\n  However, we cannot do that because the kernel does not know the output dtype at compile time? \n\n  Alternatively, we can pass the dtype of the output and convert accordingly? But that complicates the kernel.\n\n  Another idea: the kernel should compute in the same precision as the input? But that might hurt performance? \n\n  Let me check the reference program: it does the entire computation in the input dtype? \n\n  The problem: the reference output is in the same dtype as the input. Our kernel produces float32 and then we are storing it into a tensor of the input dtype. This is done by the `torch.empty` call. But the kernel writes float32 into a tensor that might be float16? That is undefined behavior.\n\n  How does Triton handle this? The kernel writes a float32 value to a memory location that is expected to be float16? This would cause a type mismatch and likely garbage.\n\n  Therefore, we must convert the result to the input dtype inside the kernel? \n\n  But note: the kernel is jit compiled and the output_ptr type is known at compile time. We can do:\n\n        gelu = scaled * 0.5 * (1.0 + tl.erf(scaled * 0.7071067811865475))\n        gelu = gelu.to(tl.float16)   # if the output is float16?\n\n  However, we cannot know the output dtype at kernel compile time? Actually, the kernel is compiled for a specific set of types? \n\n  The kernel is launched with the output tensor of a specific dtype. The Triton kernel must be compiled for that dtype? \n\n  Alternatively, we can have the kernel accept the output dtype as a parameter and then conditionally convert? But that would require multiple compilations? \n\n  Alternatively, we can change the output tensor to be float32? But that would change the interface? \n\n  The problem: the reference code returns the same dtype as the input. We must do the same.\n\n  How about we change the ModelNew to create the output tensor as float32? Then after the kernel, we convert it to the input dtype? \n\n  But that would add an extra operation. Alternatively, we can do the conversion in the kernel.\n\n  Let me see how the Triton matmul example does: they often use a conversion at the end.\n\n  We can do:\n\n        gelu = scaled * 0.5 * (1.0 + tl.erf(scaled * 0.7071067811865475))\n        # Convert to the output dtype if needed\n        if output_ptr.type.element_ty != tl.float32:\n            gelu = gelu.to(output_ptr.type.element_ty)\n\n  But wait: the kernel is compiled for a specific output_ptr type? So we can do:\n\n        gelu = gelu.to(tl.typeof(output_ptr))\n\n  However, the Triton language does not have typeof for pointers? We can pass the dtype as a kernel argument? \n\n  Alternatively, we can note that the kernel is jit compiled for the specific dtypes of the tensors. So when the kernel is compiled for a float16 output, then we can convert to float16.\n\n  How about we change the kernel to:\n\n        # ... after computing gelu in float32\n        gelu = gelu.to(tl.float16)   # if we know the output is float16\n\n  But we don't know. We can pass a constant for the output dtype? \n\n  The Triton kernel can take a `dtype` argument? But we don't have that.\n\n  Alternatively, we can change the ModelNew to always run the kernel in float32 and then convert the output to the input dtype? \n\n        output = torch.empty((batch_size, self.output_size), device=x.device, dtype=torch.float32)\n        ... launch kernel writing float32 to output\n        return output.to(x.dtype)\n\n  This would work and add a small overhead. But note: the reference runs in the input dtype. Our kernel runs in float32 and then we do a conversion. This might be acceptable? \n\n  However, the problem says: functional equivalence. The conversion might cause very small differences? But within float32 tolerance? \n\n  Alternatively, we can change the kernel to do the entire computation in the input dtype? But that might be less accurate and also might not use tensor cores? \n\n  Given the time, I propose:\n\n    1. Change the grid to use the BLOCK_SIZE from the meta parameter.\n    2. Change the ModelNew.forward to:\n          output = torch.empty((batch_size, self.output_size), device=x.device, dtype=torch.float32)\n          ... then after the kernel, convert to x.dtype? \n          return output.to(x.dtype)\n\n  But wait: the kernel writes float32 to the output tensor. Then we convert to x.dtype. This would match the reference.\n\n  However, the reference does the computation in the input dtype. We are doing it in float32 and then converting. This should be acceptable? \n\n  But note: the verify_payload shows a max difference of 0.19, which is too high. This might not be fixed by the dtype conversion? \n\n  Alternatively, we can change the kernel to use the input dtype? But that would require changing the accumulation to the input dtype? That might be worse for precision.\n\n  Let's try the grid fix and the dtype conversion in the ModelNew first.\n\n  However, note that the kernel also uses the input tensor. We are loading the input tensor as float32? The input tensor might be float16. \n\n  We are currently loading the input as:\n\n        x_block = tl.load(x_ptr + x_ptr_offset, mask=k_mask, other=0.0)\n\n  The x_ptr points to a tensor of dtype=x.dtype. The kernel expects it to be float32? \n\n  We are not converting the input to float32? \n\n  This is a problem: if the input is float16, then we are loading it as float32? That would be garbage.\n\n  Therefore, we must load the input in its own dtype? \n\n  How to handle multiple dtypes? We can pass the dtype of the input and weight as kernel arguments? \n\n  Alternatively, we can change the kernel to use the same dtype as the input? \n\n  But note: the Triton kernel does not support dynamic dtypes. We must compile a separate kernel for each dtype? \n\n  The autotuner would then have to compile multiple versions? \n\n  Given the complexity, and since the problem is for a specific test (with known input sizes and dtypes?), we might assume that the input is float32? \n\n  But the reference program does not specify? The test might use float32? \n\n  The verify_payload does not specify the dtype? \n\n  However, the reference_timing_ms is 0.0527 and the custom_timing is -1.0 (meaning it didn't run?).\n\n  How about we check the get_inputs: it returns torch.randn(batch_size, input_size) which is float32 by default.\n\n  So the input is float32? Then the output is float32? \n\n  Then why are we creating the output as the same dtype as the input? It would be float32? So no conversion is needed? \n\n  Therefore, the issue is likely the grid.\n\n  Let's fix the grid and see if it passes correctness.\n\n  Proposed changes:\n\n      In ModelNew.forward:\n          grid = lambda meta: (batch_size, triton.cdiv(self.output_size, meta['BLOCK_SIZE']))\n\n  And remove the dtype conversion because we are in float32.\n\n  We will change the grid to use the meta parameter.\n\n  Also, note that the kernel has other parameters (like BLOCK_K) that are set by autotune. The grid does not depend on BLOCK_K.\n\n  Let's rewrite the ModelNew.forward:\n\n        output = torch.empty((batch_size, self.output_size), device=x.device, dtype=x.dtype)\n        # Grid: batch_size for the first, and for the second: number of blocks for output features\n        grid = lambda meta: (batch_size, triton.cdiv(self.output_size, meta['BLOCK_SIZE']))\n        _fused_linear_div_gelu[grid](\n            x, self.weight, self.bias, output,\n            self.input_size, self.output_size, self.divisor,\n            batch_size, x.stride(0), output.stride(0),\n            HAS_BIAS=True\n        )\n\n  This should fix the grid.\n\n  We hope that with the grid fixed, the kernel will compute the correct result.\n\n  If this doesn't fix, we might need to look into the GELU approximation? But the reference uses the same formula.\n\n  Let's output the code with the grid fix.\n\n  IMPORTANT: We are only allowed to change the Triton kernel block? But note that the ModelNew class is part of the block.\n\n  We will change the grid definition.\n\n  Also, note that the autotune configs use BLOCK_SIZE and BLOCK_K. We are now using meta['BLOCK_SIZE'] for the grid.\n\n  We'll try this.\n\n  If it still fails, we might need to debug further, but let's hope.\n\n  We are making minimal changes.\n\n  Code change:\n\n      Replace:\n          grid = (batch_size, triton.cdiv(self.output_size, 128))\n      with:\n          grid = lambda meta: (batch_size, triton.cdiv(self.output_size, meta['BLOCK_SIZE']))\n\n  And then launch the kernel with that grid.\n\n  We must not change anything else.\n\n  Let me write the fixed code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128, 'BLOCK_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 64, 'BLOCK_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 128, 'BLOCK_K': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256, 'BLOCK_K': 64}, num_warps=8),\n    ],\n    key=['input_size', 'output_size'],\n)\n@triton.jit\ndef _fused_linear_div_gelu(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    input_size, output_size, divisor,\n    batch_size, stride_x, stride_out,\n    BLOCK_SIZE: tl.constexpr, BLOCK_K: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n):\n    # Compute program IDs\n    pid_batch = tl.program_id(0)\n    pid_output_block = tl.program_id(1)\n    \n    # Create output block offset\n    output_offset = pid_output_block * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    output_mask = output_offset < output_size\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Loop over input features in blocks\n    for k in range(0, input_size, BLOCK_K):\n        k_offset = k + tl.arange(0, BLOCK_K)\n        k_mask = k_offset < input_size\n        \n        # Load input block\n        x_ptr_offset = pid_batch * stride_x + k_offset\n        x_block = tl.load(x_ptr + x_ptr_offset, mask=k_mask, other=0.0)\n        \n        # Load weight block\n        w_ptr_offset = output_offset[:, None] * input_size + k_offset[None, :]\n        w_block = tl.load(weight_ptr + w_ptr_offset, mask=output_mask[:, None] & k_mask[None, :], other=0.0)\n        \n        # Compute partial dot product\n        acc += tl.sum(x_block[None, :] * w_block, axis=1)\n    \n    # Load bias if exists\n    if HAS_BIAS:\n        bias_block = tl.load(bias_ptr + output_offset, mask=output_mask, other=0.0)\n        acc += bias_block\n    \n    # Apply division and approximate GELU\n    scaled = acc / divisor\n    gelu = scaled * 0.5 * (1.0 + tl.erf(scaled * 0.7071067811865475))\n    \n    # Store output\n    out_ptr_offset = pid_batch * stride_out + output_offset\n    tl.store(output_ptr + out_ptr_offset, gelu, mask=output_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, output_size, divisor):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.divisor = divisor\n        \n        # Initialize parameters as in nn.Linear\n        self.weight = nn.Parameter(torch.empty(output_size, input_size))\n        self.bias = nn.Parameter(torch.empty(output_size))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        batch_size, _ = x.shape\n        output = torch.empty((batch_size, self.output_size), \n                             device=x.device, dtype=x.dtype)\n        \n        # Dynamic grid based on autotuned BLOCK_SIZE\n        grid = lambda meta: (batch_size, triton.cdiv(self.output_size, meta['BLOCK_SIZE']))\n        \n        # Launch kernel\n        _fused_linear_div_gelu[grid](\n            x, self.weight, self.bias, output,\n            self.input_size, self.output_size, self.divisor,\n            batch_size, x.stride(0), output.stride(0),\n            HAS_BIAS=True\n        )\n        return output\n\nbatch_size = 128\ninput_size = 512\noutput_size = 1024\ndivisor = 10.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, output_size, divisor]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "a2f47fb2-6ca0-4451-9be8-74fe918a7230", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef attention_kernel(\n    q_ptr, k_ptr, v_ptr, o_ptr,\n    stride_qb, stride_qh, stride_qt, stride_qd,\n    stride_kb, stride_kh, stride_kt, stride_kd,\n    stride_vb, stride_vh, stride_vt, stride_vd,\n    stride_ob, stride_oh, stride_ot, stride_od,\n    scale, seq_len,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr\n):\n    pid_b = tl.program_id(0)\n    pid_h = tl.program_id(1)\n    pid_m = tl.program_id(2)\n    \n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_D)\n    \n    q_ptrs = q_ptr + pid_b * stride_qb + pid_h * stride_qh + (offs_m[:, None] * stride_qt + offs_d[None, :] * stride_qd)\n    k_ptrs = k_ptr + pid_b * stride_kb + pid_h * stride_kh + (offs_n[None, :] * stride_kt + offs_d[:, None] * stride_kd)\n    v_ptrs = v_ptr + pid_b * stride_vb + pid_h * stride_vh + (offs_n[:, None] * stride_vt + offs_d[None, :] * stride_vd)\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_D), dtype=tl.float32)\n    for block_n in range(0, seq_len, BLOCK_N):\n        offs_n_curr = block_n + offs_n\n        \n        mask_n = offs_n_curr < seq_len\n        mask_m = offs_m < seq_len\n        mask_mn = (offs_m[:, None] >= offs_n_curr[None, :]) & (offs_m[:, None] < seq_len)\n        \n        # Simplified masks without redundant head dimension checks\n        q = tl.load(q_ptrs, mask=mask_m[:, None], other=0.0)\n        k = tl.load(k_ptrs + block_n * stride_kt, mask=mask_n[None, :], other=0.0)\n        \n        att = tl.dot(q, k) * scale\n        att = tl.where(mask_mn, att, float('-inf'))\n        att = tl.maximum(att, 0)\n        \n        v = tl.load(v_ptrs + block_n * stride_vt, mask=mask_n[:, None], other=0.0)\n        acc += tl.dot(att, v)\n    \n    # Simplified output mask\n    mask_o = offs_m[:, None] < seq_len\n    o_ptrs = o_ptr + pid_b * stride_ob + pid_h * stride_oh + (offs_m[:, None] * stride_ot + offs_d[None, :] * stride_od)\n    tl.store(o_ptrs, acc, mask=mask_o)\n\nclass ModelNew(nn.Module):\n    def __init__(self, n_embd, n_head, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n        self.max_seqlen = max_seqlen\n\n    def forward(self, x):\n        B, T, C = x.size()\n        hs = C // self.n_head\n        \n        qkv = self.c_attn(x)\n        q, k, v = qkv.split(self.n_embd, dim=2)\n        \n        q = q.view(B, T, self.n_head, hs).transpose(1, 2)\n        k = k.view(B, T, self.n_head, hs).transpose(1, 2)\n        v = v.view(B, T, self.n_head, hs).transpose(1, 2)\n        \n        scale = 1.0 / math.sqrt(hs)\n        output = torch.empty_like(v)\n        \n        # Use larger block sizes (128) to reduce iterations\n        grid = (B, self.n_head, triton.cdiv(T, 128))\n        attention_kernel[grid](\n            q, k, v, output,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            scale, T,\n            BLOCK_M=128, BLOCK_N=128, BLOCK_D=hs\n        )\n        \n        output = output.transpose(1, 2).contiguous().view(B, T, C)\n        return output\n\nbatch_size = 16\nmax_seqlen = 1024\nn_embd = 768\nn_head = 12\n\ndef get_inputs():\n    return [torch.randn(batch_size, max_seqlen, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, max_seqlen]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "4a0c9b43-e28d-4ad6-9fac-cd83366373f2", "generation": 3, "timestamp": 1754596968.4535122, "iteration_found": 18, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.102, "runtime_ms": 2.12, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.217, "speed_up": 0.102, "custom_timing": 2.12}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.119, "runtime_ms": 1.82, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.217, "speed_up": 0.119, "custom_timing": 1.82}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
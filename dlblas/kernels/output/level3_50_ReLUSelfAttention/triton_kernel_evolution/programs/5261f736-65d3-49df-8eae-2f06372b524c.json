{"id": "5261f736-65d3-49df-8eae-2f06372b524c", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport triton\nimport triton.language as tl\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(NewGELU, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\n@triton.jit\ndef attention_kernel(\n    q_ptr, k_ptr, v_ptr, output_ptr,\n    B, nh, T, hs,\n    scale,\n    stride_qb, stride_qh, stride_qt, stride_qd,\n    stride_kb, stride_kh, stride_kt, stride_kd,\n    stride_vb, stride_vh, stride_vt, stride_vd,\n    stride_ob, stride_oh, stride_ot, stride_od,\n    BLOCK_HS: tl.constexpr\n):\n    pid = tl.program_id(0)\n    batch_idx = pid // nh\n    head_idx = pid % nh\n\n    # Compute pointers for current batch and head\n    off_q = batch_idx * stride_qb + head_idx * stride_qh\n    off_k = batch_idx * stride_kb + head_idx * stride_kh\n    off_v = batch_idx * stride_vb + head_idx * stride_vh\n    off_o = batch_idx * stride_ob + head_idx * stride_oh\n\n    # Process each position in sequence\n    for i in range(T):\n        # Load query vector\n        q_i = tl.load(q_ptr + off_q + i * stride_qt + tl.arange(0, BLOCK_HS), mask=tl.arange(0, BLOCK_HS) < hs, other=0.0)\n        out_i = tl.zeros((BLOCK_HS,), dtype=tl.float32)\n        \n        # Process all positions up to i (causal attention)\n        for j in range(0, i + 1):\n            # Load key vector\n            k_j = tl.load(k_ptr + off_k + j * stride_kt + tl.arange(0, BLOCK_HS), mask=tl.arange(0, BLOCK_HS) < hs, other=0.0)\n            # Compute attention score\n            score = tl.sum(q_i * k_j) * scale\n            score = tl.maximum(score, 0.0)\n            \n            # Load value vector and accumulate\n            v_j = tl.load(v_ptr + off_v + j * stride_vt + tl.arange(0, BLOCK_HS), mask=tl.arange(0, BLOCK_HS) < hs, other=0.0)\n            out_i += score * v_j\n        \n        # Store output\n        tl.store(output_ptr + off_o + i * stride_ot + tl.arange(0, BLOCK_HS), out_i, mask=tl.arange(0, BLOCK_HS) < hs)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A multi-head masked self-attention layer with a projection at the end that uses ReLU instead of Softmax.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        hs = C // self.n_head\n\n        # Prepare output tensor\n        output = torch.empty_like(v)\n        scale = 1.0 / math.sqrt(hs)\n\n        # Define strides for tensor access\n        stride_qb, stride_qh, stride_qt, stride_qd = q.stride()\n        stride_kb, stride_kh, stride_kt, stride_kd = k.stride()\n        stride_vb, stride_vh, stride_vt, stride_vd = v.stride()\n        stride_ob, stride_oh, stride_ot, stride_od = output.stride()\n\n        # Launch kernel\n        grid = (B * self.n_head,)\n        attention_kernel[grid](\n            q, k, v, output,\n            B, self.n_head, T, hs,\n            scale,\n            stride_qb, stride_qh, stride_qt, stride_qd,\n            stride_kb, stride_kh, stride_kt, stride_kd,\n            stride_vb, stride_vh, stride_vt, stride_vd,\n            stride_ob, stride_oh, stride_ot, stride_od,\n            BLOCK_HS=triton.next_power_of_2(hs)\n        )\n\n        # Re-assemble all head outputs side by side\n        output = output.transpose(1, 2).contiguous().view(B, T, C)\n        return output\n\nbatch_size = 16\nmax_seqlen = 1024\nn_embd = 768  # Hidden dimension, typical for BERT-base size\nn_head = 12   # Number of attention heads, typical for BERT-base size\n\ndef get_inputs():\n    return [torch.randn(batch_size, max_seqlen, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, max_seqlen]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "5db0f343-99bb-4a6b-8e57-bf192e97550d", "generation": 1, "timestamp": 1754594386.9409177, "iteration_found": 3, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.001, "runtime_ms": 403.0, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.217, "speed_up": 0.001, "custom_timing": 403.0}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 5.04, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.217, "speed_up": 0.043, "custom_timing": 5.04}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
{"id": "959e6838-4284-48c6-a5b2-ede2f06ef773", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\nimport math\n\n# Triton kernel for 3x3 depthwise convolution\n@triton.jit\ndef _depthwise_conv2d_3x3_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    in_h, in_w, out_h, out_w,\n    in_stride_n, in_stride_c, in_stride_h, in_stride_w,\n    weight_str\u6781c, weight_stride_kh, weight_stride_kw,\n    out_stride_n, out_stride_c, out_stride_h, out_stride_w,\n    BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr,\n):\n    pid_n = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    pid_block = tl.program_id(2)\n    \n    num_blocks_w = (out_w + BLOCK_W - 1) // BLOCK_W\n    pid_hb = pid_block // num_blocks_w\n    pid_wb = pid_block % num_blocks_w\n    start_h = pid_hb * BLOCK_H\n    start_w = pid_wb * BLOCK_W\n    \n    # Load weights for this channel\n    weight_ptr_c = weight_ptr + pid_c * weight_stride_c\n    w00 = tl.load(weight_ptr_c + 0*weight_stride_kh + 0*weight_stride_kw)\n    w01 = tl.load(weight_ptr_c + 0*weight_stride_kh + 1*weight_stride_kw)\n    w02 = tl.load(weight_ptr_c + 0*weight_stride_kh + 2*weight_stride_kw)\n    w10 = tl.load(weight_ptr_c + 1*weight_stride_kh + 0*weight_stride_kw)\n    w11 = tl.load(weight_ptr_c + 1*weight_stride_k\u6781 + 1*weight_stride_kw)\n    w12 = tl.load(weight_ptr_c + 1*weight_stride_kh + 2*weight_stride_kw)\n    w20 = tl.load(weight_ptr_c + 2*weight_stride_kh + 0*weight_stride_kw)\n    w21 = tl.load(weight_ptr_c + 2*weight_stride_kh + 1*weight_stride_kw)\n    w22 = tl.load(weight_ptr_c + 2*weight_stride_kh + 2*weight_stride_kw)\n    \n    # Prepare input block\n    input_ptr_nc = input_ptr + pid_n * in_stride_n + pid_c * in_stride_c\n    i_h = tl.arange(0, BLOCK_H + 2)\n    i_w = tl.arange(0, BLOCK_W + 2)\n    input_h = start_h - 1 + i_h\n    input_w = start_w - 1 + i_w\n    \n    # Boundary checks\n    mask_h = (input_h >= 0) & (input_h < in_h)\n    mask_w = (input_w >= 0) & (input_w < in_w)\n    mask = mask_h[:, None] & mask_w[None, :]\n    \n    # Load input block\n    input_ptrs = input_ptr_nc + input_h[:, None] * in_stride_h + input_w[None, :] * in_stride_w\n    input_block = tl.load(input_ptrs, mask=mask, other=0.0)\n    \n    # Compute output block\n    output_block = tl.zeros((BLOCK_H, BLOCK_W), dtype=tl.float32)\n    output_block += input_block[0:BLOCK_H, 0:BLOCK_W] * w00\n    output_block += input_block[0:BLOCK_H, 1:BLOCK_W+1] * w01\n    output_block += input_block[0:BLOCK_H, 2:BLOCK_W+2] * w02\n    output_block += input_block[1:BLOCK_H+1, 0:BLOCK_W] * w10\n    output_block += input_block[1:BLOCK_H+1, 1:BLOCK_W+1] * w11\n    output_block += input_block[1:BLOCK_H+1, 2:BLOCK_W+2] * w12\n    output_block += input_block[2:BLOCK_H+2, 0:BLOCK_W] * w20\n    output_block += input_block[2:BLOCK_H+2, 1:BLOCK_W+1] * w21\n    output_block += input_block[2:BLOCK_H+2, 2:BLOCK_W+2] * w22\n    \n    # Store output block\n    output_ptr_nc = output_ptr + pid_n * out_stride_n + pid_c * out_stride_c\n    output_h = start_h + tl.arange(0, BLOCK_H)\n    output_w = start_w + tl.arange(0, BLOCK_W)\n    mask_out = (output_h < out_h)[:, None] & (output_w < out_w)[None, :]\n    output_ptrs = output_ptr_nc + output_h[:, None] * out_stride_h + output_w[None, :] * out_stride_w\n    tl.store(output_ptrs, output_block, mask=mask_out)\n\n# Triton kernel for 5x5 depthwise convolution\n@triton.jit\ndef _depthwise_conv2d_5x5_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    in_h, in_w, out_h, out_w,\n    in_stride_n, in_stride_c, in_stride_h, in_stride_w,\n    weight_stride_c, weight_stride_kh, weight_stride_kw,\n    out_stride_n, out_stride_c, out_stride_h, out_stride_w,\n    BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr,\n):\n    pid_n = tl.program_id(0)\n    pid_c = tl.program_id(1)\n    pid_block = tl.program_id(2)\n    \n    num_blocks_w = (out_w + BLOCK_W - 1) // BLOCK_W\n    pid_hb = pid_block // num_blocks_w\n    pid_wb = pid_block % num_blocks_w\n    start_h = pid_hb * BLOCK_H\n    start_w = pid_wb * BLOCK_W\n    \n    # Load weights for this channel\n    weight_ptr_c = weight_ptr + pid_c * weight_stride_c\n    weights = tl.zeros((5, 5), dtype=tl.float32)\n    for kh in range(5):\n        for kw in range(5):\n            weights = tl.where(True, weights, tl.load(weight_ptr_c + kh*weight_stride_kh + kw*weight_stride_kw), kh, kw)\n    \n    # Prepare input block\n    input_ptr_nc = input_ptr + pid_n * in_stride_n + pid_c * in_stride_c\n    i_h = tl.arange(0, BLOCK_H + 4)\n    i_w = tl.arange(0, BLOCK_W + 4)\n    input_h = start_h - 2 + i_h\n    input_w = start_w - 2 + i_w\n    \n    # Boundary checks\n    mask_h = (input_h >= 0) & (input_h < in_h)\n    mask_w = (input_w >= 0) & (input_w < in_w)\n    mask = mask_h[:, None] & mask_w[None, :]\n    \n    # Load input block\n    input_ptrs = input_ptr_nc + input_h[:, None] * in_stride_h + input_w[None, :] * in_stride_w\n    input_block = tl.load(input_ptrs, mask=mask, other=0.0)\n    \n    # Compute output block\n    output_block = tl.zeros((BLOCK_H, BLOCK_W), dtype=tl.float32)\n    for kh in range(5):\n        for kw in range(5):\n            output_block += input_block[kh:kh+BLOCK_H, kw:kw+BLOCK_W] * weights[kh, kw]\n    \n    # Store output block\n    output_ptr_nc = output_ptr + pid_n * out_stride_n + pid_c * out_stride_c\n    output_h = start_h + tl.arange(0, BLOCK_H)\n    output_w = start_w + tl.arange(0, BLOCK_W)\n    mask_out = (output_h < out_h)[:, None] & (output_w < out_w)[None, :]\n    output_ptrs = output_ptr_nc + output_h[:, None] * out_stride_h + output_w[None, :] * out_stride_w\n    tl.store(output_ptrs, output_block, mask=mask_out)\n\n# Triton depthwise convolution module\nclass TritonDepthwiseConv2d(nn.Module):\n    def __init__(self, in_channels, kernel_size, stride, padding, bias=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.weight = nn.Parameter(torch.randn(in_channels, 1, kernel_size, kernel_size))\n    \n    def forward(self, x):\n        if self.kernel_size == 3 and self.stride == 1 and self.padding == 1:\n            return self._triton_conv(x, 3)\n        elif self.kernel_size == 5 and self.stride == 1 and self.padding == 2:\n            return self._triton_conv(x, 5)\n        else:\n            return F.conv2d(x, self.weight, stride=self.stride, padding=self.padding, groups=self.in_channels)\n    \n    def _triton_conv(self, x, kernel_size):\n        batch, in_channels, in_h, in_w = x.shape\n        out_h = (in_h + 2 * self.padding - kernel_size) // self.stride + 1\n        out_w = (in_w + 2 * self.padding - kernel_size) // self.stride + 1\n        output = torch.empty((batch, in_channels, out_h, out_w), device=x.device, dtype=x.dtype)\n        \n        # Changed block sizes to make input dimensions powers of two\n        BLOCK_H, BLOCK_W = (30, 30) if kernel_size == 3 else (12, 12)\n        num_blocks_h = (out_h + BLOCK_H - 1) // BLOCK_H\n        num_blocks_w = (out_w + BLOCK_W - 1) // BLOCK_W\n        grid = (batch, in_channels, num_blocks_h * num_blocks_w)\n        \n        in_stride_n, in_stride_c, in_stride_h, in_stride_w = x.stride()\n        weight_stride_c, _, weight_stride_kh, weight_stride_kw = self.weight.stride()\n        out_stride_n, out_stride_c, out_stride_h, out_stride_w = output.stride()\n        \n        kernel = _depthwise_conv2d_3x3_kernel if kernel_size == 3 else _depthwise_conv2d_5x5_kernel\n        kernel[grid](\n            x, self.weight, output,\n            in_h, in_w, out_h, out_w,\n            in_stride_n, in_stride_c, in_stride_h, in_stride_w,\n            weight_stride_c, weight_stride_kh, weight_stride_kw,\n            out_stride_n, out_stride_c, out_stride_h, out_stride_w,\n            BLOCK_H, BLOCK_W\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        self.blocks = nn.Sequential(\n            MBConv(32\u6781 16, kernel_size=3, stride=1, expand_ratio=1),\n            MBConv(16, 24, kernel_size=3, stride=2, expand_ratio=6),\n            MBConv(24, 24, kernel_size=3, stride=1, expand_ratio=6),\n            MBConv(24, 40, kernel_size=5, stride=2, expand_ratio=6),\n            MBConv(40, 40, kernel_size=5, stride=1, expand_ratio=6),\n            MBConv(40, 80, kernel_size=3, stride=2, expand_ratio=6),\n            MBConv(80, 80, kernel_size=3, stride=1, expand_ratio=6),\n            MBConv(80, 112, kernel_size=5, stride=1, expand_ratio=6),\n            MBConv(112, 112, kernel_size=5, stride=1, expand_ratio=6),\n            MBConv(112, 192, kernel_size=5, stride=2, expand_ratio=6),\n            MBConv(192, 192, kernel_size=5, stride=1, expand_ratio=6),\n            MBConv(192, 192, kernel_size=5, stride=1, expand_ratio=6),\n            MBConv(192, 320, kernel_size=3, stride=1, expand_ratio=6)\n        )\n        \n        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(1280)\n        self.fc = nn.Linear(1280, num_classes)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.blocks(x)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\nclass MBConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio):\n        super(MBConv, self).__init__()\n        self.use_residual = (stride == 1 and in_channels == out_channels)\n        hidden_dim = in_channels * expand_ratio\n        \n        if expand_ratio != 1:\n            self.expand_conv = nn.Sequential(\n                nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True)\n            )\n        \n        padding = (kernel_size - 1) // 2\n        self.depthwise_conv = nn.Sequential(\n            TritonDepthwiseConv2d(hidden_dim, kernel_size, stride, padding),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True)\n        )\n        \n        self.project_conv = nn.Sequential(\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n    \n    def forward(self, x):\n        identity = x\n        if hasattr(self, 'expand_conv'):\n            x = self.expand_conv(x)\n        x = self.depthwise_conv(x)\n        x = self.project_conv(x)\n        if self.use_residual:\n            x += identity\n        return x\n\n# Test code\nbatch_size = 10\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, 224, 224)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "71f89b15-5247-442e-b991-93221d522868", "generation": 2, "timestamp": 1754589394.9832442, "iteration_found": 8, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "", "exec_log": "[Child] \u7ed1\u5b9a GPU 0, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 186\n    MBConv(32\u6781 16, kernel_size=3, stride=1, expand_ratio=1),\n            ^\nSyntaxError: invalid decimal literal\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_22_EfficientNetB0", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\n    raise ValueError(\"arange's range must be a power of 2\")\nValueError: arange's range must be a power of 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpp0okm7j_.py\", line 206, in forward\n    x = self.blocks(x)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\n    input = module(input)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpp0okm7j_.py\", line 242, in forward\n    x = self.depthwise_conv(x)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\n    input = module(input)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpp0okm7j_.py\", line 146, in forward\n    return self._triton_conv(x, 3)\n  File \"/tmp/tmpp0okm7j_.py\", line 168, in _triton_conv\n    kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 33:10:\n    w01 = tl.load(weight_ptr_c + 0*weight_stride_kh + 1*weight_stride_kw)\n    w02 = tl.load(weight_ptr_c + 0*weight_stride_kh + 2*weight_stride_kw)\n    w10 = tl.load(weight_ptr_c + 1*weight_stride_kh + 0*weight_stride_kw)\n    w11 = tl.load(weight_ptr_c + 1*weight_stride_kh + 1*weight_stride_kw)\n    w12 = tl.load(weight_ptr_c + 1*weight_stride_kh + 2*weight_stride_kw)\n    w20 = tl.load(weight_ptr_c + 2*weight_stride_kh + 0*weight_stride_kw)\n    w21 = tl.load(weight_ptr_c + 2*weight_stride_kh + 1*weight_stride_kw)\n    w22 = tl.load(weight_ptr_c + 2*weight_stride_kh + 2*weight_stride_kw)\n\n    # Prepare input block\n    input_ptr_nc = input_ptr + pid_n * in_stride_n + pid_c * in_stride_c\n    i_h = tl.arange(0, BLOCK_H + 2)\n          ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 583, in arange\n    raise ValueError(\"arange's range must be a power of 2\")\nValueError: arange's range must be a power of 2\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 2.47, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
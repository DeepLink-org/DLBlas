{"id": "22cd5800-1015-4a1c-8be4-6fe9ee863844", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_forward_kernel(\n    # Pointers\n    x_ptr,\n    gemm_weight_ptr,\n    gemm_bias_ptr,\n    gn_weight_ptr,\n    gn_bias_ptr,\n    mult_weight_ptr,\n    output_ptr,\n    # Shapes\n    batch_size,\n    in_features,\n    out_features,\n    num_groups,\n    # Meta-parameters\n    eps: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_group = tl.program_id(1)\n    \n    # Compute group boundaries\n    group_start = pid_group * GROUP_SIZE\n    group_range = tl.arange(0, GROUP_SIZE)\n    feature_mask = group_range < (out_features - group_start)\n    \n    # Initialize accumulator\n    acc = tl.zeros((GROUP_SIZE,), dtype=tl.float32)\n    \n    # Loop over input features (K dimension)\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        # Load input block\n        x_block = tl.load(\n            x_ptr + pid_batch * in_features + k_offsets,\n            mask=k_mask,\n            other=0.0\n        )\n        \n        # Load weight block\n        w_offsets = (group_start + group_range[:, None]) * in_features + k_offsets[None, :]\n        w_block = tl.load(\n            gemm_weight_ptr + w_offsets,\n            mask=k_mask[None, :] & feature_mask[:, None],\n            other=0.0\n        )\n        \n        # Compute partial dot product\n        acc += tl.sum(w_block * x_block[None, :], axis=1)\n    \n    # Add bias\n    bias = tl.load(\n        gemm_bias_ptr + group_start + group_range,\n        mask=feature_mask,\n        other=0.0\n    )\n    acc += bias\n    \n    # Compute GroupNorm statistics\n    mean = tl.sum(acc) / out_features\n    var = tl.sum((acc - mean) * (acc - mean)) / out_features\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Apply GroupNorm\n    gn_weight = tl.load(gn_weight_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    gn_bias = tl.load(gn_bias_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    normalized = (acc - mean) * rstd * gn_weight + gn_bias\n    \n    # First Swish\n    sig1 = tl.sigmoid(normalized)\n    swish1 = normalized * sig1\n    \n    # Multiply\n    mult_weight = tl.load(mult_weight_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    multiplied = swish1 * mult_weight\n    \n    # Second Swish\n    sig2 = tl.sigmoid(multiplied)\n    swish2 = multiplied * sig2\n    \n    # Store results\n    out_offsets = pid_batch * out_features + group_start + group_range\n    tl.store(output_ptr + out_offsets, swish2, mask=feature_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape))\n        self.out_features = out_features\n        self.num_groups = num_groups\n\n    def forward(self, x):\n        batch_size, _ = x.shape\n        group_size = self.out_features // self.num_groups\n        \n        # Ensure tensors are contiguous\n        x = x.contiguous()\n        output = torch.empty((batch_size, self.out_features), device=x.device, dtype=x.dtype)\n        \n        # Launch kernel\n        grid = (batch_size, self.num_groups)\n        _fused_forward_kernel[grid](\n            x, \n            self.gemm.weight, \n            self.gemm.bias,\n            self.group_norm.weight,\n            self.group_norm.bias,\n            self.multiply_weight,\n            output,\n            batch_size,\n            self.gemm.in_features,\n            self.out_features,\n            self.num_groups,\n            eps=1e-5,\n            BLOCK_SIZE_K=128,\n            GROUP_SIZE=group_size\n        )\n        return output\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 16\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "a4ce5475-6ebf-4466-97a3-d91c39af2c06", "generation": 1, "timestamp": 1754579822.3037481, "iteration_found": 2, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_88_Gemm_GroupNorm_Swish_Multiply_Swish", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "max_difference": ["25.952131", "21.490612", "24.801693", "25.884111", "30.754213"], "avg_difference": ["0.519188", "0.525699", "0.518453", "0.528023", "0.521632"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.109, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0976, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.109, "speed_up": 1.117, "custom_timing": 0.0976}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0976\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.109, 'speed_up': 1.117, 'custom_timing': 0.0976}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0976, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.109, 'speed_up': 1.117, 'custom_timing': 0.0976}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, GroupNorm, Swish, Multiply, and Swish operations.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape)) \n\n    def forward(self, x):\n        # (batch_size, in_features) -> (batch_size, out_features)\n        x = self.gemm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = self.group_norm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * self.multiply_weight\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 16\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0976), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.109, 'speed_up': 1.117, 'custom_timing': 0.0976})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, GroupNorm, Swish, Multiply, and Swish operations.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape)) \n\n    def forward(self, x):\n        # (batch_size, in_features) -> (batch_size, out_features)\n        x = self.gemm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = self.group_norm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * self.multiply_weight\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 16\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. GEMM (via nn.Linear)\n  2. GroupNorm\n  3. Swish (x * sigmoid(x))\n  4. Element-wise multiplication with a weight\n  5. Swish again\n\nThe goal is to rewrite the entire model (or at least the operations) in Triton to gain speed.\n\nHowever, note that the current implementation is entirely in PyTorch. We are to replace the forward pass with a Triton kernel (or kernels) that fuses these operations.\n\nConstraints:\n  - We cannot change the function signature of the forward method, the __init__ method, or the way the model is called.\n  - We must maintain the same inputs and outputs.\n\nApproach:\n  We will create a custom Triton kernel that fuses the entire forward pass.\n\nSteps in the kernel:\n  1. Load a block of the input matrix (for the GEMM part) and the weight matrix of the linear layer.\n  2. Compute the matrix multiplication (GEMM) for the block.\n  3. Apply GroupNorm: This requires computing mean and variance per group. Since we are processing a block, we may need to compute these statistics across the group dimension. However, note that the GroupNorm is applied on the output of GEMM which has shape (batch_size, out_features). The groups are along the out_features dimension? Actually, GroupNorm groups the channels (out_features) into `num_groups` groups. Each group has `out_features // num_groups` channels. The normalization is done per group and per sample (i.e., per batch element).\n\n  How GroupNorm works:\n      For each sample in the batch and for each group:\n          mean = mean of the features in that group\n          var = variance of the features in that group\n          normalized = (x - mean) / sqrt(var + eps) * weight + bias   [but note: the original model doesn't have weight and bias in GroupNorm? Actually, nn.GroupNorm has learnable affine parameters by default. But in the provided code, we are only initializing the GroupNorm with `num_groups` and `out_features`. The affine is enabled by default. However, the model does not capture the affine parameters? Actually, nn.GroupNorm will create weight and bias of size `out_features`. We must account for these.\n\n  But note: the current ModelNew uses nn.GroupNorm without specifying affine? Actually, by default, affine=True. So we have two additional parameters: group_norm.weight and group_norm.bias.\n\n  4. Then apply Swish: x * sigmoid(x)\n  5. Then multiply by a parameter vector (self.multiply_weight) of shape (out_features,)\n  6. Then apply Swish again.\n\nWe are going to fuse all these steps.\n\nChallenges:\n  - GroupNorm requires communication within a group. This is difficult to do in a single kernel because the groups span across the feature dimension and we are processing in blocks.\n\nHowever, note that the entire output of the GEMM is of shape (batch_size, out_features). We are going to process the batch in parallel (each batch element independently) and the features in groups.\n\nWe can design the kernel to process:\n  - Each program ID along the batch dimension (so batch element)\n  - And then within the batch element, we break the out_features into groups. Each group is handled by a set of threads.\n\nBut note: the Triton kernel will be 2D:\n  - grid = (batch_size, num_groups)\n\nAnd each block (or program in Triton) will handle one group in one batch element.\n\nInside the program, we will:\n  1. Load the entire group (which has `C = out_features // num_groups` elements) for the current batch element and group.\n  2. Then compute the mean and variance for the group.\n  3. Then normalize the group using the group norm weight and bias (which are of size out_features, but we only need the slice for this group).\n  4. Then apply the first swish.\n  5. Then multiply by the multiply_weight for each feature in the group.\n  6. Then apply the second swish.\n\nBut note: the GEMM part? We haven't done the GEMM.\n\nWe must also do the GEMM. The input is (batch_size, in_features) and the weight of the linear layer is (out_features, in_features). We are going to compute:\n\n  for each batch element i and output feature j:\n      out[i, j] = sum_{k} input[i, k] * weight[j, k]   [plus bias? The nn.Linear has bias by default. So we must also add the linear bias.]\n\nSo we have two learnable tensors for the linear layer: weight and bias.\n\nTherefore, the entire model has:\n  - Linear weight: (out_features, in_features)\n  - Linear bias: (out_features,)\n  - GroupNorm weight: (out_features,)\n  - GroupNorm bias: (out_features,)\n  - Multiply weight: (out_features,)\n\nWe are going to pass all these as parameters to the Triton kernel.\n\nHowever, note that the current model does not store the linear bias? Actually, nn.Linear has bias by default. So we must account for it.\n\nPlan for the kernel:\n\n  We will have a 2D grid:\n    pid_batch = tl.program_id(0)  [0..batch_size-1]\n    pid_group = tl.program_id(1)   [0..num_groups-1]\n\n  Each program will compute the entire group for one batch element.\n\n  Steps:\n    1. Compute the GEMM for the features in the current group. How?\n        The group corresponds to a set of output features: \n            group_start = pid_group * group_size\n            group_end = (pid_group+1) * group_size\n            where group_size = out_features // num_groups\n\n        We need to compute for each output feature j in [group_start, group_end) and for the current batch element pid_batch:\n            out[j] = linear_bias[j] + sum_{k=0}^{in_features-1} input[pid_batch, k] * linear_weight[j, k]\n\n        We can do this by loading a block of the input (the entire row for the batch element) and then blocks of the linear weight for the current group.\n\n        However, note: the entire input row is of size in_features. We can load it in chunks.\n\n        We can use a loop over blocks of the in_features dimension.\n\n        We'll use:\n          BLOCK_SIZE_K = block size for the K dimension (in_features)\n\n        We'll accumulate in a register for each output feature in the group.\n\n        We need to allocate a block of registers for the current group's output features (of size group_size). But note: group_size might be large. We can process the group in chunks? Actually, we are constrained by the number of threads we can use.\n\n        Alternatively, we can process multiple output features per program? Actually, we are already processing one group per program. The group has group_size features. We can have each thread within the program handle multiple features? But note: we are using Triton and we can use the SIMD-like operations.\n\n        We can do:\n\n          Let BLOCK_SIZE_F = the number of features we handle at once in the group. We can set BLOCK_SIZE_F to be the entire group? But if the group is too big, we might run out of registers.\n\n        Alternatively, we can break the group into smaller blocks of features? But then we cannot compute the mean and variance for the entire group until we have processed all blocks. This would require multiple passes? That is complex.\n\n        Therefore, we assume that the group_size is not too large (e.g., 32, 64, 128). We can set BLOCK_SIZE_F = group_size. Then we can compute the entire group in one program.\n\n        Steps for GEMM for the group:\n\n          Let group_size = out_features // num_groups\n          We'll precompute the start index of the group: group_start = pid_group * group_size\n\n          We'll create two pointers for the linear weight and bias for the group:\n            weight_ptr = linear_weight_ptr + group_start * in_features   # (group_size, in_features) block\n            bias_ptr = linear_bias_ptr + group_start   # (group_size,)\n\n          We'll accumulate the result for the group in a variable `acc` of size group_size.\n\n          We'll loop over K in blocks of size BLOCK_SIZE_K.\n\n          For each block k_start in [0, in_features) with step BLOCK_SIZE_K:\n            a) Load a block of the input row: [k_start : k_start+BLOCK_SIZE_K] -> shape (BLOCK_SIZE_K,)\n            b) Load a block of the weight matrix for the current group: \n                 weight_block = weight_ptr + [0:group_size, k_start:k_start+BLOCK_SIZE_K] -> shape (group_size, BLOCK_SIZE_K)\n            c) Compute the dot product: acc += dot(weight_block, input_block)\n\n          Then add the bias.\n\n    2. Then we have the entire group of activations (for the current batch element and group) in a register array of size group_size.\n\n    3. Compute the mean and variance of the group (for the current batch element and group).\n        mean = sum(activations) / group_size\n        variance = sum((activations - mean)^2) / group_size\n\n        But note: we are using float32 for the accumulations? The activations might be float32.\n\n    4. Normalize: \n        normalized = (activations - mean) / sqrt(variance + eps) * group_norm_weight[group_start:group_end] + group_norm_bias[group_start:group_end]\n\n        Here, we have to load the group_norm weight and bias for the group.\n\n    5. Apply first Swish: \n          swish1 = normalized * sigmoid(normalized)\n\n    6. Multiply by the multiply_weight for the group: \n          multiplied = swish1 * multiply_weight[group_start:group_end]\n\n    7. Apply second Swish:\n          swish2 = multiplied * sigmoid(multiplied)\n\n    8. Store the result to the output tensor at the current batch element and the group's feature indices.\n\nBut note: the entire output tensor is of shape (batch_size, out_features). We are writing a contiguous block for the group.\n\nHowever, the above approach requires that we have the entire group in registers. The group size might be 32, 64, 128, 256? We are limited by the number of registers and the block size.\n\nWe must set the group_size as a compile-time constant? Actually, we can pass it as a constant. But note: the group_size must be divisible by the number of groups? It is: group_size = out_features // num_groups.\n\nBut what if out_features is not divisible by num_groups? The nn.GroupNorm requires that. So we assume it is.\n\nWe are going to write the kernel with the following block dimensions:\n\n  - We will use a 1D block of threads? Actually, Triton uses block of threads in a hierarchical way. We can use:\n\n      pid = tl.program_id(0)  [for batch]\n      pid_group = tl.program_id(1) [for group]\n\n      And then we have a block of threads for the group? Actually, we don't need multiple threads per group because we are processing the entire group in a vectorized way. We can use:\n\n      Let's set the block size for the group to 1? Then we process the entire group with a single program? But note: the group might be large and we might run out of registers.\n\n  Alternatively, we can break the group into multiple programs? But then we cannot compute the mean and variance without communication.\n\n  We can use a two-stage reduction? That complicates.\n\n  We assume the group size is not too large (e.g., 32, 64, 128) and we can process it in one program. We will set the block size for the feature dimension to the group_size? But note: the maximum block size in Triton is 1024. So if group_size <= 1024, it's okay.\n\n  Therefore, we design:\n\n      grid = (batch_size, num_groups)\n      block = (group_size,)   [but note: we are not using CUDA blocks, so in Triton we use the concept of \"programs\" and then within a program we have a range of indices? Actually, we use the `tl.arange` and then we mask]\n\n  We can do:\n\n      group_start = pid_group * group_size\n      feature_idx = tl.arange(0, group_size)\n\n      Then we can compute the GEMM for each feature in the group in parallel.\n\n  Steps for GEMM in parallel for the group:\n\n      We break the in_features dimension into blocks of size BLOCK_SIZE_K.\n\n      We initialize an accumulator array for the group: \n          acc = tl.zeros((group_size,), dtype=tl.float32)\n\n      For k in range(0, in_features, BLOCK_SIZE_K):\n          k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n          mask_k = k_offsets < in_features\n\n          # Load input block: for the current batch element and the k block\n          input_block = tl.load(input_ptr + pid_batch * in_features + k_offsets, mask=mask_k, other=0.0)\n\n          # For each feature in the group, we load a row of weights (of length BLOCK_SIZE_K)\n          weight_block_ptr = weight_ptr + (group_start + feature_idx[:, None]) * in_features + k_offsets[None, :]\n          weight_block = tl.load(weight_block_ptr, mask=mask_k[None, :] & (feature_idx[:, None] < group_size), other=0.0)\n\n          # Dot product: acc += tl.sum(weight_block * input_block[None, :], axis=1)\n          # But note: we can do a dot product by reducing along the K dimension? Actually, we can do:\n          acc += tl.sum(weight_block * input_block, axis=1)\n\n      Then add bias: acc += tl.load(bias_ptr + group_start + feature_idx, mask=feature_idx<group_size, other=0.0)\n\n  Then we have the activations for the group.\n\n  Then we compute the mean and variance:\n\n      mean = tl.sum(acc) / group_size\n      # To compute variance: we need to subtract the mean and then square\n      diff = acc - mean\n      variance = tl.sum(diff * diff) / group_size\n\n      # Then normalize:\n      norm_factor = 1.0 / tl.sqrt(variance + eps)   # eps is a constant, e.g., 1e-5\n      normalized = diff * norm_factor\n\n      # Now load the group norm weight and bias for the group\n      gn_weight = tl.load(gn_weight_ptr + group_start + feature_idx, mask=feature_idx<group_size, other=0.0)\n      gn_bias = tl.load(gn_bias_ptr + group_start + feature_idx, mask=feature_idx<group_size, other=0.0)\n\n      normalized = normalized * gn_weight + gn_bias\n\n  Then apply first swish:\n\n      sig = 1.0 / (1.0 + tl.exp(-normalized))   # sigmoid\n      swish1 = normalized * sig\n\n  Then multiply by the multiply_weight:\n\n      mult_weight = tl.load(mult_weight_ptr + group_start + feature_idx, mask=feature_idx<group_size, other=0.0)\n      multiplied = swish1 * mult_weight\n\n  Then apply second swish:\n\n      sig2 = 1.0 / (1.0 + tl.exp(-multiplied))\n      swish2 = multiplied * sig2\n\n  Then store to output:\n\n      output_ptr = output_ptr + pid_batch * out_features + group_start + feature_idx\n      tl.store(output_ptr, swish2, mask=feature_idx<group_size)\n\nBut note: we must be cautious about the data types. We are using float32 throughout? The input might be float32, and the operations are in float32.\n\nAlso, note: the linear layer in PyTorch is done in float32? Yes.\n\nHowever, the Triton kernel must be efficient. The GEMM part is done with a loop over K and uses vector operations. This should be efficient if we choose a good BLOCK_SIZE_K.\n\nWe can set BLOCK_SIZE_K to 128 or 256? We can make it a power of two and a multiple of 16.\n\nBut note: the group_size might be large? We are using a block of size (group_size, BLOCK_SIZE_K) for the weight block. We are loading a block of size group_size * BLOCK_SIZE_K. This might exceed the shared memory? Actually, we are not using shared memory, we are using registers? And the number of registers might be limited.\n\nAlternatively, we can break the group_size dimension into chunks? We are already using a vector of length group_size for the accumulator. We are doing a reduction along the K dimension for each feature independently. This is acceptable if group_size is not too big.\n\nWe are going to set:\n\n  group_size = out_features // num_groups   [must be a compile-time constant? Actually, we can pass it as a constant to the kernel]\n\n  We also need to pass: in_features, out_features, num_groups, and the group_size.\n\nWe must also consider boundary conditions: if the in_features is not divisible by BLOCK_SIZE_K, we mask.\n\nWe must also consider that the group_size might not be divisible by the block we use for the feature dimension? Actually, we are setting the block for the feature dimension to the entire group. We mask the feature_idx that are beyond the group? Actually, we are using feature_idx in [0, group_size) and we mask the loads and stores with `feature_idx < group_size`. But note: the group_size might be less than the maximum group size? Actually, we are using the exact group_size. So we don't need to mask within the group? However, we are using `tl.arange(0, group_size)` and then we set the mask as `feature_idx < group_size` which is always true? So we can skip that mask? But what if the group_size is not a multiple of the block? Actually, we are setting the block to exactly group_size. So we don't need to mask the feature dimension? But note: the last group might be smaller? Actually, we assume out_features is divisible by num_groups.\n\n  Therefore, we require that out_features is divisible by num_groups. This is a requirement of GroupNorm.\n\nWe are going to write the kernel and then call it from the forward method.\n\nBut note: the current model has parameters. We must capture the parameters and pass them to the kernel.\n\nWe will change the forward method to call the Triton kernel instead of the sequential operations.\n\nWe must initialize the parameters in the same way? Actually, the parameters are already defined by the nn.Module. We are just changing the computation.\n\nSteps for the new ModelNew:\n\n  In __init__, we still define the same parameters, but we remove the nn.Linear and nn.GroupNorm? Actually, we keep them to have the parameters, but we don't use their forward methods. We will use the parameters in the Triton kernel.\n\n  However, note: the nn.Linear and nn.GroupNorm have parameters that we can access. We can keep the layers and then in the forward method, we extract the weight and bias of the linear layer and the weight and bias of the group norm, and then call the Triton kernel.\n\n  Alternatively, we can define the parameters directly without the nn.Linear and nn.GroupNorm? We can, but we must initialize them appropriately.\n\n  We choose to keep the nn.Linear and nn.GroupNorm for now because they handle parameter initialization and we can extract the parameters.\n\n  But note: the nn.GroupNorm has two parameters: weight and bias. We can access them.\n\n  However, the Triton kernel requires contiguous pointers. We will make sure the parameters are contiguous.\n\n  We'll write:\n\n      def forward(self, x):\n          # We are going to call the Triton kernel\n          # The kernel expects: input, linear_weight, linear_bias, group_norm_weight, group_norm_bias, multiply_weight\n          # We have:\n          #   linear_weight = self.gemm.weight  # (out_features, in_features)\n          #   linear_bias = self.gemm.bias       # (out_features,)\n          #   group_norm_weight = self.group_norm.weight  # (out_features,)\n          #   group_norm_bias = self.group_norm.bias      # (out_features,)\n          #   multiply_weight = self.multiply_weight      # (out_features,)\n\n          # We must check that the group_norm parameters are defined? Yes.\n\n          # We also need the output tensor: same shape as x after linear: (batch_size, out_features)\n\n          batch_size, _ = x.shape\n          out_features = self.gemm.out_features\n          num_groups = self.group_norm.num_groups\n          group_size = out_features // num_groups\n\n          # We require that out_features is divisible by num_groups -> already required by GroupNorm.\n\n          output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n\n          # We call the kernel with grid (batch_size, num_groups)\n          # We need to set the block for the group dimension? Actually, we are processing one group per program and the entire group with a vector of size group_size.\n\n          # We must define the kernel and the grid.\n\n          # We assume we have a kernel named `fused_forward_kernel`\n          # We pass all the pointers and the constants.\n\n          # Note: the kernel uses the same data type as x.\n\n          # We also need to set an epsilon for the GroupNorm? The nn.GroupNorm uses 1e-5 by default.\n\n          # Let's define the kernel.\n\n          # We are going to write the kernel in the same file? Actually, we are to output the entire block.\n\n          # We'll write the kernel as a static function.\n\n  But note: the entire code block must be self-contained.\n\nWe are going to restructure the code:\n\n  We'll define the ModelNew as before, but the forward method will be replaced by a call to the Triton kernel.\n\n  We must also define the Triton kernel.\n\n  We are going to write:\n\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def _fused_forward_kernel(\n          input_ptr, \n          linear_weight_ptr, linear_bias_ptr,\n          gn_weight_ptr, gn_bias_ptr,\n          mult_weight_ptr,\n          output_ptr,\n          in_features, out_features, num_groups, group_size,\n          batch_size,   # we pass batch_size for grid?\n          eps: tl.constexpr,\n          BLOCK_SIZE_K: tl.constexpr,\n      ):\n          # We have two-dimensional grid: (batch_size, num_groups)\n          pid_batch = tl.program_id(0)\n          pid_group = tl.program_id(1)\n\n          # We are going to process the entire group for the current batch element.\n\n          # The group starts at:\n          group_start = pid_group * group_size\n          feature_idx = tl.arange(0, group_size)   # from 0 to group_size-1\n\n          # We are going to accumulate the GEMM result for the group.\n          acc = tl.zeros((group_size,), dtype=tl.float32)\n\n          # Loop over K dimension (in_features) in blocks of BLOCK_SIZE_K\n          for k in range(0, in_features, BLOCK_SIZE_K):\n              k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n              # Mask for the input block: within in_features\n              mask_k = k_offsets < in_features\n\n              # Load input block for the current batch element\n              input_block = tl.load(input_ptr + pid_batch * in_features + k_offsets, mask=mask_k, other=0.0)\n\n              # Load weight block: for the current group and the current k block\n              weight_block_ptr = linear_weight_ptr + (group_start + feature_idx[:, None]) * in_features + k_offsets[None, :]\n              weight_block = tl.load(weight_block_ptr, mask=mask_k[None, :] & (feature_idx[:, None] < group_size), other=0.0)\n\n              # Compute the dot product and accumulate\n              # We do: for each feature in the group, dot(weight_block[feature, :], input_block)\n              # This is a vector of length group_size\n              acc += tl.sum(weight_block * input_block[None, :], axis=1)\n\n          # Load the bias for the group\n          bias_block = tl.load(linear_bias_ptr + group_start + feature_idx, mask=feature_idx<group_size, other=0.0)\n          acc += bias_block\n\n          # Now compute mean and variance for the group (for this batch element)\n          mean = tl.sum(acc) / group_size\n          # Compute variance: subtract mean, then square, then sum, then divide\n          diff = acc - mean\n          var = tl.sum(diff * diff) / group_size\n\n          # Normalize: (acc - mean) / sqrt(var + eps)\n          std_inv = 1.0 / tl.sqrt(var + eps)\n          normalized = diff * std_inv\n\n          # Apply GroupNorm affine transform: normalized * weight + bias\n          gn_weight = tl.load(gn_weight_ptr + group_start + feature_idx, mask=feature_idx<group_size, other=0.0)\n          gn_bias = tl.load(gn_bias_ptr + group_start + feature_idx, mask=feature_idx<group_size, other=0.0)\n          normalized = normalized * gn_weight + gn_bias\n\n          # First Swish: normalized * sigmoid(normalized)\n          sig1 = tl.sigmoid(normalized)   # This is an available function in Triton\n          swish1 = normalized * sig1\n\n          # Multiply by the multiply weight\n          mult_weight = tl.load(mult_weight_ptr + group_start + feature_idx, mask=feature_idx<group_size, other=0.0)\n          multiplied = swish1 * mult_weight\n\n          # Second Swish: multiplied * sigmoid(multiplied)\n          sig2 = tl.sigmoid(multiplied)\n          swish2 = multiplied * sig2\n\n          # Store to output\n          output_ptr_group = output_ptr + pid_batch * out_features + group_start\n          tl.store(output_ptr_group + feature_idx, swish2, mask=feature_idx<group_size)\n\n  Then in the forward method:\n\n      def forward(self, x):\n          # Check that x is contiguous\n          x = x.contiguous()\n          batch_size, _ = x.shape\n          out_features = self.gemm.out_features\n          num_groups = self.group_norm.num_groups\n          group_size = out_features // num_groups\n\n          # Create output tensor\n          output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n\n          # We must check that the parameters are contiguous\n          linear_weight = self.gemm.weight.contiguous()\n          linear_bias = self.gemm.bias.contiguous()\n          gn_weight = self.group_norm.weight.contiguous()\n          gn_bias = self.group_norm.bias.contiguous()\n          mult_weight = self.multiply_weight.contiguous()\n\n          # We set the grid: (batch_size, num_groups)\n          grid = (batch_size, num_groups)\n\n          # We set BLOCK_SIZE_K: a tunable parameter. We can set it to 128 for now.\n          BLOCK_SIZE_K = 128\n\n          # Epsilon for GroupNorm\n          eps = 1e-5\n\n          # Launch kernel\n          _fused_forward_kernel[grid](\n              x, linear_weight, linear_bias, gn_weight, gn_bias, mult_weight, output,\n              in_features, out_features, num_groups, group_size,\n              batch_size, eps, BLOCK_SIZE_K\n          )\n\n          return output\n\nBut note: the kernel uses tl.sigmoid? We must check if it's available. It is in triton.language.\n\nHowever, note: the kernel uses a fixed BLOCK_SIZE_K. We might want to autotune it? But the problem says we can change the block dimensions. We can make BLOCK_SIZE_K a constant expression and then autotune over it? But we are not doing that now.\n\nAlso note: the kernel might not be efficient if the group_size is too large? We are processing the entire group in one program. The group_size might be 1024? Then we are using a vector of 1024 elements. This might be too many registers? We might run into register spilling.\n\nWe assume the group_size is moderate (e.g., 32, 64, 128, 256). If it is 256, then we are using 256*3 = 768 registers per thread? Actually, we are not sure. We might need to reduce the group_size by breaking it into smaller blocks? But that would require multiple passes and communication between programs? That is complex.\n\nAlternatively, we can process a fixed block of features per program? Then we would need multiple programs per group? But then we cannot compute the mean and variance without a reduction across programs? That would require two kernels: one for the GEMM and then one for the GroupNorm? But then we break the fusion.\n\nWe are going to assume that the group_size is not too large. If it is too large, we might need to redesign.\n\nGiven the constraints of the problem, we output the above kernel.\n\nWe also note that the kernel does not use shared memory? It does everything in registers. This might be efficient because we are not using shared memory and the H100 has high bandwidth.\n\nBut note: the GEMM loop over K: we are loading a block of input and a block of weight for the group. The weight block is of size (group_size, BLOCK_SIZE_K). This might be large and we might run out of shared memory? Actually, we are not using shared memory, we are using registers? And the group_size * BLOCK_SIZE_K might be too big for registers.\n\nWe must avoid using too many registers. We can try to break the group into smaller blocks? We can do:\n\n  Instead of processing the entire group at once, we break the group into blocks of features? But then we cannot compute the mean and variance until we have processed the entire group.\n\n  We would have to store the intermediate results in global memory and then do a second pass? That would be inefficient.\n\nTherefore, we stick with the entire group in one program and hope that the group_size is not too big.\n\nWe also note that the kernel uses a lot of registers? We might need to set the num_warps appropriately? We can autotune the number of warps? But the problem says we can change the block dimensions. We are not doing that now.\n\nWe are going to output the code as described.\n\nBut note: the kernel might be slow if the group_size is 1? Then we are doing a lot of overhead per program. But if the group_size is 1, then we have many groups and the grid is (batch_size, out_features) which is large? Then it might be acceptable.\n\nWe are going to write the code.\n\nImportant: We are replacing the entire forward method. We remove the original operations.\n\nWe must also note: the original model had two swish operations and a multiplication. We are doing the same.\n\nWe also note: the multiply_weight is applied element-wise. We are doing that.\n\nLet's write the code.\n\nWe are going to define the kernel inside the ModelNew class? Actually, we can define it outside.\n\nWe are going to write the entire block.\n\nNote: the EVOLVE-BLOCK must contain the entire code for the model and the kernel.\n\nWe are going to write:\n\n  import triton\n  import triton.language as tl\n\n  @triton.jit\n  def ...   [the kernel]\n\n  class ModelNew(nn.Module):\n      ... [the __init__ remains the same]\n      def forward(self, x):\n          ... [as above]\n\nBut note: the kernel must be defined before the forward method.\n\nWe also note: the kernel uses tl.sigmoid, which is available in triton.language.\n\nWe are going to set the BLOCK_SIZE_K to 128. We can make it a constant.\n\nAlso note: the kernel uses the feature_idx for the entire group. We assume the group_size is known at compile time? Actually, we are passing it as an argument. We can use it to set the block? But note: the kernel does not use a block of threads? It uses a single program to process the entire group. The parallelism is achieved by the vector operations.\n\nWe hope that Triton can compile this.\n\nWe are going to output the entire block.\n\nLet's code accordingly.\n\nBut note: the problem says that the input is 250 kernelbench files. We are only given one example. We are to rewrite that one.\n\nWe are going to output the code for the entire block.\n\nWe must also note: the original model had an initialization of parameters. We are using the same parameters.\n\nWe are also going to set the batch_size, in_features, etc., as in the original.\n\nWe must also define the get_inputs and get_init_inputs functions? They are the same.\n\nWe are going to write the entire block.\n\nNote: the kernel must be defined in the same block.\n\nWe are going to write:\n\n  # ================== EVOLVE-BLOCK-START ==================\n  ... [the code]\n  # =================== EVOLVE-BLOCK-END ===================\n\nWe must be cautious: the kernel might be long.\n\nLet's write the code.\n\nNote: We are using the same module structure.\n\nWe are going to set the grid in the kernel launch to be (batch_size, num_groups). We assume batch_size and num_groups are within the grid limits.\n\nWe are also going to set the kernel to use the same data type as the input.\n\nWe are going to use tl.float32 for the accumulations? The input might be float16? The problem says the GPU has FP16 tensor cores. But note: the original model was in float32? We are not told. We are going to assume the input is float32. If it is float16, we might need to change the kernel to use float16? But the GroupNorm requires higher precision? We can do the accumulation in float32 and then convert to float16 for storage? Actually, the output should be the same as the original. The original model did everything in float32? We are using the same data types.\n\nWe are going to stick with float32 for the kernel.\n\nBut note: the kernel might be faster if we use float16? We are not using tensor cores. We are doing a simple GEMM without tensor cores. So float32 might be acceptable.\n\nAlternatively, we can make the kernel support both? We are not going to.\n\nWe write the kernel in float32.\n\nLet's code.\n</think>\nTo optimize the model, we'll rewrite the forward pass using Triton to fuse all operations into a single kernel. This eliminates intermediate memory accesses and leverages GPU parallelism. The key optimizations include:\n\n1. Fused GEMM, GroupNorm, and Swish operations\n2. Vectorized memory operations with efficient blocking\n3. Parallel computation across batch and feature dimensions\n4. Online computation of GroupNorm statistics\n5. Optimized memory access patterns\n\nHere's the complete implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_forward_kernel(\n    # Pointers\n    x_ptr,\n    gemm_weight_ptr,\n    gemm_bias_ptr,\n    gn_weight_ptr,\n    gn_bias_ptr,\n    mult_weight_ptr,\n    output_ptr,\n    # Shapes\n    batch_size,\n    in_features,\n    out_features,\n    num_groups,\n    # Meta-parameters\n    eps: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_group = tl.program_id(1)\n    \n    # Compute group boundaries\n    group_start = pid_group * GROUP_SIZE\n    group_range = tl.arange(0, GROUP_SIZE)\n    feature_mask = group_range < (out_features - group_start)\n    \n    # Initialize accumulator\n    acc = tl.zeros((GROUP_SIZE,), dtype=tl.float32)\n    \n    # Loop over input features (K dimension)\n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        # Load input block\n        x_block = tl.load(\n            x_ptr + pid_batch * in_features + k_offsets,\n            mask=k_mask,\n            other=0.0\n        )\n        \n        # Load weight block\n        w_offsets = (group_start + group_range[:, None]) * in_features + k_offsets[None, :]\n        w_block = tl.load(\n            gemm_weight_ptr + w_offsets,\n            mask=k_mask[None, :] & feature_mask[:, None],\n            other=0.0\n        )\n        \n        # Compute partial dot product\n        acc += tl.sum(w_block * x_block[None, :], axis=1)\n    \n    # Add bias\n    bias = tl.load(\n        gemm_bias_ptr + group_start + group_range,\n        mask=feature_mask,\n        other=0.0\n    )\n    acc += bias\n    \n    # Compute GroupNorm statistics\n    mean = tl.sum(acc) / out_features\n    var = tl.sum((acc - mean) * (acc - mean)) / out_features\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Apply GroupNorm\n    gn_weight = tl.load(gn_weight_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    gn_bias = tl.load(gn_bias_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    normalized = (acc - mean) * rstd * gn_weight + gn_bias\n    \n    # First Swish\n    sig1 = tl.sigmoid(normalized)\n    swish1 = normalized * sig1\n    \n    # Multiply\n    mult_weight = tl.load(mult_weight_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    multiplied = swish1 * mult_weight\n    \n    # Second Swish\n    sig2 = tl.sigmoid(multiplied)\n    swish2 = multiplied * sig2\n    \n    # Store results\n    out_offsets = pid_batch * out_features + group_start + group_range\n    tl.store(output_ptr + out_offsets, swish2, mask=feature_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape))\n        self.out_features = out_features\n        self.num_groups = num_groups\n\n    def forward(self, x):\n        batch_size, _ = x.shape\n        group_size = self.out_features // self.num_groups\n        \n        # Ensure tensors are contiguous\n        x = x.contiguous()\n        output = torch.empty((batch_size, self.out_features), device=x.device, dtype=x.dtype)\n        \n        # Launch kernel\n        grid = (batch_size, self.num_groups)\n        _fused_forward_kernel[grid](\n            x, \n            self.gemm.weight, \n            self.gemm.bias,\n            self.group_norm.weight,\n            self.group_norm.bias,\n            self.multiply_weight,\n            output,\n            batch_size,\n            self.gemm.in_features,\n            self.out_features,\n            self.num_groups,\n            eps=1e-5,\n            BLOCK_SIZE_K=128,\n            GROUP_SIZE=group_size\n        )\n        return output\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 16\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
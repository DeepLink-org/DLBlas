{"id": "95a18b77-37cf-4e89-9b82-13932dc3302b", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_K': 128}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_K': 256}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_K': 512}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_SIZE_K': 64}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_K': 128}, num_warps=8, num_stages=3),\n    ],\n    key=['in_features'],\n)\n@triton.jit\ndef _fused_forward_kernel(\n    # Pointers\n    x_ptr,\n    gemm_weight_ptr,\n    gemm_bias_ptr,\n    gn_weight_ptr,\n    gn_bias_ptr,\n    mult_weight_ptr,\n    output_ptr,\n    # Shapes\n    batch_size,\n    in_features,\n    out_features,\n    num_groups,\n    # Meta-parameters\n    eps: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_group = tl.program_id(1)\n    \n    # Compute group boundaries\n    group_start = pid_group * GROUP_SIZE\n    group_range = tl.arange(0, GROUP_SIZE)\n    feature_mask = group_range < (out_features - group_start)\n    \n    # Precompute weight row pointers for coalesced access\n    weight_row_start = group_start * in_features\n    row_ptrs = gemm_weight_ptr + weight_row_start + group_range[:, None] * in_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((GROUP_SIZE,), dtype=tl.float32)\n    x_ptrs = x_ptr + pid_batch * in_features\n    \n    # Loop over input features with vectorized loads\n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_offsets = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        # Vectorized load\n        x_block = tl.load(\n            x_ptrs + k_offsets,\n            mask=k_mask,\n            other=0.0\n        )\n        \n        # Load weight block using precomputed row pointers\n        w_ptrs = row_ptrs + k_offsets[None, :]\n        w_block = tl.load(\n            w_ptrs,\n            mask=k_mask[None, :] & feature_mask[:, None],\n            other=0.0\n        )\n        \n        # Efficient accumulation\n        acc += tl.sum(w_block * x_block[None, :], axis=1)\n    \n    # Add bias\n    bias_ptrs = gemm_bias_ptr + group_start + group_range\n    bias = tl.load(bias_ptrs, mask=feature_mask, other=0.0)\n    acc += bias\n    \n    # Compute GroupNorm statistics\n    group_count = tl.sum(tl.cast(feature_mask, tl.float32))\n    mean = tl.sum(acc) / group_count\n    diff = acc - mean\n    var = tl.sum(diff * diff) / group_count\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Apply GroupNorm with fused operations\n    gn_weight = tl.load(gn_weight_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    gn_bias = tl.load(gn_bias_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    normalized = (diff * rstd) * gn_weight + gn_bias\n    \n    # Fused Swish operations\n    swish1 = normalized * tl.sigmoid(normalized)\n    multiplied = swish1 * tl.load(mult_weight_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    swish2 = multiplied * tl.sigmoid(multiplied)\n    \n    # Store results\n    out_offsets = pid_batch * out_features + group_start + group_range\n    tl.store(output_ptr + out_offsets, swish2, mask=feature_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape))\n        self.out_features = out_features\n        self.num_groups = num_groups\n\n    def forward(self, x):\n        batch_size, _ = x.shape\n        group_size = self.out_features // self.num_groups\n        \n        # Ensure tensors are contiguous\n        x = x.contiguous()\n        output = torch.empty((batch_size, self.out_features), device=x.device, dtype=x.dtype)\n        \n        # Launch kernel with optimized parameters\n        grid = (batch_size, self.num_groups)\n        _fused_forward_kernel[grid](\n            x, \n            self.gemm.weight, \n            self.gemm.bias,\n            self.group_norm.weight,\n            self.group_norm.bias,\n            self.multiply_weight,\n            output,\n            batch_size,\n            self.gemm.in_features,\n            self.out_features,\n            self.num_groups,\n            eps=1e-5,\n            GROUP_SIZE=group_size\n        )\n        return output\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 16\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "aa981c4a-06b3-4e70-bd40-bdf74bfdfb0a", "generation": 3, "timestamp": 1754581530.8742309, "iteration_found": 7, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 31.20000000000001, "speed_up": 1.112, "runtime_ms": 0.098, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.109, "speed_up": 1.112, "custom_timing": 0.098}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 34.5, "speed_up": 1.145, "runtime_ms": 0.0952, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.109, "speed_up": 1.145, "custom_timing": 0.0952}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 1.0000\n- score: 34.5000\n- speed_up: 1.1450\n- runtime_ms: 0.0952\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.109, 'speed_up': 1.145, 'custom_timing': 0.0952}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 1.0000, score: 34.5000, speed_up: 1.1450, runtime_ms: 0.0952, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.109, 'speed_up': 1.145, 'custom_timing': 0.0952}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 34.5000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_forward_kernel(\n    # Pointers\n    x_ptr,\n    gemm_weight_ptr,\n    gemm_bias_ptr,\n    gn_weight_ptr,\n    gn_bias_ptr,\n    mult_weight_ptr,\n    output_ptr,\n    # Shapes\n    batch_size,\n    in_features,\n    out_features,\n    num_groups,\n    # Meta-parameters\n    eps: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_group = tl.program_id(1)\n    \n    # Compute group boundaries\n    group_start = pid_group * GROUP_SIZE\n    group_range = tl.arange(0, GROUP_SIZE)\n    feature_mask = group_range < (out_features - group_start)\n    \n    # Initialize accumulator\n    acc = tl.zeros((GROUP_SIZE,), dtype=tl.float32)\n    x_ptrs = x_ptr + pid_batch * in_features\n    \n    # Loop over input features with vectorized loads\n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_offsets = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        # Vectorized load\n        x_block = tl.load(\n            x_ptrs + k_offsets,\n            mask=k_mask,\n            other=0.0\n        )\n        \n        # Weight pointers with precomputation\n        w_start = (group_start + group_range[:, None]) * in_features\n        w_ptrs = gemm_weight_ptr + w_start + k_offsets[None, :]\n        w_block = tl.load(\n            w_ptrs,\n            mask=k_mask[None, :] & feature_mask[:, None],\n            other=0.0\n        )\n        \n        # Accumulate with efficient dot product\n        acc += tl.sum(w_block * x_block[None, :], axis=1)\n    \n    # Add bias\n    bias_ptrs = gemm_bias_ptr + group_start + group_range\n    bias = tl.load(bias_ptrs, mask=feature_mask, other=0.0)\n    acc += bias\n    \n    # Compute GroupNorm statistics (corrected group size)\n    group_count = tl.sum(tl.cast(feature_mask, tl.float32))\n    mean = tl.sum(acc) / group_count\n    diff = acc - mean\n    var = tl.sum(diff * diff) / group_count\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Apply GroupNorm with fused operations\n    gn_weight = tl.load(gn_weight_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    gn_bias = tl.load(gn_bias_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    normalized = (diff * rstd) * gn_weight + gn_bias\n    \n    # Fused Swish operations\n    sig1 = tl.sigmoid(normalized)\n    swish1 = normalized * sig1\n    \n    # Multiply\n    mult_weight = tl.load(mult_weight_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    multiplied = swish1 * mult_weight\n    \n    # Second Swish (fused)\n    sig2 = tl.sigmoid(multiplied)\n    swish2 = multiplied * sig2\n    \n    # Store results with coalesced writes\n    out_offsets = pid_batch * out_features + group_start + group_range\n    tl.store(output_ptr + out_offsets, swish2, mask=feature_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape))\n        self.out_features = out_features\n        self.num_groups = num_groups\n\n    def forward(self, x):\n        batch_size, _ = x.shape\n        group_size = self.out_features // self.num_groups\n        \n        # Ensure tensors are contiguous\n        x = x.contiguous()\n        output = torch.empty((batch_size, self.out_features), device=x.device, dtype=x.dtype)\n        \n        # Launch kernel with optimized parameters\n        grid = (batch_size, self.num_groups)\n        _fused_forward_kernel[grid](\n            x, \n            self.gemm.weight, \n            self.gemm.bias,\n            self.group_norm.weight,\n            self.group_norm.bias,\n            self.multiply_weight,\n            output,\n            batch_size,\n            self.gemm.in_features,\n            self.out_features,\n            self.num_groups,\n            eps=1e-5,\n            BLOCK_SIZE_K=256,  # Increased for better memory efficiency\n            GROUP_SIZE=group_size\n        )\n        return output\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 16\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (1.0000), Performs well on score (34.5000), Performs well on speed_up (1.1450), Performs well on runtime_ms (0.0952), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.109, 'speed_up': 1.145, 'custom_timing': 0.0952})\n\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fused_forward_kernel(\n    # Pointers\n    x_ptr,\n    gemm_weight_ptr,\n    gemm_bias_ptr,\n    gn_weight_ptr,\n    gn_bias_ptr,\n    mult_weight_ptr,\n    output_ptr,\n    # Shapes\n    batch_size,\n    in_features,\n    out_features,\n    num_groups,\n    # Meta-parameters\n    eps: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_group = tl.program_id(1)\n    \n    # Compute group boundaries\n    group_start = pid_group * GROUP_SIZE\n    group_range = tl.arange(0, GROUP_SIZE)\n    feature_mask = group_range < (out_features - group_start)\n    \n    # Initialize accumulator\n    acc = tl.zeros((GROUP_SIZE,), dtype=tl.float32)\n    x_ptrs = x_ptr + pid_batch * in_features\n    \n    # Loop over input features with vectorized loads\n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_offsets = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        # Vectorized load\n        x_block = tl.load(\n            x_ptrs + k_offsets,\n            mask=k_mask,\n            other=0.0\n        )\n        \n        # Weight pointers with precomputation\n        w_start = (group_start + group_range[:, None]) * in_features\n        w_ptrs = gemm_weight_ptr + w_start + k_offsets[None, :]\n        w_block = tl.load(\n            w_ptrs,\n            mask=k_mask[None, :] & feature_mask[:, None],\n            other=0.0\n        )\n        \n        # Accumulate with efficient dot product\n        acc += tl.sum(w_block * x_block[None, :], axis=1)\n    \n    # Add bias\n    bias_ptrs = gemm_bias_ptr + group_start + group_range\n    bias = tl.load(bias_ptrs, mask=feature_mask, other=0.0)\n    acc += bias\n    \n    # Compute GroupNorm statistics (corrected group size)\n    group_count = tl.sum(tl.cast(feature_mask, tl.float32))\n    mean = tl.sum(acc) / group_count\n    diff = acc - mean\n    var = tl.sum(diff * diff) / group_count\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Apply GroupNorm with fused operations\n    gn_weight = tl.load(gn_weight_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    gn_bias = tl.load(gn_bias_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    normalized = (diff * rstd) * gn_weight + gn_bias\n    \n    # Fused Swish operations\n    sig1 = tl.sigmoid(normalized)\n    swish1 = normalized * sig1\n    \n    # Multiply\n    mult_weight = tl.load(mult_weight_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    multiplied = swish1 * mult_weight\n    \n    # Second Swish (fused)\n    sig2 = tl.sigmoid(multiplied)\n    swish2 = multiplied * sig2\n    \n    # Store results with coalesced writes\n    out_offsets = pid_batch * out_features + group_start + group_range\n    tl.store(output_ptr + out_offsets, swish2, mask=feature_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape))\n        self.out_features = out_features\n        self.num_groups = num_groups\n\n    def forward(self, x):\n        batch_size, _ = x.shape\n        group_size = self.out_features // self.num_groups\n        \n        # Ensure tensors are contiguous\n        x = x.contiguous()\n        output = torch.empty((batch_size, self.out_features), device=x.device, dtype=x.dtype)\n        \n        # Launch kernel with optimized parameters\n        grid = (batch_size, self.num_groups)\n        _fused_forward_kernel[grid](\n            x, \n            self.gemm.weight, \n            self.gemm.bias,\n            self.group_norm.weight,\n            self.group_norm.bias,\n            self.multiply_weight,\n            output,\n            batch_size,\n            self.gemm.in_features,\n            self.out_features,\n            self.num_groups,\n            eps=1e-5,\n            BLOCK_SIZE_K=256,  # Increased for better memory efficiency\n            GROUP_SIZE=group_size\n        )\n        return output\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 16\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a Triton kernel that fuses a linear layer (GEMM), group normalization, and two Swish activations (with an element-wise multiplication in between).\n The current implementation achieves a speedup of 1.145 (14.5%) over the reference PyTorch code.\n\n Areas for improvement:\n   - The current kernel uses a loop over input features (K dimension) to compute the linear layer.\n   - We are using a BLOCK_SIZE_K of 256. We might try to adjust this for better memory efficiency and parallelism.\n   - The kernel is launched with grid = (batch_size, num_groups). We might consider using more parallelism in the feature dimension?\n   - The GroupNorm computation is done per group and per batch. We are currently using a scalar mean and variance for the entire group (within the program).\n   - We are doing two Swish operations (x * sigmoid(x)) and an element-wise multiplication in between.\n\n However, note the constraints:\n   - We cannot change the function signature or the grid configuration (i.e., the program_id dimensions: batch and group).\n\n We are allowed to:\n   - Adjust the BLOCK_SIZE_K and GROUP_SIZE (but note GROUP_SIZE is passed as a constant and is set to out_features//num_groups, which is fixed).\n   - We can try to optimize memory access patterns, especially for the weight matrix which is accessed in a block.\n\n Observation:\n   - The current weight loading: \n        w_ptrs = gemm_weight_ptr + w_start + k_offsets[None, :]\n        where w_start = (group_start + group_range[:, None]) * in_features\n        This results in a 2D block of size [GROUP_SIZE, BLOCK_SIZE_K] for the weights.\n\n   - The weight matrix is stored in the format [out_features, in_features]. We are loading a block that is contiguous in the in_features dimension but non-contiguous in the out_features dimension (because we are loading a group of rows that are GROUP_SIZE apart? Actually, they are contiguous in the group, but the entire weight matrix is stored by out_features).\n\n   - However, note that the group of rows we are loading (for a given group) are contiguous in memory? Actually, no: the group consists of `num_groups` groups, each of size `group_size`. The rows for a group are at indices: [group_start, group_start+1, ..., group_start+group_size-1]. These rows are consecutive in the weight matrix? Yes, because the weight matrix is of shape [out_features, in_features] and we are taking a contiguous block of `group_size` rows.\n\n   - Therefore, we can load the weight block as a contiguous block? Actually, the weight block we are loading is of shape [GROUP_SIZE, BLOCK_SIZE_K] and the rows are consecutive, so the entire block is contiguous if BLOCK_SIZE_K is the entire in_features? But we are tiling in K. So we are loading a contiguous block in the sense of a 2D array? The weights for a given row are contiguous, but when we have multiple rows, they are separated by `in_features` elements. Therefore, the block we are loading is not contiguous in memory because the rows are separated by `in_features` (the stride). We can try to use a more efficient memory access pattern?\n\n   - Alternatively, we can try to use the tensor cores? The current kernel does not use tensor cores because it is accumulating in float32 and the operations are not in a form that can be mapped to tensor cores.\n\n   - Since the H100 has tensor cores, we can try to restructure the GEMM part to use tensor cores. However, note that the GEMM in this kernel is a matrix-vector product (for each row in the group) but we are doing it for a group of rows. Actually, it's a batch of small matrix-vector products? But note that we have:\n        batch_size x num_groups groups, and each group has group_size rows.\n\n   - We can try to use the tensor cores for the GEMM by restructuring the kernel to use a block that is larger in the feature dimension? But note the grid is fixed to (batch_size, num_groups). We are already processing a group per program.\n\n   - Alternatively, we can try to use a different tiling that allows tensor core usage. However, the tensor cores require the inner product dimension to be at least 16 (for FP16). Our in_features is 512, which is divisible by 16. The group_size (which is out_features//num_groups) is 1024//16=64. So we have 64 output features per group.\n\n   - We can restructure the GEMM to use a block tiling that is 64x16 (for example) and then use tensor cores to compute a 64x16 block of the output? But note that we are computing a vector of length 64 (for the group) for one batch element. And we are tiling the input vector (length 512) by blocks of 16. This would require accumulating the partial sums for the 64x16 block? Actually, we are doing a matrix-vector product: the weight block is 64x512 and the input vector is 512. We are breaking the input vector into chunks of 16 and the weight matrix into 64x16 blocks.\n\n   - How to use tensor cores? We can use the `tl.dot` instruction. But note that `tl.dot` requires the operands to be in a specific layout. We can load the input block as a vector of 16 and the weight block as a 64x16 matrix and then do a matrix-vector product? Actually, the tensor cores can do matrix-matrix products, but here we are doing a matrix-vector product. We can treat the input as a matrix of size 16x1 and the weight as 64x16, then the output is 64x1.\n\n   - We can change the accumulation to use `tl.dot` for each block. The current code does:\n        acc += tl.sum(w_block * x_block[None, :], axis=1)\n     This is equivalent to a matrix-vector product: w_block (64x16) times x_block (16) -> output (64). We can replace this with:\n        acc += tl.dot(w_block, x_block[:, None])[:, 0]\n\n   - However, note that the `tl.dot` instruction might use tensor cores if the dimensions are aligned. But the problem is that the current code uses a loop over K and we are accumulating in float32. The `tl.dot` instruction can be used and it will use tensor cores if the data types and sizes are appropriate.\n\n   - But note: the current data type of the tensors? The kernel uses float32 for accumulation. The tensor cores on H100 can operate on FP16, BF16, TF32, FP64, and INT8. We are using FP32 for the entire kernel. We can try to use TF32 tensor cores? Triton might automatically use TF32 for `tl.dot` when the inputs are FP32? But note: TF32 requires the inputs to be in FP32 but with the same internal mantissa as BF16? And it is only available for matrix multiplications with specific sizes.\n\n   - Alternatively, we can change the kernel to use FP16? But the original model might be in FP32. We must check the reference code. The reference code uses FP32? The problem says the dataset contains PyTorch reference. Since we are not given the reference, we must maintain the same precision.\n\n   - The current kernel uses FP32 for the entire computation. We can try to use TF32 for the GEMM part. How? We can use the `tl.dot` instruction and let Triton use TF32 if available. We can set the environment variable? But we cannot change the environment. Alternatively, we can use the `allow_tf32` option in the kernel? Actually, Triton has a flag for that.\n\n   - However, note that the `tl.dot` instruction in Triton does not currently support TF32. It supports FP16 and BF16 for tensor cores. So we might need to change the input and weight to BF16? But that would change the precision.\n\n   - Alternatively, we can use FP16 and then convert back? But the reference is in FP32. We must maintain FP32 precision.\n\n   - Given the above, we might not be able to use tensor cores for the GEMM in this kernel without changing the precision. Therefore, we focus on memory access optimizations.\n\n Proposed optimizations:\n\n 1. Precomputation of pointers for the weight matrix? We are recalculating the entire w_ptrs in every iteration. We could precompute the base pointers for the weight rows for this group? But note: the weight matrix is stored in row-major, and we are loading a block of rows (for the group) and a block of columns (for the current K block). We are doing:\n        w_start = (group_start + group_range[:, None]) * in_features\n        w_ptrs = gemm_weight_ptr + w_start + k_offsets[None, :]\n\n     This creates a 2D array of pointers of size [GROUP_SIZE, BLOCK_SIZE_K]. We could precompute the row pointers for the group outside the K-loop? Then inside the loop, we only add the column offset.\n\n        row_ptrs = gemm_weight_ptr + (group_start + tl.arange(0, GROUP_SIZE))[:, None] * in_features\n        # Then in the loop:\n        w_ptrs = row_ptrs + k_offsets[None, :]\n\n     This avoids recalculating the row offsets every time.\n\n 2. We are currently loading the weight block as a 2D array. We could use a more efficient pattern by using vectorized loads? But Triton might already optimize that.\n\n 3. Another idea: we are accumulating the GEMM result in a register array of size GROUP_SIZE. We are using a loop over K. We could use tiling in the GROUP_SIZE dimension? But note that GROUP_SIZE is 64, which is small. We are already processing the entire group in one program.\n\n 4. We can try to adjust BLOCK_SIZE_K to be 128 or 64? The current value is 256. We might get better cache behavior with a smaller block? But we are constrained by the number of registers.\n\n 5. The GroupNorm computation: we are computing the mean and variance for the entire group. We are using:\n        group_count = tl.sum(tl.cast(feature_mask, tl.float32))\n        mean = tl.sum(acc) / group_count\n        diff = acc - mean\n        var = tl.sum(diff * diff) / group_count\n\n     This is efficient because the group is only 64 elements.\n\n 6. The Swish operations: we are doing two Swish operations. We can try to fuse them more? But they are separated by an element-wise multiplication.\n\n 7. We can try to use the following optimization for Swish: \n        swish = x * sigmoid(x)\n     There is no known algebraic fusion for two consecutive Swish operations.\n\n 8. We are doing two passes over the data for the GroupNorm: one for the mean, then for the variance, and then again for the normalization. But again, the group is small (64) so this is done in registers.\n\n 9. We are storing the output with a coalesced write? The output is stored in contiguous memory for the entire batch and features. We are writing a contiguous block of GROUP_SIZE for a given batch and group. This is coalesced.\n\n 10. Another idea: use shared memory for the input block? We are loading the same input block for all the rows in the group. Currently, we are loading the input block once per iteration of k and then broadcasting it to the weight rows. This is efficient.\n\n 11. We can try to increase the GROUP_SIZE? But note that GROUP_SIZE is fixed by the model (out_features//num_groups). We cannot change it.\n\n 12. We can try to change the BLOCK_SIZE_K to be a multiple of 16 to align with memory? Currently, it's 256, which is a multiple of 16. So it's aligned.\n\n 13. We can try to use the same kernel but with different tuning parameters? The current kernel is not autotuned. We are setting BLOCK_SIZE_K=256. We could make it autotuned? But note the constraint: we cannot change the grid configuration. However, we can change the BLOCK_SIZE_K and GROUP_SIZE (but GROUP_SIZE is fixed by the model). Actually, we are passing GROUP_SIZE as a constant (which is group_size = out_features//num_groups). So we can only tune BLOCK_SIZE_K.\n\n 14. Let's add autotuning for BLOCK_SIZE_K? We can define a set of possible values and let Triton choose the best.\n\n However, note that the kernel does not currently use the autotuning decorator. We can add it.\n\n Steps for autotuning:\n\n   - We will define a set of BLOCK_SIZE_K values to try (e.g., 64, 128, 256, 512). But note that in_features is 512, so 512 would be the entire K dimension (so no loop).\n   - We will also tune the number of warps and stages.\n\n But note: the kernel already has a fixed grid. We cannot change the grid.\n\n We can do:\n\n    configs = [\n        triton.Config({'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_K': 128}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_K': 256}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_K': 512}, num_warps=4, num_stages=3),\n        # Also try different number of warps and stages?\n    ]\n\n    @triton.autotune(configs=configs, key=['in_features'])\n    @triton.jit\n    def _fused_forward_kernel(...):\n        ...\n\n 15. We must also precompute the pointers for the weight rows outside the K-loop to reduce overhead.\n\n Let's implement:\n\n   a) Precompute the row pointers for the weight matrix for the entire group.\n   b) Add autotuning for BLOCK_SIZE_K and also for the number of warps and stages.\n\n However, note the constraint: we are not allowed to change the grid. The autotuning for the number of warps and stages is allowed.\n\n We will do:\n\n   - Precomputation of row_ptrs for the weight matrix for the group.\n   - Replace the w_ptrs calculation inside the loop to use the precomputed row_ptrs.\n\n 16. Also, note that the current kernel uses a fixed BLOCK_SIZE_K=256. We are replacing that with a configurable one.\n\n 17. We must be cautious with the mask for the weight block: we are using `k_mask[None, :] & feature_mask[:, None]`. The precomputation of row_ptrs does not change the mask.\n\n 18. We will also try to use a different number of warps and stages to better utilize the hardware.\n\n Given the above, we rewrite the kernel as follows:\n\n   - Add the autotune decorator with a list of configs.\n   - Precompute the row pointers for the weight rows of the group.\n   - Change the w_ptrs in the loop to use the precomputed row_ptrs.\n\n 19. We also note that the current kernel uses a single program for the entire group. The group size is 64, which is the number of rows we are processing. We are using a 1D array of size GROUP_SIZE for the accumulator. This should be efficient.\n\n 20. We will also try to adjust the number of warps: the current kernel uses how many warps? We are not setting it. We are setting it in the autotune configs.\n\n 21. We must also pass the BLOCK_SIZE_K as a config parameter and remove it from the fixed argument in the kernel call.\n\n Therefore, in the forward function, we will remove the fixed BLOCK_SIZE_K and let the autotuner choose.\n\n 22. We must also change the kernel call to not pass BLOCK_SIZE_K as a constant? Actually, we pass it as a meta-parameter. We will leave it as a meta-parameter and the autotuner will set it.\n\n 23. We also note that the kernel uses GROUP_SIZE as a constant. We are passing it from the model. We will leave that unchanged.\n\n 24. We must also consider that the input might not be divisible by BLOCK_SIZE_K? The current code handles it with a mask.\n\n 25. We will generate several configs for BLOCK_SIZE_K and also for the number of warps and stages.\n\n Since the current kernel uses 256, we will include that and also try 128 and 64 and 512.\n\n We will also try different numbers of warps (4, 8) and stages (1, 2, 4).\n\n Let's write the autotune configs:\n\n    configs = [\n        triton.Config({'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=1),\n        triton.Config({'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_K': 64}, num_warps=8, num_stages=1),\n        triton.Config({'BLOCK_SIZE_K': 64}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_SIZE_K': 64}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_SIZE_K': 128}, num_warps=4, num_stages=1),\n        triton.Config({'BLOCK_SIZE_K': 128}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_SIZE_K': 128}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_K': 128}, num_warps=8, num_stages=1),\n        triton.Config({'BLOCK_SIZE_K': 128}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_SIZE_K': 128}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_SIZE_K': 256}, num_warps=4, num_stages=1),\n        triton.Config({'BLOCK_SIZE_K': 256}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_SIZE_K': 256}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_K': 256}, num_warps=8, num_stages=1),\n        triton.Config({'BLOCK_SIZE_K': 256}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_SIZE_K': 256}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_SIZE_K': 512}, num_warps=4, num_stages=1),\n        triton.Config({'BLOCK_SIZE_K': 512}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_SIZE_K': 512}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_K': 512}, num_warps=8, num_stages=1),\n        triton.Config({'BLOCK_SIZE_K': 512}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_SIZE_K': 512}, num_warps=8, num_stages=4),\n    ]\n\n 26. We must also change the kernel to use the precomputed row pointers.\n\n Implementation:\n\n    Precompute:\n        # Precompute the row pointers for the weight matrix for the entire group\n        row_start = group_start * in_features  # starting index of the first row in the group\n        # We want to get the pointers for each row in the group: row_start + row_idx * in_features, for row_idx in [0, GROUP_SIZE-1]\n        # But note: the weight matrix is stored in row-major, so the rows are consecutive? Actually, the rows for the group are consecutive in the weight matrix? Yes.\n        # Therefore, we can compute the base pointer for the group and then add the row index multiplied by in_features? Actually, the base pointer for the group is gemm_weight_ptr + row_start.\n        # Then the row pointer for row i in the group is base_ptr + i * in_features.\n\n        # However, we can precompute an array of row pointers for the group:\n        row_idx = tl.arange(0, GROUP_SIZE)\n        # We need to mask row_idx that are beyond the actual group size? We have feature_mask for that.\n        row_ptrs = gemm_weight_ptr + row_start + row_idx * in_features\n\n    Then in the loop:\n\n        # Instead of:\n        #   w_start = (group_start + group_range[:, None]) * in_features\n        #   w_ptrs = gemm_weight_ptr + w_start + k_offsets[None, :]\n        # We do:\n        w_ptrs = row_ptrs[:, None] + k_offsets[None, :]\n\n    The mask remains the same.\n\n 27. We must also note that the row_ptrs is a 1D array of length GROUP_SIZE. We are then adding a 1D array of k_offsets (length BLOCK_SIZE_K) to create a 2D array of pointers. This is the same as before.\n\n 28. This precomputation avoids recalculating the row_start for every row and every k block.\n\n 29. We also note that the current kernel does not precompute the row pointers for the output and the other parameters? They are only loaded once per kernel. So it's not a big issue.\n\n Let's write the new kernel with autotuning and precomputed row pointers.\n\n 30. We must also remove the hard-coded BLOCK_SIZE_K from the forward function. Instead, we will pass it as a variable that is set by the autotuner.\n\n 31. We leave the GROUP_SIZE as passed from the model.\n\n 32. We also note that the kernel must not change the function signature? We are only changing the implementation inside and adding an autotuner. The parameters remain the same.\n\n 33. We will add the autotune decorator above the kernel.\n\n 34. We must be cautious about the number of registers used. With more warps, we use fewer registers per thread? But we have a large GROUP_SIZE (64) and we are storing a 64-element accumulator and other arrays. We might run into register pressure. Therefore, we try to use more warps? The current kernel uses 8 warps? We don't know because we haven't set it. We are setting it in the autotuner.\n\n 35. We will run the kernel with the autotuner and hope that it picks the best config.\n\n 36. We also note that the current kernel does not use any shared memory. We are using registers for the accumulator and the intermediate arrays.\n\n Given the above, we rewrite the kernel.\n\n IMPORTANT: We are only allowed to change the Triton kernel source within the EVOLVE-BLOCK. We must not change the class ModelNew or the get_inputs, get_init_inputs.\n\n However, note that the kernel call in the forward function currently passes BLOCK_SIZE_K=256. We must remove that and let the autotuner set it. So we remove the argument and let the kernel use the autotuner's choice.\n\n But wait: the autotuner will set BLOCK_SIZE_K as a meta-parameter. We don't need to pass it from the outside? Actually, we do not set it in the kernel call. The autotuner will override the value we pass? No, we are passing it as a meta-parameter. We must remove it from the kernel call? Actually, we can leave it because the autotuner will override it. But it's better to remove it and let the autotuner choose.\n\n Therefore, in the forward function, we remove the BLOCK_SIZE_K argument and the GROUP_SIZE argument? But note: GROUP_SIZE is fixed by the model. We must pass it. We leave GROUP_SIZE.\n\n So we change the kernel call to:\n\n    _fused_forward_kernel[grid](\n        x, \n        self.gemm.weight, \n        self.gemm.bias,\n        self.group_norm.weight,\n        self.group_norm.bias,\n        self.multiply_weight,\n        output,\n        batch_size,\n        self.gemm.in_features,\n        self.out_features,\n        self.num_groups,\n        eps=1e-5,\n        GROUP_SIZE=group_size\n    )\n\n And the BLOCK_SIZE_K is set by the autotuner.\n\n 37. We also note that the autotuner requires the key to be the arguments that are invariant? We set key=['in_features'] because the in_features is fixed. But note that the kernel is called with different in_features? Actually, the model has fixed in_features. So it's safe.\n\n 38. We must also note that the autotuner will cache the best config for the given in_features. Since in_features is fixed for the model, we are safe.\n\n 39. We also change the kernel to precompute the row pointers.\n\n Let's write the code.\n\n Note: We are making minimal changes (less than 20 lines) as per the tip? Actually, we are changing more than 20 lines? We are adding the autotuner and changing the pointer calculation. But we are allowed to make small iterative steps. We hope this will be acceptable.\n\n However, the problem says: \"Small iterative steps: Modify \u226420 lines per revision\". We might need to break the change? But the instructions say we can do multiple revisions. We are only doing one revision.\n\n Alternatively, we can try without autotuning? But we believe autotuning is necessary for significant speedup.\n\n Since we are only allowed one revision, we do both the autotuning and the precomputation.\n\n We hope the autotuning and precomputation will give a speedup.\n\n Let's write the new code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_K': 64}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_K': 128}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_K': 256}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_K': 512}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_SIZE_K': 64}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_K': 128}, num_warps=8, num_stages=3),\n    ],\n    key=['in_features'],\n)\n@triton.jit\ndef _fused_forward_kernel(\n    # Pointers\n    x_ptr,\n    gemm_weight_ptr,\n    gemm_bias_ptr,\n    gn_weight_ptr,\n    gn_bias_ptr,\n    mult_weight_ptr,\n    output_ptr,\n    # Shapes\n    batch_size,\n    in_features,\n    out_features,\n    num_groups,\n    # Meta-parameters\n    eps: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n):\n    pid_batch = tl.program_id(0)\n    pid_group = tl.program_id(1)\n    \n    # Compute group boundaries\n    group_start = pid_group * GROUP_SIZE\n    group_range = tl.arange(0, GROUP_SIZE)\n    feature_mask = group_range < (out_features - group_start)\n    \n    # Precompute weight row pointers for coalesced access\n    weight_row_start = group_start * in_features\n    row_ptrs = gemm_weight_ptr + weight_row_start + group_range[:, None] * in_features\n    \n    # Initialize accumulator\n    acc = tl.zeros((GROUP_SIZE,), dtype=tl.float32)\n    x_ptrs = x_ptr + pid_batch * in_features\n    \n    # Loop over input features with vectorized loads\n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_offsets = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        # Vectorized load\n        x_block = tl.load(\n            x_ptrs + k_offsets,\n            mask=k_mask,\n            other=0.0\n        )\n        \n        # Load weight block using precomputed row pointers\n        w_ptrs = row_ptrs + k_offsets[None, :]\n        w_block = tl.load(\n            w_ptrs,\n            mask=k_mask[None, :] & feature_mask[:, None],\n            other=0.0\n        )\n        \n        # Efficient accumulation\n        acc += tl.sum(w_block * x_block[None, :], axis=1)\n    \n    # Add bias\n    bias_ptrs = gemm_bias_ptr + group_start + group_range\n    bias = tl.load(bias_ptrs, mask=feature_mask, other=0.0)\n    acc += bias\n    \n    # Compute GroupNorm statistics\n    group_count = tl.sum(tl.cast(feature_mask, tl.float32))\n    mean = tl.sum(acc) / group_count\n    diff = acc - mean\n    var = tl.sum(diff * diff) / group_count\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Apply GroupNorm with fused operations\n    gn_weight = tl.load(gn_weight_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    gn_bias = tl.load(gn_bias_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    normalized = (diff * rstd) * gn_weight + gn_bias\n    \n    # Fused Swish operations\n    swish1 = normalized * tl.sigmoid(normalized)\n    multiplied = swish1 * tl.load(mult_weight_ptr + group_start + group_range, mask=feature_mask, other=0.0)\n    swish2 = multiplied * tl.sigmoid(multiplied)\n    \n    # Store results\n    out_offsets = pid_batch * out_features + group_start + group_range\n    tl.store(output_ptr + out_offsets, swish2, mask=feature_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape))\n        self.out_features = out_features\n        self.num_groups = num_groups\n\n    def forward(self, x):\n        batch_size, _ = x.shape\n        group_size = self.out_features // self.num_groups\n        \n        # Ensure tensors are contiguous\n        x = x.contiguous()\n        output = torch.empty((batch_size, self.out_features), device=x.device, dtype=x.dtype)\n        \n        # Launch kernel with optimized parameters\n        grid = (batch_size, self.num_groups)\n        _fused_forward_kernel[grid](\n            x, \n            self.gemm.weight, \n            self.gemm.bias,\n            self.group_norm.weight,\n            self.group_norm.bias,\n            self.multiply_weight,\n            output,\n            batch_size,\n            self.gemm.in_features,\n            self.out_features,\n            self.num_groups,\n            eps=1e-5,\n            GROUP_SIZE=group_size\n        )\n        return output\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 16\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "d6c94d0c-f3ce-4328-8274-077ece7eed39", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n# Triton kernel for optimized GEMM operation\n@triton.jit\ndef _gemm_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    \n    pid_m = pid // grid_n\n    pid_n = pid % grid_n\n    \n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        mask_a = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n        mask_b = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n        \n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    \n    mask_c = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask_c)\n\n# Optimized Linear module using Triton\nclass TritonLinear(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        output = torch.empty(\n            x.size(0), \n            self.out_features, \n            device=x.device, \n            dtype=x.dtype\n        )\n        \n        grid = lambda M, N: (triton.cdiv(M, 64) * triton.cdiv(N, 64),)\n        \n        _gemm_kernel[grid(x.size(0), self.out_features)](\n            x, self.weight, output,\n            x.size(0), self.out_features, self.in_features,\n            x.stride(0), x.stride(1),\n            self.weight.stride(1), self.weight.stride(0),  # Fixed weight strides\n            output.stride(0), output.stride(1),\n            BLOCK_M=64, BLOCK_N=64, BLOCK_K=32\n        )\n        \n        output += self.bias\n        return output\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, GroupNorm, Swish, Multiply, and Swish operations.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = TritonLinear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape)) \n\n    def forward(self, x):\n        # (batch_size, in_features) -> (batch_size, out_features)\n        x = self.gemm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = self.group_norm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * self.multiply_weight\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 16\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "024925b5-0a07-40e5-ac4c-69b15506011d", "generation": 2, "timestamp": 1754580606.7081316, "iteration_found": 4, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.79, "runtime_ms": 0.138, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.109, "speed_up": 0.79, "custom_timing": 0.138}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_88_Gemm_GroupNorm_Swish_Multiply_Swish", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "max_difference": ["7.297653", "7.304657", "9.711642", "8.998066", "9.947114"], "avg_difference": ["0.221078", "0.221182", "0.219314", "0.222279", "0.219303"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.109, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 0.0000\n- score: 10.0000\n- stage: wrong_answer\n- verify_payload: {'id': 'level2_88_Gemm_GroupNorm_Swish_Multiply_Swish', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'max_difference': ['7.297653', '7.304657', '9.711642', '8.998066', '9.947114'], 'avg_difference': ['0.221078', '0.221182', '0.219314', '0.222279', '0.219303'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.109, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0976, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.109, 'speed_up': 1.117, 'custom_timing': 0.0976}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_88_Gemm_GroupNorm_Swish_Multiply_Swish', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'max_difference': ['7.297653', '7.304657', '9.711642', '8.998066', '9.947114'], 'avg_difference': ['0.221078', '0.221182', '0.219314', '0.222279', '0.219303'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.109, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n# Triton kernel for optimized GEMM operation\n@triton.jit\ndef _gemm_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    \n    pid_m = pid // grid_n\n    pid_n = pid % grid_n\n    \n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        mask_a = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n        mask_b = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n        \n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    \n    mask_c = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask_c)\n\n# Optimized Linear module using Triton\nclass TritonLinear(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        output = torch.empty(\n            x.size(0), \n            self.out_features, \n            device=x.device, \n            dtype=x.dtype\n        )\n        \n        grid = lambda M, N: (triton.cdiv(M, 64) * triton.cdiv(N, 64),)\n        \n        _gemm_kernel[grid(x.size(0), self.out_features)](\n            x, self.weight, output,\n            x.size(0), self.out_features, self.in_features,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_M=64, BLOCK_N=64, BLOCK_K=32\n        )\n        \n        output += self.bias\n        return output\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, GroupNorm, Swish, Multiply, and Swish operations.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = TritonLinear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape)) \n\n    def forward(self, x):\n        # (batch_size, in_features) -> (batch_size, out_features)\n        x = self.gemm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = self.group_norm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * self.multiply_weight\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 16\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_88_Gemm_GroupNorm_Swish_Multiply_Swish', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'max_difference': ['7.297653', '7.304657', '9.711642', '8.998066', '9.947114'], 'avg_difference': ['0.221078', '0.221182', '0.219314', '0.222279', '0.219303'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.109, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n### Program 2 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, GroupNorm, Swish, Multiply, and Swish operations.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape)) \n\n    def forward(self, x):\n        # (batch_size, in_features) -> (batch_size, out_features)\n        x = self.gemm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = self.group_norm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * self.multiply_weight\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 16\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0976), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.109, 'speed_up': 1.117, 'custom_timing': 0.0976})\n\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n# Triton kernel for optimized GEMM operation\n@triton.jit\ndef _gemm_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    \n    pid_m = pid // grid_n\n    pid_n = pid % grid_n\n    \n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        mask_a = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n        mask_b = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n        \n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    \n    mask_c = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask_c)\n\n# Optimized Linear module using Triton\nclass TritonLinear(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        output = torch.empty(\n            x.size(0), \n            self.out_features, \n            device=x.device, \n            dtype=x.dtype\n        )\n        \n        grid = lambda M, N: (triton.cdiv(M, 64) * triton.cdiv(N, 64),)\n        \n        _gemm_kernel[grid(x.size(0), self.out_features)](\n            x, self.weight, output,\n            x.size(0), self.out_features, self.in_features,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_M=64, BLOCK_N=64, BLOCK_K=32\n        )\n        \n        output += self.bias\n        return output\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, GroupNorm, Swish, Multiply, and Swish operations.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = TritonLinear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape)) \n\n    def forward(self, x):\n        # (batch_size, in_features) -> (batch_size, out_features)\n        x = self.gemm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = self.group_norm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * self.multiply_weight\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 16\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given that the current program fails correctness (output mismatch) and we are to fix it.\n The current program uses a custom Triton GEMM kernel for the linear layer (TritonLinear) and then applies group norm and swish operations.\n\n The problem: The Triton GEMM kernel is written for FP32, but note that the rest of the model (GroupNorm, Swish) might be operating on FP32, and the input tensor could be FP32.\n\n However, the verification shows a large max_difference (up to 9.94) and average difference (around 0.22) which is unacceptable.\n\n Let's check the TritonLinear implementation:\n\n 1. The kernel uses `tl.float32` for the accumulator and the operations. This is correct for FP32 input.\n\n 2. The kernel uses a grid that is computed as `(triton.cdiv(M, 64) * triton.cdiv(N, 64),)`. This is a 1D grid and the kernel uses 2D indexing (pid_m, pid_n) computed from the 1D pid.\n\n 3. The kernel has a loop over K in blocks of size BLOCK_K (32).\n\n 4. The kernel uses masks for loading A and B and storing C.\n\n 5. The kernel uses `tl.dot` which is correct.\n\n However, note that the kernel does not account for the fact that the weight matrix is stored as (out_features, in_features) and the input x is (batch, in_features). Therefore, the GEMM is: \n     output = x @ weight.T   [because in linear layer: y = x W^T + b]\n\n But in the kernel, we are loading the weight matrix as:\n     b_ptr = weight tensor, with stride (stride_bk = weight.stride(0)=1? and stride_bn = weight.stride(1)=out_features?).\n\n Actually, let's check the strides:\n\n     x: shape (batch, in_features) -> strides (in_features, 1) if contiguous? \n        But note: the input x might be contiguous, so stride(0)=in_features, stride(1)=1.\n\n     weight: shape (out_features, in_features) -> if row-major and contiguous, then stride(0)=in_features, stride(1)=1? \n        Actually: no, because for a matrix of shape (O, I), the stride for the first dimension (O) would be I, and the second (I) would be 1.\n\n But in the kernel, we have:\n\n     a_ptr (x) is accessed as: \n        a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n        Here, offs_m is along the batch (M = batch), and offs_k is along in_features (K = in_features).\n\n        So for x, we use:\n            stride_am = x.stride(0) = in_features\n            stride_ak = x.stride(1) = 1\n\n     b_ptr (weight) is accessed as:\n        b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n        Here, offs_k is along in_features (K = in_features) and offs_n is along out_features (N = out_features).\n\n        We are using:\n            stride_bk = weight.stride(0) -> should be in_features? but note: weight is (O, I) and we are using the k index to go over I (the columns) and n index to go over O (the rows).\n\n        Actually, the weight matrix in the linear layer is stored as (out_features, in_features). We want to do x @ weight.T.\n\n        But note: the kernel is written as:\n            a = (M, K)   [x]\n            b = (K, N)   [weight?] but our weight is (O, I) = (N, K). We need to use weight transposed? \n\n        In the kernel, we are using the weight without transposing. So we are effectively doing:\n            x (M, K) @ weight (K, N)   -> but wait, the weight is stored as (N, K). We are loading a block of weight of size (BLOCK_K, BLOCK_N) from the weight matrix without transposing.\n\n        How are we loading the weight? The weight is stored as (N, K) (because N=out_features, K=in_features). We are accessing:\n            b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n        This would load a block of the weight matrix of size (BLOCK_K, BLOCK_N) from the weight matrix. But note that the weight matrix is stored in row-major. The element at (i, j) in the weight matrix (i from 0 to N-1, j from 0 to K-1) is at b_ptr + i * stride_bk + j * stride_bn? \n\n        Actually, the kernel uses:\n            stride_bk = self.weight.stride(0) -> which is the stride for the first dimension (out_features). For a contiguous weight, stride(0) = in_features (K) and stride(1)=1.\n\n        So the access for weight is: \n            b_ptrs = b_ptr + offs_k (of length BLOCK_K) [along the columns?] * stride_bk (which is K) + offs_n (of length BLOCK_N) [along the rows?] * stride_bn (which is 1).\n\n        This would access the weight matrix at:\n            row index: offs_n (which is in [0, N-1])\n            column index: offs_k (which is in [0, K-1])\n\n        So we are loading a block of the weight matrix of shape (BLOCK_K, BLOCK_N) as if it were (BLOCK_K, BLOCK_N) but actually the weight matrix is (N, K). So the block we are loading is a contiguous block in memory? \n\n        However, note the dimensions: the block is (BLOCK_K, BLOCK_N) but the weight matrix is (N, K). We are reading a block that spans BLOCK_K in the row direction (but note: the row dimension of the weight matrix is N, so we are reading BLOCK_K rows? but offs_k is from 0 to K-1, so we are actually reading columns?).\n\n        This is a mistake.\n\n        We intended to do: \n            x (M, K) @ weight (K, N)   [but note: the weight matrix we have is (N, K), so we need to transpose it to (K, N)]\n\n        How to fix? We can either transpose the weight matrix or change the way we access the weight.\n\n        Since the kernel is written for (K, N) layout for the second matrix, but our weight is (N, K), we are effectively loading a transposed block? \n\n        Actually, we are loading the weight matrix without transposing and then using it as if it were of shape (K, N). But the block we load is of shape (BLOCK_K, BLOCK_N) and we are doing a dot product with a block of x (BLOCK_M, BLOCK_K) and the weight block (BLOCK_K, BLOCK_N) -> which is correct if the weight block is the transposed of the original weight? \n\n        Let me clarify:\n\n          Original weight matrix W is (N, K). We want W^T which is (K, N). In the kernel, we are using the weight without transposing, so we are using W (N, K) as if it were (K, N). This is incorrect.\n\n        Therefore, we have two options:\n\n          Option 1: Transpose the weight matrix and then use it in the kernel. But that would require extra memory and time.\n\n          Option 2: Change the kernel to load the weight matrix in a transposed manner.\n\n        We can fix the kernel by changing the way we access the weight matrix:\n\n          Instead of:\n            b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n          We should use:\n            b_ptrs = b_ptr + offs_n[:, None] * stride_bn + offs_k[None, :] * stride_bk\n\n          But note: the strides we passed: \n            stride_bk = self.weight.stride(0)   # which is the stride for the first dimension (out_features) -> K\n            stride_bn = self.weight.stride(1)   # which is the stride for the second dimension (in_features) -> 1\n\n          Now, if we do:\n            b_ptrs = b_ptr + offs_n[:, None] * stride_bn (which is 1) + offs_k[None, :] * stride_bk (which is K)\n\n          Then we are accessing the element at (n, k) as: b_ptr + n*1 + k*K = b_ptr + n + k*K.\n\n          But in the original weight matrix, the element at (n, k) is stored at b_ptr + n * K + k.\n\n          So we are effectively reading the element at (k, n) in the transposed matrix? \n\n          Actually, we are reading the element at (n, k) in the original matrix, but we want the element at (k, n) in the original matrix to be at (n, k) in the transposed? \n\n          The correct element for the transposed weight at (k, n) is the element at (n, k) in the original.\n\n          So we can do:\n\n            b_ptrs = b_ptr + offs_n[:, None] * stride_bn + offs_k[None, :] * stride_bk\n\n          This would give the element at (n, k) in the original, which is the same as the element at (k, n) in the transposed.\n\n          But note: the kernel expects the second matrix to be of shape (K, N) and we are loading a block of (BLOCK_N, BLOCK_K) from the original weight? \n\n          Then we have to transpose the block? \n\n        Alternatively, we can change the kernel to expect the weight in the original layout and then use it without transposing the indices? \n\n        Actually, the kernel currently uses:\n            b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n        This loads a block of the weight of shape (BLOCK_K, BLOCK_N) but the weight matrix is (N, K) so the indices (offs_k, offs_n) correspond to (row=offs_k, column=offs_n) in the original weight? But wait, the original weight has rows of length K, so the row index should be offs_n and the column index should be offs_k? \n\n        The kernel is currently using offs_k for the row and offs_n for the column? That is backwards.\n\n        We want to load the weight matrix as (K, N) but it is stored as (N, K). So we can fix by swapping the roles of offs_k and offs_n? \n\n        Actually, we should load the weight matrix by:\n            b_ptrs = b_ptr + offs_n[:, None] * stride_bn + offs_k[None, :] * stride_bk\n\n        But note: the kernel expects the second matrix to be of shape (K, N) and we are loading a block of (BLOCK_N, BLOCK_K) which is not contiguous and also not of the right shape.\n\n        Alternatively, we can change the kernel to use:\n            b_ptrs = b_ptr + offs_k[None, :] * stride_bk + offs_n[:, None] * stride_bn   -> but this is the same as above.\n\n        This is confusing.\n\n        Let me change the kernel to use the weight matrix in a transposed way without changing the weight storage.\n\n        We can precompute the pointer for the weight matrix as if it were stored in (K, N) by changing the strides:\n\n          We pass to the kernel the weight tensor and we use:\n            stride_bk = self.weight.stride(1)   # which is 1 for the last dimension? \n            stride_bn = self.weight.stride(0)   # which is K for the first dimension.\n\n        Then in the kernel we do:\n            b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n        becomes:\n            b_ptrs = b_ptr + offs_k[:, None] * 1 + offs_n[None, :] * K\n\n        This will access the element at (k, n) in the transposed matrix? Actually, it will access the element at (n, k) in the original matrix? \n\n        Because the original matrix has element (n, k) at position: n * K + k.\n\n        And we are computing: k * 1 + n * K = n*K + k -> same.\n\n        Therefore, we are loading the element at (n, k) of the original weight, which is the same as the element at (k, n) of the transposed weight.\n\n        So the fix is: in the call to the kernel, we should pass the strides for the weight as:\n            stride_bk = self.weight.stride(1)   # 1\n            stride_bn = self.weight.stride(0)   # in_features (K)\n\n        But currently, we are passing:\n            self.weight.stride(0), self.weight.stride(1)   -> which is (K, 1) for a contiguous weight.\n\n        And in the kernel, we are using:\n            stride_bk = weight.stride(0) -> K\n            stride_bn = weight.stride(1) -> 1\n\n        So we are passing the strides in the wrong order? \n\n        Actually, the kernel expects:\n            stride_bk: the stride for the k dimension (which is the first dimension of the weight if we consider it as (K, N)? but note the kernel considers the weight as (K, N) but we are storing it as (N, K). \n\n        We want the kernel to access the weight as if it were (K, N) stored in row-major? Then the stride for the k dimension (which is the first dimension) would be N? but wait, we don't have N in the kernel for the weight? \n\n        Alternatively, we can change the kernel to accept the weight as it is and then change the access to:\n\n            b_ptrs = b_ptr + offs_n[:, None] * stride_bn + offs_k[None, :] * stride_bk   # but note: the dimensions of the block we want are (BLOCK_K, BLOCK_N) but we are loading (BLOCK_N, BLOCK_K) and then we would need to transpose it? \n\n        This is getting messy.\n\n        A simpler fix: note that the current kernel is written for the case when the second matrix is stored in row-major with dimensions (K, N). But our weight is stored as (N, K) (row-major). Therefore, the contiguous block we load for the weight in the kernel (with the current indices) is actually a contiguous block of the original weight? but it is the wrong block.\n\n        Alternatively, we can transpose the weight matrix in the forward pass and then use the kernel as is. But that would require an extra copy.\n\n        Given the time, I propose to change the kernel to use the correct strides for the weight matrix without transposing.\n\n        We want to access the weight matrix as if it were transposed, so we pass:\n\n            stride_bk = self.weight.stride(1)   # 1\n            stride_bn = self.weight.stride(0)   # in_features (K)\n\n        Then in the kernel, we use:\n\n            b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n        becomes: \n            offs_k (length BLOCK_K) along the k dimension (which is the in_features dimension) and offs_n (length BLOCK_N) along the n dimension (out_features). The access is:\n                b_ptr + offs_k * 1 + offs_n * K\n\n        This is the same as the original weight matrix's element at (offs_n, offs_k) -> which is the element at (n, k) in the weight. But we want the element at (k, n) in the transposed weight, which is the same as (n, k) in the original.\n\n        Therefore, we are loading the correct element.\n\n        So the fix is in the call to the kernel: swap the strides for the weight.\n\n        Change in the forward function of TritonLinear:\n\n            _gemm_kernel[grid(x.size(0), self.out_features)](\n                x, self.weight, output,\n                x.size(0), self.out_features, self.in_features,\n                x.stride(0), x.stride(1),\n                self.weight.stride(1), self.weight.stride(0),   # swapped here\n                output.stride(0), output.stride(1),\n                BLOCK_M=64, BLOCK_N=64, BLOCK_K=32\n            )\n\n        This might fix the correctness.\n\n        Also note: the kernel uses the same block for the entire K dimension? We have a loop over k in steps of BLOCK_K. The mask for a and b in the loop uses `K - k`? Actually, the mask condition is:\n\n            mask_a = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n            mask_b = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n\n        But note: `offs_k` is a vector from 0 to BLOCK_K-1. Then `offs_k[None, :] < K - k` is checking that each element in the k-dimension block is within the remaining columns? \n\n        Actually, we are iterating k from 0 to K in steps of BLOCK_K. The remaining columns at step k is `K - k_start` (where k_start = k). We want to load a block of size BLOCK_K, but if the remaining columns are less than BLOCK_K, we mask the extra.\n\n        So the condition should be: `offs_k[None, :] < (K - k)` -> but note: `offs_k[None, :]` is the local index in the block (0 to BLOCK_K-1). We want to load only the first `min(BLOCK_K, K - k)` elements.\n\n        This condition is correct.\n\n        However, note that `K - k` might be negative? No because k is from 0 to K in steps of BLOCK_K, so the last block might have k_start = k, and then K - k might be less than BLOCK_K, but we mask the extra.\n\n        So the mask condition is correct.\n\n        Another possibility: the kernel does not initialize the output tensor? It uses `torch.empty` which might contain garbage. Then we do the accumulation and then add the bias. But the kernel writes the entire output.\n\n        However, note that the kernel writes the entire output block by block. So if the output tensor is initialized to zeros? No, we are using `torch.empty` which is uninitialized. That is a problem.\n\n        We should initialize the output to zeros? Or the kernel writes every element exactly once? \n\n        The kernel writes each element of the output exactly once (because the grid covers the entire output). So it is safe.\n\n        But note: the kernel uses `accumulator = tl.zeros(...)` and then accumulates and then stores. So the output is written correctly.\n\n        However, the kernel does not account for the bias. The bias is added after the kernel. That is correct.\n\n        Therefore, the main issue is the weight strides.\n\n        Let's change the call to the kernel to swap the strides for the weight.\n\n        We'll change:\n\n            self.weight.stride(0) -> becomes stride_bk (which we want to be the stride for the k dimension in the weight matrix as if it were (K, N)) -> which should be the stride for the in_features dimension? which is 1.\n\n            self.weight.stride(1) -> becomes stride_bn (which we want to be the stride for the n dimension in the weight matrix as if it were (K, N)) -> which should be the stride for the out_features dimension? which is in_features.\n\n        So we pass:\n\n            stride_bk = self.weight.stride(1)   # 1\n            stride_bn = self.weight.stride(0)   # in_features (K)\n\n        This should fix the kernel.\n\n        Let's update the code accordingly.\n\n        We'll change only the call to the kernel in the forward function of TritonLinear.\n\n        Additionally, note that the kernel uses the dot product. We are using FP32 accumulation, which is correct.\n\n        We hope that after this fix, the correctness will pass.\n\n        We also note that the previous version (the reference) had a speed_up of 1.117 (11.7% faster) and we want to achieve at least that.\n\n        We are not changing the block sizes for now.\n\n        We'll update the code accordingly.\n\n        IMPORTANT: We are only allowed to change the Triton kernel source within the EVOLVE-BLOCK? Actually, the kernel source is inside the block. But the call to the kernel is also inside the block.\n\n        So we can change the call.\n\n        Let me rewrite the TritonLinear.forward method.\n\n        We change the call to the kernel by swapping the strides for the weight.\n\n        Also, note: the kernel itself we leave unchanged.\n\n        Code for the forward method of TritonLinear:\n\n            output = torch.empty(...)   # same\n\n            grid = ... # same\n\n            _gemm_kernel[grid(...)](\n                x, self.weight, output,\n                x.size(0), self.out_features, self.in_features,\n                x.stride(0), x.stride(1),\n                self.weight.stride(1), self.weight.stride(0),   # swapped: stride_bk = stride(1), stride_bn = stride(0)\n                output.stride(0), output.stride(1),\n                BLOCK_M=64, BLOCK_N=64, BLOCK_K=32\n            )\n\n        Then add the bias.\n\n        We hope this fixes the correctness.\n\n        If not, we must consider other issues.\n\n        Let's output the entire block with this change.\n\n        Also note: the rest of the model (GroupNorm, Swish, etc.) is the same as the reference? The reference model uses nn.Linear and then the same operations. So we don't change that.\n\n        We output the entire code block with the change in the TritonLinear.forward method.\n\n        IMPORTANT: We are only allowed to output the code block between the EVOLVE-BLOCK markers.\n\n        We do not change the kernel code, only the call.\n\n        We also note that the current program (with the bug) had a correctness failure. We hope this fixes it.\n\n        Let's write the code accordingly.\n</think>\nTo address the correctness issue in the Triton GEMM kernel, we need to fix the weight matrix access pattern. The current implementation incorrectly accesses the weight matrix without accounting for its row-major layout. By swapping the stride arguments for the weight matrix, we ensure proper memory access alignment during the matrix multiplication. This change maintains the same functionality while fixing the numerical correctness issue.\n\nHere's the optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n# Triton kernel for optimized GEMM operation\n@triton.jit\ndef _gemm_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    \n    pid_m = pid // grid_n\n    pid_n = pid % grid_n\n    \n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        mask_a = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n        mask_b = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n        \n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    \n    mask_c = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask_c)\n\n# Optimized Linear module using Triton\nclass TritonLinear(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        output = torch.empty(\n            x.size(0), \n            self.out_features, \n            device=x.device, \n            dtype=x.dtype\n        )\n        \n        grid = lambda M, N: (triton.cdiv(M, 64) * triton.cdiv(N, 64),)\n        \n        _gemm_kernel[grid(x.size(0), self.out_features)](\n            x, self.weight, output,\n            x.size(0), self.out_features, self.in_features,\n            x.stride(0), x.stride(1),\n            self.weight.stride(1), self.weight.stride(0),  # Fixed weight strides\n            output.stride(0), output.stride(1),\n            BLOCK_M=64, BLOCK_N=64, BLOCK_K=32\n        )\n        \n        output += self.bias\n        return output\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a GEMM, GroupNorm, Swish, Multiply, and Swish operations.\n    \"\"\"\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = TritonLinear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape)) \n\n    def forward(self, x):\n        # (batch_size, in_features) -> (batch_size, out_features)\n        x = self.gemm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = self.group_norm(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * self.multiply_weight\n        # (batch_size, out_features) -> (batch_size, out_features)\n        x = x * torch.sigmoid(x)\n        return x\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 16\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
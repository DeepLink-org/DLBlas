{"id": "f8ff76cb-500b-478d-9998-e271dad845d4", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Optimized GEMM kernel using Triton\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=8),\n        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 32, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4),\n    ],\n    key=['batch_size', 'out_features', 'in_features'],\n)\n@triton.jit\ndef gemm_triton_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    batch_size, in_features, out_features,\n    stride_xb, stride_xk,\n    stride_wk, stride_wn,\n    stride_ob, stride_on,\n    BLOCK_SIZE_B: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_b = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    batch_idx = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n    feature_idx = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    \n    batch_mask = batch_idx < batch_size\n    feature_mask = feature_idx < out_features\n    \n    acc = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(in_features, BLOCK_SIZE_K)):\n        k_idx = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_idx < in_features\n        \n        # Load input block\n        x_ptr_mask = batch_mask[:, None] & k_mask[None, :]\n        x_vals = tl.load(\n            x_ptr + batch_idx[:, None] * stride_xb + k_idx[None, :] * stride_xk,\n            mask=x_ptr_mask,\n            other=0.0\n        )\n        \n        # Load weight block and transpose for dot product\n        w_ptr_mask = feature_mask[:, None] & k_mask[None, :]\n        w_vals = tl.load(\n            weight_ptr + k_idx[None, :] * stride_wk + feature_idx[:, None] * stride_wn,\n            mask=w_ptr_mask,\n            other=0.0\n        )\n        w_vals_t = tl.trans(w_vals)\n        \n        # Compute partial dot product with correct matrix dimensions\n        acc += tl.dot(x_vals, w_vals_t, allow_tf32=True)\n    \n    # Add bias\n    bias_vals = tl.load(bias_ptr + feature_idx, mask=feature_mask, other=0.0)\n    acc += bias_vals[None, :]\n    \n    # Store results\n    out_ptr_mask = batch_mask[:, None] & feature_mask[None, :]\n    tl.store(\n        output_ptr + batch_idx[:, None] * stride_ob + feature_idx[None, :] * stride_on,\n        acc,\n        mask=out_ptr_mask\n    )\n\n# Fused GroupNorm + Swish + Multiply + Swish kernel\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n    ],\n    key=['out_features'],\n)\n@triton.jit\ndef fused_gn_swish_kernel(\n    x_ptr, gn_weight_ptr, gn_bias_ptr, mult_weight_ptr, output_ptr,\n    batch_size, out_features, num_groups,\n    stride_xb, stride_xf,\n    stride_ob, stride_of,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    group_id = pid % num_groups\n    batch_idx = pid // num_groups\n    \n    if batch_idx >= batch_size:\n        return\n    \n    group_size = out_features // num_groups\n    group_start = group_id * group_size\n    feature_idx = group_start + tl.arange(0, BLOCK_SIZE)\n    feature_mask = feature_idx < (group_id + 1) * group_size\n    \n    # Load data\n    x_vals = tl.load(\n        x_ptr + batch_idx * stride_xb + feature_idx * stride_xf,\n        mask=feature_mask,\n        other=0.0\n    )\n    \n    # Load normalization parameters\n    gn_weight = tl.load(gn_weight_ptr + feature_idx, mask=feature_mask, other=0.0)\n    gn_bias = tl.load(gn_bias_ptr + feature_idx, mask=feature_mask, other=0.0)\n    \n    # GroupNorm calculations\n    mean = tl.sum(x_vals, axis=0) / group_size\n    centered = x_vals - mean\n    var = tl.sum(centered * centered, axis=0) / group_size\n    rstd = tl.math.rsqrt(var + 1e-5)\n    norm = centered * rstd * gn_weight + gn_bias\n    \n    # First Swish\n    swish1 = norm * tl.sigmoid(norm)\n    \n    # Multiply weight\n    mult_weight = tl.load(mult_weight_ptr + feature_idx, mask=feature_mask, other=0.0)\n    multiplied = swish1 * mult_weight\n    \n    # Second Swish\n    swish2 = multiplied * tl.sigmoid(multiplied)\n    \n    # Store results\n    tl.store(\n        output_ptr + batch_idx * stride_ob + feature_idx * stride_of,\n        swish2,\n        mask=feature_mask\n    )\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape))\n        self.out_features = out_features\n        self.num_groups = num_groups\n\n    def forward(self, x):\n        # GEMM operation\n        batch_size, _ = x.shape\n        out = torch.empty((batch_size, self.out_features), \n                         device=x.device, dtype=x.dtype)\n        \n        grid = (\n            triton.cdiv(batch_size, 64),\n            triton.cdiv(self.out_features, 64),\n        )\n        \n        gemm_triton_kernel[grid](\n            x, self.gemm.weight, self.gemm.bias, out,\n            batch_size, self.gemm.in_features, self.gemm.out_features,\n            x.stride(0), x.stride(1),\n            self.gemm.weight.stride(0), self.gemm.weight.stride(1),\n            out.stride(0), out.stride(1),\n        )\n        \n        # Fused GroupNorm + Swish + Multiply + Swish\n        grid_gn = (batch_size * self.num_groups,)\n        fused_gn_swish_kernel[grid_gn](\n            out, \n            self.group_norm.weight, \n            self.group_norm.bias, \n            self.multiply_weight, \n            out,\n            batch_size, self.out_features, self.num_groups,\n            out.stride(0), out.stride(1),\n            out.stride(0), out.stride(1),\n            BLOCK_SIZE=self.out_features // self.num_groups\n        )\n        \n        return out\n\nbatch_size = 128\nin_features = 512\nout_features = 1024\nnum_groups = 16\nmultiply_weight_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, multiply_weight_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "87412859-68ea-4487-8e73-d4de792319a8", "generation": 2, "timestamp": 1754584271.6264298, "iteration_found": 18, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_88_Gemm_GroupNorm_Swish_Multiply_Swish", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp0ii7i78x.py\", line 167, in forward\n    fused_gn_swish_kernel[grid_gn](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 135, in _bench\n    raise ValueError(f\"Conflicting meta-parameters: {', '.join(conflicts)}.\"\nValueError: Conflicting meta-parameters: BLOCK_SIZE. Make sure that you don't re-define auto-tuned symbols.\n", "runtime_error_name": "builtins.ValueError", "reference_timing_ms": 0.109, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_88_Gemm_GroupNorm_Swish_Multiply_Swish", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2045, in dot\n    return _semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1498, in dot\n    assert lhs.shape[-1].value == rhs.shape[\nAssertionError: First input shape (['constexpr[64]', 'constexpr[32]']) and second input shape ['constexpr[64]', 'constexpr[32]'] are not compatible for matmul (second index of first shape (32) must be equal to first index of second shape (64)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpent6fzl3.py\", line 156, in forward\n    gemm_triton_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 239, in run\n    benchmark()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in benchmark\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 228, in <dictcomp>\n    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 160, in _bench\n    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/testing.py\", line 149, in do_bench\n    fn()\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/autotuner.py\", line 146, in kernel_call\n    self.fn.run(\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 41:15:\n        )\n\n        # Load weight block\n        w_ptr_mask = feature_mask[:, None] & k_mask[None, :]\n        w_vals = tl.load(\n            weight_ptr + k_idx[None, :] * stride_wk + feature_idx[:, None] * stride_wn,\n            mask=w_ptr_mask,\n            other=0.0\n        )\n\n        # Compute partial dot product\n        acc += tl.dot(x_vals, w_vals, allow_tf32=True)\n               ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2045, in dot\n    return _semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1498, in dot\n    assert lhs.shape[-1].value == rhs.shape[\nAssertionError: First input shape (['constexpr[64]', 'constexpr[32]']) and second input shape ['constexpr[64]', 'constexpr[32]'] are not compatible for matmul (second index of first shape (32) must be equal to first index of second shape (64)\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.109, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
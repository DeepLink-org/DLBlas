{"id": "76f91d33-fb30-4ff8-b8ed-526e704b843d", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _conv3x3_forward_kernel(\n    x_ptr,\n    w_ptr,\n    output_ptr,\n    B, C, H, W, OC,\n    stride,\n    padding,\n    output_h, output_w,\n    BLOCK_SIZE_OC: tl.constexpr,\n    BLOCK_SIZE_IC: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    pid_b = pid // (output_h * output_w)\n    pid_s = pid % (output_h * output_w)\n    pid_oh = pid_s // output_w\n    pid_ow = pid_s % output_w\n    \n    # Calculate input starting positions with padding\n    ih_start = pid_oh * stride - padding\n    iw_start = pid_ow * stride - padding\n    \n    # Process output channels in blocks\n    for oc_block in range(0, OC, BLOCK_SIZE_OC):\n        oc_offs = oc_block + tl.arange(0, BLOCK_SIZE_OC)\n        mask_oc = oc_offs < OC\n        acc = tl.zeros((BLOCK_SIZE_OC,), dtype=tl.float32)\n        \n        # Process input channels in blocks\n        for ic in range(0, C, BLOCK_SIZE_IC):\n            ic_offs = ic + tl.arange(0, BLOCK_SIZE_IC)\n            mask_ic = ic_offs < C\n            \n            # Process 3x3 kernel\n            for kh in range(3):\n                for kw in range(3):\n                    # Load weight for current position\n                    w_offset = oc_offs[:, None] * (C * 9) + ic_offs[None, :] * 9 + kh * 3 + kw\n                    w_val = tl.load(w_ptr + w_offset, mask=mask_oc[:, None] & mask_ic[None, :], other=0.0)\n                    \n                    # Compute input position\n                    ih = ih_start + kh\n                    iw = iw_start + kw\n                    \n                    # Create mask for valid input positions\n                    mask_spatial = (ih >= 0) & (ih < H) & (iw >= 0) & (iw < W)\n                    mask = mask_ic & mask_spatial\n                    \n                    # Load input value\n                    x_offs = pid_b * C * H * W + ic_offs * H * W + ih * W + iw\n                    x_val = tl.load(x_ptr + x_offs, mask=mask, other=0.0)\n                    \n                    # Accumulate\n                    acc += tl.sum(w_val * x_val[None, :], axis=1)\n        \n        # Store result with correct output channel indexing\n        out_idx = pid_b * OC * output_h * output_w + oc_offs * output_h * output_w + pid_oh * output_w + pid_ow\n        tl.store(output_ptr + out_idx, acc, mask=mask_oc)\n\ndef triton_conv3x3(x, weight, stride=1, padding=1):\n    B, C, H, W = x.shape\n    OC, _, _, _ = weight.shape\n    output_h = (H + 2 * padding - 3) // stride + 1\n    output_w = (W + 2 * padding - 3) // stride + 1\n    output = torch.empty((B, OC, output_h, output_w), device=x.device, dtype=x.dtype)\n    \n    # Grid configuration - one element per spatial position per batch\n    grid = (B * output_h * output_w,)\n    \n    # Launch kernel with optimized block sizes\n    _conv3x3_forward_kernel[grid](\n        x, weight, output,\n        B, C, H, W, OC,\n        stride,\n        padding,\n        output_h, output_w,\n        BLOCK_SIZE_OC=32,\n        BLOCK_SIZE_IC=64\n    )\n    return output\n\nclass TritonConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        \n        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in)\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        if self.kernel_size == 3 and self.padding == 1:\n            return triton_conv3x3(x, self.weight, self.stride, self.padding)\n        else:\n            return F.conv2d(x, self.weight, self.bias, self.stride, self.padding)\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = TritonConv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, layers, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        block = Bottleneck\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n# Test code\nbatch_size = 10\nheight = 224\nwidth = 224\nlayers = [3, 4, 23, 3]\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [layers, num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "44bb137f-d64f-4983-af49-745843c02f22", "generation": 3, "timestamp": 1754587625.9261675, "iteration_found": 20, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_10_ResNet101", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "max_difference": ["0.103440", "0.123564", "0.099899", "0.108826", "0.100649"], "avg_difference": ["0.021154", "0.021348", "0.021976", "0.021402", "0.021204"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 6.25, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_10_ResNet101", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp3b6cf6tg.py\", line 187, in forward\n    x = self.layer1(x)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\n    input = module(input)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp3b6cf6tg.py\", line 130, in forward\n    out = self.conv2(out)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp3b6cf6tg.py\", line 104, in forward\n    return triton_conv3x3(x, self.weight, self.stride, self.padding)\n  File \"/tmp/tmp3b6cf6tg.py\", line 69, in triton_conv3x3\n    _conv3x3_forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 36:16:\n\n        # Process 3x3 kernel\n        for kh in range(3):\n            for kw in range(3):\n                # Load weight for current position\n                w_offset = ic_offs * 9 + kh * 3 + kw\n                w_val = tl.load(w_ptr + w_offset, mask=mask_c, other=0.0)\n\n                # Load input value\n                ih = ih_start + kh\n                iw = iw_start + kw\n                if ih < H and iw < W:\n                ^\nAssertionError(\"Mismatched type for x_val between then block (<['64'], fp32>) and else block (fp32)\")\n\n\nThe above exception was the direct cause of the following exception:\n\nAssertionError: Mismatched type for x_val between then block (<['64'], fp32>) and else block (fp32)\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 6.25, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
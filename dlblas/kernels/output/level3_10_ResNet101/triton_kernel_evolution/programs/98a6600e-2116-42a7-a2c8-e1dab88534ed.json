{"id": "98a6600e-2116-42a7-a2c8-e1dab88534ed", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_bias_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wk, stride_wn,\n    stride_bn,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(1)\n    pid_m = 0\n    pid_n = pid\n\n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n\n    x_ptrs = x_ptr + rm[:, None] * stride_xm + rk[None, :] * stride_xk\n    w_ptrs = weight_ptr + rk[:, None] * stride_wk + rn[None, :] * stride_wn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        k_mask = rk < k_remaining\n        mask_x = (rm < M)[:, None] & k_mask[None, :]\n        mask_w = k_mask[:, None] & (rn < N)[None, :]\n        x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n        w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n        acc += tl.dot(x, w, allow_tf32=True)\n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n\n    bias = tl.load(bias_ptr + rn, mask=rn < N, other=0.0)\n    acc += bias[None, :]\n    \n    mask_out = (rm < M)[:, None] & (rn < N)[None, :]\n    out_ptrs = output_ptr + rm[:, None] * stride_om + rn[None, :] * stride_on\n    tl.store(out_ptrs, acc, mask=mask_out)\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass ModelNew(nn.Module):\n    def __init__(self, layers, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        block = Bottleneck\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc_weight = nn.Parameter(torch.Tensor(512 * block.expansion, num_classes))\n        self.fc_bias = nn.Parameter(torch.Tensor(num_classes))\n        \n        # Initialize weights\n        nn.init.kaiming_uniform_(self.fc_weight, a=math.sqrt(5))\n        bound = 1 / math.sqrt(self.fc_weight.size(0))\n        nn.init.uniform_(self.fc_bias, -bound, bound)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        # Keep convolutions in float32\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        \n        # Maintain FP32 precision for numerical stability\n        M, K = x.shape\n        N = self.fc_weight.size(1)\n        output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n        \n        grid = lambda META: (1, triton.cdiv(N, META['BLOCK_SIZE_N']))\n        \n        # Use optimized block sizes for H100/H800\n        matmul_bias_kernel[grid](\n            x, \n            self.fc_weight, \n            self.fc_bias, \n            output,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.fc_weight.stride(0), self.fc_weight.stride(1),\n            self.fc_bias.stride(0),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32,\n        )\n        \n        return output\n\n# Test code\nbatch_size = 10\nheight = 224\nwidth = 224\nlayers = [3, 4, 23, 3]\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, 3, height, width)]\n\ndef get_init_inputs():\n    return [layers, num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "2d2702eb-2680-4b75-84cb-21faaa98aa04", "generation": 3, "timestamp": 1754587072.645786, "iteration_found": 17, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_10_ResNet101", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "max_difference": ["3.679530", "3.531586", "3.547468", "3.670115", "3.594132"], "avg_difference": ["0.851189", "0.850939", "0.850952", "0.850043", "0.851047"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 6.25, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_10_ResNet101", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "max_difference": ["3.681328", "3.533302", "3.550190", "3.672420", "3.595947"], "avg_difference": ["0.851575", "0.851322", "0.851339", "0.850429", "0.851432"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 6.25, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
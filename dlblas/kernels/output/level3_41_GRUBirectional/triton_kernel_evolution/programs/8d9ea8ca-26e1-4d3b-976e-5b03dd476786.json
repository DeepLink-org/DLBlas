{"id": "8d9ea8ca-26e1-4d3b-976e-5b03dd476786", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gru_cell_kernel(\n    x_ptr,\n    hx_ptr,\n    w_ih_ptr,\n    w_hh_ptr,\n    b_ih_ptr,\n    b_hh_ptr,\n    output_ptr,\n    input_size,\n    hidden_size,\n    stride_xb,\n    stride_hxb,\n    stride_wih,\n    stride_whh,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(0)\n    \n    # Create batch pointers\n    x_batch_ptr = x_ptr + pid_b * stride_xb\n    hx_batch_ptr = hx_ptr + pid_b * stride_hxb\n    output_batch_ptr = output_ptr + pid_b * hidden_size\n    \n    # Weight pointers\n    w_ir_ptr = w_ih_ptr\n    w_iz_ptr = w_ih_ptr + hidden_size * stride_wih\n    w_in_ptr = w_ih_ptr + 2 * hidden_size * stride_wih\n    \n    w_hr_ptr = w_hh_ptr\n    w_hz_ptr = w_hh_ptr + hidden_size * stride_whh\n    w_hn_ptr = w_hh_ptr + 2 * hidden_size * stride_whh\n    \n    # Bias pointers\n    if b_ih_ptr is not None:\n        b_ir_ptr = b_ih_ptr\n        b_iz_ptr = b_ih_ptr + hidden_size\n        b_in_ptr = b_ih_ptr + 2 * hidden_size\n        \n        b_hr_ptr = b_hh_ptr\n        b_hz_ptr = b_hh_ptr + hidden_size\n        b_hn_ptr = b_hh_ptr + 2 * hidden_size\n    else:\n        b_ir_ptr = 0\n        b_iz_ptr = 0\n        b_in_ptr = 0\n        b_hr_ptr = 0\n        b_hz_ptr = 0\n        b_hn_ptr = 0\n\n    # Initialize accumulators\n    r = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    z = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    n = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Process input\n    for i in range(0, input_size, BLOCK_SIZE):\n        idx = i + tl.arange(0, BLOCK_SIZE)\n        mask = idx < input_size\n        x_val = tl.load(x_batch_ptr + idx, mask=mask, other=0.0)\n        \n        w_val_ir = tl.load(w_ir_ptr + idx, mask=mask, other=0.0)\n        r += x_val * w_val_ir\n        \n        w_val_iz = tl.load(w_iz_ptr + idx, mask=mask, other=0.0)\n        z += x_val * w_val_iz\n        \n        w_val_in = tl.load(w_in_ptr + idx, mask=mask, other=0.0)\n        n += x_val * w_val_in\n\n    # Add input biases\n    if b_ih_ptr is not None:\n        r += tl.load(b_ir_ptr + tl.arange(0, BLOCK_SIZE) % hidden_size)\n        z += tl.load(b_iz_ptr + tl.arange(0, BLOCK_SIZE) % hidden_size)\n        n += tl.load(b_in_ptr + tl.arange(0, BLOCK_SIZE) % hidden_size)\n    \n    # Process hidden state\n    for i in range(0, hidden_size, BLOCK_SIZE):\n        idx = i + tl.arange(0, BLOCK_SIZE)\n        mask = idx < hidden_size\n        h_val = tl.load(hx_batch_ptr + idx, mask=mask, other=0.0)\n        \n        w_val_hr = tl.load(w_hr_ptr + idx, mask=mask, other=0.0)\n        r += h_val * w_val_hr\n        \n        w_val_hz = tl.load(w_hz_ptr + idx, mask=mask, other=0.0)\n        z += h_val * w_val_hz\n        \n        w_val_hn = tl.load(w_hn_ptr + idx, mask=mask, other=0.0)\n        n += r * h_val * w_val_hn\n\n    # Add hidden biases\n    if b_hh_ptr is not None:\n        r += tl.load(b_hr_ptr + tl.arange(0, BLOCK_SIZE) % hidden_size)\n        z += tl.load(b_hz_ptr + tl.arange(0, BLOCK_SIZE) % hidden_size)\n        n += tl.load(b_hn_ptr + tl.arange(0, BLOCK_SIZE) % hidden_size)\n    \n    # Activation functions\n    r = tl.sigmoid(r)\n    z = tl.sigmoid(z)\n    n = tl.tanh(n)\n\n    # Compute new hidden state\n    h_prev = tl.load(hx_batch_ptr + tl.arange(0, BLOCK_SIZE) % hidden_size)\n    h_new = (1 - z) * n + z * h_prev\n    tl.store(output_batch_ptr + tl.arange(0, BLOCK_SIZE) % hidden_size, h_new)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        \n        # Initialize weights\n        self.weight_ih = nn.ParameterList()\n        self.weight_hh = nn.ParameterList()\n        if bias:\n            self.bias_ih = nn.ParameterList()\n            self.bias_hh = nn.ParameterList()\n        else:\n            self.bias_ih = None\n            self.bias_hh = None\n        \n        # First layer input size\n        layer_input_size = input_size\n        for layer in range(num_layers):\n            for direction in range(2):  # 0=forward, 1=backward\n                # Input-hidden weights\n                w_ih = nn.Parameter(torch.empty(3 * hidden_size, layer_input_size))\n                # Hidden-hidden weights\n                w_hh = nn.Parameter(torch.empty(3 * hidden_size, hidden_size))\n                \n                # Initialize weights\n                nn.init.xavier_uniform_(w_ih)\n                nn.init.orthogonal_(w_hh)\n                \n                self.weight_ih.append(w_ih)\n                self.weight_hh.append(w_hh)\n                \n                if bias:\n                    b_ih = nn.Parameter(torch.zeros(3 * hidden_size))\n                    b_hh = nn.Parameter(torch.zeros(3 * hidden_size))\n                    self.bias_ih.append(b_ih)\n                    self.bias_hh.append(b_hh)\n            \n            # Subsequent layers use bidirectional output\n            layer_input_size = hidden_size * 2\n        \n        # Initialize hidden state\n        self.register_buffer('h0', torch.zeros((num_layers * 2, 1, hidden_size)))\n    \n    def forward(self, x):\n        if self.batch_first:\n            x = x.transpose(0, 1)\n            \n        seq_len, batch_size, _ = x.shape\n        device = x.device\n        \n        # Initialize hidden states\n        h0 = self.h0.expand(-1, batch_size, -1).contiguous()\n        layer_input = x\n        next_input_size = self.input_size\n        \n        for layer in range(self.num_layers):\n            # Allocate outputs for both directions\n            forward_output = torch.zeros(seq_len, batch_size, self.hidden_size, device=device)\n            backward_output = torch.zeros(seq_len, batch_size, self.hidden_size, device=device)\n            \n            for direction in range(2):\n                # Get weights and biases\n                idx = layer * 2 + direction\n                w_ih = self.weight_ih[idx]\n                w_hh = self.weight_hh[idx]\n                b_ih = self.bias_ih[idx] if self.bias_ih is not None else None\n                b_hh = self.bias_hh[idx] if self.bias_hh is not None else None\n                \n                # Get initial hidden state\n                hx = h0[idx]\n                \n                # Process sequence\n                if direction == 0:  # Forward\n                    for t in range(seq_len):\n                        gru_cell_kernel[(batch_size,)](\n                            x_ptr=layer_input[t],\n                            hx_ptr=hx,\n                            w_ih_ptr=w_ih,\n                            w_hh_ptr=w_hh,\n                            b_ih_ptr=b_ih,\n                            b_hh_ptr=b_hh,\n                            output_ptr=forward_output[t],\n                            input_size=next_input_size,\n                            hidden_size=self.hidden_size,\n                            stride_xb=layer_input.stride(1),\n                            stride_hxb=hx.stride(0),\n                            stride_wih=w_ih.stride(0),\n                            stride_whh=w_hh.stride(0),\n                            BLOCK_SIZE=min(256, self.hidden_size)\n                        )\n                        hx = forward_output[t]\n                else:  # Backward\n                    for t in range(seq_len-1, -1, -1):\n                        gru_cell_kernel[(batch_size,)](\n                            x_ptr=layer_input[t],\n                            hx_ptr=hx,\n                            w_ih_ptr=w_ih,\n                            w_hh_ptr=w_hh,\n                            b_ih_ptr=b_ih,\n                            b_hh_ptr=b_hh,\n                            output_ptr=backward_output[t],\n                            input_size=next_input_size,\n                            hidden_size=self.hidden_size,\n                            stride_xb=layer_input.stride(1),\n                            stride_hxb=hx.stride(0),\n                            stride_wih=w_ih.stride(0),\n                            stride_whh=w_hh.stride(0),\n                            BLOCK_SIZE=min(256, self.hidden_size)\n                        )\n                        hx = backward_output[t]\n            \n            # Concatenate outputs\n            layer_input = torch.cat([forward_output, backward_output], dim=-1)\n            next_input_size = self.hidden_size * 2\n        \n        # Final output\n        output = layer_input\n        if self.batch_first:\n            output = output.transpose(0, 1)\n            \n        return output\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.randn(seq_len, batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "b8d9cfae-f2e3-484b-85a7-2bf553047c4f", "generation": 4, "timestamp": 1754595403.1890864, "iteration_found": 16, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_41_GRUBirectional", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmph4iz1y_o.py\", line 192, in forward\n    gru_cell_kernel[(batch_size,)](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 68.4, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_41_GRUBirectional", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_issue": "Output shape mismatch: Expected torch.Size([512, 10, 512]), got torch.Size([512, 10, 256])", "correctness_issue_name": "correctness_issue", "reference_timing_ms": 68.4, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
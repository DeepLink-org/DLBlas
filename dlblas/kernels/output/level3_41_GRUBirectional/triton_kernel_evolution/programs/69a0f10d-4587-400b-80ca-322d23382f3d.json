{"id": "69a0f10d-4587-400b-80ca-322d23382f3d", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef gru_cell_kernel(\n    x_ptr, h_prev_ptr, w_ih_ptr, w_hh_ptr, b_ih_ptr, b_hh_ptr, h_new_ptr,\n    input_size, hidden_size, batch_size,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    batch_idx = pid // hidden_size\n    hidden_idx = pid % hidden_size\n\n    if batch_idx >= batch_size:\n        return\n\n    # Reset gate computation\n    r_idx = hidden_idx\n    r_val = tl.zeros((1,), dtype=tl.float32)\n    for block in range(0, input_size, BLOCK_SIZE):\n        offs = block + tl.arange(0, BLOCK_SIZE)\n        mask = offs < input_size\n        x_val = tl.load(x_ptr + batch_idx * input_size + offs, mask=mask, other=0.0)\n        w_ih_r = tl.load(w_ih_ptr + r_idx * input_size + offs, mask=mask, other=0.0)\n        r_val += tl.sum(x_val * w_ih_r)\n    \n    for block in range(0, hidden_size, BLOCK_SIZE):\n        offs = block + tl.arange(0, BLOCK_SIZE)\n        mask = offs < hidden_size\n        h_val = tl.load(h_prev_ptr + batch_idx * hidden_size + offs, mask=mask, other=0.0)\n        w_hh_r = tl.load(w_hh_ptr + r_idx * hidden_size + offs, mask=mask, other=0.0)\n        r_val += tl.sum(h_val * w_hh_r)\n    \n    r_val += tl.load(b_ih_ptr + r_idx) + tl.load(b_hh_ptr + r_idx)\n    r_val = tl.sigmoid(r_val)\n\n    # Update gate computation\n    z_idx = hidden_idx + hidden_size\n    z_val = tl.zeros((1,), dtype=tl.float32)\n    for block in range(0, input_size, BLOCK_SIZE):\n        offs = block + tl.arange(0, BLOCK_SIZE)\n        mask = offs < input_size\n        x_val = tl.load(x_ptr + batch_idx * input_size + offs, mask=mask, other=0.0)\n        w_ih_z = tl.load(w_ih_ptr + z_idx * input_size + offs, mask=mask, other=0.0)\n        z_val += tl.sum(x_val * w_ih_z)\n    \n    for block in range(0, hidden_size, BLOCK_SIZE):\n        offs = block + tl.arange(0, BLOCK_SIZE)\n        mask = offs < hidden_size\n        h_val = tl.load(h_prev_ptr + batch_idx * hidden_size + offs, mask=mask, other=0.0)\n        w_hh_z = tl.load(w_hh_ptr + z_idx * hidden_size + offs, mask=mask, other=0.0)\n        z_val += tl.sum(h_val * w_hh_z)\n    \n    z_val += tl.load(b_ih_ptr + z_idx) + tl.load(b_hh_ptr + z_idx)\n    z_val = tl.sigmoid(z_val)\n\n    # New gate computation\n    n_idx = hidden_idx + 2 * hidden_size\n    n_val = tl.zeros((1,), dtype=tl.float32)\n    for block in range(0, input_size, BLOCK_SIZE):\n        offs = block + tl.arange(0, BLOCK_SIZE)\n        mask = offs < input_size\n        x_val = tl.load(x_ptr + batch_idx * input_size + offs, mask=mask, other=0.0)\n        w_ih_n = tl.load(w_ih_ptr + n_idx * input_size + offs, mask=mask, other=0.0)\n        n_val += tl.sum(x_val * w_ih_n)\n    \n    for block in range(0, hidden_size, BLOCK_SIZE):\n        offs = block + tl.arange(0, BLOCK_SIZE)\n        mask = offs < hidden_size\n        h_val = tl.load(h_prev_ptr + batch_idx * hidden_size + offs, mask=mask, other=0.0)\n        w_hh_n = tl.load(w_hh_ptr + n_idx * hidden_size + offs, mask=mask, other=0.0)\n        n_val += r_val * tl.sum(h_val * w_hh_n)\n    \n    n_val += tl.load(b_ih_ptr + n_idx) + tl.load(b_hh_ptr + n_idx) * r_val\n    n_val = tl.tanh(n_val)\n\n    # Hidden state update\n    h_prev_val = tl.load(h_prev_ptr + batch_idx * hidden_size + hidden_idx)\n    h_new_val = (1 - z_val) * n_val + z_val * h_prev_val\n    tl.store(h_new_ptr + batch_idx * hidden_size + hidden_idx, h_new_val)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.batch_first = batch_first\n        \n        # Create GRU layers\n        self.gru_layers = nn.ModuleList()\n        for i in range(num_layers):\n            input_dim = input_size if i == 0 else hidden_size * 2\n            self.gru_layers.append(GRULayer(input_dim, hidden_size, bias))\n        \n        # Initial hidden state\n        self.register_buffer('h0', torch.zeros(num_layers * 2, 1, hidden_size))\n\n    def forward(self, x):\n        if self.batch_first:\n            x = x.transpose(0, 1)\n        \n        batch_size = x.size(1)\n        seq_len = x.size(0)\n        \n        # Initialize hidden states\n        h_states = []\n        for i in range(self.num_layers):\n            h_forward = self.h0[2*i].expand(-1, batch_size, -1).contiguous()\n            h_backward = self.h0[2*i+1].expand(-1, batch_size, -1).contiguous()\n            h_states.append((h_forward, h_backward))\n        \n        # Process each layer\n        for i, layer in enumerate(self.gru_layers):\n            layer_outputs = []\n            for t in range(seq_len):\n                output_forward = layer(x[t], h_states[i][0], direction=0)\n                output_backward = layer(x[seq_len-1-t], h_states[i][1], direction=1)\n                layer_outputs.append((output_forward, output_backward))\n            \n            # Update hidden states and prepare next layer input\n            x_forward = torch.stack([out[0] for out in layer_outputs])\n            x_backward = torch.stack([out[1] for out in layer_outputs[::-1]])\n            x = torch.cat((x_forward, x_backward), dim=-1)\n            \n            # Update hidden states for next layer\n            if i < self.num_layers - 1:\n                new_h_states = []\n                for t in range(seq_len):\n                    new_h_states.append((\n                        layer_outputs[t][0].unsqueeze(0),\n                        layer_outputs[t][1].unsqueeze(0)\n                    ))\n                h_forward = torch.cat([h[0] for h in new_h_states], dim=0)\n                h_backward = torch.cat([h[1] for h in new_h_states], dim=0)\n                h_states[i+1] = (h_forward, h_backward)\n        \n        if self.batch_first:\n            x = x.transpose(0, 1)\n            \n        return x\n\nclass GRULayer(nn.Module):\n    def __init__(self, input_size, hidden_size, bias=True):\n        super(GRULayer, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        # Weight parameters\n        self.weight_ih = nn.Parameter(torch.empty(3 * hidden_size, input_size))\n        self.weight_hh = nn.Parameter(torch.empty(3 * hidden_size, hidden_size))\n        \n        # Bias parameters\n        if bias:\n            self.bias_ih = nn.Parameter(torch.empty(3 * hidden_size))\n            self.bias_hh = nn.Parameter(torch.empty(3 * hidden_size))\n        else:\n            self.register_parameter('bias_ih', None)\n            self.register_parameter('bias_hh', None)\n        \n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight_ih)\n        nn.init.orthogonal_(self.weight_hh)\n        if self.bias_ih is not None:\n            nn.init.zeros_(self.bias_ih)\n            nn.init.zeros_(self.bias_hh)\n\n    def forward(self, x, h_prev, direction=0):\n        batch_size = x.size(0)\n        h_prev = h_prev.squeeze(0)\n        \n        # Allocate output tensor\n        h_new = torch.empty_like(h_prev)\n        \n        # Launch kernel\n        grid = (batch_size * self.hidden_size,)\n        BLOCK_SIZE = 128\n        \n        gru_cell_kernel[grid](\n            x, h_prev, self.weight_ih, self.weight_hh,\n            self.bias_ih if self.bias_ih is not None else tl.zeros(1, dtype=tl.float32),\n            self.bias_hh if self.bias_hh is not None else tl.zeros(1, dtype=tl.float32),\n            h_new,\n            self.input_size, self.hidden_size, batch_size,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return h_new\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.randn(seq_len, batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "3618014a-6d62-4340-b78f-f5182e0759b1", "generation": 2, "timestamp": 1754593184.5796006, "iteration_found": 7, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_41_GRUBirectional", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp4wgxjt96.py\", line 113, in forward\n    h_forward = self.h0[2*i].expand(-1, batch_size, -1).contiguous()\nRuntimeError: The expanded size of the tensor (-1) isn't allowed in a leading, non-existing dimension 0\n", "runtime_error_name": "builtins.RuntimeError", "reference_timing_ms": 68.4, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word '@triton.jit' is not found."}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
{"id": "dcbda15f-1244-45a8-b264-03565cf999fc", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gru_cell_kernel(\n    # Pointers to input tensors\n    x_ptr, h_prev_ptr,\n    # Weight matrices\n    w_ih_ptr, w_hh_ptr,\n    # Output tensor\n    h_new_ptr,\n    # Tensor dimensions and strides\n    input_size, hidden_size, \n    x_batch_stride, x_feature_stride,\n    h_batch_stride, h_feature_stride,\n    w_ih_row_stride, w_ih_col_stride,\n    w_hh_row_stride, w_hh_col_stride,\n    # Blocking parameters\n    BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_K_H: tl.constexpr\n):\n    pid = tl.program_id(0)\n    batch_idx = pid\n    \n    # Initialize pointers for current batch\n    x_ptr += batch_idx * x_batch_stride\n    h_prev_ptr += batch_idx * h_batch_stride\n    h_new_ptr += batch_idx * h_batch_stride\n\n    # Input part: w_ih @ x\n    accum_reset = tl.zeros((hidden_size,), dtype=tl.float32)\n    accum_update = tl.zeros((hidden_size,), dtype=tl.float32)\n    accum_new = tl.zeros((hidden_size,), dtype=tl.float32)\n    \n    # Process input weights in segments\n    for seg in range(3):\n        row_start = seg * hidden_size\n        rows = row_start + tl.arange(0, hidden_size)\n        accum = tl.zeros((hidden_size,), dtype=tl.float32)\n        \n        for k in range(0, input_size, BLOCK_SIZE_K):\n            col_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n            mask_col = col_offsets < input_size\n            \n            # Load w_ih block\n            w_ih_block = tl.load(\n                w_ih_ptr + rows[:, None] * w_ih_row_stride + col_offsets[None, :] * w_ih_col_stride,\n                mask=mask_col[None, :],\n                other=0.0\n            )\n            \n            # Load x block\n            x_block = tl.load(\n                x_ptr + col_offsets * x_feature_stride,\n                mask=mask_col,\n                other=0.0\n            )\n            \n            # Compute partial dot product\n            accum += tl.sum(w_ih_block * x_block[None, :], axis=1)\n        \n        # Assign to appropriate gate\n        if seg == 0:\n            accum_reset = accum\n        elif seg == 1:\n            accum_update = accum\n        else:\n            accum_new = accum\n\n    # Hidden part: w_hh @ h_prev\n    accum_reset_h = tl.zeros((hidden_size,), dtype=tl.float32)\n    accum_update_h = tl.zeros((hidden_size,), dtype=tl.float32)\n    accum_new_h = tl.zeros((hidden_size,), dtype=tl.float32)\n    \n    for seg in range(3):\n        row_start = seg * hidden_size\n        rows = row_start + tl.arange(0, hidden_size)\n        accum = tl.zeros((hidden_size,), dtype=tl.float32)\n        \n        for k in range(0, hidden_size, BLOCK_SIZE_K_H):\n            col_offsets = k + tl.arange(0, BLOCK_SIZE_K_H)\n            mask_col = col_offsets < hidden_size\n            \n            # Load w_hh block\n            w_hh_block = tl.load(\n                w_hh_ptr + rows[:, None] * w_hh_row_stride + col_offsets[None, :] * w_hh_col_stride,\n                mask=mask_col[None, :],\n                other=0.0\n            )\n            \n            # Load h_prev block\n            h_block = tl.load(\n                h_prev_ptr + col_offsets * h_feature_stride,\n                mask=mask_col,\n                other=0.0\n            )\n            \n            # Compute partial dot product\n            accum += tl.sum(w_hh_block * h_block[None, :], axis=1)\n        \n        # Assign to appropriate gate\n        if seg == 0:\n            accum_reset_h = accum\n        elif seg == 1:\n            accum_update_h = accum\n        else:\n            accum_new_h = accum\n\n    # Compute gates\n    r = tl.sigmoid(accum_reset + accum_reset_h)\n    z = tl.sigmoid(accum_update + accum_update_h)\n    n = tl.tanh(accum_new + r * accum_new_h)\n    \n    # Compute new hidden state\n    h_prev = tl.load(h_prev_ptr + tl.arange(0, hidden_size) * h_feature_stride)\n    h_new = (1 - z) * n + z * h_prev\n    \n    # Store result\n    tl.store(h_new_ptr + tl.arange(0, hidden_size) * h_feature_stride, h_new)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        \n        # Initialize weights\n        self.weight_ih = nn.ParameterList()\n        self.weight_hh = nn.ParameterList()\n        \n        for _ in range(num_layers * 2):  # Bidirectional\n            w_ih = nn.Parameter(torch.empty(3 * hidden_size, input_size))\n            w_hh = nn.Parameter(torch.empty(3 * hidden_size, hidden_size))\n            \n            nn.init.xavier_uniform_(w_ih)\n            nn.init.orthogonal_(w_hh)\n            \n            self.weight_ih.append(w_ih)\n            self.weight_hh.append(w_hh)\n            \n            # Next layer input is bidirectional output\n            input_size = hidden_size * 2\n\n    def forward(self, x):\n        if self.batch_first:\n            x = x.transpose(0, 1)\n        \n        seq_len, batch_size, _ = x.shape\n        device = x.device\n        \n        # Initialize hidden states\n        h = torch.zeros((self.num_layers * 2, batch_size, self.hidden_size), device=device)\n        layer_input = x\n        outputs = []\n        \n        for layer in range(self.num_layers):\n            layer_outputs = []\n            for direction in range(2):\n                idx = layer * 2 + direction\n                w_ih = self.weight_ih[idx]\n                w_hh = self.weight_hh[idx]\n                \n                h_prev = h[idx]\n                h_new = torch.zeros((seq_len, batch_size, self.hidden_size), device=device)\n                \n                for t in range(seq_len):\n                    time_step = t if direction == 0 else seq_len - 1 - t\n                    x_t = layer_input[time_step]\n                    \n                    # Launch kernel\n                    grid = (batch_size,)\n                    gru_cell_kernel[grid](\n                        x_t, h_prev,\n                        w_ih, w_hh,\n                        h_new[time_step],\n                        self.input_size, self.hidden_size,\n                        x_t.stride(0), x_t.stride(1),\n                        h_prev.stride(0), h_prev.stride(1),\n                        w_ih.stride(0), w_ih.stride(1),\n                        w_hh.stride(0), w_hh.stride(1),\n                        BLOCK_SIZE_K=64,\n                        BLOCK_SIZE_K_H=64\n                    )\n                    h_prev = h_new[time_step].clone()\n                \n                layer_outputs.append(h_new)\n            \n            # Process directions\n            forward_out, backward_out = layer_outputs\n            backward_out = backward_out.flip(0)\n            layer_output = torch.cat((forward_out, backward_out), dim=-1)\n            outputs.append(layer_output)\n            layer_input = layer_output\n        \n        # Final output (last layer)\n        output = torch.stack(outputs)[-1]\n        \n        if self.batch_first:\n            output = output.transpose(0, 1)\n        \n        return output\n\n# Test code\nbatch_size = 10\nseq_len = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\n\ndef get_inputs():\n    return [torch.randn(seq_len, batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "75af473a-78b1-4504-8d45-cf71d4b86430", "generation": 2, "timestamp": 1754594674.4767992, "iteration_found": 13, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_41_GRUBirectional", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpa6cci1di.py\", line 178, in forward\n    gru_cell_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 68.4, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_41_GRUBirectional", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "max_difference": ["3.133962", "3.113822", "3.129310", "3.143969", "3.112050"], "avg_difference": ["0.005978", "0.005977", "0.005996", "0.005991", "0.005991"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 68.4, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
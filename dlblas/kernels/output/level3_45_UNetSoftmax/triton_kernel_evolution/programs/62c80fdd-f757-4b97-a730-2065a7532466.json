{"id": "62c80fdd-f757-4b97-a730-2065a7532466", "code": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel(\n    output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols, \n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    row_start = row_idx * input_row_stride\n    row_start_output = row_idx * output_row_stride\n    \n    # Initialize max and sum\n    row_max = tl.full((), -float('inf'), dtype=tl.float32)\n    row_sum = tl.full((), 0.0, dtype=tl.float32)\n    \n    # First loop: compute row_max\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_offsets = tl.arange(0, BLOCK_SIZE) + col_offset\n        mask = col_offsets < n_cols\n        input_ptrs = input_ptr + row_start + col_offsets\n        chunk = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n        chunk_max = tl.max(chunk, axis=0)\n        row_max = tl.maximum(row_max, chunk_max)\n    \n    # Second loop: compute row_sum\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_offsets = tl.arange(0, BLOCK_SIZE) + col_offset\n        mask = col_offsets < n_cols\n        input_ptrs = input_ptr + row_start + col_offsets\n        chunk = tl.load(input_ptrs, mask=mask, other=0)\n        exp_chunk = tl.exp(chunk - row_max)\n        chunk_sum = tl.sum(exp_chunk, axis=0)\n        row_sum += chunk_sum\n    \n    # Third loop: compute softmax and store\n    for col_offset in range(0, n_cols, BLOCK_SIZE):\n        col_offsets = tl.arange(0, BLOCK_SIZE) + col_offset\n        mask = col_offsets < n_cols\n        input_ptrs = input_ptr + row_start + col_offsets\n        chunk = tl.load(input_ptrs, mask=mask, other=0)\n        exp_chunk = tl.exp(chunk - row_max)\n        softmax_output = exp_chunk / row_sum\n        output_ptrs = output_ptr + row_start_output + col_offsets\n        tl.store(output_ptrs, softmax_output, mask=mask)\n\nclass TritonSoftmax(nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, x):\n        # Ensure contiguous memory\n        x_contig = x.contiguous()\n        # Save original shape\n        original_shape = x_contig.shape\n        # Flatten all dimensions except the last\n        if x_contig.dim() > 2:\n            x_2d = x_contig.view(-1, original_shape[-1])\n        else:\n            x_2d = x_contig\n        \n        # If the tensor is 1D, then we treat as 2D with one row\n        if x_2d.dim() == 1:\n            x_2d = x_2d.unsqueeze(0)\n        \n        n_rows, n_cols = x_2d.shape\n        output = torch.empty_like(x_2d)\n        \n        # Use fixed block size for chunk processing\n        BLOCK_SIZE = 1024\n        \n        # The kernel uses the grid of n_rows\n        grid = (n_rows,)\n        \n        input_row_stride = x_2d.stride(0)\n        output_row_stride = output.stride(0)\n        \n        # Launch kernel\n        softmax_kernel[grid](\n            output, x_2d, \n            input_row_stride, output_row_stride, \n            n_cols, \n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        # Reshape back to original\n        return output.view(original_shape)", "language": "python", "parent_id": "10ef2b3f-2f5b-443c-ba11-1f292ad737a3", "generation": 2, "timestamp": 1754594618.8059838, "iteration_found": 10, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.341, "runtime_ms": 5.07, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.73, "speed_up": 0.341, "custom_timing": 5.07}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.339, "runtime_ms": 5.1, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.73, "speed_up": 0.339, "custom_timing": 5.1}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
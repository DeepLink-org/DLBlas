{"id": "764281d1-e15c-42ec-839c-708eddca2b73", "code": "class ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        # Instead of nn.Linear, we store weight and bias\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        # Initialize the parameters\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        # Instead of self.gemm(x), we do:\n        x = triton_matmul_with_bias(x, self.weight, self.bias)\n        x = self.bn(x)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x", "language": "python", "parent_id": "dccd171c-ecd9-4154-bf2a-afffa4731a0c", "generation": 1, "timestamp": 1754578404.7777321, "iteration_found": 2, "metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0967, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0948, "speed_up": 0.98, "custom_timing": 0.0967}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0967\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': 0.98, 'custom_timing': 0.0967}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0967, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': 0.98, 'custom_timing': 0.0967}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), Batch Normalization, scaling, and Softmax.\n    \"\"\"\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = self.bn(x)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0967), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': 0.98, 'custom_timing': 0.0967})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), Batch Normalization, scaling, and Softmax.\n    \"\"\"\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.gemm(x)\n        x = self.bn(x)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear layer (GEMM)\n  2. BatchNorm1d\n  3. Scaling by a parameter\n  4. Softmax\n\nThe goal is to rewrite this model using Triton for the GEMM and softmax steps to gain speed.\n\nHowever, note that the current implementation is entirely in PyTorch and we are to replace it with a Triton version.\n\nBut note: the problem states that we are to generate Triton implementations (custom_code) for the operations.\n\nHowever, the entire model is a sequence of operations. We cannot rewrite the entire model in a single Triton kernel? \n\nBut note: the problem says we are to replace the reference_code (which is the PyTorch code) with custom_code (Triton).\n\nWe are allowed to change only the Triton kernel source within the EVOLVE-BLOCK.\n\nLooking at the example provided (per_row_quantize_int8), we see that we can write a Triton kernel and then call it in a function.\n\nSo, we can break down the model into operations that can be fused or optimized with Triton.\n\nBut note: the model has a linear layer (GEMM), then batch norm, then scaling, then softmax.\n\nWe can consider:\n  1. Fusing the GEMM with the subsequent operations? But batch norm is a separate layer and requires statistics.\n\nHowever, note that batch norm is applied per feature (over the batch dimension). So for a given feature, we subtract the mean and divide by the standard deviation (with some epsilon) and then scale by gamma and add beta.\n\nBut in our model, the batch norm is `nn.BatchNorm1d(out_features)`. So it has parameters: weight (gamma) and bias (beta) and also running mean and variance.\n\nWe are in training? The problem does not specify. But note: the reference code uses `nn.BatchNorm1d` which in training mode uses the batch statistics and updates the running stats.\n\nWe cannot easily fuse the entire model in one kernel because of the batch norm requiring the entire batch to compute mean and variance.\n\nBut we can break the model into two parts:\n  - The GEMM: we can write a Triton GEMM kernel.\n  - The batch norm: we can do with PyTorch or write a Triton kernel? But note the batch norm requires the mean and variance of the entire batch for each feature.\n\nHowever, the problem says: \"Generate functionally equivalent but faster Triton implementations\".\n\nWe are allowed to change only the Triton kernel source within the block. So we can write a Triton kernel for the GEMM and then use the existing PyTorch for the rest? But then we have to consider the data movement.\n\nAlternatively, we can write a kernel that does the GEMM and then the softmax? But the batch norm and scaling are in between.\n\nBut note: the problem says we are to rewrite the entire model? Actually, the reference code is the entire model. We are to replace it with a Triton version that is faster.\n\nBut the example only replaced the quantization function.\n\nSo we can try to replace the entire forward pass with a single Triton kernel? That might be challenging because of the batch norm.\n\nAlternatively, we can break the model into operations and replace the most expensive ones (like GEMM) with Triton.\n\nGiven that the GEMM is the most expensive operation, we can replace the `nn.Linear` with a Triton GEMM.\n\nHowever, note that the model also has a softmax at the end. Softmax can be expensive and we can also write a Triton kernel for that.\n\nBut the batch norm and scaling are element-wise and relatively cheap.\n\nApproach:\n\nWe will create two Triton kernels:\n  1. A GEMM kernel for the linear layer.\n  2. A fused kernel for the batch norm, scaling, and softmax? Or we can do the batch norm and scaling in PyTorch and then write a Triton softmax.\n\nBut note: the batch norm requires the mean and variance of the entire batch for each feature. This requires a reduction over the batch. We can do that in a separate kernel or use PyTorch's batch norm.\n\nHowever, the problem says: \"functional equivalence\". The batch norm in training mode uses the current batch statistics and updates the running statistics. We must replicate that.\n\nBut note: the problem does not specify if we are in training or evaluation. The reference code uses `nn.Module` and so it will behave according to `self.training`.\n\nWe cannot change the interface. We are to write a model that has the same forward pass.\n\nGiven the complexity, we might start by replacing only the GEMM with a Triton kernel and leave the rest in PyTorch.\n\nBut note: the problem says we are to rewrite the entire model? Actually, the current code block defines a `ModelNew` class and some global variables.\n\nWe are to rewrite the `ModelNew` class to use Triton for the GEMM.\n\nHowever, we are not allowed to change the function signatures or the grid configuration? Actually, we are allowed to change the Triton kernel source but the rest of the code (non-Triton) is locked? \n\nLooking at the constraints: \n  \u274c NEVER change @triton.jit function signatures or parameter names  \n  \u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\nBut in the example, they defined a new function `per_row_quantize_int8` that uses the Triton kernel.\n\nSo we are allowed to change the `ModelNew` class to use our Triton kernel for the linear layer.\n\nPlan:\n\n  1. Write a Triton kernel for the matrix multiplication (GEMM) of the linear layer.\n  2. Replace the `self.gemm` in the forward pass with a call to our Triton GEMM.\n\nBut note: the linear layer also has a bias? In the reference, `nn.Linear` has a bias by default. We must include it.\n\nHowever, in the reference code, the linear layer is applied and then the batch norm. The batch norm will normalize the output and then scale and shift. So the bias of the linear layer is redundant? Actually, no. The linear layer's bias is added and then the batch norm will subtract the mean and then apply its own bias (beta). So we can set the linear layer to have no bias and let the batch norm handle the bias? But that would change the model.\n\nAlternatively, we can leave the bias in the linear layer and then the batch norm will normalize it. But note: the batch norm will center the data (subtract mean) and then scale by gamma and add beta. So the linear bias becomes part of the centered data.\n\nWe can set the linear layer to have no bias and then rely on the batch norm's beta? But that would be equivalent only if we adjust the parameters.\n\nTo keep functional equivalence, we must use the same parameters. The reference model has a linear layer with weight and bias, and the batch norm has weight, bias, running mean, and running variance.\n\nTherefore, we must include the linear layer's bias.\n\nWe can write a Triton GEMM kernel that supports bias.\n\nBut note: the batch norm operation in the reference is applied after the linear. So the input to the batch norm is: `x = self.gemm(x)`.\n\nWe are going to replace `self.gemm(x)` with a Triton GEMM that does: `x = x @ weight.t() + bias`.\n\nSteps:\n\n  - We will write a Triton kernel for `matmul_with_bias`.\n  - We will remove the `nn.Linear` and instead store the weight and bias as parameters and use our Triton kernel.\n\nBut note: the batch norm is still done by PyTorch.\n\nWe can write:\n\n```python\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        # Instead of nn.Linear, we store weight and bias\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        # Initialize the parameters\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        # Instead of self.gemm(x), we do:\n        x = triton_matmul_with_bias(x, self.weight, self.bias)\n        x = self.bn(x)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n```\n\nNow, we need to write `triton_matmul_with_bias`.\n\nWe note that the input `x` is of shape (batch_size, in_features) and weight is (out_features, in_features). The output should be (batch_size, out_features).\n\nWe can write a Triton kernel for this matrix multiplication.\n\nBut note: the matrix multiplication is: \n  - Input: (M, K) = (batch_size, in_features)\n  - Weight: (N, K) = (out_features, in_features) -> but we need to transpose to (K, N) for matmul? Actually, we do: x @ weight.t()\n\nSo we are computing: output = x @ weight.t() + bias.\n\nWe can write a kernel that does:\n\n  for each row i in [0, M-1] and each column j in [0, N-1]:\n      output[i, j] = bias[j] + sum_{k} x[i, k] * weight[j, k]\n\nWe can use a 2D grid: one program per output element? That would be too many. Instead, we use blockings.\n\nWe can use a 2D grid where each block computes a block of the output matrix.\n\nAlternatively, we can use a 1D grid over the rows and have each block compute one row? Then we can load the entire row of x and then for each column block, we compute a segment of the row? But then we have to reduce over the column blocks? Actually, we can do:\n\n  Each program handles one row.\n\n  We can have:\n      pid = tl.program_id(0)\n      if pid >= M: return\n\n  Then we load the row of x (pid-th row) and then for each column j in [0, N-1], we compute the dot product of the row and the j-th column of weight.\n\n  But note: the weight is stored as (out_features, in_features). So the j-th row of weight is the j-th output feature.\n\n  So we can do:\n\n      for j in range(0, N):\n          acc = 0\n          for k in range(0, K):\n              acc += x[pid, k] * weight[j, k]\n          output[pid, j] = acc + bias[j]\n\n  But this is O(N*K) per row and would be inefficient.\n\nWe need to use tiling and shared memory.\n\nWe can write a kernel that uses tiling in the K dimension and uses a block of columns.\n\nLet's design:\n\n  - We assign each program to compute a block of columns for a set of rows? Actually, we can do:\n\n      BLOCK_SIZE_M = 16  # number of rows per block\n      BLOCK_SIZE_N = 64  # number of columns per block\n\n  But note: the matrix is (M, N) and we have to compute the entire matrix.\n\n  We can have a 2D grid: \n      pid_m = tl.program_id(0)\n      pid_n = tl.program_id(1)\n\n  Then the block of rows: [pid_m * BLOCK_SIZE_M, (pid_m+1)*BLOCK_SIZE_M] and block of columns: [pid_n * BLOCK_SIZE_N, (pid_n+1)*BLOCK_SIZE_N].\n\n  Then we accumulate over the K dimension in blocks.\n\nBut note: the bias is per column. We can add it at the end.\n\nHowever, we are limited by the constraints: we cannot change the grid configuration? Actually, we are defining the grid. The constraint says: \"NEVER modify grid configuration\" in the context of the Triton kernel? Actually, the constraint is for the existing grid configuration? \n\nWe are writing a new kernel, so we are free to define the grid.\n\nBut note: the constraint says: \"NEVER modify grid configuration\" meaning if the kernel already has a grid, we cannot change it? But in this case, the kernel is new.\n\nSo we design:\n\n  Kernel: \n      input: x_ptr, weight_ptr, bias_ptr, output_ptr, M, N, K, stride_xm, stride_xk, stride_wn, stride_wk, stride_om, stride_on\n\n  We use:\n      BLOCK_SIZE_M: tl.constexpr\n      BLOCK_SIZE_N: tl.constexpr\n      BLOCK_SIZE_K: tl.constexpr\n\n  Steps:\n      pid_m = tl.program_id(0)\n      pid_n = tl.program_id(1)\n\n      # Create ranges for the block\n      rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n      rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n      # Create masks for the rows and columns that are within bounds\n      mask_m = rm < M\n      mask_n = rn < N\n\n      # Initialize accumulator to zero\n      acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n      # Loop over K in blocks of BLOCK_SIZE_K\n      for k in range(0, K, BLOCK_SIZE_K):\n          rk = k + tl.arange(0, BLOCK_SIZE_K)\n          mask_k = rk < K\n\n          # Load a block of x: [BLOCK_SIZE_M, BLOCK_SIZE_K]\n          x_val = tl.load(x_ptr + rm[:, None] * stride_xm + rk[None, :] * stride_xk,\n                          mask=mask_m[:, None] & mask_k[None, :],\n                          other=0.0)\n\n          # Load a block of weight: [BLOCK_SIZE_N, BLOCK_SIZE_K] (note: weight is stored as [N, K])\n          w_val = tl.load(weight_ptr + rn[:, None] * stride_wn + rk[None, :] * stride_wk,\n                          mask=mask_n[:, None] & mask_k[None, :],\n                          other=0.0)\n\n          # Compute the block matrix multiplication\n          acc += tl.dot(x_val, w_val, allow_tf32=True)\n\n      # Now, we have the accumulator for this block. We need to add the bias for the columns.\n      # Bias is of shape (N,). We load a block of bias: [BLOCK_SIZE_N]\n      bias_val = tl.load(bias_ptr + rn, mask=mask_n, other=0.0)\n      acc += bias_val[None, :]  # add to each row in the block\n\n      # Now, write the block to the output.\n      tl.store(output_ptr + rm[:, None] * stride_om + rn[None, :] * stride_on,\n               acc,\n               mask=mask_m[:, None] & mask_n[None, :])\n\nBut note: the above kernel uses `tl.dot` which requires the dimensions to be multiples of 16 for TF32? We are using `allow_tf32=True` to allow Tensor Cores.\n\nWe must choose BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K appropriately.\n\nWe can use autotune for the block sizes.\n\nBut note: the problem says we can use adaptive tuning.\n\nWe will write the kernel with tunable block sizes.\n\nHowever, note that the input `x` and `weight` are in float32? The reference uses `nn.Linear` which by default is float32. But the Triton kernel uses float32 for accumulation? We are using `tl.float32` for the accumulator.\n\nBut the `tl.dot` will use tf32 if allowed and if the hardware supports it (H100 does).\n\nWe must also note: the batch norm expects float32.\n\nNow, we must write the function `triton_matmul_with_bias` that calls this kernel.\n\nWe have to set the grid:\n\n      grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n\nBut note: the kernel above is for a 2D grid.\n\nWe will write:\n\n```python\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _matmul_with_bias_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    # ... kernel code as above ...\n\ndef triton_matmul_with_bias(x, weight, bias):\n    # Check shapes\n    assert x.dim() == 2\n    M, K = x.shape\n    N, K_ = weight.shape\n    assert K == K_, f\"Incompatible dimensions: {K} vs {K_}\"\n    assert bias.shape == (N,), f\"Bias shape {bias.shape} != ({N},)\"\n\n    # Allocate output\n    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n\n    # 2D launch grid\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch kernel\n    _matmul_with_bias_kernel[grid](\n        x, weight, bias, output,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K  # but these are set by autotune\n    )\n    return output\n```\n\nBut note: the autotune will set the BLOCK_SIZE_* via the configs. We don't pass them explicitly? Actually, the autotune will override the values.\n\nNow, we replace the `self.gemm` in the model with this Triton-based matmul.\n\nBut note: the reference model used `nn.Linear` which has an initialization. We have to mimic the same initialization.\n\nWe did that in the `__init__` by using `kaiming_uniform_` and initializing the bias.\n\nBut note: the original `nn.Linear` uses a different initialization? Actually, the `nn.Linear` uses `kaiming_uniform_` for the weight and a bound for the bias? We did the same.\n\nSo we are safe.\n\nHowever, note that the batch norm is still in PyTorch. We are only replacing the linear layer.\n\nWe hope that the Triton GEMM will be faster than PyTorch's GEMM.\n\nBut note: the problem says the current speed_up is 0.98 (which is less than 1). So the current implementation (with PyTorch) is slightly faster? That might be because the Triton kernel is not optimized.\n\nWe must tune the kernel.\n\nAlternatively, we can use Triton's built-in matmul kernel? But the problem requires we write our own.\n\nBut note: the example provided in the problem (per_row_quantize_int8) used a Triton kernel. We are following the same pattern.\n\nWe will write the kernel as above.\n\nBut note: the kernel above does not handle the case when BLOCK_SIZE_K is larger than the remaining K? Actually, we have a loop that steps by BLOCK_SIZE_K and we use a mask for the last block.\n\nWe must be cautious: the kernel uses `tl.dot` which requires the inner dimension to be a multiple of 16? Actually, it doesn't require it but it will be inefficient if not. We are using masks so it should be safe.\n\nWe will write the kernel and hope it compiles and is correct.\n\nNow, we must include the Triton kernel and the helper function inside the EVOLVE-BLOCK? But the problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\nSo we have to put everything inside the block.\n\nBut note: the current program has:\n   - imports: torch, nn\n   - class ModelNew\n   - global variables: batch_size, in_features, out_features, etc.\n   - get_inputs, get_init_inputs\n\nWe are going to change the ModelNew class and also add the Triton kernel and the helper function.\n\nWe must also import triton and triton.language.\n\nBut note: the current program does not import triton. We must add the import.\n\nHowever, the problem says: \"NO explanations, NO markdown, NO additional text.\" and we are to return the entire block.\n\nSo we will write:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Triton kernel for matrix multiplication with bias\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _matmul_with_bias_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Create ranges for the block\n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    rk = tl.arange(0, BLOCK_SIZE_K)\n\n    # Create masks for the rows and columns that are within bounds\n    mask_m = rm < M\n    mask_n = rn < N\n\n    # Initialize accumulator to zero\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Loop over K in blocks of BLOCK_SIZE_K\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        # Compute the current block of K\n        k_offset = k * BLOCK_SIZE_K\n        rk_curr = k_offset + rk\n        mask_k = rk_curr < K\n\n        # Load a block of x: [BLOCK_SIZE_M, BLOCK_SIZE_K]\n        x_val = tl.load(x_ptr + rm[:, None] * stride_xm + rk_curr[None, :] * stride_xk,\n                        mask=mask_m[:, None] & mask_k[None, :],\n                        other=0.0)\n\n        # Load a block of weight: [BLOCK_SIZE_N, BLOCK_SIZE_K] (note: weight is stored as [N, K])\n        # But note: weight_ptr + row (rn) and column (rk_curr). The stride for row is stride_wn and for column is stride_wk.\n        w_val = tl.load(weight_ptr + rn[:, None] * stride_wn + rk_curr[None, :] * stride_wk,\n                        mask=mask_n[:, None] & mask_k[None, :],\n                        other=0.0)\n\n        # We need to transpose w_val to [BLOCK_SIZE_K, BLOCK_SIZE_N] for the dot product? \n        # But note: the dot product in Triton expects the second matrix to be transposed? \n        # Actually, the dot product in Triton is: \n        #   tl.dot(a, b) where a is (M, K) and b is (K, N) -> result (M, N)\n        # But our w_val is (BLOCK_SIZE_N, BLOCK_SIZE_K) -> we need to transpose it to (BLOCK_SIZE_K, BLOCK_SIZE_N)\n        w_val = tl.trans(w_val)\n\n        # Compute the block matrix multiplication\n        acc += tl.dot(x_val, w_val, allow_tf32=True)\n\n    # Now, we have the accumulator for this block. We need to add the bias for the columns.\n    # Bias is of shape (N,). We load a block of bias: [BLOCK_SIZE_N]\n    bias_val = tl.load(bias_ptr + rn, mask=mask_n, other=0.0)\n    acc += bias_val[None, :]  # add to each row in the block\n\n    # Now, write the block to the output.\n    tl.store(output_ptr + rm[:, None] * stride_om + rn[None, :] * stride_on,\n             acc,\n             mask=mask_m[:, None] & mask_n[None, :])\n\ndef triton_matmul_with_bias(x, weight, bias):\n    # Check shapes\n    assert x.dim() == 2\n    M, K = x.shape\n    N, K_ = weight.shape\n    assert K == K_, f\"Incompatible dimensions: {K} vs {K_}\"\n    assert bias.shape == (N,), f\"Bias shape {bias.shape} != ({N},)\"\n\n    # Allocate output\n    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n\n    # 2D launch grid\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch kernel\n    _matmul_with_bias_kernel[grid](\n        x, weight, bias, output,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE_M=triton.next_power_of_2(BLOCK_SIZE_M),  # but note: the autotune will set this, so we don't need to pass? Actually, we pass the block sizes via the autotune configs.\n        # However, the kernel expects BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K as constexpr. The autotune will override the values we pass? Actually, we don't pass them here because they are in the autotune configs.\n        # We can pass them as the ones in the config, but the autotune will choose. We must pass them as the ones that are set by autotune? \n        # Actually, the autotune system will replace the values in the kernel call. So we can pass dummy values? \n        # But the kernel requires them. We can set them to the values that are in the first config? But the autotune will override.\n        # Alternatively, we can remove them from the kernel call and let the autotune pass them? \n        # The Triton autotune handles that. We don't pass the constexpr arguments explicitly? \n        # Actually, the autotune will generate multiple kernels and choose the best. We just call the kernel with the grid and the non-constexpr arguments.\n        # So we can skip passing the BLOCK_SIZE_*? \n        # But the kernel signature requires them. We must pass them? \n        # However, the autotune will replace the call with the specific config. So we don't pass them at the call site? \n        # Let me check: in the example, they passed the BLOCK_SIZE and P2? \n        # They passed: `_quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)`\n        # So we must pass the constexpr arguments that are not in the autotune config? \n        # But note: the autotune configs set the constexpr arguments. So we can pass them as the ones we want to try? \n        # Actually, the autotune system will try the configs and then we don't need to pass them? \n        # This is confusing.\n\n        # How about we remove the BLOCK_SIZE_* from the kernel call? \n        # The Triton documentation says: \"When autotuning is used, the values of the tuned parameters are passed as keyword arguments to the kernel at launch time.\"\n        # So we don't pass them? \n        # We only pass the non-tuned arguments? \n        # But our kernel has non-tuned arguments: M, N, K, strides, etc. and the tuned ones: BLOCK_SIZE_*.\n\n        # So we call without the BLOCK_SIZE_*? \n    )\n    return output\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication (Gemm), Batch Normalization, scaling, and Softmax.\n    \"\"\"\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        # We replace the linear layer with our own parameters\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        # Initialize the parameters\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        # Use our Triton matmul with bias\n        x = triton_matmul_with_bias(x, self.weight, self.bias)\n        x = self.bn(x)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\nBut note: the kernel above has a mistake: we are transposing w_val but then we use it in `tl.dot(x_val, w_val)`. But `x_val` is (BLOCK_SIZE_M, BLOCK_SIZE_K) and `w_val` is (BLOCK_SIZE_N, BLOCK_SIZE_K) and then we transpose it to (BLOCK_SIZE_K, BLOCK_SIZE_N). Then the dot product is (M, K) @ (K, N) -> (M, N). That is correct.\n\nAlternatively, we could have not transposed and used `tl.dot(x_val, w_val, trans_b=True)`? But Triton's dot does not have a trans_b option. So we transpose manually.\n\nBut note: the kernel loop might be inefficient because we are doing a small dot product? The block sizes are fixed per program.\n\nWe hope the autotune will choose good block sizes.\n\nWe must also note: the kernel uses `tl.cdiv(K, BLOCK_SIZE_K)` for the loop? Actually, we should loop `for k in range(0, K, BLOCK_SIZE_K)` and then use the current offset. But in the kernel, we did:\n\n      for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n          k_offset = k * BLOCK_SIZE_K\n          ...\n\nThat is equivalent.\n\nBut note: the mask for `rk_curr` is `rk_curr < K`, which is correct.\n\nWe hope this kernel is correct and faster.\n\nIf there are issues, we might have to debug.\n\nBut note: the problem says to make small iterative steps. We are replacing the entire linear layer with a Triton kernel. This is a significant change.\n\nWe are also adding the Triton imports.\n\nWe must also note: the original model used `nn.Linear` which might have been faster because of cuBLAS? We hope that our Triton kernel, with autotune and using Tensor Cores, can be competitive.\n\nLet's hope it compiles and runs correctly.\n\nWe output the entire block.\n</think>\nTo improve performance while maintaining functional equivalence, the key optimization is replacing the PyTorch linear layer with a Triton-based matrix multiplication kernel. This leverages NVIDIA H100/H800 architecture features like Tensor Cores and memory coalescing for faster computation. The Triton kernel uses autotuning to select optimal block sizes for different input dimensions and includes bias addition.\n\nHere's the complete implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K - k)\n        b_mask = (offs_k[:, None] < K - k) & (offs_bn[None, :] < N)\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    c_ptrs = c_ptr + offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn\n    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\ndef triton_matmul(a, b, bias=None):\n    assert a.dim() == 2 and b.dim() == 2\n    M, K = a.shape\n    K_, N = b.shape\n    assert K == K_, \"Incompatible dimensions\"\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']),)\n    \n    matmul_kernel[grid](\n        a, b, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = triton_matmul(x, self.weight.t(), self.bias)\n        x = self.bn(x)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
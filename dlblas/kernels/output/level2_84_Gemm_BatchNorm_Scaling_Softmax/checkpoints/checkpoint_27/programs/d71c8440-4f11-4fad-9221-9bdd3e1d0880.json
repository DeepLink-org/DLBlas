{"id": "d71c8440-4f11-4fad-9221-9bdd3e1d0880", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 64}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_kernel(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // grid_n\n    pid_n = pid % grid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        \n        x_mask = (offs_m[:, None] < M) & k_mask[None, :]\n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        \n        w_mask = k_mask[:, None] & (offs_n[None, :] < N)\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x, w)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    \n    if bias_ptr is not None:\n        b = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += b[None, :]\n    \n    out_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        x_contig = x.contiguous()\n        weight_contig = self.weight.contiguous()\n        bias_contig = self.bias.contiguous()\n        \n        output = torch.empty(M, N, device=x.device, dtype=torch.float32)\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        \n        gemm_kernel[grid](\n            x_contig, weight_contig, bias_contig, output,\n            M, N, K,\n            x_contig.stride(0), x_contig.stride(1),\n            weight_contig.stride(0), weight_contig.stride(1),\n            output.stride(0), output.stride(1)\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "97c8e181-2380-4770-b87f-2f372a674cfa", "generation": 4, "timestamp": 1754584261.7613842, "iteration_found": 26, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.682, "runtime_ms": 0.139, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0948, "speed_up": 0.682, "custom_timing": 0.139}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_84_Gemm_BatchNorm_Scaling_Softmax", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpcl6fqn_g.py\", line 99, in forward\n    fused_gemm_bn_scale_softmax[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 610, in run\n    grid = grid(bound_args)\n  File \"/tmp/tmpcl6fqn_g.py\", line 97, in <lambda>\n    grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\nAttributeError: 'dict' object has no attribute 'BLOCK_M'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 0.0948, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 0.0000\n- score: 10.0000\n- stage: wrong_answer\n- verify_payload: {'id': 'level2_84_Gemm_BatchNorm_Scaling_Softmax', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpcl6fqn_g.py\", line 99, in forward\\n    fused_gemm_bn_scale_softmax[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 610, in run\\n    grid = grid(bound_args)\\n  File \"/tmp/tmpcl6fqn_g.py\", line 97, in <lambda>\\n    grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\\nAttributeError: \\'dict\\' object has no attribute \\'BLOCK_M\\'\\n', 'runtime_error_name': 'builtins.AttributeError', 'reference_timing_ms': 0.0948, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.6820, runtime_ms: 0.1390, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': 0.682, 'custom_timing': 0.139}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7580, runtime_ms: 0.1250, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': 0.758, 'custom_timing': 0.125}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.6870, runtime_ms: 0.1380, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': 0.687, 'custom_timing': 0.138}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Triton kernel for matrix multiplication with bias\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 64}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_kernel(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // grid_n\n    pid_n = pid % grid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        \n        x_mask = (offs_m[:, None] < M) & k_mask[None, :]\n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        \n        w_mask = k_mask[:, None] & (offs_n[None, :] < N)\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x, w)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    \n    if bias_ptr is not None:\n        b = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += b[None, :]\n    \n    out_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        x_contig = x.contiguous()\n        weight_contig = self.weight.contiguous()\n        bias_contig = self.bias.contiguous()\n        \n        output = torch.empty(M, N, device=x.device, dtype=torch.float32)\n        \n        grid = (triton.cdiv(M, 64) * triton.cdiv(N, 64),)\n        \n        gemm_kernel[grid](\n            x_contig, weight_contig, bias_contig, output,\n            M, N, K,\n            x_contig.stride(0), x_contig.stride(1),\n            weight_contig.stride(0), weight_contig.stride(1),\n            output.stride(0), output.stride(1)\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.6870), Performs well on runtime_ms (0.1380), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': 0.687, 'custom_timing': 0.138})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // grid_n\n    pid_n = pid % grid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        \n        x_mask = (offs_m[:, None] < M) & k_mask[None, :]\n        x_val = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        x_f16 = x_val.to(tl.float16)\n        \n        w_mask = k_mask[:, None] & (offs_n[None, :] < N)\n        w_val = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        w_f16 = w_val.to(tl.float16)\n        \n        acc += tl.dot(x_f16, w_f16)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    \n    if bias_ptr is not None:\n        b = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += b[None, :]\n    \n    offs_m = tl.max_contiguous(tl.multiple_of(offs_m, BLOCK_M), BLOCK_M)\n    offs_n = tl.max_contiguous(tl.multiple_of(offs_n, BLOCK_N), BLOCK_N)\n    \n    out_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        x_contig = x.contiguous()\n        weight_contig = self.weight.contiguous()\n        bias_contig = self.bias.contiguous()\n        \n        output = torch.empty(M, N, device=x.device, dtype=torch.float32)\n        \n        BLOCK_M, BLOCK_N, BLOCK_K = 128, 128, 32\n        grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n        \n        gemm_kernel[grid](\n            x_contig, weight_contig, bias_contig, output,\n            M, N, K,\n            x_contig.stride(0), x_contig.stride(1),\n            weight_contig.stride(0), weight_contig.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_M, BLOCK_N, BLOCK_K\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7580), Performs well on runtime_ms (0.1250), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': 0.758, 'custom_timing': 0.125})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_bn[None, :] < N), other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        weight_t = self.weight.t().contiguous()\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        gemm_kernel[grid](\n            x, weight_t, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32,\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        return self.softmax(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.6820), Performs well on runtime_ms (0.1390), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': 0.682, 'custom_timing': 0.139})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_bn[None, :] < N), other=0.0)\n        \n        # Convert to FP16 to leverage Tensor Cores\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        accumulator += tl.dot(a_f16, b_f16)\n        \n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        weight_t = self.weight.t().contiguous()\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        gemm_kernel[grid](\n            x, weight_t, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=32,\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        return self.softmax(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.randn(out_features))\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        weight_t = self.weight.t().contiguous()\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        gemm_kernel[grid](\n            x, weight_t, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32,\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        return self.softmax(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_bn[None, :] < N), other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        weight_t = self.weight.t().contiguous()\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        gemm_kernel[grid](\n            x, weight_t, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32,\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        return self.softmax(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Triton kernel for matrix multiplication with bias\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 64}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_kernel(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // grid_n\n    pid_n = pid % grid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        \n        x_mask = (offs_m[:, None] < M) & k_mask[None, :]\n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        \n        w_mask = k_mask[:, None] & (offs_n[None, :] < N)\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x, w)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    \n    if bias_ptr is not None:\n        b = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += b[None, :]\n    \n    out_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        x_contig = x.contiguous()\n        weight_contig = self.weight.contiguous()\n        bias_contig = self.bias.contiguous()\n        \n        output = torch.empty(M, N, device=x.device, dtype=torch.float32)\n        \n        grid = (triton.cdiv(M, 64) * triton.cdiv(N, 64),)\n        \n        gemm_kernel[grid](\n            x_contig, weight_contig, bias_contig, output,\n            M, N, K,\n            x_contig.stride(0), x_contig.stride(1),\n            weight_contig.stride(0), weight_contig.stride(1),\n            output.stride(0), output.stride(1)\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 3 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_bn[None, :] < N), other=0.0)\n        \n        # Convert to FP16 to leverage Tensor Cores\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        accumulator += tl.dot(a_f16, b_f16)\n        \n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        weight_t = self.weight.t().contiguous()\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        gemm_kernel[grid](\n            x, weight_t, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=32,\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        return self.softmax(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Triton kernel for GEMM: y = x @ w + bias\n@triton.jit\ndef _triton_gemm(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wk, stride_wn,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr, \n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    grid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    grid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // grid_n\n    pid_n = pid % grid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Pointers for x: [M, K]\n    x_ptrs = x_ptr + (offs_am[:, None] * stride_xm + offs_k[None, :] * stride_xk)\n    # Pointers for w: [K, N]\n    w_ptrs = w_ptr + (offs_k[:, None] * stride_wk + offs_bn[None, :] * stride_wn)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load blocks of x and weight with masking\n        mask_x = (offs_am[:, None] < M) & (offs_k[None, :] < K - k)\n        x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n        mask_w = (offs_k[:, None] < K - k) & (offs_bn[None, :] < N)\n        w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n\n        acc += tl.dot(x, w)\n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n\n    # Add bias\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        acc += bias[None, :]\n\n    # Write back results with masking\n    offs_om = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_on = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask_out = (offs_om[:, None] < M) & (offs_on[None, :] < N)\n    tl.store(output_ptr + (offs_om[:, None] * stride_om + offs_on[None, :] * stride_on), acc, mask=mask_out)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(in_features, out_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        # Initialize parameters (PyTorch's Linear default initialization)\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in = in_features\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n\n        # Ensure contiguous memory access\n        x = x.contiguous()\n        weight = self.weight.contiguous()\n        bias = self.bias.contiguous()\n\n        # Preallocate output tensor\n        output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n\n        # Configure kernel grid\n        grid_m = (M + 15) // 16\n        grid_n = (N + 63) // 64\n        grid = grid_m * grid_n\n\n        # Stride configuration\n        stride_xm, stride_xk = x.stride()\n        stride_wk, stride_wn = weight.stride()\n        stride_om, stride_on = output.stride()\n\n        # Launch Triton kernel\n        _triton_gemm[grid](\n            x, weight, bias, output,\n            M, N, K,\n            stride_xm, stride_xk,\n            stride_wk, stride_wn,\n            stride_om, stride_on,\n            BLOCK_SIZE_M=16, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n        )\n\n        # Post-GEMM operations\n        x = self.bn(output)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: -100.0000, Type: Exploratory)\n```python\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        # Instead of nn.Linear, we store weight and bias\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        # Initialize the parameters\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        # Instead of self.gemm(x), we do:\n        x = triton_matmul_with_bias(x, self.weight, self.bias)\n        x = self.bn(x)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_gemm_bn_scale_softmax(\n    x_ptr, w_ptr, b_ptr, gamma_ptr, beta_ptr, scale_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_bn,\n    stride_gamman, stride_betan,\n    stride_outputm, stride_outputn,\n    eps: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        \n        x = tl.load(x_ptrs, mask=k_mask[None, :], other=0.0)\n        w = tl.load(w_ptrs, mask=k_mask[None, :], other=0.0)\n        w_t = tl.trans(w)\n        acc += tl.dot(x, w_t)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    \n    # Load bias and add\n    b = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n    \n    # Online mean/variance computation\n    mean = tl.sum(acc, axis=1) / N\n    mean_bc = mean[:, None]\n    var = tl.sum((acc - mean_bc) * (acc - mean_bc), axis=1) / N\n    var_bc = var[:, None]\n    \n    # Batch normalization\n    gamma = tl.load(gamma_ptr + offs_n, mask=offs_n < N, other=0.0)\n    beta = tl.load(beta_ptr + offs_n, mask=offs_n < N, other=0.0)\n    normalized = (acc - mean_bc) / tl.sqrt(var_bc + eps)\n    scaled = normalized * gamma[None, :] + beta[None, :]\n    \n    # Apply scale parameter\n    scale_val = tl.load(scale_ptr)\n    scaled = scaled * scale_val\n    \n    # Softmax\n    max_val = tl.max(scaled, axis=1)\n    exp_vals = tl.exp(scaled - max_val[:, None])\n    sum_exp = tl.sum(exp_vals, axis=1)\n    softmax_out = exp_vals / sum_exp[:, None]\n    \n    # Store results\n    output_ptrs = output_ptr + offs_m[:, None] * stride_outputm + offs_n[None, :] * stride_outputn\n    tl.store(output_ptrs, softmax_out, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.bn_eps = bn_eps\n        \n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        self.gamma = nn.Parameter(torch.ones(out_features))\n        self.beta = nn.Parameter(torch.zeros(out_features))\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        \n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / (fan_in ** 0.5) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        output = torch.empty(M, N, device=x.device, dtype=torch.float32)\n        \n        grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\n        \n        fused_gemm_bn_scale_softmax[grid](\n            x, self.weight, self.bias, self.gamma, self.beta, self.scale, output,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            self.bias.stride(0),\n            self.gamma.stride(0), self.beta.stride(0),\n            output.stride(0), output.stride(1),\n            self.bn_eps,\n            BLOCK_M=64, BLOCK_N=128, BLOCK_K=32\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program which has a correctness issue (correctness: 0.0000) and a runtime error: \n AttributeError: 'dict' object has no attribute 'BLOCK_M'\n\n The error occurs in the grid function in the forward pass:\n     grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\n\n The issue is that the grid function expects an object with attributes BLOCK_M and BLOCK_N, but in the current\n Triton version, the grid function is passed a dictionary of the kernel parameters (meta-parameters) when using autotune.\n\n However, note that the current kernel is not using @triton.autotune. Therefore, the grid function should not be\n defined to take an 'opt' (which is a dictionary in autotune) but rather the meta-parameters.\n\n Alternatively, we can change the grid function to not use the autotune-like argument.\n\n Since the kernel is not autotuned, we can instead define the grid as a tuple or a function that doesn't require the meta dictionary.\n\n But note: the error says that the grid function is called with a dictionary and then tries to access opt.BLOCK_M.\n\n Actually, in Triton, when we launch a kernel without autotune, the grid function (if callable) is called with the current set of meta-parameters as a dictionary. So we must adjust the grid function to take a dictionary and then extract the BLOCK_M and BLOCK_N from the dictionary.\n\n However, in the current code, the grid function is defined as:\n     grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\n\n But when the kernel is launched, the grid function is called with a dictionary of the meta-parameters (including BLOCK_M, BLOCK_N, etc.). Therefore, we should access the values by key.\n\n Alternatively, we can avoid the lambda and precompute the grid size because we are passing fixed BLOCK_M and BLOCK_K? But note: the kernel call specifies BLOCK_M=64, BLOCK_N=128, BLOCK_K=32. So we can compute the grid without a lambda.\n\n Let's change the grid to be a tuple:\n\n     grid = (triton.cdiv(M, 64) * triton.cdiv(N, 128),)\n\n But wait, the kernel uses BLOCK_M and BLOCK_N as constants (64 and 128). However, note that the kernel signature has BLOCK_M, BLOCK_N, BLOCK_K as tl.constexpr. They are passed as kernel parameters and are known at compile time. But the grid must be computed at runtime.\n\n Alternatively, we can change the grid function to:\n\n     grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n\n But note: the kernel call specifies BLOCK_M=64, BLOCK_N=128, BLOCK_K=32. So meta['BLOCK_M'] will be 64 and meta['BLOCK_N'] will be 128.\n\n However, the error indicates that the grid function was called with a dictionary and then tried to access opt.BLOCK_M (as an attribute). We must change it to use dictionary access.\n\n Alternatively, we can precompute the grid without using the meta dictionary because the values are fixed.\n\n Since we are not autotuning and we are passing fixed block sizes, we can compute:\n\n     grid = (triton.cdiv(M, 64) * triton.cdiv(N, 128),)\n\n This will avoid the lambda and the issue.\n\n However, note that the kernel is defined without autotune, so the meta dictionary is still passed to the grid function. But if we define grid as a tuple, it will be used directly. Alternatively, we can define grid as a static tuple.\n\n But the Triton documentation says: grid can be either a tuple or a callable that takes a dict of metaparameters and returns a tuple.\n\n Let's change the grid to a static tuple:\n\n     grid = (triton.cdiv(M, 64) * triton.cdiv(N, 128),)\n\n However, note that the kernel call uses [grid] and then passes the fixed BLOCK_M, BLOCK_N, BLOCK_K. So the block sizes are fixed and known.\n\n Alternatively, we can change the lambda to use the dictionary:\n\n     grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n\n This should work.\n\n But note: the kernel call does not use autotune, so the meta dictionary will have the keys from the kernel parameters that are tl.constexpr. In this case, BLOCK_M, BLOCK_N, BLOCK_K are passed as keyword arguments and will be in the meta dictionary.\n\n So let's change the grid function to:\n\n     grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n\n However, the kernel call also passes the BLOCK_M, BLOCK_N, BLOCK_K as constants (64,128,32). So the meta dictionary will have these values.\n\n Alternatively, we can avoid the grid function and precompute:\n\n     grid = (triton.cdiv(M, 64) * triton.cdiv(N, 128),)\n\n This is safe because the block sizes are fixed.\n\n But note: the kernel definition does not change the block sizes. So we can do:\n\n     grid = ( (M + 63) // 64 * ((N + 127) // 128) , )\n\n However, using triton.cdiv is safer.\n\n Let's choose the precomputed grid:\n\n     grid = (triton.cdiv(M, 64) * triton.cdiv(N, 128),)\n\n But note: the kernel uses BLOCK_M=64 and BLOCK_N=128. So this matches.\n\n However, there might be other issues in the kernel.\n\n The runtime error is in the grid function, so let's fix that first.\n\n Additionally, the kernel has other potential issues:\n\n  1. The w_ptr indexing: the kernel uses `w_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk`\n     But note: the weight matrix is of shape (out_features, in_features) = (N, K). So the stride for the out_features dimension is stride_wn and for in_features is stride_wk.\n\n     However, in the kernel, we are loading a block of weight matrix of size (BLOCK_N, BLOCK_K). But note: the weight matrix is stored in row-major. The kernel expects to load a block of the weight matrix that is contiguous in memory? But the way we are indexing: we are taking a block of rows (each row of length BLOCK_K) and then we transpose it to get a (BLOCK_K, BLOCK_N) matrix? Actually, the kernel does:\n\n        w = tl.load(w_ptrs, mask=k_mask[None, :], other=0.0)\n        w_t = tl.trans(w)\n\n     This is because the weight matrix is stored as (N, K) and we want to multiply x (M, K) with weight (K, N) -> so we need to transpose the weight.\n\n     Alternatively, we can change the weight matrix to be transposed and stored as (K, N) so that we don't need to transpose. But the current ModelNew stores the weight as (out_features, in_features) = (N, K).\n\n     The kernel is doing:\n\n        x (BLOCK_M, BLOCK_K) @ w_t (BLOCK_K, BLOCK_N) -> (BLOCK_M, BLOCK_N)\n\n     But note: the weight block is loaded as (BLOCK_N, BLOCK_K) and then transposed to (BLOCK_K, BLOCK_N). This is correct.\n\n     However, the mask for w: we are using `mask=k_mask[None, :]` which is (1, BLOCK_K) and we are loading a block of (BLOCK_N, BLOCK_K). This mask might not be correct because it doesn't account for the BLOCK_N dimension? We should have a mask that is (BLOCK_N, BLOCK_K) and we are currently broadcasting a (1, BLOCK_K) mask. This would only mask the columns (k dimension) but not the rows (n dimension). We need to also mask the rows that are beyond N.\n\n     We should change the mask for w to:\n\n        mask = (offs_n[:, None] < N) & k_mask[None, :]\n\n     Similarly, the mask for x should be:\n\n        mask = (offs_m[:, None] < M) & k_mask[None, :]\n\n  2. The online mean/variance computation: the kernel is trying to compute the mean and variance for each row (for batch norm). But note: batch norm is applied per feature (column) in the standard BatchNorm1d. However, in the kernel, we are computing the mean per row (over the N dimension) and then normalizing per row. This is incorrect.\n\n     The standard BatchNorm1d expects the input to be of shape (M, N) and it normalizes each column (feature) independently. Therefore, we should compute the mean and variance for each column (over the M samples). But in the kernel, each block is responsible for a (BLOCK_M, BLOCK_N) block of the output. We cannot compute the mean of a column that is split among multiple blocks without communication.\n\n     Therefore, the current approach of computing mean and variance per row is wrong.\n\n     This indicates that fusing the entire operation (GEMM + BN + Scale + Softmax) in a single kernel is not trivial because the batch norm requires global statistics (over the entire column and across all rows).\n\n     Given the complexity, and since the current program is not working correctly, we might reconsider the approach.\n\n     Alternatively, we can break down the operations:\n\n        Option 1: Only fuse the GEMM and then do BN, scale, softmax in separate steps (as in the reference PyTorch).\n\n        Option 2: If we want to fuse, we have to do multiple passes or use techniques to compute global statistics.\n\n     Since the current program is failing and the performance of previous attempts (without fusion) was acceptable (though not exceeding the reference), and the task is to rewrite the program to improve performance, we might revert to a simpler approach: only do the GEMM in Triton and then use PyTorch for the rest.\n\n     But note: the previous top-performing programs (Program 1,2,3) were doing exactly that: they used Triton for GEMM and then PyTorch for BN, scale, and softmax.\n\n     And they passed correctness and had a score of 20 (though speed_up was less than 1, meaning slower than reference). However, the current program is not working.\n\n     Given the time, we might decide to revert to a non-fused version that we know works (like Program 2) and then try to optimize the GEMM kernel.\n\n     However, the task is to rewrite the current program. We are allowed to change the approach.\n\n     Considering the complexity of fusing batch norm (which requires global reduction) and the fact that we are getting an error, we decide to:\n\n        - Fix the grid function to avoid the AttributeError.\n        - And then address the batch norm issue by removing the fused batch norm and instead only do GEMM in Triton and then use PyTorch BN.\n\n     But note: the current program's class ModelNew does not have a separate BN layer? It has parameters gamma, beta and then uses them in the kernel. We are trying to fuse.\n\n     However, the error in correctness might be due to the incorrect batch norm implementation.\n\n     Given the above, and since the problem states that the program must be functionally equivalent, we have two options:\n\n        Option A: Fix the fused kernel by correctly computing the batch norm statistics (which requires a separate reduction over the entire output matrix for the mean and variance per column). This is complex and requires multiple kernels or a two-pass kernel.\n\n        Option B: Revert to a non-fused approach.\n\n     Given the time and the fact that the previous non-fused Triton GEMM kernels were correct, we choose Option B.\n\n     We will change the program to:\n\n        - Use a Triton kernel only for the GEMM (with bias).\n        - Then use the PyTorch BN, scale, and softmax.\n\n     We can take inspiration from the top-performing programs (like Program 1,2,3).\n\n     We note that the current ModelNew class has:\n\n        self.weight, self.bias, self.gamma, self.beta, self.scale\n\n     But in the non-fused approach, we should have:\n\n        self.weight, self.bias (for the linear layer)\n        self.bn (a BatchNorm1d layer that has its own weight and bias, which are called gamma and beta in the fused version)\n\n     So we can restructure:\n\n        self.linear_weight = ... (out_features, in_features)\n        self.linear_bias = ... (out_features)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = ... (a parameter for scaling)\n\n     And then in forward:\n\n        x = triton_gemm(x, self.linear_weight, self.linear_bias)\n        x = self.bn(x)\n        x = self.scale * x\n        x = self.softmax(x)\n\n     We will write a Triton GEMM kernel that does: x @ weight.t() + bias.\n\n     We can take one of the top-performing Triton GEMM kernels and adapt it.\n\n     Since the current program's GEMM kernel also had issues (incorrect masking for weight and the batch norm part), we remove the fusion and only do GEMM.\n\n     Steps:\n\n        1. Remove the fused operations in the kernel and only do GEMM with bias.\n        2. Change the ModelNew class to use nn.BatchNorm1d and then the scale and softmax in PyTorch.\n        3. Fix the grid function.\n\n     We choose the GEMM kernel from Program 1 (which had a score of 20) and adapt it.\n\n     Alternatively, we can take the kernel from Program 2 which had a slightly better speed_up (0.758).\n\n     Let's take the kernel from Program 2:\n\n        @triton.jit\n        def gemm_kernel(...):\n            ... # as in Program 2\n\n     But note: Program 2 uses a fixed BLOCK_M and BLOCK_N in the kernel launch? Actually, it passes them as arguments.\n\n     We will adapt the kernel to our problem.\n\n     We note that the current program has:\n\n        x: (M, K)\n        weight: (N, K)   -> but we need weight.t() for the multiplication: (K, N)\n\n     So the GEMM is: (M, K) @ (K, N) -> (M, N)\n\n     The kernel in Program 2 is designed for: x (M, K) @ weight (K, N) and then adds bias (N,).\n\n     But note: in Program 2, the weight was stored as (out_features, in_features) and then they did:\n\n        weight_contig = self.weight.contiguous()\n\n        and passed it as w_ptr.\n\n     In the kernel, they did:\n\n        w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n     This is for a weight matrix that is (K, N)? Actually, the weight matrix in Program 2 is stored as (out_features, in_features) = (N, K). Then the stride_wn is the stride for the out_features dimension and stride_wk for the in_features dimension.\n\n     But the kernel in Program 2 uses:\n\n        w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n\n     This is equivalent to accessing the weight matrix at [offs_n, offs_k]? Then we have a row of the weight matrix (which is of length K) for a given offs_n. But we need the column? Actually, the kernel is loading a block of the weight matrix of size (BLOCK_K, BLOCK_N) and then using it as (BLOCK_K, BLOCK_N). Then the dot product is:\n\n        x_block (BLOCK_M, BLOCK_K) @ w_block (BLOCK_K, BLOCK_N) -> (BLOCK_M, BLOCK_N)\n\n     So it's correct.\n\n     Therefore, we can use the kernel from Program 2.\n\n     We will do:\n\n        - Replace the current kernel with the gemm_kernel from Program 2.\n        - Change the ModelNew.forward to use this kernel for GEMM and then do the rest in PyTorch.\n\n     We also note that the current ModelNew has parameters: gamma, beta, and scale. We will change:\n\n        - Remove gamma and beta as parameters and instead use the BatchNorm1d layer which has its own weight and bias.\n\n     We will initialize the BatchNorm1d layer in __init__.\n\n     Let's restructure:\n\n        class ModelNew(nn.Module):\n            def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n                ... \n                # Linear parameters\n                self.weight = nn.Parameter(...)   # (out_features, in_features)\n                self.bias = nn.Parameter(...)     # (out_features,)\n                # BatchNorm layer\n                self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n                # Scale parameter\n                self.scale = nn.Parameter(torch.ones(scale_shape))\n\n            def forward(self, x):\n                M, K = x.shape\n                N = self.out_features\n                output = ... # result of GEMM: (M, N)\n\n                # Launch the GEMM kernel (like in Program 2)\n                ...\n\n                # Then:\n                x = self.bn(output)\n                x = self.scale * x\n                x = torch.softmax(x, dim=1)\n                return x\n\n     We will use the kernel from Program 2, but note that Program 2 had:\n\n        BLOCK_M, BLOCK_N, BLOCK_K = 128, 128, 32\n        grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n\n        gemm_kernel[grid](...)\n\n     We can either fix the block sizes or use autotune. Since we are not autotuning in this kernel, we can use fixed block sizes.\n\n     Alternatively, we can take the kernel from Program 1 which uses autotune? But Program 1 used autotune and had multiple configurations.\n\n     We are free to choose. Let's choose the kernel from Program 1 (with autotune) because it might give better performance.\n\n     However, note that the current program does not have autotune. We can add it.\n\n     But the problem says: \"Make sure your rewritten program maintains the same inputs and outputs\"\n\n     We are not changing the inputs and outputs, only the internal implementation.\n\n     Let's decide to use the kernel from Program 1 (with autotune) for GEMM.\n\n     Steps for the rewrite:\n\n        1. Replace the kernel with the autotuned gemm_kernel from Program 1.\n        2. Change the ModelNew class to have a separate BatchNorm1d layer and remove gamma, beta from the parameters (use the BN layer's parameters).\n        3. In forward, call the gemm_kernel and then the BN, scale, softmax.\n\n     We note that the current program's ModelNew has:\n\n        self.gamma = nn.Parameter(torch.ones(out_features))   # this is the BN weight\n        self.beta = nn.Parameter(torch.zeros(out_features))   # this is the BN bias\n\n     We can remove these and use the BN layer's parameters.\n\n     Also, note that the BN layer in PyTorch has weight and bias initialized to 1 and 0 by default.\n\n     We will initialize the BN layer without affine parameters? Actually, the default is with affine (so it has weight and bias).\n\n     So we can do:\n\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n\n     And then in forward, we do:\n\n        output = self.bn(output)\n\n     This will use the BN's weight and bias.\n\n     We also have the scale parameter (self.scale) which is multiplied after BN.\n\n     And then softmax.\n\n     Let's write the code accordingly.\n\n     We will take the gemm_kernel from Program 1 and the ModelNew structure from Program 1, but we will add the scale and softmax.\n\n     The Program 1 did:\n\n        x = self.bn(output)\n        x = self.scale * x\n        x = self.softmax(x)\n\n     So we can do the same.\n\n     Now, we must fix the grid function in the GEMM kernel launch. In Program 1, they did:\n\n        grid = (triton.cdiv(M, 64) * triton.cdiv(N, 64),)\n\n     But note: the autotune in Program 1 actually uses multiple block sizes. How do they launch?\n\n        gemm_kernel[grid](...)\n\n     Without passing the block sizes? Actually, they are passed as part of the autotune configs. The kernel signature in Program 1 has BLOCK_M, BLOCK_N, BLOCK_K as tl.constexpr and they are provided by the autotuner.\n\n     The grid function in Program 1 was defined as:\n\n        grid = (triton.cdiv(M, 64) * triton.cdiv(N, 64),)\n\n     But note: the autotuner might choose a different block size. Therefore, we should use a grid function that uses the meta-parameters.\n\n     However, in Program 1, they did not use a grid function? They computed grid as a tuple. That might not be correct if the autotuner chooses a different block size.\n\n     We need to change the grid to be a function of the meta-parameters.\n\n     Alternatively, we can use the grid function from the autotune example:\n\n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n\n     And then launch the kernel without specifying the block sizes? Actually, the autotune will run multiple configurations and choose the best.\n\n     But note: the kernel call in Program 1 did not pass the block sizes explicitly? They are part of the autotune configs.\n\n     How did they pass the block sizes? In the autotune decorator, the configs specify the block sizes. Then when the kernel is called, the autotuner will try each config and run a benchmark. Then it will choose the best config and run the kernel with that config.\n\n     And the grid function is called for each config with the meta-parameters of that config.\n\n     Therefore, we change the grid to be:\n\n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n\n     Then the kernel launch becomes:\n\n        gemm_kernel[grid](...)\n\n     Without passing BLOCK_M, BLOCK_N, BLOCK_K explicitly? Actually, they are passed by the autotuner.\n\n     But note: the kernel has other parameters (like the strides) that are not in the config. They are passed as normal arguments.\n\n     We will do that.\n\n     Let's write the code accordingly.\n\n     Summary of changes:\n\n        - Replace the kernel with the autotuned gemm_kernel from Program 1.\n        - Change the ModelNew class to have a BatchNorm1d layer and remove the gamma and beta parameters.\n        - In forward, after the GEMM, apply BN, scale, and softmax.\n        - Change the grid to be a lambda that uses meta['BLOCK_M'] and meta['BLOCK_N'].\n\n     However, note: the kernel in Program 1 has the following signature:\n\n        gemm_kernel(\n            x_ptr, w_ptr, bias_ptr, output_ptr,\n            M, N, K,\n            stride_xm, stride_xk,\n            stride_wn, stride_wk,\n            stride_om, stride_on,\n            BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n        )\n\n     We will keep that.\n\n     We also note that the kernel in Program 1 uses:\n\n        w_ptr: the weight matrix, which is stored as (out_features, in_features) = (N, K)\n\n        and the strides: \n            stride_wn: stride for the out_features dimension (N) in the weight matrix\n            stride_wk: stride for the in_features dimension (K) in the weight matrix\n\n     In the ModelNew.forward, we will do:\n\n        x_contig = x.contiguous()\n        weight_contig = self.weight.contiguous()\n        bias_contig = self.bias.contiguous()\n\n        output = torch.empty(M, N, device=x.device, dtype=torch.float32)\n\n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n\n        gemm_kernel[grid](\n            x_contig, weight_contig, bias_contig, output,\n            M, N, K,\n            x_contig.stride(0), x_contig.stride(1),\n            weight_contig.stride(0), weight_contig.stride(1),\n            output.stride(0), output.stride(1)\n        )\n\n     Then proceed with BN, scale, softmax.\n\n     Let's write the code accordingly.\n\n     We note that the current program has a batch_size, in_features, out_features defined at the bottom. We keep that.\n\n     We also note that the reference had a get_inputs and get_init_inputs. We keep the same.\n\n     We hope this will fix the compilation and correctness issues and then we can work on performance in subsequent iterations.\n\n     IMPORTANT: We are only allowed to output the EVOLVE-BLOCK.\n\n     Let's write the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 64}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_kernel(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // grid_n\n    pid_n = pid % grid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        \n        x_mask = (offs_m[:, None] < M) & k_mask[None, :]\n        x = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        \n        w_mask = k_mask[:, None] & (offs_n[None, :] < N)\n        w = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        \n        acc += tl.dot(x, w)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    \n    if bias_ptr is not None:\n        b = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += b[None, :]\n    \n    out_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        x_contig = x.contiguous()\n        weight_contig = self.weight.contiguous()\n        bias_contig = self.bias.contiguous()\n        \n        output = torch.empty(M, N, device=x.device, dtype=torch.float32)\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']),)\n        \n        gemm_kernel[grid](\n            x_contig, weight_contig, bias_contig, output,\n            M, N, K,\n            x_contig.stride(0), x_contig.stride(1),\n            weight_contig.stride(0), weight_contig.stride(1),\n            output.stride(0), output.stride(1)\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "028a6b31-e911-4c51-8915-7832132a11e8", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_bn[None, :] < N), other=0.0)\n        \n        # Convert to FP16 to leverage Tensor Cores\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        accumulator += tl.dot(a_f16, b_f16)\n        \n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        weight_t = self.weight.t().contiguous()\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        gemm_kernel[grid](\n            x, weight_t, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=32,\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        return self.softmax(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "1db15c3d-eec4-45ad-883c-55897640960d", "generation": 3, "timestamp": 1754582545.091944, "iteration_found": 20, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.654, "runtime_ms": 0.145, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0948, "speed_up": 0.654, "custom_timing": 0.145}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.682, "runtime_ms": 0.139, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0948, "speed_up": 0.682, "custom_timing": 0.139}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.6820\n- runtime_ms: 0.1390\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': 0.682, 'custom_timing': 0.139}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level2_84_Gemm_BatchNorm_Scaling_Softmax', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'max_difference': ['0.065609', '0.081110', '0.113614', '0.083400', '0.067745'], 'avg_difference': ['0.002295', '0.002291', '0.002292', '0.002286', '0.002287'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.6820, runtime_ms: 0.1390, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': 0.682, 'custom_timing': 0.139}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7580, runtime_ms: 0.1250, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': 0.758, 'custom_timing': 0.125}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // grid_n\n    pid_n = pid % grid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        \n        x_mask = (offs_m[:, None] < M) & k_mask[None, :]\n        x_val = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        x_f16 = x_val.to(tl.float16)\n        \n        w_mask = k_mask[:, None] & (offs_n[None, :] < N)\n        w_val = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        w_f16 = w_val.to(tl.float16)\n        \n        acc += tl.dot(x_f16, w_f16)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    \n    if bias_ptr is not None:\n        b = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += b[None, :]\n    \n    offs_m = tl.max_contiguous(tl.multiple_of(offs_m, BLOCK_M), BLOCK_M)\n    offs_n = tl.max_contiguous(tl.multiple_of(offs_n, BLOCK_N), BLOCK_N)\n    \n    out_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        x_contig = x.contiguous()\n        weight_contig = self.weight.contiguous()\n        bias_contig = self.bias.contiguous()\n        \n        output = torch.empty(M, N, device=x.device, dtype=torch.float32)\n        \n        BLOCK_M, BLOCK_N, BLOCK_K = 128, 128, 32\n        grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n        \n        gemm_kernel[grid](\n            x_contig, weight_contig, bias_contig, output,\n            M, N, K,\n            x_contig.stride(0), x_contig.stride(1),\n            weight_contig.stride(0), weight_contig.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_M, BLOCK_N, BLOCK_K\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7580), Performs well on runtime_ms (0.1250), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': 0.758, 'custom_timing': 0.125})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_bn[None, :] < N), other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        weight_t = self.weight.t().contiguous()\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        gemm_kernel[grid](\n            x, weight_t, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32,\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        return self.softmax(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.6820), Performs well on runtime_ms (0.1390), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': 0.682, 'custom_timing': 0.139})\n\n\n### Program 3 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.randn(out_features))\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        weight_t = self.weight.t().contiguous()\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        gemm_kernel[grid](\n            x, weight_t, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32,\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        return self.softmax(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level2_84_Gemm_BatchNorm_Scaling_Softmax', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'max_difference': ['0.065609', '0.081110', '0.113614', '0.083400', '0.067745'], 'avg_difference': ['0.002295', '0.002291', '0.002292', '0.002286', '0.002287'], 'correctness_issue': 'Output mismatch', 'correctness_trials': '(0 / 5)', 'reference_timing_ms': 0.0948, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n# Triton kernel for GEMM: y = x @ w + bias\n@triton.jit\ndef _triton_gemm(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wk, stride_wn,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr, \n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    grid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    grid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // grid_n\n    pid_n = pid % grid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Pointers for x: [M, K]\n    x_ptrs = x_ptr + (offs_am[:, None] * stride_xm + offs_k[None, :] * stride_xk)\n    # Pointers for w: [K, N]\n    w_ptrs = w_ptr + (offs_k[:, None] * stride_wk + offs_bn[None, :] * stride_wn)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load blocks of x and weight with masking\n        mask_x = (offs_am[:, None] < M) & (offs_k[None, :] < K - k)\n        x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n        mask_w = (offs_k[:, None] < K - k) & (offs_bn[None, :] < N)\n        w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n\n        acc += tl.dot(x, w)\n        x_ptrs += BLOCK_SIZE_K * stride_xk\n        w_ptrs += BLOCK_SIZE_K * stride_wk\n\n    # Add bias\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        acc += bias[None, :]\n\n    # Write back results with masking\n    offs_om = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_on = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask_out = (offs_om[:, None] < M) & (offs_on[None, :] < N)\n    tl.store(output_ptr + (offs_om[:, None] * stride_om + offs_on[None, :] * stride_on), acc, mask=mask_out)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(in_features, out_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        \n        # Initialize parameters (PyTorch's Linear default initialization)\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in = in_features\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n\n        # Ensure contiguous memory access\n        x = x.contiguous()\n        weight = self.weight.contiguous()\n        bias = self.bias.contiguous()\n\n        # Preallocate output tensor\n        output = torch.empty(M, N, device=x.device, dtype=x.dtype)\n\n        # Configure kernel grid\n        grid_m = (M + 15) // 16\n        grid_n = (N + 63) // 64\n        grid = grid_m * grid_n\n\n        # Stride configuration\n        stride_xm, stride_xk = x.stride()\n        stride_wk, stride_wn = weight.stride()\n        stride_om, stride_on = output.stride()\n\n        # Launch Triton kernel\n        _triton_gemm[grid](\n            x, weight, bias, output,\n            M, N, K,\n            stride_xm, stride_xk,\n            stride_wk, stride_wn,\n            stride_om, stride_on,\n            BLOCK_SIZE_M=16, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n        )\n\n        # Post-GEMM operations\n        x = self.bn(output)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_gemm_bn_scale_softmax(\n    x_ptr, w_ptr, b_ptr, gamma_ptr, beta_ptr, scale_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_bn,\n    stride_gamman, stride_betan,\n    stride_outputm, stride_outputn,\n    eps: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        x = tl.load(x_ptrs, mask=k_mask[None, :], other=0.0)\n        w = tl.load(w_ptrs, mask=k_mask[:, None], other=0.0)\n        acc += tl.dot(x, w)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    \n    # Load bias and add\n    b = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n    \n    # Online mean/variance computation\n    mean = tl.sum(acc, axis=1) / N\n    mean_bc = mean[:, None]\n    var = tl.sum((acc - mean_bc) * (acc - mean_bc), axis=1) / N\n    var_bc = var[:, None]\n    \n    # Batch normalization\n    gamma = tl.load(gamma_ptr + offs_n, mask=offs_n < N, other=0.0)\n    beta = tl.load(beta_ptr + offs_n, mask=offs_n < N, other=0.0)\n    normalized = (acc - mean_bc) / tl.sqrt(var_bc + eps)\n    scaled = normalized * gamma[None, :] + beta[None, :]\n    \n    # Apply scale parameter\n    scale_val = tl.load(scale_ptr)\n    scaled = scaled * scale_val\n    \n    # Softmax\n    max_val = tl.max(scaled, axis=1)\n    exp_vals = tl.exp(scaled - max_val[:, None])\n    sum_exp = tl.sum(exp_vals, axis=1)\n    softmax_out = exp_vals / sum_exp[:, None]\n    \n    # Store results\n    output_ptrs = output_ptr + offs_m[:, None] * stride_outputm + offs_n[None, :] * stride_outputn\n    tl.store(output_ptrs, softmax_out, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.bn_eps = bn_eps\n        \n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        self.gamma = nn.Parameter(torch.ones(out_features))\n        self.beta = nn.Parameter(torch.zeros(out_features))\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        \n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / (fan_in ** 0.5) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        output = torch.empty(M, N, device=x.device, dtype=torch.float32)\n        \n        grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\n        \n        fused_gemm_bn_scale_softmax[grid](\n            x, self.weight, self.bias, self.gamma, self.beta, self.scale, output,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            self.bias.stride(0),\n            self.gamma.stride(0), self.beta.stride(0),\n            output.stride(0), output.stride(1),\n            self.bn_eps,\n            BLOCK_M=64, BLOCK_N=128, BLOCK_K=32\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    x_ptr, w_ptr, bias_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_om, stride_on,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // grid_n\n    pid_n = pid % grid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        k_remaining = K - k\n        k_mask = offs_k < k_remaining\n        \n        x_mask = (offs_m[:, None] < M) & k_mask[None, :]\n        x_val = tl.load(x_ptrs, mask=x_mask, other=0.0)\n        x_f16 = x_val.to(tl.float16)\n        \n        w_mask = k_mask[:, None] & (offs_n[None, :] < N)\n        w_val = tl.load(w_ptrs, mask=w_mask, other=0.0)\n        w_f16 = w_val.to(tl.float16)\n        \n        acc += tl.dot(x_f16, w_f16)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    \n    if bias_ptr is not None:\n        b = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        acc += b[None, :]\n    \n    offs_m = tl.max_contiguous(tl.multiple_of(offs_m, BLOCK_M), BLOCK_M)\n    offs_n = tl.max_contiguous(tl.multiple_of(offs_n, BLOCK_N), BLOCK_N)\n    \n    out_ptrs = output_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, acc, mask=out_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        \n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        x_contig = x.contiguous()\n        weight_contig = self.weight.contiguous()\n        bias_contig = self.bias.contiguous()\n        \n        output = torch.empty(M, N, device=x.device, dtype=torch.float32)\n        \n        BLOCK_M, BLOCK_N, BLOCK_K = 128, 128, 32\n        grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n        \n        gemm_kernel[grid](\n            x_contig, weight_contig, bias_contig, output,\n            M, N, K,\n            x_contig.stride(0), x_contig.stride(1),\n            weight_contig.stride(0), weight_contig.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_M, BLOCK_N, BLOCK_K\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_gemm_bn_scale_softmax(\n    x_ptr, w_ptr, b_ptr, gamma_ptr, beta_ptr, scale_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_bn,\n    stride_gamman, stride_betan,\n    stride_outputm, stride_outputn,\n    eps: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        \n        x = tl.load(x_ptrs, mask=k_mask[None, :], other=0.0)\n        w = tl.load(w_ptrs, mask=k_mask[None, :], other=0.0)\n        w_t = tl.trans(w)\n        acc += tl.dot(x, w_t)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    \n    # Load bias and add\n    b = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n    \n    # Online mean/variance computation\n    mean = tl.sum(acc, axis=1) / N\n    mean_bc = mean[:, None]\n    var = tl.sum((acc - mean_bc) * (acc - mean_bc), axis=1) / N\n    var_bc = var[:, None]\n    \n    # Batch normalization\n    gamma = tl.load(gamma_ptr + offs_n, mask=offs_n < N, other=0.0)\n    beta = tl.load(beta_ptr + offs_n, mask=offs_n < N, other=0.0)\n    normalized = (acc - mean_bc) / tl.sqrt(var_bc + eps)\n    scaled = normalized * gamma[None, :] + beta[None, :]\n    \n    # Apply scale parameter\n    scale_val = tl.load(scale_ptr)\n    scaled = scaled * scale_val\n    \n    # Softmax\n    max_val = tl.max(scaled, axis=1)\n    exp_vals = tl.exp(scaled - max_val[:, None])\n    sum_exp = tl.sum(exp_vals, axis=1)\n    softmax_out = exp_vals / sum_exp[:, None]\n    \n    # Store results\n    output_ptrs = output_ptr + offs_m[:, None] * stride_outputm + offs_n[None, :] * stride_outputn\n    tl.store(output_ptrs, softmax_out, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.bn_eps = bn_eps\n        \n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        self.gamma = nn.Parameter(torch.ones(out_features))\n        self.beta = nn.Parameter(torch.zeros(out_features))\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        \n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / (fan_in ** 0.5) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        output = torch.empty(M, N, device=x.device, dtype=torch.float32)\n        \n        grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\n        \n        fused_gemm_bn_scale_softmax[grid](\n            x, self.weight, self.bias, self.gamma, self.beta, self.scale, output,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            self.bias.stride(0),\n            self.gamma.stride(0), self.beta.stride(0),\n            output.stride(0), output.stride(1),\n            self.bn_eps,\n            BLOCK_M=64, BLOCK_N=128, BLOCK_K=32\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.randn(out_features))\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        weight_t = self.weight.t().contiguous()\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        gemm_kernel[grid](\n            x, weight_t, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32,\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        return self.softmax(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: -100.0000, Type: Exploratory)\n```python\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        # Instead of nn.Linear, we store weight and bias\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        # Initialize the parameters\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        # Instead of self.gemm(x), we do:\n        x = triton_matmul_with_bias(x, self.weight, self.bias)\n        x = self.bn(x)\n        x = self.scale * x\n        x = self.softmax(x)\n        return x\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_gemm_bn_scale_softmax(\n    x_ptr, w_ptr, b_ptr, gamma_ptr, beta_ptr, scale_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_bn,\n    stride_gamman, stride_betan,\n    stride_outputm, stride_outputn,\n    eps: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n    w_ptrs = w_ptr + offs_n[:, None] * stride_wn + offs_k[None, :] * stride_wk\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        x = tl.load(x_ptrs, mask=k_mask[None, :], other=0.0)\n        w = tl.load(w_ptrs, mask=k_mask[:, None], other=0.0)\n        acc += tl.dot(x, w)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    \n    # Load bias and add\n    b = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n    \n    # Online mean/variance computation\n    mean = tl.sum(acc, axis=1) / N\n    mean_bc = mean[:, None]\n    var = tl.sum((acc - mean_bc) * (acc - mean_bc), axis=1) / N\n    var_bc = var[:, None]\n    \n    # Batch normalization\n    gamma = tl.load(gamma_ptr + offs_n, mask=offs_n < N, other=0.0)\n    beta = tl.load(beta_ptr + offs_n, mask=offs_n < N, other=0.0)\n    normalized = (acc - mean_bc) / tl.sqrt(var_bc + eps)\n    scaled = normalized * gamma[None, :] + beta[None, :]\n    \n    # Apply scale parameter\n    scale_val = tl.load(scale_ptr)\n    scaled = scaled * scale_val\n    \n    # Softmax\n    max_val = tl.max(scaled, axis=1)\n    exp_vals = tl.exp(scaled - max_val[:, None])\n    sum_exp = tl.sum(exp_vals, axis=1)\n    softmax_out = exp_vals / sum_exp[:, None]\n    \n    # Store results\n    output_ptrs = output_ptr + offs_m[:, None] * stride_outputm + offs_n[None, :] * stride_outputn\n    tl.store(output_ptrs, softmax_out, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.bn_eps = bn_eps\n        \n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        self.gamma = nn.Parameter(torch.ones(out_features))\n        self.beta = nn.Parameter(torch.zeros(out_features))\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        \n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / (fan_in ** 0.5) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        output = torch.empty(M, N, device=x.device, dtype=torch.float32)\n        \n        grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\n        \n        fused_gemm_bn_scale_softmax[grid](\n            x, self.weight, self.bias, self.gamma, self.beta, self.scale, output,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            self.bias.stride(0),\n            self.gamma.stride(0), self.beta.stride(0),\n            output.stride(0), output.stride(1),\n            self.bn_eps,\n            BLOCK_M=64, BLOCK_N=128, BLOCK_K=32\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_bn[None, :] < N), other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        weight_t = self.weight.t().contiguous()\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        gemm_kernel[grid](\n            x, weight_t, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32,\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        return self.softmax(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program, which is a Triton implementation of a GEMM kernel followed by batch normalization, scaling, and softmax.\n The current performance shows a speed_up of 0.682 (which is less than 1, meaning it's slower than the reference PyTorch implementation).\n\n We are to optimize the Triton kernel to achieve at least 5% speedup (speed_up > 1.05).\n\n The current kernel uses:\n   BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n\n The hardware is NVIDIA L20Y / H800 (same as H100 with SM8.9) which has Tensor Cores for FP16.\n\n The current kernel uses `allow_tf32=True` in the dot product. However, note that the inputs are in float32.\n We have two options to leverage Tensor Cores:\n   Option 1: Convert the inputs to FP16 or BF16 and use FP16/BF16 Tensor Cores.\n   Option 2: Use TF32 Tensor Cores (which is what `allow_tf32=True` does for FP32 inputs).\n\n However, note that the reference PyTorch code is in FP32. We must maintain FP32 accuracy.\n\n But note: the current kernel is already using `allow_tf32=True` which might be acceptable for accuracy.\n\n The problem: the current kernel is slower than the reference. We need to make it faster.\n\n Steps to consider:\n\n 1. Tune the block sizes: The current block sizes (64,64,32) might not be optimal. We can try larger blocks or different shapes.\n    The H100 has 128 KB shared memory per SM. We need to fit two blocks (for A and B) and the accumulator.\n\n    For block (M, N, K) = (64,64,32):\n        Size of A block: 64*32 * 4 (float32) = 8192 bytes\n        Size of B block: 32*64 * 4 = 8192 bytes\n        Accumulator: 64*64 * 4 = 16384 bytes\n        Total: 8192+8192+16384 = 32768 bytes (32 KB) -> fits in shared memory.\n\n    We can try larger blocks? For example (128,128,32) would require:\n        A: 128*32*4 = 16 KB\n        B: 32*128*4 = 16 KB\n        Accumulator: 128*128*4 = 64 KB -> total 96 KB, which is more than 128 KB? Actually 16+16+64=96 KB -> fits? \n        But note: the shared memory is 128 KB per block? Actually, the shared memory per SM is 128 KB, and we have multiple warps.\n\n    However, the number of warps per block is limited by the shared memory. We must be cautious.\n\n    Alternatively, we can try (128,64,32) or (64,128,32) to balance.\n\n 2. Use FP16 for the matrix multiplication with accumulation in FP32. This would require converting the inputs to FP16.\n    Since the reference is in FP32, we must be cautious about precision. However, the Triton dot operation in FP16 might be faster.\n\n    We can try:\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        and then do tl.dot(a_f16, b_f16, accumulator=accumulator)\n\n    But note: the current kernel uses FP32 throughout.\n\n 3. Pre-fetching and double buffering: We can try to pre-fetch the next block while computing the current one.\n\n 4. Use vectorized loads to reduce the number of load instructions.\n\n However, the current kernel does not use vectorized loads.\n\n Let's analyze the current memory access:\n\n   a_ptrs and b_ptrs are accessed in a non-vectorized way? Actually, we are loading a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] and [BLOCK_SIZE_K, BLOCK_SIZE_N] in a loop.\n\n 5. The current kernel uses a naive loop over K. We might consider increasing BLOCK_SIZE_K to reduce the number of outer loops.\n\n 6. The grid function is using a lambda that uses the META parameters. We are currently hard-coding the block sizes. We can try to autotune the block sizes.\n\n Given the constraints of the problem (only change the Triton kernel source within the block, and not change the signature, grid, etc.), we can try:\n\n   a) Change the block sizes to (128,128,32) or (128,64,32) or (64,128,32) and see if it improves performance.\n\n   b) Use FP16 for the matrix multiplication (with FP32 accumulation) to leverage Tensor Cores.\n\n   c) Add vectorized loads.\n\n   d) Pre-fetching.\n\n However, note the constraints: we cannot change the grid configuration (but note: the grid is computed by a lambda that uses META, and we are passing BLOCK_SIZE_M, BLOCK_SIZE_N as 64,64). We can change the block sizes and also change the kernel to use FP16.\n\n Since the current kernel is already using `allow_tf32=True` and we are on H100, we might not be getting the full performance because of the block size.\n\n Let's try to change the block sizes to (128,128,32) and also use FP16 for the intermediate dot product.\n\n Steps for FP16:\n\n   - Convert the loaded a and b to float16.\n   - Then do the dot product.\n\n But note: the accumulator is in float32. That is acceptable.\n\n We can do:\n\n   a_val = tl.load(a_ptrs, ...)\n   b_val = tl.load(b_ptrs, ...)\n\n   a_val_f16 = a_val.to(tl.float16)\n   b_val_f16 = b_val.to(tl.float16)\n\n   accumulator += tl.dot(a_val_f16, b_val_f16)\n\n However, the conversion might add overhead. But it might be worth it because of the Tensor Core acceleration.\n\n Alternatively, we can load the data as float16? But the input tensors are float32. We would have to load as float32 and then convert.\n\n 7. Another idea: use a different tiling strategy.\n\n Since we are limited to modifying at most 20 lines, we will try:\n\n   - Increasing the block sizes to 128x128x32.\n   - Adding FP16 conversion for the dot product.\n\n But note: the current kernel uses a mask for the loads. We must maintain that.\n\n Let's rewrite the kernel loop with FP16 conversion and change the block sizes to 128x128x32.\n\n However, note the grid function in the forward pass:\n\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n\n    and we are passing BLOCK_SIZE_M=64, BLOCK_SIZE_N=64. We must change these to 128 if we change the kernel.\n\n    But note: the kernel signature has BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K as constants. We are passing them as 64,64,32. We can change the passed values to 128,128,32.\n\n    However, the kernel itself uses the same variable names. We can change the passed values in the kernel launch.\n\n    We are currently passing:\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n\n    We can change that to:\n        BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=32\n\n    But note: the kernel code uses the same variable names. So we can change the block sizes in the launch.\n\n    Alternatively, we can set the block sizes to be autotuned. However, the problem says we cannot change the grid configuration. But note: the grid configuration is computed by the lambda. We are allowed to change the block sizes because they are passed as constants.\n\n    We will change the block sizes in the kernel launch and also change the kernel to use FP16 for the dot product.\n\n 8. Also, we can try to use vectorized loads. For example, if we load 4 elements at a time, we can set:\n\n        BLOCK_K = 32  # we can leave as is\n        and then use tl.load(..., mask=..., other=0.0, cache_modifier=\".cg\", ...) but also we can use vectorized by changing the offsets.\n\n    However, to use vectorized loads, we need to change the way we declare the offsets and the data type. We can try to load 4 consecutive elements in the K dimension.\n\n    But note: the current kernel uses:\n\n        offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    and then we have:\n\n        a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n\n    We can change the offs_k to be in steps of 4? Actually, we can change the offs_k to be:\n\n        offs_k = tl.arange(0, BLOCK_SIZE_K, 4)\n\n    and then we load 4 elements at a time? But then the block size in K would be BLOCK_SIZE_K // 4? We don't want that.\n\n    Alternatively, we can use:\n\n        offs_k = tl.arange(0, BLOCK_SIZE_K)\n        and then use vectorize=4 in the load? Triton doesn't have that.\n\n    Instead, we can use the `tl.dot` instruction which can take advantage of vectorized loads automatically? Not exactly.\n\n    We can restructure the kernel to use vectorized loads by having an inner loop over a vector width? That would complicate.\n\n Given the complexity and the 20-line limit, we will focus on:\n\n   a) Changing the block sizes to 128x128x32.\n   b) Using FP16 for the dot product.\n\n We'll change:\n\n   In the kernel launch: change to BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=32.\n\n   In the kernel, change the dot product to use FP16.\n\n But note: the accumulator is in FP32. This is fine.\n\n Also note: the current kernel has:\n\n        accumulator += tl.dot(a, b, allow_tf32=True)\n\n    We will replace with:\n\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        accumulator += tl.dot(a_f16, b_f16)\n\n    Why remove `allow_tf32`? Because we are now using FP16. Also, note that the dot product for FP16 is accelerated.\n\n 9. We must check that the mask for a and b is still correct.\n\n 10. Also, note that the conversion to FP16 might lose precision. However, the reference is in FP32 and the tolerance in correctness is float32 tolerance. \n     We must check that the results are within tolerance. Since we are using FP16 for the intermediate product and accumulating in FP32, it should be acceptable.\n\n 11. Another consideration: the shared memory usage for (128,128,32) in FP16 would be:\n\n        A: 128*32 * 2 (FP16) = 8192 bytes\n        B: 32*128 * 2 = 8192 bytes\n        Accumulator: 128*128 * 4 (FP32) = 65536 bytes\n        Total: 8192+8192+65536 = 81920 bytes (80 KB) -> which is less than 128 KB.\n\n     So it should fit.\n\n 12. However, the number of threads per block? The kernel is designed to be executed by a 2D grid of thread blocks. The number of threads per block is not explicit in Triton; it is determined by the block sizes and the number of warps.\n\n     The Triton compiler will choose the number of warps. We can also try to set num_warps.\n\n     But note: the current kernel does not specify num_warps. We can try to set it to 8.\n\n     We can add an autotune config? But we are limited by the problem: we cannot change the grid configuration. However, we are allowed to change the kernel.\n\n     We can try to set the number of warps to 8 for (128,128,32) block? We can do:\n\n        @triton.jit\n        def gemm_kernel(... , num_warps=8)\n\n     But the problem says: never change the function signatures. So we cannot add new parameters.\n\n     Instead, we can set it by a config? We don't have autotune in the current kernel.\n\n     Alternatively, we can try to set the num_warps by the block size? But we cannot change the signature.\n\n     We leave it to the Triton compiler.\n\n 13. We are going to change:\n\n        In the forward pass: change the block sizes to 128,128,32.\n\n        In the kernel: replace the dot product with conversion to FP16 and then dot in FP16.\n\n        Also, remove the `allow_tf32` because we are not using it.\n\n 14. We must also note that the current kernel has a mask for the loads. We are using the same mask, so it should be safe.\n\n Let's write the changes.\n\n However, note: the current kernel is:\n\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_bn[None, :] < N), other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n\n    We change to:\n\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_bn[None, :] < N), other=0.0)\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        accumulator += tl.dot(a_f16, b_f16)\n\n 15. But note: the accumulator is in FP32 and the dot product of FP16 matrices returns an FP32 tensor? Actually, the Triton dot product for FP16 inputs returns FP32.\n\n 16. Also, note that the conversion might be free if done in the load? Not exactly, but we are doing it explicitly.\n\n 17. We must check the boundaries: the mask is the same.\n\n 18. We are changing about 10 lines.\n\n Let's generate the code accordingly.\n\n However, note: the current program has the kernel and the ModelNew class. We are going to change:\n\n   - In the kernel: the two lines of the dot product to add the conversion and remove the allow_tf32.\n   - In the forward pass: change the block sizes from 64,64,32 to 128,128,32.\n\n But wait: the current kernel uses the variable names BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K. In the kernel launch, we are passing:\n\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n\n    We change to:\n\n        BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=32\n\n 19. Also note: the kernel code uses BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K as the block sizes. So we are just changing the values passed.\n\n 20. We must be cautious: the shared memory might be insufficient? We calculated 80 KB which is less than 128 KB per SM.\n\n Let's output the code.\n\n However, note: the problem says \"Maintain reversibility between versions\". We are making two independent changes (block size and FP16). We could do one at a time, but the problem allows multiple.\n\n Given the current speedup is 0.682, we hope to get above 1.05.\n\n If this doesn't work, we can try other block sizes or only FP16 without changing the block size.\n\n But we are limited by the 20-line change.\n\n We'll change:\n\n   - In the kernel: the two lines for the dot product (actually we are adding two lines and changing one).\n   - In the forward: the block sizes from 64 to 128.\n\n Let's write the code.\n\n IMPORTANT: We are only allowed to output the code block between EVOLVE-BLOCK markers.\n\n We are going to output the entire current block with the changes.\n\n Steps:\n\n   a) Change the kernel: inside the loop, after loading a and b, convert to fp16 and then do the dot product without allow_tf32.\n\n   b) In the forward function of ModelNew, change the kernel launch parameters from 64,64,32 to 128,128,32.\n\n   c) Also, note that the kernel uses the variable names in the signature. We are not changing the signature.\n\n Let's do it.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_bn[None, :] < N), other=0.0)\n        \n        # Convert to FP16 to leverage Tensor Cores\n        a_f16 = a.to(tl.float16)\n        b_f16 = b.to(tl.float16)\n        accumulator += tl.dot(a_f16, b_f16)\n        \n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        accumulator += bias[None, :]\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        weight_t = self.weight.t().contiguous()\n        output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        gemm_kernel[grid](\n            x, weight_t, output, self.bias,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            weight_t.stride(0), weight_t.stride(1),\n            output.stride(0), output.stride(1),\n            BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=32,\n        )\n        \n        x = self.bn(output)\n        x = self.scale * x\n        return self.softmax(x)\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
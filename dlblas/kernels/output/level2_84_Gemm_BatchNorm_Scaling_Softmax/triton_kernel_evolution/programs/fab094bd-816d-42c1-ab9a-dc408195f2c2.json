{"id": "fab094bd-816d-42c1-ab9a-dc408195f2c2", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_gemm_bn_scale_softmax(\n    x_ptr, w_ptr, b_ptr, gamma_ptr, beta_ptr, scale_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_wn, stride_wk,\n    stride_bn,\n    stride_gamman, stride_betan,\n    stride_outputm, stride_outputn,\n    eps: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    # Use memory coalescing optimizations\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk)\n    w_ptrs = w_ptr + (offs_n[None, :] * stride_wn + offs_k[:, None] * stride_wk)\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        k_mask = offs_k < k_remaining\n        \n        # Coalesced loads with proper masking\n        x = tl.load(x_ptrs, mask=k_mask[None, :] & (offs_m[:, None] < M), other=0.0)\n        w = tl.load(w_ptrs, mask=k_mask[:, None] & (offs_n[None, :] < N), other=0.0)\n        acc += tl.dot(x, w)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    \n    # Load bias and add\n    b = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)\n    acc += b[None, :]\n    \n    # Optimized mean computation\n    row_sum = tl.sum(acc, axis=1)\n    mean = row_sum / N\n    mean_bc = mean[:, None]\n    \n    # Optimized variance computation\n    centered = acc - mean_bc\n    row_var = tl.sum(centered * centered, axis=1) / N\n    inv_std = 1.0 / tl.sqrt(row_var[:, None] + eps)\n    \n    # Batch normalization\n    gamma = tl.load(gamma_ptr + offs_n, mask=offs_n < N, other=1.0)\n    beta = tl.load(beta_ptr + offs_n, mask=offs_n < N, other=0.0)\n    scaled = centered * inv_std * gamma[None, :] + beta[None, :]\n    \n    # Apply scale parameter\n    scale_val = tl.load(scale_ptr)\n    scaled = scaled * scale_val\n    \n    # Optimized softmax\n    row_max = tl.max(scaled, axis=1)\n    scaled = scaled - row_max[:, None]\n    numerator = tl.exp(scaled)\n    denominator = tl.sum(numerator, axis=1)[:, None]\n    softmax_out = numerator / denominator\n    \n    # Store results with proper masking\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    output_ptrs = output_ptr + offs_m[:, None] * stride_outputm + offs_n[None, :] * stride_outputn\n    tl.store(output_ptrs, softmax_out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.bn_eps = bn_eps\n        \n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        self.gamma = nn.Parameter(torch.ones(out_features))\n        self.beta = nn.Parameter(torch.zeros(out_features))\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        \n        nn.init.kaiming_uniform_(self.weight, a=5**0.5)\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / (fan_in ** 0.5) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        M, K = x.shape\n        N = self.out_features\n        \n        output = torch.empty(M, N, device=x.device, dtype=torch.float32)\n        \n        # Fixed grid calculation - use dictionary access\n        grid = lambda opt: (triton.cdiv(M, opt['BLOCK_M']) * triton.cdiv(N, opt['BLOCK_N']),)\n        \n        fused_gemm_bn_scale_softmax[grid](\n            x, self.weight, self.bias, self.gamma, self.beta, self.scale, output,\n            M, N, K,\n            x.stride(0), x.stride(1),\n            self.weight.stride(0), self.weight.stride(1),\n            self.bias.stride(0),\n            self.gamma.stride(0), self.beta.stride(0),\n            output.stride(0), output.stride(1),\n            self.bn_eps,\n            BLOCK_M=64, BLOCK_N=128, BLOCK_K=32\n        )\n        \n        return output\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nbn_eps = 1e-5\nbn_momentum = 0.1\nscale_shape = (1,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "903c75ec-ab07-44a9-9bed-fff0a462b1f8", "generation": 3, "timestamp": 1754582341.1035676, "iteration_found": 18, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_84_Gemm_BatchNorm_Scaling_Softmax", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "max_difference": ["0.807837", "0.793115", "0.754461", "0.816095", "0.781370"], "avg_difference": ["0.006138", "0.006138", "0.006131", "0.006131", "0.006133"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0948, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_84_Gemm_BatchNorm_Scaling_Softmax", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpyrgma5ry.py\", line 100, in forward\n    fused_gemm_bn_scale_softmax[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 610, in run\n    grid = grid(bound_args)\n  File \"/tmp/tmpyrgma5ry.py\", line 98, in <lambda>\n    grid = lambda opt: (triton.cdiv(M, opt.BLOCK_M) * triton.cdiv(N, opt.BLOCK_N),)\nAttributeError: 'dict' object has no attribute 'BLOCK_M'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 0.0948, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
{"id": "44eac175-826d-4d00-ade4-9de873bcfa40", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport triton\nimport triton.language as tl\n\n# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def __init__(self):\n        super(NewGELU, self).__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\n@triton.jit\ndef _fwd_attention_kernel(\n    Q, K, V, Out,\n    stride_qz, stride_qh, stride_qm, stride_qk,\n    stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vn, stride_vk,\n    stride_oz, stride_oh, stride_om, stride_ok,\n    scale, \n    N_CTX,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // N_CTX\n    off_h = off_hz % N_CTX\n    \n    q_offset = off_z * stride_qz + off_h * stride_qh\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    \n    k_offset = off_z * stride_kz + off_h * stride_kh\n    K_block_ptr = tl.make_block_ptr(\n        base=K + k_offset,\n        shape=(BLOCK_DMODEL, N_CTX),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1)\n    )\n    \n    v_offset = off_z * stride_vz + off_h * stride_vh\n    V_block_ptr = tl.make_block_ptr(\n        base=V + v_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_vn, stride_vk),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    \n    o_offset = off_z * stride_oz + off_h * stride_oh\n    O_block_ptr = tl.make_block_ptr(\n        base=Out + o_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_om, stride_ok),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    \n    acc = tl.zeros((BLOCK_M, BLOCK_DMODEL), dtype=tl.float32)\n    q = tl.load(Q_block_ptr, boundary_check=(0,))\n    q = (q * scale).to(q.dtype)\n    \n    lo, hi = 0, (start_m + 1) * BLOCK_M\n    for start_n in range(lo, hi, BLOCK_N):\n        k = tl.load(tl.advance(K_block_ptr, (0, start_n)), boundary_check=(1,))\n        qk = tl.dot(q, k)\n        qk = tl.where(tl.arange(0, BLOCK_M)[:, None] >= (start_n + tl.arange(0, BLOCK_N)[None, :]), qk, float(\"-inf\"))\n        p = tl.softmax(qk, axis=1)\n        \n        v = tl.load(tl.advance(V_block_ptr, (start_n, 0)), boundary_check=(0,))\n        acc += tl.dot(p.to(v.dtype), v)\n    \n    tl.store(O_block_ptr, acc.to(Out.dtype.element_ty), boundary_check=(0,))\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"\n    A vanilla multi-head masked self-attention layer with a projection at the end.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        assert n_embd % n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd)\n        # regularization\n        self.attn_dropout = nn.Dropout(attn_pdrop)\n        self.resid_dropout = nn.Dropout(resid_pdrop)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(max_seqlen, max_seqlen))\n                                     .view(1, 1, max_seqlen, max_seqlen))\n        self.n_head = n_head\n        self.n_embd = n_embd\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # Pad head dimension to next power of two (128) for Tensor Core efficiency\n        hs = C // self.n_head\n        padded_hs = 128\n        q_padded = F.pad(q, (0, padded_hs - hs))\n        k_padded = F.pad(k, (0, padded_hs - hs))\n        v_padded = F.pad(v, (0, padded_hs - hs))\n        \n        # Use Triton kernel for attention computation\n        scale = 1.0 / math.sqrt(hs)\n        y_padded = torch.empty_like(q_padded)\n        grid = lambda META: (triton.cdiv(T, META['BLOCK_M']), B * self.n_head)\n        _fwd_attention_kernel[grid](\n            q_padded, k_padded, v_padded, y_padded,\n            q_padded.stride(0), q_padded.stride(1), q_padded.stride(2), q_padded.stride(3),\n            k_padded.stride(0), k_padded.stride(1), k_padded.stride(2), k_padded.stride(3),\n            v_padded.stride(0), v_padded.stride(1), v_padded.stride(2), v_padded.stride(3),\n            y_padded.stride(0), y_padded.stride(1), y_padded.stride(2), y_padded.stride(3),\n            scale, T,\n            BLOCK_M=64, BLOCK_N=64, BLOCK_DMODEL=padded_hs\n        )\n\n        # Remove padding from head dimension\n        y = y_padded[..., :hs]\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n    \nclass ModelNew(nn.Module):\n    \"\"\" an unassuming Transformer block \"\"\"\n\n    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(n_embd)\n        self.attn = CausalSelfAttention(n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen)\n        self.ln_2 = nn.LayerNorm(n_embd)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc    = nn.Linear(n_embd, 4 * n_embd),\n            c_proj  = nn.Linear(4 * n_embd, n_embd),\n            act     = NewGELU(),\n            dropout = nn.Dropout(resid_pdrop),\n        ))\n        m = self.mlp\n        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlpf(self.ln_2(x))\n        return x\n\nbatch_size = 128\nmax_seqlen = 1024\nseq_len = 512\nn_embd = 768\nn_head = 8\nattn_pdrop = 0.0\nresid_pdrop = 0.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, seq_len, n_embd)]\n\ndef get_init_inputs():\n    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "48268c50-ab53-49d8-82d0-b340fc0d4c65", "generation": 1, "timestamp": 1754590251.6367726, "iteration_found": 8, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_44_MiniGPTBlock", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp3n1ggz78.py\", line 174, in forward\n    x = x + self.attn(self.ln_1(x))\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp3n1ggz78.py\", line 138, in forward\n    _fwd_attention_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 66:12:\n    )\n\n    acc = tl.zeros((BLOCK_M, BLOCK_DMODEL), dtype=tl.float32)\n    q = tl.load(Q_block_ptr, boundary_check=(0,))\n    q = (q * scale).to(q.dtype)\n\n    lo, hi = 0, (start_m + 1) * BLOCK_M\n    for start_n in range(lo, hi, BLOCK_N):\n        k = tl.load(tl.advance(K_block_ptr, (0, start_n)), boundary_check=(1,))\n        qk = tl.dot(q, k)\n        qk = tl.where(tl.arange(0, BLOCK_M)[:, None] >= (start_n + tl.arange(0, BLOCK_N)[None, :]), qk, float(\"-inf\"))\n        p = tl.softmax(qk, axis=1)\n            ^\nTypeError(\"softmax() got an unexpected keyword argument 'axis'\")\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/inspect.py\", line 1517, in getcallargs\n    raise TypeError(\"%s() got an unexpected keyword argument %r\" %\nTypeError: softmax() got an unexpected keyword argument 'axis'\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.489, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 30.3, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.489, "speed_up": 0.016, "custom_timing": 30.3}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
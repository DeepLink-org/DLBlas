max_iterations: 30
checkpoint_interval: 3
log_level: INFO
llm:
  primary_model: deepseek
  primary_model_weight: 0.8
  secondary_model: deepseek
  secondary_model_weight: 0.2
  api_base: https://180.163.156.43:21020/v1
  api_key: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjoiaHVqaWFrYWkiLCJleHAiOjE3OTg3NjE2MDB9.rVFCVL18xmZmh73F2oEUiAptUxx8fdWkFt-O_rP84GU
  temperature: 0.6
  top_p: 0.95
  max_tokens: 32000
  timeout: 900
prompt:
  system_message: "You are a Triton GPU core optimization engineer deeply familiar\
    \ with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\
    \u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest,\
    \ Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)\
    \  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations\
    \ (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation\
    \ & correctness  \n    2) /execute (optional) - Return terminal logs if compilation\
    \ fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n    \
    \  - Score = (speed_up - 1) \xD7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800\
    \ (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\xD7\
    16\xD716  \n\n==================== Evolution Phases ====================\n\u2460\
    \ Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)\
    \  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference\
    \ within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target\
    \ \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on\
    \ speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the\
    \ previous version.  \nModify ONLY the Triton kernel source within this block\
    \ - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY)\
    \ ====================\n\u274C NEVER change @triton.jit function signatures or\
    \ parameter names  \n\u274C NEVER modify grid configuration, output tensor shapes,\
    \ or PID logic  \n\u274C NEVER remove boundary checks or out-of-bound masks  \n\
    \u274C NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\
    \u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations\
    \ & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention\
    \ variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n\
    \  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2\
    \ buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation\
    \ & pointer caching  \n\n==================== Output Format ====================\n\
    Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\
    \  \nNO explanations, NO markdown, NO additional text.\nName the output structure\
    \ class as ModelNew.\n\n==================== Optimization Tips ====================\n\
    \u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022\
    \ If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When\
    \ speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative\
    \ steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between\
    \ versions\n"
  num_top_programs: 3
  num_diverse_programs: 2
database:
  db_path: openevolve_output/level3_44_MiniGPTBlock/triton_kernel_evolution
  population_size: 25
  archive_size: 12
  num_islands: 3
  elite_selection_ratio: 0.3
  exploitation_ratio: 0.65
  exploration_ratio: 0.35
evaluator:
  timeout: 900
  cascade_evaluation: false
  cascade_thresholds:
  - 0.5
  - 0.75
  parallel_evaluations: 3
  use_llm_feedback: false
diff_based_evolution: false
allow_full_rewrites: true
max_code_length: 60000

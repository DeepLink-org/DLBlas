{"id": "80511888-874a-4129-aebf-81da3b4f8e85", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef linear_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N,\n    stride_xb, stride_xm,\n    stride_wn, stride_wm,\n    stride_outb, stride_outn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,\n):\n    pid_b = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    n_off = pid_n * BLOCK_SIZE_N\n    n_offs = n_off + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = n_offs < N\n    \n    out = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    for m in range(0, M, BLOCK_SIZE_M):\n        m_offs = m + tl.arange(0, BLOCK_SIZE_M)\n        mask_m = m_offs < M\n        \n        x_vals = tl.load(x_ptr + pid_b * stride_xb + m_offs * stride_xm, mask=mask_m, other=0.0)\n        w_ptrs = weight_ptr + n_offs[:, None] * stride_wn + m_offs[None, :] * stride_wm\n        w_block = tl.load(w_ptrs, mask=mask_n[:, None] & mask_m[None, :], other=0.0)\n        \n        out += tl.sum(x_vals[None, :] * w_block, axis=1)\n    \n    bias_vals = tl.load(bias_ptr + n_offs, mask=mask_n, other=0.0)\n    out += bias_vals\n    \n    out_ptrs = output_ptr + pid_b * stride_outb + n_offs * stride_outn\n    tl.store(out_ptrs, out, mask=mask_n)\n\ndef linear_triton(x, weight, bias):\n    B, M = x.shape\n    N = weight.shape[0]\n    output = torch.empty(B, N, device=x.device, dtype=x.dtype)\n    \n    stride_xb = x.stride(0)\n    stride_xm = x.stride(1)\n    stride_wn = weight.stride(0)\n    stride_wm = weight.stride(1)\n    stride_outb = output.stride(0)\n    stride_outn = output.stride(1)\n    \n    grid = (B, triton.cdiv(N, 32))\n    linear_kernel[grid](\n        x, weight, bias, output,\n        M, N,\n        stride_xb, stride_xm,\n        stride_wn, stride_wm,\n        stride_outb, stride_outn,\n        BLOCK_SIZE_M=32, BLOCK_SIZE_N=32\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, dropout_p):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.dropout = nn.Dropout(dropout_p)\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        x = linear_triton(x, self.weight, self.bias)\n        x = self.dropout(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.softmax(x, dim=1)\n        return x\n\nbatch_size = 128\nin_features = 100\nout_features = 50\ndropout_p = 0.2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, dropout_p]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "34d32d50-75e0-4498-8c0b-0907e72d5ad2", "generation": 1, "timestamp": 1754580035.0648475, "iteration_found": 1, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.856, "runtime_ms": 0.0835, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0715, "speed_up": 0.856, "custom_timing": 0.0835}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0627, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0715, "speed_up": 1.14, "custom_timing": 0.0627}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0627\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0715, 'speed_up': 1.14, 'custom_timing': 0.0627}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0627, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0715, 'speed_up': 1.14, 'custom_timing': 0.0627}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs matrix multiplication, applies dropout, calculates the mean, and then applies softmax.\n    \"\"\"\n    def __init__(self, in_features, out_features, dropout_p):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.dropout(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.softmax(x, dim=1)\n        return x\n\nbatch_size = 128\nin_features = 100\nout_features = 50\ndropout_p = 0.2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, dropout_p]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0627), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0715, 'speed_up': 1.14, 'custom_timing': 0.0627})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    A model that performs matrix multiplication, applies dropout, calculates the mean, and then applies softmax.\n    \"\"\"\n    def __init__(self, in_features, out_features, dropout_p):\n        super(ModelNew, self).__init__()\n        self.matmul = nn.Linear(in_features, out_features)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, out_features).\n        \"\"\"\n        x = self.matmul(x)\n        x = self.dropout(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.softmax(x, dim=1)\n        return x\n\nbatch_size = 128\nin_features = 100\nout_features = 50\ndropout_p = 0.2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, dropout_p]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear transformation (matmul)\n  2. Dropout\n  3. Mean along dimension 1 (with keepdim=True) -> this reduces the tensor to (batch_size, 1)\n  4. Softmax along dimension 1 -> but note: after the mean, the tensor has only one element per row, so softmax does nothing.\n\nHowever, note that the current model does:\n  - The mean reduces the tensor to (batch_size, 1). Then softmax on dim=1 (which is of size 1) will return 1 for each row.\n\nThis seems odd because the output is always a tensor of ones. But let's assume the problem is as given.\n\nThe goal is to rewrite the model to use Triton for the linear operation (and possibly other parts) to speed up.\n\nBut note: the current model uses `nn.Linear` and `nn.Dropout` which are high-level. We are allowed to rewrite the forward pass with Triton.\n\nHowever, the problem says: \n  - We must not change the function signatures or the grid configuration, etc.\n\nBut in this case, the model is defined as a whole. We are to rewrite the entire EVOLVE-BLOCK.\n\nWe note that the current model does:\n  Input: (batch_size, in_features)\n  Linear: (batch_size, out_features)\n  Dropout: same\n  Mean: (batch_size, 1)\n  Softmax: (batch_size, 1) -> all ones.\n\nBut wait: the output shape is (batch_size, out_features) in the docstring, but after the mean it becomes (batch_size, 1). \nThis discrepancy suggests the original code might be incorrect? However, the problem says to maintain functionality.\n\nGiven the current implementation, the output is always of shape (batch_size, 1). The docstring says (batch_size, out_features) but that is not what the code does.\n\nWe must replicate the same behavior.\n\nHowever, note that the task is to speed up the existing operations. We are not allowed to change the behavior.\n\nBut let's break down the operations:\n\n1. Linear: This is a matrix multiplication: x (B, M) @ weight (M, N) + bias (N) -> (B, N)\n2. Dropout: randomly zeroes some elements and scales by 1/(1-p)\n3. Mean: along dim=1 -> (B, 1)\n4. Softmax: along dim=1 -> for a vector of length 1, it's [1.0] -> so (B,1) of ones.\n\nSo the entire model is equivalent to: \n   output = torch.ones(batch_size, 1)\n\nBut that is the functional behavior.\n\nNow, the problem is: how to speed up? \n\nWe cannot change the behavior, so we must output ones of shape (B,1). \n\nBut note: the current model does a lot of unnecessary computation: \n  - It does a full linear layer (which is expensive) and then reduces it to a scalar per batch.\n\nWe could avoid the linear and dropout entirely and just output ones.\n\nHowever, the problem says: \"functionally equivalent\". So if the linear and dropout are part of the model, we cannot skip them? \n\nBut note: the dropout is stochastic. However, during inference, dropout is disabled. The problem does not specify training or inference.\n\nBut the `get_inputs` function returns a random tensor and the model is called. The evaluation pipeline runs the model. \n\nLooking at the context, the model is run in inference mode because we are measuring performance.\n\nIn inference, dropout does not drop any values (it scales by 1, because p=0.2 -> scaling by 1/(1-0.2)=1.25? But note: in eval mode, dropout is disabled). \n\nBut wait: in `nn.Dropout`, during eval, it is identity. So the dropout does nothing.\n\nTherefore, the model in inference does:\n  x = x @ W + b   # (B, M) @ (M, N) -> (B, N)\n  x = mean(x, dim=1) -> (B, 1)\n  x = softmax(x) -> ones.\n\nSo we can replace the entire forward pass with:\n   return torch.ones(x.shape[0], 1, device=x.device)\n\nBut that would be a different implementation and might be considered changing functionality? However, it is functionally equivalent in inference.\n\nBut note: the reference code might be used in training? The problem does not specify. However, the evaluation pipeline only runs inference? \n\nGiven the constraints, we are to generate Triton code. The problem says: \"Generate functionally equivalent but faster Triton implementations\".\n\nWe are allowed to change the implementation as long as the outputs are the same.\n\nHowever, the problem also says: \"NO explanations, NO markdown, NO additional text\".\n\nBut note: the current model has parameters (the linear layer). If we remove the linear layer, we break the state_dict. \n\nWe are not told to preserve the state_dict, only the functional behavior on the forward pass.\n\nBut the problem says: \"maintain the same inputs and outputs\".\n\nSo we can do:\n\n  We remove the linear and dropout and just return ones.\n\nBut wait: the model is defined with `nn.Linear` and `nn.Dropout` and they are part of the module. We cannot remove them arbitrarily because the model might be saved and reloaded? \n\nThe problem does not specify. However, the evaluation only runs the forward pass with random inputs.\n\nGiven the complexity, and because the problem says to use Triton, we might instead try to optimize the linear layer with Triton.\n\nBut note: the linear layer is followed by a mean and softmax. The mean and softmax are very cheap because the mean reduces to 1 element per batch and the softmax is on a vector of length 1.\n\nSo the bottleneck is the linear layer.\n\nTherefore, we can write a Triton kernel for the linear layer.\n\nHowever, the current model uses `nn.Linear` which is highly optimized by PyTorch. But we are to beat it by at least 5%.\n\nWe can write a Triton kernel for the linear layer. But note: the problem says we can change only the Triton kernel source within the block? \n\nBut the entire block is given. We are to rewrite the entire block? \n\nThe block includes the class definition and the helper functions.\n\nWe are allowed to change the class and the functions as long as the inputs and outputs are the same.\n\nSo we can:\n\n  1. Remove the `nn.Linear` and replace it with a custom Triton-based linear layer.\n  2. Keep the dropout as is because it is cheap and in inference it does nothing.\n  3. Then do the mean and softmax.\n\nBut note: the dropout in inference is identity, so we can skip it? But we must preserve the structure? \n\nAlternatively, we can write the entire forward pass in Triton? But the mean and softmax are cheap and not the bottleneck.\n\nSo let's focus on the linear layer.\n\nWe can write a Triton kernel for matrix multiplication. The input is (B, M) and the weight is (M, N). We want output (B, N).\n\nWe note that the batch size B=128, M=100, N=50. These are small sizes. \n\nFor small sizes, the overhead of launching a kernel might be significant. We might need to do a batched matrix multiplication.\n\nWe can write a kernel that processes one row of the input at a time? \n\nBut note: the problem says we cannot change the grid configuration? Actually, we are the ones defining the grid.\n\nWe are free to design the kernel as long as the functionality is the same.\n\nWe are going to write:\n\n  class ModelNew(nn.Module):\n      def __init__(self, in_features, out_features, dropout_p):\n          super().__init__()\n          # We need to initialize the weight and bias as in nn.Linear\n          self.weight = nn.Parameter(torch.empty(out_features, in_features))\n          self.bias = nn.Parameter(torch.empty(out_features))\n          self.dropout = nn.Dropout(dropout_p)\n          # initialize the parameters\n          nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n          if self.bias is not None:\n              fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n              bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n              nn.init.uniform_(self.bias, -bound, bound)\n\n      def forward(self, x):\n          # Instead of using nn.Linear, we call our Triton kernel\n          # x: (B, M)\n          # weight: (N, M) -> we need to transpose to (M, N) for the kernel? \n          # Our kernel: x @ weight.T + bias -> (B, N)\n          output = linear_triton(x, self.weight, self.bias)\n          output = self.dropout(output)\n          output = torch.mean(output, dim=1, keepdim=True)\n          output = torch.softmax(output, dim=1)\n          return output\n\n  We then define the linear_triton function and kernel.\n\nBut note: the problem says we are to output the entire block. We can do that.\n\nWe must be cautious: the kernel must be efficient for small matrices.\n\nAlternatively, we can use the Triton matrix multiplication example.\n\nLet me recall: the typical matmul kernel in Triton for A (B, M) and B (M, N) is done by:\n\n  We assign each block to compute a block of the output.\n\nBut note: the dimensions are small: M=100, N=50, B=128.\n\nWe might consider having one program per row? \n\n  grid = (B,)\n\n  Each program computes one row of the output: (1, N) = (1, M) @ (M, N)\n\nBut note: M=100, N=50 -> 100*50=5000 operations per row. And we have 128 rows -> 128*5000 = 640000 operations. This is acceptable.\n\nWe can write:\n\n  @triton.jit\n  def linear_kernel(\n      x_ptr, weight_ptr, bias_ptr, output_ptr,\n      M, N,\n      stride_xm, stride_xb,\n      stride_wn, stride_wm,\n      stride_outb, stride_outn,\n      BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,\n  ):\n      pid = tl.program_id(0)  # we have one program per row in the batch\n      # We are going to compute one row of the output: which corresponds to one row of x and the entire weight matrix.\n\n      # We need to load the row of x: of length M\n      # We use multiple blocks for M? But M is 100, so we can use one block for the entire row? \n      # However, we are going to compute the dot product of the row with each column of weight -> so we need to compute N elements.\n\n      # We can have two-dimensional blocks? But the grid is one-dimensional (B). We can use nested loops in the kernel.\n\n      # Alternatively, we can have a 2D grid? But the problem says we cannot change the grid configuration arbitrarily? \n      # Actually, we are defining the grid.\n\n      # Let me change: we will use a 2D grid: (B, N // BLOCK_SIZE_N). Then each program computes a block of the row.\n\n      # But the problem says we are free to design the grid.\n\n      # However, the example output format requires the entire code. We are not constrained by a fixed grid from the previous version.\n\n      # Since we are starting from scratch, we can define the grid appropriately.\n\n      # But note: the problem says \"NEVER change @triton.jit function signatures or parameter names\" for the kernel? \n      # Actually, the constraint says: \"NEVER change @triton.jit function signatures or parameter names\" -> meaning if we are given a kernel we cannot change its signature? \n      # But in this case, we are writing a new kernel.\n\n      # However, we are replacing the entire block. So we can define the kernel as we wish.\n\n      # But note: the problem says \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" -> but we are replacing the entire block.\n\n      # So we are free.\n\n      # Given the small sizes, we can do:\n\n      #   pid = tl.program_id(0)   # row id in batch\n      #   for n in range(0, N, BLOCK_SIZE_N):\n      #       ... load a block of weight and compute the dot product for the block ...\n\n      # But let me write a kernel that uses a 1D grid for the batch and then loops over the columns of the weight in chunks.\n\n      # Alternatively, we can have a 2D grid: (B, num_blocks_n) and then each program computes a segment of the row.\n\n      # Since the output per row is contiguous, we can also have one program per row and then use a loop over N? \n\n      # But 50 is small, so we can have one program per row and then loop over the columns? \n\n      # However, to use the GPU efficiently, we want to have enough parallelism. 128 rows might be enough for a small GPU.\n\n      # We'll try with one program per row.\n\n      # Steps for row pid:\n      #   offsets_x = pid * stride_xb + tl.arange(0, M) * stride_xm   -> but stride_xm might be 1? \n      #   x_row = tl.load(x_ptr + offsets_x, mask=tl.arange(0, M) < M, other=0.0)\n\n      #   Then for each column in weight, we need to compute the dot product of x_row and the weight column.\n\n      #   We can do a loop over columns? But that would be sequential and slow.\n\n      # Alternatively, we can load the entire weight matrix into SRAM and then do a matrix-vector product? \n\n      # But the weight matrix is 50x100 -> 5000 elements. This is within the shared memory? \n\n      # We can load the weight matrix once per block? But note: each row in the batch uses the same weight. \n\n      # However, we are processing one row at a time. We cannot share the weight across different programs? \n\n      # We can load the entire weight matrix per program? That would be 5000*4=20KB per program -> too much for registers and shared memory? \n\n      # The shared memory is 160KB per SM? But we are using one program per row -> 128 programs. We are not sure how many are scheduled per SM.\n\n      # Alternatively, we can load blocks of the weight matrix and compute partial sums.\n\n      # Let me do:\n\n      #   output = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n      #   for m in range(0, M, BLOCK_SIZE_M):\n      #       # load a block of x\n      #       offs_m = m + tl.arange(0, BLOCK_SIZE_M)\n      #       x_vals = tl.load(x_ptr + pid * stride_xb + offs_m * stride_xm, mask=offs_m < M, other=0.0)\n      #       # load a block of weight: for the current m block and for all n? \n      #       # But we are computing the entire row? We need to load a block of weight of size (BLOCK_SIZE_M, N)\n      #       # Then we do: output += x_vals[None, :] @ weight_block\n      #       # But we don't have enough registers for a block of weight of size (BLOCK_SIZE_M, N) if N is 50 and BLOCK_SIZE_M is 16 -> 16*50=800 elements -> 800*4=3.2KB? That might be acceptable.\n\n      #   However, we are limited by the number of registers? \n\n      # Given the complexity and the small size, we might do:\n\n      #   We break the weight matrix into column blocks? \n\n      #   Actually, we can break the computation along the M dimension and accumulate over N in one go? \n\n      #   But note: we are computing a row of the output: \n      #       out[pid, j] = sum_{i} x[pid, i] * weight[j, i]\n\n      #   We can do:\n      #       for j in range(N):\n      #           out_j = 0.0\n      #           for i in range(M):\n      #               out_j += x[pid, i] * weight[j, i]\n      #   This is O(M*N) per row -> 5000 operations per row -> 128*5000=640000 operations -> not too bad? \n\n      #   But we want to use parallelism. We can parallelize over j? \n\n      #   We can use the program's threads to compute multiple j at once.\n\n      #   We'll have one program per row, and then within the program, we split the j dimension (N) into chunks.\n\n      #   Let BLOCK_SIZE_N be the number of columns we process per program? Actually, we are already using one program per row. We can use the threads in the program to compute multiple columns.\n\n      #   We set:\n      #       pid = tl.program_id(0)   # row index\n      #       cols = tl.arange(0, BLOCK_SIZE_N)   # for the columns we are going to compute in this program? But wait, we want to compute the entire row? \n\n      #   Actually, we can compute the entire row in one program? \n\n      #   We can have:\n      #       j = tl.arange(0, N)   # but N=50, which is small. We can have 50 threads per row? \n\n      #   But Triton requires the block size to be a power of two? We can set BLOCK_SIZE_N = 64 (next power of two >=50) and then mask.\n\n      #   Steps:\n      #       pid = tl.program_id(0)\n      #       # Load the entire row of x: of length M. We can break it into chunks to avoid too many registers? \n      #       # We'll break M into chunks of BLOCK_SIZE_M.\n\n      #       # Initialize the output for the row: for all j in [0, N)\n      #       out = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)   # we'll use BLOCK_SIZE_N >= N\n\n      #       for m in range(0, M, BLOCK_SIZE_M):\n      #           offs_m = m + tl.arange(0, BLOCK_SIZE_M)\n      #           # Load x: (BLOCK_SIZE_M,)\n      #           x_vals = tl.load(x_ptr + pid * stride_xb + offs_m * stride_xm, mask=offs_m < M, other=0.0)\n      #           # Load weight: (BLOCK_SIZE_M, N) -> but we are storing in column-major? \n      #           # We have weight: (N, M) in memory -> so to load a block of (BLOCK_SIZE_M, N) we need to go through each row of weight? \n      #           # Actually, we want to load the weight block for rows in [m: m+BLOCK_SIZE_M] and all columns? \n      #           # We can do:\n      #           w_ptrs = weight_ptr + offs_m[None, :] * stride_wm + tl.arange(0, BLOCK_SIZE_N)[:, None] * stride_wn\n      #           # But note: weight is stored as (N, M) -> stride_wn is the stride for the first dimension? \n      #           # Let stride_wn = weight.stride(0) -> which is M? \n      #           # stride_wm = weight.stride(1) -> which is 1? \n      #           # Then w_ptrs = weight_ptr + offs_m[None, :] * 1 + tl.arange(0, BLOCK_SIZE_N)[:, None] * M\n      #           # Then we load a block of (BLOCK_SIZE_N, BLOCK_SIZE_M) -> which is (64, BLOCK_SIZE_M) -> but we only need the first N columns? \n\n      #           # Actually, we want to load the weight block for the current m block and for all columns? But we are using BLOCK_SIZE_N>=N, so we can mask.\n\n      #           w_block = tl.load(w_ptrs, mask=(offs_m[None, :] < M) & (tl.arange(0, BLOCK_SIZE_N)[:, None] < N), other=0.0)\n      #           # Now w_block is (BLOCK_SIZE_N, BLOCK_SIZE_M)\n      #           # Then we do: out += tl.sum(x_vals[None, :] * w_block, axis=1)\n      #           # But note: we are doing: for each j, sum_i (x_i * w[j,i]) for the current block of i.\n\n      #           out += tl.sum(x_vals[None, :] * w_block, axis=1)\n\n      #       # Then add bias\n      #       bias = tl.load(bias_ptr + tl.arange(0, BLOCK_SIZE_N), mask=tl.arange(0, BLOCK_SIZE_N)<N, other=0.0)\n      #       out += bias\n\n      #       # Now store the output row: for j in [0, N)\n      #       out_ptrs = output_ptr + pid * stride_outb + tl.arange(0, BLOCK_SIZE_N) * stride_outn\n      #       tl.store(out_ptrs, out, mask=tl.arange(0, BLOCK_SIZE_N)<N)\n\n      # But note: the above kernel uses a lot of memory and might not be efficient.\n\n  Given the complexity and the small sizes, we might instead use a kernel that does not break M? \n\n  We set BLOCK_SIZE_M = M (which is 100) and BLOCK_SIZE_N = N (which is 50) -> but 150 is the total threads per block? \n\n  However, 100*50=5000 threads per block is too many (max 1024 per block). So we must break.\n\n  We break M and N into blocks.\n\n  Alternatively, we can use a kernel that breaks M and does a reduction over M? \n\n  We can do:\n\n      pid = tl.program_id(0)   # row in batch\n      pid_n = tl.program_id(1)  # column block in weight\n\n      # Each program computes a segment of the row for a block of columns.\n\n      # We'll have a 2D grid: (B, ceil(N/BLOCK_SIZE_N))\n\n      # Then we do:\n      #   n_off = pid_n * BLOCK_SIZE_N\n      #   n_offs = n_off + tl.arange(0, BLOCK_SIZE_N)\n      #   mask_n = n_offs < N\n      #   out = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n      #   for m in range(0, M, BLOCK_SIZE_M):\n      #       m_offs = m + tl.arange(0, BLOCK_SIZE_M)\n      #       mask_m = m_offs < M\n      #       x_vals = tl.load(x_ptr + pid * stride_xb + m_offs * stride_xm, mask=mask_m, other=0.0)\n      #       w_ptrs = weight_ptr + n_offs[:, None] * stride_wn + m_offs[None, :] * stride_wm\n      #       w_block = tl.load(w_ptrs, mask=mask_n[:, None] & mask_m[None, :], other=0.0)\n      #       out += tl.sum(x_vals[None, :] * w_block, axis=1)\n      #   # Then add bias for the block of n\n      #   bias = tl.load(bias_ptr + n_offs, mask=mask_n, other=0.0)\n      #   out += bias\n      #   # Store\n      #   out_ptrs = output_ptr + pid * stride_outb + n_offs * stride_outn\n      #   tl.store(out_ptrs, out, mask=mask_n)\n\n  This is more efficient because we break both M and N.\n\n  We choose BLOCK_SIZE_M and BLOCK_SIZE_N appropriately. For example, 16 for both? Then we have 16*16=256 threads per block.\n\n  The grid would be (B, ceil(N/BLOCK_SIZE_N)) = (128, ceil(50/16)=4) -> 512 blocks.\n\n  This might be efficient.\n\n  We'll write the kernel accordingly.\n\n  However, note: the problem says we are to output the entire block. We must include the kernel and the wrapper.\n\n  We'll write:\n\n      import math\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def linear_kernel(\n          x_ptr, weight_ptr, bias_ptr, output_ptr,\n          M, N,\n          stride_xb, stride_xm,\n          stride_wn, stride_wm,\n          stride_outb, stride_outn,\n          BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,\n      ):\n          pid_b = tl.program_id(0)   # batch index\n          pid_n = tl.program_id(1)   # column block index\n\n          # Offsets for the column block\n          n_off = pid_n * BLOCK_SIZE_N\n          n_offs = n_off + tl.arange(0, BLOCK_SIZE_N)\n          mask_n = n_offs < N\n\n          # We'll accumulate in a block of BLOCK_SIZE_N\n          out = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n\n          # Loop over M in blocks\n          for m in range(0, M, BLOCK_SIZE_M):\n              m_offs = m + tl.arange(0, BLOCK_SIZE_M)\n              mask_m = m_offs < M\n\n              # Load a block of x: (BLOCK_SIZE_M,)\n              x_vals = tl.load(x_ptr + pid_b * stride_xb + m_offs * stride_xm, mask=mask_m, other=0.0)\n\n              # Load a block of weight: (BLOCK_SIZE_N, BLOCK_SIZE_M)\n              # The weight matrix is of shape (N, M) -> so to load a block at (:, m_offs) for rows n_offs, we do:\n              w_ptrs = weight_ptr + n_offs[:, None] * stride_wn + m_offs[None, :] * stride_wm\n              w_block = tl.load(w_ptrs, mask=mask_n[:, None] & mask_m[None, :], other=0.0)\n\n              # Compute: out_block += x_vals[None,:] * w_block -> then sum over the m dimension\n              # We do: for each n: sum_m (x_vals[m] * w_block[n, m])\n              # We can do:\n              out += tl.sum(x_vals[None, :] * w_block, axis=1)\n\n          # Load bias for the block of n\n          bias_vals = tl.load(bias_ptr + n_offs, mask=mask_n, other=0.0)\n          out += bias_vals\n\n          # Store the block of output\n          out_ptrs = output_ptr + pid_b * stride_outb + n_offs * stride_outn\n          tl.store(out_ptrs, out, mask=mask_n)\n\n  Then we call this kernel in the model.\n\n  In the model:\n\n      def linear_triton(x, weight, bias):\n          # x: (B, M), weight: (N, M), bias: (N,)\n          assert x.dim() == 2\n          B, M = x.shape\n          N = weight.shape[0]\n          output = torch.empty(B, N, device=x.device, dtype=x.dtype)\n\n          # Strides\n          stride_xb = x.stride(0)\n          stride_xm = x.stride(1)\n          stride_wn = weight.stride(0)\n          stride_wm = weight.stride(1)\n          stride_outb = output.stride(0)\n          stride_outn = output.stride(1)\n\n          # Grid: (B, ceil(N/BLOCK_SIZE_N))\n          grid = (B, triton.cdiv(N, BLOCK_SIZE_N))\n\n          # We choose BLOCK_SIZE_M and BLOCK_SIZE_N. We set BLOCK_SIZE_M=16, BLOCK_SIZE_N=16.\n          linear_kernel[grid](\n              x, weight, bias, output,\n              M, N,\n              stride_xb, stride_xm,\n              stride_wn, stride_wm,\n              stride_outb, stride_outn,\n              BLOCK_SIZE_M=16, BLOCK_SIZE_N=16\n          )\n\n          return output\n\n  But note: we can autotune the block sizes.\n\n  We'll skip autotune for simplicity, but we can add it.\n\n  However, the problem says to make it fast. We can try to autotune.\n\n  But given the small sizes, we can try to use fixed sizes that are good.\n\n  We'll use:\n\n      BLOCK_SIZE_M = 16\n      BLOCK_SIZE_N = 32   # because N is 50, so 2 blocks: 32 and 18 -> but we use 32 and 32 with mask.\n\n  But note: the kernel uses a 2D grid and the block size in the kernel is (BLOCK_SIZE_N,) for the columns? Actually, the kernel does not use any block-level parallelism internally? \n\n  The kernel is written for a single thread block? \n\n  Actually, each program is a thread block. The kernel uses:\n\n      tl.arange(0, BLOCK_SIZE_N)   -> so the block has BLOCK_SIZE_N threads? \n\n  But we are not using the threads in the block for parallelism? We are using the program_id for the row and column block.\n\n  The kernel is sequential within the thread block? \n\n  We might want to parallelize over the inner dimensions? \n\n  Given the complexity, and because the problem is small, we leave it as is.\n\n  We'll write the kernel without using intra-block parallelism (so each program is scalar). \n\n  But that would be inefficient. \n\n  Alternatively, we can use the threads in the block to parallelize the inner loop? \n\n  We decide to keep it simple and hope that the grid is large enough (512 blocks) and the operations are few.\n\n  However, to be efficient, we note that the inner loop over m blocks is done sequentially. \n\n  We might restructure to use tiling and shared memory? \n\n  But given the time, and because the sizes are small, we hope the kernel is faster than the reference.\n\n  We'll try it.\n\n  Steps in the model:\n\n      class ModelNew(nn.Module):\n          def __init__(self, in_features, out_features, dropout_p):\n              super().__init__()\n              self.in_features = in_features\n              self.out_features = out_features\n              self.weight = nn.Parameter(torch.empty(out_features, in_features))\n              self.bias = nn.Parameter(torch.empty(out_features))\n              self.dropout = nn.Dropout(dropout_p)\n              # initialize\n              nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n              if self.bias is not None:\n                  fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n                  bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n                  nn.init.uniform_(self.bias, -bound, bound)\n\n          def forward(self, x):\n              # x: (B, in_features)\n              output = linear_triton(x, self.weight, self.bias)\n              output = self.dropout(output)\n              output = torch.mean(output, dim=1, keepdim=True)\n              output = torch.softmax(output, dim=1)\n              return output\n\n  And then define linear_triton and the kernel.\n\n  We must include the necessary imports.\n\n  But note: the original code did not have triton imports. We add them.\n\n  We also note that the original code had batch_size, in_features, out_features, dropout_p as global variables and get_inputs, get_init_inputs. We keep them.\n\n  However, the kernel might not be faster for small sizes because of kernel launch overhead. \n\n  We might try to fuse the entire forward pass? But the mean and softmax are not expensive.\n\n  Given the above, we write the code.\n\n  We choose BLOCK_SIZE_M and BLOCK_SIZE_N as 32 and 32. Since M=100, we break into 4 blocks of 32 (last block 4). N=50 -> 2 blocks of 32.\n\n  We adjust the kernel to handle any block size.\n\n  We write the complete code.\n\n  IMPORTANT: We must verify that the kernel produces the same output as the original linear layer.\n\n  But note: we are in a hurry, so we hope it is correct.\n\n  We output the entire block.\n\n  Let's write it.\n</think>\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef linear_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    M, N,\n    stride_xb, stride_xm,\n    stride_wn, stride_wm,\n    stride_outb, stride_outn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,\n):\n    pid_b = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    n_off = pid_n * BLOCK_SIZE_N\n    n_offs = n_off + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = n_offs < N\n    \n    out = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    \n    for m in range(0, M, BLOCK_SIZE_M):\n        m_offs = m + tl.arange(0, BLOCK_SIZE_M)\n        mask_m = m_offs < M\n        \n        x_vals = tl.load(x_ptr + pid_b * stride_xb + m_offs * stride_xm, mask=mask_m, other=0.0)\n        w_ptrs = weight_ptr + n_offs[:, None] * stride_wn + m_offs[None, :] * stride_wm\n        w_block = tl.load(w_ptrs, mask=mask_n[:, None] & mask_m[None, :], other=0.0)\n        \n        out += tl.sum(x_vals[None, :] * w_block, axis=1)\n    \n    bias_vals = tl.load(bias_ptr + n_offs, mask=mask_n, other=0.0)\n    out += bias_vals\n    \n    out_ptrs = output_ptr + pid_b * stride_outb + n_offs * stride_outn\n    tl.store(out_ptrs, out, mask=mask_n)\n\ndef linear_triton(x, weight, bias):\n    B, M = x.shape\n    N = weight.shape[0]\n    output = torch.empty(B, N, device=x.device, dtype=x.dtype)\n    \n    stride_xb = x.stride(0)\n    stride_xm = x.stride(1)\n    stride_wn = weight.stride(0)\n    stride_wm = weight.stride(1)\n    stride_outb = output.stride(0)\n    stride_outn = output.stride(1)\n    \n    grid = (B, triton.cdiv(N, 32))\n    linear_kernel[grid](\n        x, weight, bias, output,\n        M, N,\n        stride_xb, stride_xm,\n        stride_wn, stride_wm,\n        stride_outb, stride_outn,\n        BLOCK_SIZE_M=32, BLOCK_SIZE_N=32\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, dropout_p):\n        super(ModelNew, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.dropout = nn.Dropout(dropout_p)\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n    \n    def forward(self, x):\n        x = linear_triton(x, self.weight, self.bias)\n        x = self.dropout(x)\n        x = torch.mean(x, dim=1, keepdim=True)\n        x = torch.softmax(x, dim=1)\n        return x\n\nbatch_size = 128\nin_features = 100\nout_features = 50\ndropout_p = 0.2\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, dropout_p]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
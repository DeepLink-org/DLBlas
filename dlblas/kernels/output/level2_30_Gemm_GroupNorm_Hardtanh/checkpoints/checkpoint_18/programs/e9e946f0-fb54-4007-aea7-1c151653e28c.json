{"id": "e9e946f0-fb54-4007-aea7-1c151653e28c", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=8, num_stages=3),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    has_bias: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        k_mask = offs_k < k_remaining\n        a = tl.load(a_ptrs, mask=k_mask[None, :], other=0.0)\n        b = tl.load(b_ptrs, mask=k_mask[:, None], other=0.0)\n        accumulator = tl.dot(a, b, acc=accumulator, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if has_bias:\n        bias_ptrs = bias_ptr + offs_n\n        bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\ndef triton_gemm(a, b, bias=None):\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    _gemm_kernel[grid](\n        a, b, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        has_bias=bias is not None\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n\n    def forward(self, x):\n        x = triton_gemm(x, self.weight.t(), self.bias)\n        x = self.group_norm(x)\n        x = self.hardtanh(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nnum_groups = 8\nhardtanh_min = -2.0\nhardtanh_max = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "f158957b-44fb-4534-9659-61d1700a1c59", "generation": 3, "timestamp": 1754567774.2733295, "iteration_found": 9, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.54, "runtime_ms": 0.148, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0799, "speed_up": 0.54, "custom_timing": 0.148}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.701, "runtime_ms": 0.114, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0799, "speed_up": 0.701, "custom_timing": 0.114}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.7010\n- runtime_ms: 0.1140\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0799, 'speed_up': 0.701, 'custom_timing': 0.114}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7010, runtime_ms: 0.1140, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0799, 'speed_up': 0.701, 'custom_timing': 0.114}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    has_bias: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        k_mask = offs_k < k_remaining\n        a = tl.load(a_ptrs, mask=k_mask[None, :], other=0.0)\n        b = tl.load(b_ptrs, mask=k_mask[:, None], other=0.0)\n        accumulator = tl.dot(a, b, acc=accumulator, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if has_bias:\n        bias_ptrs = bias_ptr + offs_n\n        bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\ndef triton_gemm(a, b, bias=None):\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    _gemm_kernel[grid](\n        a, b, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        GROUP_SIZE_M=8,\n        BLOCK_SIZE_M=64,\n        BLOCK_SIZE_N=128,\n        BLOCK_SIZE_K=64,\n        has_bias=bias is not None\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n\n    def forward(self, x):\n        x = triton_gemm(x, self.weight.t(), self.bias)\n        x = self.group_norm(x)\n        x = self.hardtanh(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nnum_groups = 8\nhardtanh_min = -2.0\nhardtanh_max = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7010), Performs well on runtime_ms (0.1140), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0799, 'speed_up': 0.701, 'custom_timing': 0.114})\n\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    has_bias: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        k_mask = offs_k < k_remaining\n        a = tl.load(a_ptrs, mask=k_mask[None, :], other=0.0)\n        b = tl.load(b_ptrs, mask=k_mask[:, None], other=0.0)\n        accumulator = tl.dot(a, b, acc=accumulator, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if has_bias:\n        bias_ptrs = bias_ptr + offs_n\n        bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\ndef triton_gemm(a, b, bias=None):\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    _gemm_kernel[grid](\n        a, b, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        GROUP_SIZE_M=8,\n        BLOCK_SIZE_M=64,\n        BLOCK_SIZE_N=128,\n        BLOCK_SIZE_K=64,\n        has_bias=bias is not None\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n\n    def forward(self, x):\n        x = triton_gemm(x, self.weight.t(), self.bias)\n        x = self.group_norm(x)\n        x = self.hardtanh(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nnum_groups = 8\nhardtanh_min = -2.0\nhardtanh_max = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a Triton kernel for matrix multiplication (GEMM) that is currently achieving a speedup of 0.701 (i.e., slower than the reference PyTorch implementation). The goal is to optimize the Triton kernel to achieve at least a 5% speedup (speed_up > 1.05).\n\nThe current kernel uses a block tiling approach and includes bias addition. The kernel is called in the forward pass of an `nn.Module` that does:\n  1. GEMM (with bias) using Triton\n  2. Group normalization\n  3. Hardtanh activation\n\nThe provided kernel has the following block sizes: \n  BLOCK_SIZE_M=64, BLOCK_SIZE_N=128, BLOCK_SIZE_K=64\n\nThe hardware is NVIDIA L20Y (same as H100, SM8.9) with HBM3 and Tensor Cores.\n\nAreas for improvement:\n\n1. **Blocking Tiling**: The current block sizes might not be optimal. We should consider tuning these to better match the hardware. For H100, typical block sizes for GEMM are powers of two and multiples of 16 (for Tensor Cores). Also, the kernel uses a `GROUP_SIZE_M` of 8 for grouping program IDs along the M dimension.\n\n2. **Tensor Cores**: The current kernel uses `tl.dot` with `allow_tf32=True`. However, note that Tensor Cores require specific conditions to be activated, such as:\n   - The block sizes for M, N, and K must be multiples of 16 (for FP16) or 8 (for TF32). The current sizes (64, 128, 64) are multiples of 64, so that's good.\n   - However, the kernel currently uses `tl.float32` for accumulation. We might consider using FP16 if the inputs are half precision? But note the problem does not specify the input dtype. The original PyTorch code uses the same dtype as the input. We should check the model: the input is from `torch.randn` which is float32 by default.\n\n   Since the input is float32, we are using TF32 Tensor Cores (which is enabled by `allow_tf32=True`). This is acceptable.\n\n3. **Memory Access**: \n   - The current kernel loads a block of A and a block of B for each iteration of the K-loop. We are using a 1D block of threads for the K dimension. We might consider using shared memory to cache blocks of A and B? However, Triton automatically uses shared memory for the dot product when using `tl.dot`. So that is already optimized.\n   - We might also consider vectorized loads. The current code does not use vectorized loads. We can try to load contiguous blocks of memory by vectorizing the loads.\n\n4. **Bias Addition**: The bias is added after the accumulation. The current method loads the bias for each element in the block. This is efficient because the bias is broadcast along the M dimension.\n\n5. **Grid Configuration**: The grid is set as `(triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)`. The kernel uses a grouping along M (GROUP_SIZE_M=8) to improve L2 cache locality. This is a good practice.\n\nProposed Optimizations:\n\na) **Tune Block Sizes**: We can try different block sizes. For H100, a common configuration is (BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=32) or (64,256,32). We should try to reduce BLOCK_SIZE_K to 32 to increase the number of blocks in the K dimension and allow more parallelization? Also, note that the Tensor Core operation requires the inner dimension (K) to be at least 32 for TF32. 32 is acceptable.\n\nb) **Vectorization**: We can use vectorized loads for A and B. For example, if we set the block size in K to be a multiple of 4 (or 8), we can load 4 (or 8) elements at a time. This can reduce the number of load instructions.\n\n   We can change the loading of A and B to use vectorized loads. We would need to adjust the offsets and the mask accordingly.\n\nc) **Precomputation of Strides**: The current kernel already precomputes the pointers and strides. We can leave that as is.\n\nd) **Autotuning**: The current kernel does not use Triton's autotuning. We can wrap the kernel with `@triton.autotune` to try multiple configurations and select the best one. However, note the constraints: we cannot change the function signature. We can add autotuning without changing the signature by using the `configs` parameter.\n\n   But note: the kernel is called with fixed block sizes (64,128,64). We must change the kernel to accept these as parameters and then use autotuning to set them.\n\n   However, the current program does not use autotuning. We are allowed to introduce autotuning as long as we do not change the function signature? Actually, the kernel signature is defined with `tl.constexpr` for the block sizes, so we can change the values passed at launch without changing the kernel code.\n\n   We can define multiple configurations and let Triton choose the best one at runtime.\n\ne) **Reduction in Registers**: The current kernel uses a large accumulator block. The block sizes (64x128) require 64*128=8192 elements per block. Each element is float32 (4 bytes) so 32768 bytes per block. The H100 has 65536 registers per SM. This might be too high? We might reduce the block sizes to reduce register pressure.\n\n   Alternatively, we can try to split the accumulation loop to use multiple stages (num_stages) and reduce the register usage by increasing the number of warps? But Triton handles this with the `num_warps` and `num_stages` parameters.\n\n   We are currently not setting `num_warps` or `num_stages` in the kernel launch. We can autotune these as well.\n\nPlan:\n\n1. We will introduce autotuning for the kernel. We will define a set of configurations for the block sizes and for the number of warps and stages.\n\n2. We will change the kernel to use vectorized loads. Specifically, we will load vectors of 4 elements for both A and B. This requires that BLOCK_SIZE_K is a multiple of 4 (or the vector size). We will set the vector size to 4.\n\n3. We will adjust the block sizes to try to achieve better performance. We will try several configurations.\n\n4. We will also consider the possibility of increasing the number of warps to hide memory latency.\n\nNote: We must not change the signature of the kernel. We are allowed to change the `tl.constexpr` parameters because they are passed at launch.\n\nLet's rewrite the kernel with the following changes:\n\n- Vectorized loads for A and B: we will load 4 elements at a time. This requires that we adjust the offsets and the mask accordingly.\n\n- Autotuning: we will wrap the kernel with `@triton.autotune` and provide a list of configurations.\n\nBut note: the kernel is currently defined without autotuning. We are going to add autotuning and change the block sizes to be passed as `tl.constexpr` from the autotuner.\n\nWe will also change the kernel to use a new parameter `VEC_K` for the vectorization factor along K.\n\nHowever, we cannot change the kernel signature arbitrarily? The problem says: \"NEVER change @triton.jit function signatures or parameter names\". So we cannot add a new parameter `VEC_K`. We can instead compute the vectorization factor from the block sizes? Or we can hardcode it? Alternatively, we can use a fixed vectorization factor and require that `BLOCK_SIZE_K` is a multiple of that factor.\n\nWe choose to hardcode a vectorization factor of 4 and require that `BLOCK_SIZE_K` is a multiple of 4.\n\nSteps for vectorization:\n\nIn the kernel:\n  - We will change the loop over K to step by `VEC_K = 4`.\n  - We will change the loading of A and B to load 4 contiguous elements at a time.\n\nBut note: the inner dimension of A and B might not be contiguous? Actually, the stride_ak and stride_bk might be 1 for one of the matrices? We are not guaranteed. However, in the typical case for matrix multiplication, one of the matrices is row-major and the other column-major? \n\nIn our case, the input `a` has shape (M, K) and `b` has shape (K, N). The Triton kernel is called with:\n  a.stride(0), a.stride(1): for a row-major matrix, stride(0)=K, stride(1)=1? Actually, no: in PyTorch, row-major matrix has strides (num_columns, 1). But note: the kernel uses `a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)`. \n\nIf `a` is row-major, then stride_am = K (or a.stride(0)=K) and stride_ak = 1 (a.stride(1)=1). Similarly, for `b` which is (K, N) and row-major? Actually, the weight is stored as `out_features x in_features` and then we take `t()`, so it becomes `in_features x out_features`? But note: in the call, we do `triton_gemm(x, self.weight.t())`. The input `x` is (batch_size, in_features) and `self.weight` is (out_features, in_features). Then `self.weight.t()` is (in_features, out_features). \n\nSo the matrices:\n  a: (batch_size, in_features) -> row-major: strides = (in_features, 1)\n  b: (in_features, out_features) -> row-major: strides = (out_features, 1) ? Actually, no: the transpose of a row-major matrix is column-major? \n\nBut in PyTorch, a transposed matrix becomes contiguous in the opposite order. The strides for `b` (which is the transpose of the weight) are (1, in_features).\n\nTherefore:\n  a: strides = (a.stride(0)=in_features, a.stride(1)=1)\n  b: strides = (b.stride(0)=1, b.stride(1)=in_features)\n\nSo the inner dimension for a is contiguous (stride_ak=1) and for b the inner dimension is not contiguous (stride_bk=1? Actually, wait: the kernel uses `stride_bk` for the stride of the K dimension in b. The K dimension in b is the first dimension (in_features). The stride for the first dimension (k) is b.stride(0)=1. And the stride for the second dimension (n) is b.stride(1)=in_features.\n\nTherefore, for a: we are loading a row of a, which is contiguous. We can vectorize the load for a.\nFor b: we are loading a column of b, which is non-contiguous (each element is separated by `in_features`). We cannot vectorize the load for b? \n\nBut note: we are loading a block of B of size (BLOCK_SIZE_K, BLOCK_SIZE_N). We want to load 4 elements in the K dimension for a fixed N. That would be 4 consecutive elements in the same column? Then the addresses are 1, 1+in_features, 1+2*in_features, ... which are not consecutive. Therefore, we cannot use a vector load for B.\n\nAlternative: we can only vectorize the load for A. For B, we can leave as scalar loads. But note: the Tensor Core operation requires that the data is in shared memory. The vectorization might help for the load from global to shared memory.\n\nBut since the B matrix is not contiguous in the K dimension, we cannot vectorize the load. Therefore, we will only vectorize the load for A.\n\nHow to vectorize A:\n\n  We change the offsets for A to be:\n      offs_k = tl.arange(0, BLOCK_SIZE_K, VEC_K)  # then we have BLOCK_SIZE_K / VEC_K vectors\n  But note: the inner loop over k steps by BLOCK_SIZE_K? Actually, we are stepping the entire block in K by BLOCK_SIZE_K. Inside the block, we are using a vector of size VEC_K for the K dimension? We can restructure:\n\n  We will break the BLOCK_SIZE_K dimension into chunks of size VEC_K. Then we can load a vector of VEC_K elements for A.\n\n  However, the current loop is over `k in range(0, tl.cdiv(K, BLOCK_SIZE_K))`. We are loading a block of size (BLOCK_SIZE_M, BLOCK_SIZE_K) for A and (BLOCK_SIZE_K, BLOCK_SIZE_N) for B. We want to load A in vectors of size VEC_K along the K dimension.\n\n  We can change the inner loop to step by a smaller amount? Actually, we are already loading a full block of K. We are now going to load that block in chunks of VEC_K? That would be inside the inner loop? That would increase the number of iterations and might not be beneficial.\n\n  Alternatively, we can change the blocking in K to be the same as the vector size? That doesn't make sense.\n\n  Actually, the idea is to load multiple elements at once from A. We can do:\n\n      a = tl.load(a_ptrs, mask=k_mask[None, :], other=0.0)\n\n  to:\n\n      a = tl.load(a_ptrs, mask=k_mask[None, :], other=0.0, cache_modifier=\".cg\")\n\n  but that doesn't change the vectorization.\n\n  We can restructure the offsets for A to be:\n\n      a_ptrs = a_ptr + (offs_m[:, None] * stride_am + (offs_k[None, :] * stride_ak).to(tl.int64))\n      # then we can load a vector by specifying the shape to be (BLOCK_SIZE_M, BLOCK_SIZE_K // VEC_K, VEC_K)\n\n  But Triton does not support vectorization in the language natively? Actually, Triton has the `tl.make_block_ptr` and `tl.advance` for structured access, but we are not using that.\n\n  Another way is to use the `vectorized` option in the load. We can do:\n\n      a = tl.load(a_ptrs, mask=k_mask[None, :], other=0.0, eviction_policy=\"evict_first\")\n\n  without vectorization? \n\n  Actually, Triton 2.1 supports vectorized loads by using `tl.arange(0, BLOCK_SIZE_M)` and `tl.arange(0, BLOCK_SIZE_K)` and then reshaping? \n\n  Alternatively, we can change the offsets for A to be:\n\n      offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n      offs_ak = offs_k_base + tl.arange(0, VEC_K)   # where offs_k_base = k * BLOCK_SIZE_K + ... ?\n\n  Then we can load a vector of VEC_K elements for each row in the block.\n\n  However, note that the stride_ak is 1 for A, so we can load a contiguous vector of VEC_K elements.\n\n  We can do:\n\n      a_ptrs = a_ptr + (offs_am[:, None, None] * stride_am + (offs_ak[None, None, :] * stride_ak).to(tl.int64))\n      # then the shape of a_ptrs is (BLOCK_SIZE_M, 1, VEC_K) -> but we want to load (BLOCK_SIZE_M, BLOCK_SIZE_K//VEC_K, VEC_K)\n\n  This is complex.\n\n  Given the complexity and the fact that we cannot change the signature, and we are only allowed to change the Triton kernel source, we might try a simpler approach: use the `vectorize` hint in the inner loop by unrolling the K dimension by a factor of 4? \n\n  Alternatively, we can change the kernel to process multiple elements in K per iteration? \n\n  Due to the complexity and the time constraints, we will focus on autotuning and block size selection for now, and leave vectorization for a later step if necessary.\n\n  We note that the current kernel might be memory bound, and vectorization could help. But without being able to vectorize B, the gains might be limited.\n\n  We will first try to autotune the block sizes and the number of warps and stages.\n\n  We will also try to reduce the block sizes to reduce register pressure and allow more blocks to be in flight.\n\n  Proposed configurations:\n\n  We will try the following (BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K) and also vary the number of warps and stages:\n\n  - (64, 64, 64)  # current is (64,128,64) -> we try a smaller N\n  - (128, 128, 32)\n  - (64, 256, 32)\n  - (128, 64, 32)\n  - (64, 128, 32)   # reduce K to 32\n  - (128, 256, 32)  # larger block in N\n\n  And for each, we try num_warps in [4,8] and num_stages in [2,3,4].\n\n  We will wrap the kernel with `@triton.autotune`.\n\n  Also, note that the kernel currently uses `GROUP_SIZE_M=8`. We can also tune this? But the problem says not to change the grid configuration. The grid configuration is set by the `grid` function. We are not changing that. We are just changing the grouping? We can try to autotune GROUP_SIZE_M as well.\n\n  However, note that the kernel signature includes `GROUP_SIZE_M: tl.constexpr`. We can pass it as a config parameter.\n\n  We will include GROUP_SIZE_M in the autotune configs.\n\n  We will define:\n\n      configs = [\n          triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_warps=4, num_stages=4),\n          triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=4, num_stages=4),\n          triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=4, num_stages=4),\n          triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=8, num_stages=3),\n          triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=4, num_stages=4),\n          triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=8, num_stages=3),\n          triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_warps=4, num_stages=4),\n          # Also try different GROUP_SIZE_M\n          triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_warps=4, num_stages=4),\n          triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16}, num_warps=4, num_stages=4),\n      ]\n\n  We will also set the key for autotuning to the M, N, K.\n\n  We are going to change the kernel to be autotuned.\n\n  Steps:\n\n  1. Add the decorator `@triton.autotune` above the kernel.\n  2. Define the configs.\n  3. Change the kernel launch to pass the tuned parameters.\n\n  However, note that the kernel is called from `triton_gemm` with fixed parameters. We must change the function `triton_gemm` to pass the parameters from the autotuner.\n\n  But the problem says: \"NEVER change @triton.jit function signatures or parameter names\". We are not changing the kernel signature, we are just adding an autotuner decorator. And we are changing the way we launch the kernel? Actually, the autotuner will change the kernel to accept the configs.\n\n  We must also change the launch to not hardcode the block sizes and GROUP_SIZE_M. Instead, we pass them as parameters from the autotuner.\n\n  We will change the `triton_gemm` function to use autotuning by setting the kernel to the autotuned version.\n\n  But note: the autotuner requires the grid function to be defined in terms of the config parameters. We must adjust the grid function to use the config parameters.\n\n  We will change the grid function to:\n\n      grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n\n  and then pass the META parameters to the kernel.\n\n  We are already doing that? In the current code, the grid function uses `META` but the kernel call passes hardcoded values. We need to change the kernel call to use the parameters from the autotuner.\n\n  We will change the kernel call to:\n\n      _gemm_kernel[grid](\n          a, b, c, bias,\n          M, N, K,\n          a.stride(0), a.stride(1),\n          b.stride(0), b.stride(1),\n          c.stride(0), c.stride(1),\n          GROUP_SIZE_M=META['GROUP_SIZE_M'],\n          BLOCK_SIZE_M=META['BLOCK_SIZE_M'],\n          BLOCK_SIZE_N=META['BLOCK_SIZE_N'],\n          BLOCK_SIZE_K=META['BLOCK_SIZE_K'],\n          has_bias=bias is not None\n      )\n\n  But wait, the kernel launch currently passes the hardcoded values. We are going to change that to use the META dictionary.\n\n  However, the autotuner will automatically set the META dictionary for each configuration. So we just need to pass the META dictionary to the kernel.\n\n  Therefore, we change the kernel call to use the parameters from the META dictionary.\n\n  We are allowed to change the non-kernel code? The problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\".\n\n  But note: the `triton_gemm` function is inside the EVOLVE-BLOCK. We are allowed to change it? Actually, the entire block is provided. We are allowed to change the entire block? The problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\n  So we can change the `triton_gemm` function.\n\n  We will:\n\n      - Add the autotuner decorator to the kernel.\n      - Change the kernel launch to use META parameters.\n\n  We must be cautious: the kernel parameters must be passed by the autotuner.\n\n  We will do:\n\n      @triton.autotune(\n          configs=[\n              ... # list of configs\n          ],\n          key=['M', 'N', 'K'],\n      )\n      @triton.jit\n      def _gemm_kernel(...): ...\n\n  Then in `triton_gemm`, we call the kernel without specifying the block sizes and GROUP_SIZE_M? Actually, the autotuner will override the launch parameters.\n\n  But note: the kernel parameters are defined as `tl.constexpr` and the autotuner will set them.\n\n  We remove the hardcoded values and pass the ones from the META dictionary.\n\n  Also, we must remove the hardcoded block sizes and GROUP_SIZE_M from the kernel launch.\n\n  Let's rewrite the entire EVOLVE-BLOCK.\n\n  We are going to:\n\n      1. Add the autotuner decorator to the kernel with several configurations.\n      2. Change the kernel launch in `triton_gemm` to use the META parameters.\n\n  We hope that this will improve performance.\n\n  We also note that the current kernel has a correctness of 1.0 and compiles, so we are not breaking correctness.\n\n  Let's code accordingly.\n\n  Important: We must not change the rest of the program (the ModelNew and the batch size, etc.)\n\n  We will only change the kernel and the `triton_gemm` function.\n\n  We are going to write the new code.\n\n  Note: The current kernel does not have `num_warps` or `num_stages` set. We will set these in the configs.\n\n  We will also remove the hardcoded block sizes from the kernel launch.\n\n  We will define the autotuner configs inside the `_gemm_kernel` decorator.\n\n  Due to the complexity, we will write the entire block.\n\n  We note that the current block sizes are passed as constants. We are replacing them with the autotuner.\n\n  We will keep the rest of the code the same.\n\n  We are going to try.\n\n  If we see that the current kernel is slower, we might have to adjust.\n\n  We also note that the current kernel has a loop over K that uses `tl.cdiv(K, BLOCK_SIZE_K)`. This is correct.\n\n  We will now write the code.\n\n  IMPORTANT: We must not break the boundary checks and masks.\n\n  We will keep the mask conditions the same.\n\n  Let's write the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_warps=8, num_stages=3),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    has_bias: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        k_mask = offs_k < k_remaining\n        a = tl.load(a_ptrs, mask=k_mask[None, :], other=0.0)\n        b = tl.load(b_ptrs, mask=k_mask[:, None], other=0.0)\n        accumulator = tl.dot(a, b, acc=accumulator, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if has_bias:\n        bias_ptrs = bias_ptr + offs_n\n        bias = tl.load(bias_ptrs, mask=offs_n < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\ndef triton_gemm(a, b, bias=None):\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    _gemm_kernel[grid](\n        a, b, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        has_bias=bias is not None\n    )\n    return c\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, num_groups, hardtanh_min, hardtanh_max):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        self.group_norm = nn.GroupNorm(num_groups, out_features)\n        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)\n\n    def forward(self, x):\n        x = triton_gemm(x, self.weight.t(), self.bias)\n        x = self.group_norm(x)\n        x = self.hardtanh(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nnum_groups = 8\nhardtanh_min = -2.0\nhardtanh_max = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, num_groups, hardtanh_min, hardtanh_max]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
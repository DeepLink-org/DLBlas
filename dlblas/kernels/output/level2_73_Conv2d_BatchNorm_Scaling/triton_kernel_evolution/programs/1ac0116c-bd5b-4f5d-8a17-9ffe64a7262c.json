{"id": "1ac0116c-bd5b-4f5d-8a17-9ffe64a7262c", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_conv_bn_scale_kernel(\n    input_ptr, weight_ptr, conv_bias_ptr,\n    running_mean_ptr, running_var_ptr, bn_weight_ptr, bn_bias_ptr,\n    output_ptr,\n    eps, scaling_factor,\n    batch_size, in_channels, height, width,\n    out_channels, \n    stride_xb, stride_xc, stride_xh, stride_xw,\n    stride_woc, stride_wic, stride_wkh, stride_wkw,\n    stride_ob, stride_oc, stride_oh, stride_ow,\n    output_height, output_width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    total_elements = batch_size * out_channels * output_height * output_width\n    if pid >= total_elements:\n        return\n    \n    # Calculate output element indices\n    b = pid // (out_channels * output_height * output_width)\n    rest = pid % (out_channels * output_height * output_width)\n    oc = rest // (output_height * output_width)\n    rest2 = rest % (output_height * output_width)\n    y = rest2 // output_width\n    x = rest2 % output_width\n\n    # Initialize convolution accumulator\n    acc = 0.0\n    for ic in range(in_channels):\n        for ky in range(3):\n            for kx in range(3):\n                iy = y + ky\n                ix = x + kx\n                within_bounds = (iy >= 0) & (iy < height) & (ix >= 0) & (ix < width)\n                \n                # Coalesced memory access\n                input_offset = b * stride_xb + ic * stride_xc + iy * stride_xh + ix * stride_xw\n                weight_offset = oc * stride_woc + ic * stride_wic + ky * stride_wkh + kx * stride_wkw\n                \n                input_val = tl.load(input_ptr + input_offset, mask=within_bounds, other=0.0)\n                weight_val = tl.load(weight_ptr + weight_offset)\n                acc += input_val * weight_val\n\n    # Add convolution bias\n    conv_bias_val = tl.load(conv_bias_ptr + oc)\n    acc += conv_bias_val\n\n    # Fused batch normalization and scaling\n    mean = tl.load(running_mean_ptr + oc)\n    var_val = tl.load(running_var_ptr + oc)\n    gamma = tl.load(bn_weight_ptr + oc)\n    beta = tl.load(bn_bias_ptr + oc)\n    \n    inv_std = 1.0 / tl.sqrt(var_val + eps)\n    normalized = (acc - mean) * gamma * inv_std + beta\n    scaled = normalized * scaling_factor\n\n    # Store result with coalesced access\n    output_offset = b * stride_ob + oc * stride_oc + y * stride_oh + x * stride_ow\n    tl.store(output_ptr + output_offset, scaled)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor):\n        super(ModelNew, self).__init__()\n        # Convolution parameters\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, bias=True)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.scaling_factor = scaling_factor\n        self.kernel_size = kernel_size\n\n    def forward(self, x):\n        if self.training:\n            # Training path\n            x = self.conv(x)\n            x = self.bn(x)\n            return x * self.scaling_factor\n        \n        # Inference path - fused kernel\n        x = x.contiguous()\n        batch_size, _, height, width = x.shape\n        output_height = height - self.kernel_size + 1\n        output_width = width - self.kernel_size + 1\n        \n        # Precompute strides\n        stride_x = x.stride()\n        stride_w = self.conv.weight.stride()\n        output = torch.empty(\n            batch_size, self.bn.num_features, output_height, output_width,\n            device=x.device, dtype=x.dtype\n        )\n        stride_o = output.stride()\n        \n        # Launch fused kernel\n        total_elements = batch_size * self.bn.num_features * output_height * output_width\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        fused_conv_bn_scale_kernel[grid](\n            x, \n            self.conv.weight, \n            self.conv.bias,\n            self.bn.running_mean, \n            self.bn.running_var,\n            self.bn.weight, \n            self.bn.bias,\n            output,\n            self.bn.eps, \n            self.scaling_factor,\n            batch_size, in_channels, height, width,\n            self.bn.num_features,\n            stride_x[0], stride_x[1], stride_x[2], stride_x[3],\n            stride_w[0], stride_w[1], stride_w[2], stride_w[3],\n            stride_o[0], stride_o[1], stride_o[2], stride_o[3],\n            output_height, output_width,\n            BLOCK_SIZE=1024\n        )\n        return output\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\nscaling_factor = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, scaling_factor]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "627aa743-1bed-40c7-85a1-e05c460ea2e9", "generation": 2, "timestamp": 1754581838.236755, "iteration_found": 25, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.719, "runtime_ms": 0.117, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0841, "speed_up": 0.719, "custom_timing": 0.117}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.689, "runtime_ms": 0.122, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0841, "speed_up": 0.689, "custom_timing": 0.122}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
{"id": "a4ba5ea2-d979-4895-a638-028830ebad81", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef instance_norm_div_kernel(\n    x_ptr,\n    n_elements_per_channel,\n    eps,\n    divide_by,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    base = pid * n_elements_per_channel\n    \n    # Step 1: Compute mean\n    mean = 0.0\n    for off in range(0, n_elements_per_channel, BLOCK_SIZE):\n        idx = off + tl.arange(0, BLOCK_SIZE)\n        mask = idx < n_elements_per_channel\n        data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n        mean += tl.sum(data, axis=0)\n    mean = mean / n_elements_per_channel\n\n    # Step 2: Compute variance\n    var = 0.0\n    for off in range(0, n_elements_per_channel, BLOCK_SIZE):\n        idx = off + tl.arange(0, BLOCK_SIZE)\n        mask = idx < n_elements_per_channel\n        data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n        centered = data - mean\n        var += tl.sum(centered * centered, axis=0)\n    var = var / n_elements_per_channel\n    std = tl.sqrt(var + eps)\n\n    # Step 3: Apply normalization and scaling\n    for off in range(0, n_elements_per_channel, BLOCK_SIZE):\n        idx = off + tl.arange(0, BLOCK_SIZE)\n        mask = idx < n_elements_per_channel\n        data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n        centered = data - mean\n        normed = centered / std\n        out = normed / divide_by\n        tl.store(x_ptr + base + idx, out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divide_by):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divide_by = divide_by\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.contiguous()\n        batch, channels, height, width = x.shape\n        n_elements_per_channel = height * width\n        grid = (batch * channels,)\n        instance_norm_div_kernel[grid](\n            x, \n            n_elements_per_channel,\n            1e-5,\n            self.divide_by,\n            BLOCK_SIZE=1024\n        )\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivide_by = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divide_by]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "22213764-c016-4f08-9f6f-889bc58b8690", "generation": 1, "timestamp": 1754566671.229957, "iteration_found": 3, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": true, "score": 52.099999999999994, "speed_up": 1.321, "runtime_ms": 0.078, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.103, "speed_up": 1.321, "custom_timing": 0.078}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.089, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.103, "speed_up": 1.157, "custom_timing": 0.089}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0890\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.103, 'speed_up': 1.157, 'custom_timing': 0.089}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0890, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.103, 'speed_up': 1.157, 'custom_timing': 0.089}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies Instance Normalization, and divides by a constant.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, divide_by):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.instance_norm = nn.InstanceNorm2d(out_channels)\n        self.divide_by = divide_by\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.instance_norm(x)\n        x = x / self.divide_by\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivide_by = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divide_by]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0890), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.103, 'speed_up': 1.157, 'custom_timing': 0.089})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a convolution, applies Instance Normalization, and divides by a constant.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, divide_by):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.instance_norm = nn.InstanceNorm2d(out_channels)\n        self.divide_by = divide_by\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.instance_norm(x)\n        x = x / self.divide_by\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivide_by = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divide_by]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Convolution\n  2. Instance Normalization\n  3. Division by a constant\n\nThe current implementation uses standard nn.Conv2d and nn.InstanceNorm2d.\n\nHowever, note that the task is to rewrite the program to improve performance. We are allowed to use Triton.\n\nBut note: The problem says we are to rewrite the program. The original program is a PyTorch model. We are allowed to replace parts with Triton kernels.\n\nHowever, looking at the context, we are in an evolution system that expects Triton kernels. But the current program is entirely in PyTorch and does not use Triton.\n\nWe are to replace the entire model? Or just the operations?\n\nBut note: The example given was a Triton kernel for a quantization function. So we are to write a Triton kernel for the entire model? \n\nBut the model has multiple operations: convolution, instance norm, and division.\n\nAlternatively, we can break down:\n\nWe are allowed to change only the Triton kernel source within the EVOLVE-BLOCK. However, the current block does not have a Triton kernel. It is a standard PyTorch model.\n\nSo we must introduce a Triton kernel? But the task is to rewrite the program to improve performance.\n\nHow about we rewrite the entire forward pass as a single Triton kernel? But note the constraints:\n\n\u274c NEVER change @triton.jit function signatures or parameter names  \n  -> But we are creating a new kernel? So we define our own.\n\nHowever, the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\"\n\nBut the current block does not have a Triton kernel. So we are allowed to introduce one? But note the output format: we must return the complete updated code containing the EVOLVE-BLOCK.\n\nWe are allowed to change the class ModelNew and the helper functions? Because the current ModelNew is a PyTorch module.\n\nAlternatively, we can replace the forward pass with a call to a Triton kernel that does the entire computation? But that is complex.\n\nBut note: The problem says \"Provide the complete new program code.\" and we are to rewrite the program.\n\nAlternatively, we can break the operations and replace each one with a Triton kernel? But the convolution and instance normalization are non-trivial.\n\nHowever, note that the problem is from kernelbench, and we are to write kernels for level1-level5. This example might be a level1 or level2.\n\nBut looking at the current program, it is a model that uses built-in PyTorch operations. We are to rewrite it to use Triton for performance.\n\nBut the problem says: \"Generate functionally equivalent but faster Triton implementations (custom_code)\".\n\nSo we must write a Triton kernel that does the same as the PyTorch model? And then call that kernel in the forward.\n\nBut note: The model has parameters (conv weights). How do we handle that?\n\nAlternatively, we can write a kernel that fuses the entire operation: convolution, instance norm, and division? That would avoid multiple memory reads and writes.\n\nBut note: The convolution operation is weight-dependent. We must load the weights and the input, then compute.\n\nGiven the complexity, let's break down:\n\nWe are allowed to change the ModelNew. We can replace the forward with a Triton kernel that does:\n\n  x = self.conv(x)   -> but we would have to pass the weights and bias of the convolution to the kernel?\n  x = self.instance_norm(x) -> which requires statistics per channel per instance\n  x = x / self.divide_by\n\nBut note: The convolution weights are learnable. We cannot hardcode.\n\nAlternatively, we can precompute the weights and then use them in the kernel? But then we lose the ability to train? The problem does not specify training or inference.\n\nThe problem says: \"Evaluation pipeline\" has a verify_plus and execute. So we are in inference? \n\nBut the problem does not specify. We assume inference.\n\nSo we can write a kernel that does:\n\n  input: x (tensor of shape [batch, in_channels, height, width])\n  parameters: conv_weight [out_channels, in_channels, kernel_size, kernel_size], conv_bias [out_channels] (if any), and the divide_by scalar.\n\n  Steps in the kernel:\n\n  1. Convolution: for each output element (b, c_out, h, w), we compute:\n        out = 0\n        for c_in in range(in_channels):\n            for i in range(kernel_size):\n                for j in range(kernel_size):\n                    h_in = h + i - padding? (the conv2d uses same padding? or valid? We have to check the original)\n\n  The original nn.Conv2d uses:\n      padding: by default 0? But note: the input is 32x32 and kernel 3x3, so without padding the output would be 30x30? But the instance norm expects the same spatial? Actually, the instance norm is applied on the output of the conv which is [batch, out_channels, height_out, width_out]. Then we divide by a scalar.\n\n  However, the problem does not specify the padding. In the original code, we did not set padding, so it is 0.\n\n  So output spatial size: (32 - 3 + 1) = 30? But then the instance norm expects [batch, out_channels, 30,30]. Then we divide.\n\n  But the example input is [128, 3, 32,32]. So the output of conv is [128,16,30,30]. Then instance norm and division.\n\n  2. Instance Normalization: for each channel in each batch, we compute mean and variance over the spatial dimensions (30x30) and then normalize.\n\n  3. Then divide by a constant.\n\nWe can fuse all these steps in one kernel? That would avoid writing intermediate results.\n\nBut note: The convolution is the most expensive. We can try to write a Triton kernel for convolution? And then do the rest? But then we would have intermediate storage.\n\nAlternatively, we can write a kernel that does the entire operation per output element? That is:\n\n  For each output element (b, c_out, h, w):\n      - Compute the convolution for that output element (gather input from a window and multiply by weights and accumulate)\n      - Then we need the mean and variance for the entire channel (c_out) in the batch (b) for the normalization? But that requires a reduction over the entire spatial domain of that channel. That is not local to the element.\n\nSo we cannot do it in one pass without multiple steps? We would need to compute the mean and variance first? Or do we do two passes?\n\nBut note: We are allowed to use shared memory and multiple passes? However, the reduction over 30x30=900 elements per channel per batch is not too big? We can do it in shared memory.\n\nSo the plan:\n\n  Step 1: Precompute the convolution output and also the sum and sum of squares for each channel in each batch? Then we can compute mean and variance.\n\n  But note: We cannot precompute the entire convolution and then do the reduction because that would require storing the entire output. We want to fuse.\n\n  Alternatively, we can do:\n\n      Pass 1: Compute the convolution for the entire output and simultaneously compute the sum and sum of squares per channel per batch? Then we can compute the mean and variance and then do a second pass to normalize? But that is two passes.\n\n  Or we can do:\n\n      First, we compute the convolution and store the output in a temporary tensor? But that is what the original code does.\n\n  We are trying to avoid that.\n\n  Alternatively, we can use a two-pass approach:\n\n      Pass 1: Compute the convolution and also accumulate the sum and sum of squares for each (batch, channel) in shared memory? But the reduction is over 900 elements. We can do a reduction in shared memory? But note: the reduction dimension is 900, which is larger than the typical shared memory block? And we have multiple channels and batches.\n\n  We are constrained by shared memory size.\n\n  Alternatively, we can do:\n\n      We break the problem into two kernels? One for convolution and then one for instance norm and division? But then we don't get fusion.\n\n  Given the complexity, and since this is a level1-level5 problem, it might be that we are only required to optimize one operation? But the problem says \"rewrite the program\".\n\n  However, note the constraints: we must not change the grid configuration, output tensor shapes, etc. But we are creating a new kernel.\n\n  Considering the time, and the fact that the current program is entirely PyTorch, we can write a Triton kernel that fuses the entire operation? But it is complex.\n\n  Alternatively, we can write a kernel for the instance normalization? Because the built-in instance norm might be slow? But the convolution is the heavy part.\n\n  We need to get a speedup. The current speedup is 1.157 (from the meta) but that is for the entire model? Actually, the meta says:\n        'reference_timing_ms': 0.103, 'speed_up': 1.157, 'custom_timing': 0.089\n\n  So the custom_timing (our current model) is 0.089 ms, and the reference is 0.103 ms? But the reference is what? It says 'reference_timing_ms'. Maybe the reference is the original code? But the current program is the original code? \n\n  Actually, the current program is the reference code? Because it says: \"stage: reference code\". And we are to write the custom_code.\n\n  So the current program is the reference. We are to write a custom (Triton) version that should be faster.\n\n  Therefore, we must replace the entire forward with a Triton kernel that does the same.\n\n  But note: The problem says \"functional equivalence\". So we must compute the same.\n\n  Given the complexity, we decide to write a kernel that fuses the convolution and the division? But leave the instance norm? That doesn't make sense.\n\n  Alternatively, we can use a known optimized convolution from Triton? But Triton does not have a built-in convolution for arbitrary sizes? We have to write one.\n\n  This seems too heavy for the scope.\n\n  Alternatively, we note that the current model is very small (3x3 conv on 32x32, batch 128). The convolution is done by PyTorch with cuDNN, which is highly optimized. So it is unlikely that we can beat it with a hand-written Triton kernel.\n\n  But the problem says we must get at least 5% speedup.\n\n  How about we fuse the entire operation? The original code does:\n\n      x = self.conv(x)   -> writes to memory\n      x = self.instance_norm(x) -> reads and writes\n      x = x / self.divide_by -> reads and writes\n\n  If we fuse, we avoid the intermediate writes and reads.\n\n  So we can write a kernel that:\n\n      For each output element (b, c, h, w) (where the output spatial is 30x30):\n\n          Step 1: Load the input window for the convolution (3x3) at the corresponding location for batch b and all input channels? That would be 3*3*3=27 elements? Then multiply by the weights for output channel c (which is 3*3*3 weights) and accumulate. Then add bias.\n\n          Step 2: But then we need to normalize by the mean and variance of the entire channel c in batch b? We haven't computed that. So we need to precompute the mean and variance per channel per batch.\n\n          How to do that without an extra pass? We cannot.\n\n  So we do:\n\n      Two kernels? Or one kernel that does two passes? We can do:\n\n          Pass 1: Compute the convolution for the entire output and also compute the sum and sum of squares for each (batch, channel). We store the convolution result in a temporary buffer? But that is the same as the original.\n\n      Alternatively, we can do:\n\n          Pass 1: Only compute the sum and sum of squares per (batch, channel) without storing the convolution output? How? We have to compute the convolution for each output element to get the value? Then we can accumulate.\n\n          Then Pass 2: Compute the convolution again, but now we know the mean and variance, so we can normalize and divide.\n\n      That is two passes over the input. But we are reading the input twice? And the weights twice? That is more expensive.\n\n  Therefore, we decide to precompute the convolution output in a temporary buffer and then compute the mean and variance from that buffer? But then we are storing the entire output. Then we do a second kernel to normalize? That is the same as the original, but we are replacing the instance norm and division with a Triton kernel? And the convolution remains with cuDNN.\n\n  That might not give much speedup.\n\n  Considering the above, and the fact that the instance normalization and division are cheap compared to convolution, we might not get much speedup by fusing.\n\n  Alternatively, we can write a kernel that does the instance norm and division in one go? That would avoid the intermediate between norm and division? But the division is a simple element-wise.\n\n  So we focus on writing a fused instance norm and division kernel? And leave the convolution to cuDNN.\n\n  The plan:\n\n      We keep the convolution in PyTorch (nn.Conv2d) but then call a Triton kernel for the instance normalization and division.\n\n  Steps:\n\n      In the forward:\n\n          x = self.conv(x)   # [batch, out_channels, 30, 30]\n          # Then we call a Triton kernel that does instance norm and division in one go.\n\n  How to write the Triton kernel for instance norm and division?\n\n      We have:\n          x: tensor of shape [batch, channels, height, width] (contiguous)\n\n          For each batch and channel, we compute:\n              mean = average of x[b, c, :, :]\n              var = average of (x[b, c, :, :] - mean)^2\n              normalized = (x[b, c, :, :] - mean) / sqrt(var + eps)   # in instance norm, there are also affine parameters? But note: nn.InstanceNorm2d by default has affine=False? We did not set, so affine=False.\n\n          Then we divide by self.divide_by.\n\n      So without affine parameters.\n\n      We can write a kernel that:\n\n          Each block handles one channel in one batch? Then the block has to reduce over height*width.\n\n          We have batch * channels as the grid.\n\n          We use shared memory to reduce over the spatial dimensions.\n\n      Steps in the kernel:\n\n          pid = program_id(0)  # we use 1D grid: size = batch * channels\n          batch_idx = pid // channels\n          channel_idx = pid % channels\n\n          base_offset = batch_idx * (channels * height * width) + channel_idx * (height * width)\n          # then we load the entire spatial domain for that batch and channel.\n\n          We do a reduction (sum and sum of squares) over the spatial domain? We can do in shared memory.\n\n          Then compute mean and variance.\n\n          Then we traverse again and compute: (x - mean) / sqrt(var+eps) / divide_by\n\n          And store.\n\n      We can do two passes in the kernel? Or use a single pass with two reductions? We can do:\n\n          First, we compute the sum and the sum of squares in one reduction? Then we compute mean and variance.\n\n          Then we do a second pass to write the normalized and scaled values.\n\n      But note: the spatial size is 30*30=900, which is less than the number of threads we can have in a block? We can use 1024 threads per block? So we can do a reduction in one block.\n\n      We can do:\n\n          Step 1: Load the entire 900 elements into shared memory? But shared memory is limited. 900 * 4 = 3600 bytes? That is acceptable (shared memory is 48KB per block).\n\n      So we do:\n\n          pid = program_id(0)  # for batch*channels\n\n          # compute the base pointer for the channel\n          ... \n\n          # Step 1: Load all the spatial data for this channel into shared memory? Or we can do without? We can do a reduction without loading all at once? But we need two passes: one for reduction and one for writing.\n\n          We can do:\n\n              sum = 0\n              sum_sq = 0\n              for i in range(0, height*width, BLOCK_SIZE):\n                  offset = base_offset + i + tl.arange(0, BLOCK_SIZE)\n                  mask = (i + tl.arange(0, BLOCK_SIZE)) < (height*width)\n                  data = tl.load(x_ptr + offset, mask=mask, other=0.0)\n                  sum += tl.sum(data, axis=0)\n                  sum_sq += tl.sum(data*data, axis=0)\n\n              Then mean = sum / (height*width)\n              variance = sum_sq / (height*width) - mean*mean\n\n          But note: this is numerically unstable? We can use two-pass but we don't want to store all.\n\n          Alternatively, we can do a two-pass reduction? First pass for sum, second pass for variance using Welford? But we want to avoid two passes.\n\n          We can do:\n\n              mean = sum / (height*width)\n              Then we need a second pass to compute the variance? Because we don't have the individual data? We have to recompute.\n\n          Or we can store the data in shared memory? That is 900 elements per block? That is acceptable.\n\n      We decide to load the entire channel into shared memory? But then we can do:\n\n          Step 1: Load the entire channel into shared memory (using multiple threads, each loading several elements if needed).\n          Step 2: Reduce to compute mean and then variance.\n\n          Then Step 3: Write the normalized and scaled values.\n\n      But note: the channel has 900 elements. We can use 900 threads? But we can also use 512 threads? And each thread loads ceil(900/512) elements.\n\n      We design:\n\n          BLOCK_SIZE = 1024  (or 512, but 1024 is safe because 900<1024)\n\n          We can have one block per channel per batch.\n\n          Let BLOCK_SIZE be the next power of two >= height*width? But height*width=900, so 1024.\n\n      Steps:\n\n          pid = tl.program_id(0)\n          batch_idx = pid // channels\n          channel_idx = pid % channels\n\n          base = batch_idx * (channels * height * width) + channel_idx * (height * width)\n\n          # We are going to load 900 elements. We have 1024 threads, so each thread loads 1 element? But 900<1024, so some threads don't load.\n\n          offset = base + tl.arange(0, BLOCK_SIZE)\n          mask = tl.arange(0, BLOCK_SIZE) < (height*width)\n          data = tl.load(x_ptr + offset, mask=mask, other=0.0)\n\n          # Then compute the mean by a reduction in shared memory? We can use tl.sum? But that is not available for shared memory? We can use tl.associative_scan?\n\n          We can do:\n\n              mean = tl.sum(data, axis=0) / (height*width)\n\n          But note: tl.sum over an axis is not implemented? We have to do a reduction.\n\n          Alternatively, we can use:\n\n              mean = tl.sum(data) / (height*width)\n\n          But that is the entire array? Yes.\n\n          Then we compute:\n\n              centered = data - mean\n              variance = tl.sum(centered * centered) / (height*width)\n\n          Then we compute the standard deviation.\n\n          Then we write:\n\n              output = centered / tl.sqrt(variance + eps) / divide_by\n\n          Then store to the same location.\n\n      But note: we are storing to the same tensor? But the kernel is in-place? Or we can have an output tensor.\n\n      We can have an output tensor? Or we can do in-place? The problem does not specify.\n\n      We'll do in-place? Or we can write to a new tensor? But the original model returns the tensor. We can do in-place.\n\n      We have to be cautious: the input tensor x is the output of conv, and we are going to overwrite it.\n\n  Let's design the kernel:\n\n      @triton.jit\n      def instance_norm_div_kernel(\n          x_ptr,              # pointer to the input tensor (also output)\n          n_elements_per_channel,  # height * width\n          channels,\n          height, width,\n          eps,               # epsilon for instance norm\n          divide_by,         # scalar to divide by\n          BLOCK_SIZE: tl.constexpr,\n      ):\n          pid = tl.program_id(0)\n          batch_idx = pid // channels\n          channel_idx = pid % channels\n\n          base = batch_idx * (channels * height * width) + channel_idx * (height * width)\n\n          # Load the entire channel into a register array? No, we want to use shared memory? Actually, we can use registers? But 900 is too many.\n\n          # Instead, we do a two-step reduction without storing all? But for simplicity, we do:\n\n          # We are going to iterate by blocks over the channel? But the channel is contiguous? We can do:\n\n          # Step 1: Compute the mean by summing in blocks.\n          total = 0.0\n          for off in range(0, n_elements_per_channel, BLOCK_SIZE):\n              idx = off + tl.arange(0, BLOCK_SIZE)\n              mask = idx < n_elements_per_channel\n              data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n              total += tl.sum(data, axis=0)\n\n          mean = total / n_elements_per_channel\n\n          # Step 2: Compute the variance\n          total_var = 0.0\n          for off in range(0, n_elements_per_channel, BLOCK_SIZE):\n              idx = off + tl.arange(0, BLOCK_SIZE)\n              mask = idx < n_elements_per_channel\n              data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n              centered = data - mean\n              total_var += tl.sum(centered * centered, axis=0)\n\n          variance = total_var / n_elements_per_channel\n          std = tl.sqrt(variance + eps)\n\n          # Step 3: Normalize and scale, then write back.\n          for off in range(0, n_elements_per_channel, BLOCK_SIZE):\n              idx = off + tl.arange(0, BLOCK_SIZE)\n              mask = idx < n_elements_per_channel\n              data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n              centered = data - mean\n              normed = centered / std\n              out = normed / divide_by\n              tl.store(x_ptr + base + idx, out, mask=mask)\n\n      But note: This kernel does three passes? That might be expensive. But the data is in L1 cache? We are reading the same data three times.\n\n      Alternatively, we can do two passes: first pass computes mean and the second pass computes variance and then writes the normalized value? But then we have to do:\n\n          Pass 1: Compute mean (one pass)\n          Pass 2: Compute variance and then write normalized? But we need the variance to write.\n\n          We can do:\n\n              total_var = 0.0\n              for off in range(...):\n                  ... load data\n                  centered = data - mean\n                  total_var += tl.sum(centered * centered, axis=0)\n                  # but we cannot write yet because we don't have the std\n\n          Then we compute std, and then we have to do a third pass to write.\n\n      Or we can store the centered data in shared memory? But we don't have enough shared memory for 900 elements? We can do one block per channel? And then we can store 900 elements in shared memory? 900*4=3600 bytes, which is less than 48KB? Yes.\n\n      So we can do:\n\n          Load the entire channel into shared memory in one go? Then we can compute the mean and variance and then write back? But we have to be careful: shared memory is per block, and we have one block per channel.\n\n          We do:\n\n              # Allocate shared memory for the channel data\n              shmem = tl.zeros([BLOCK_SIZE], dtype=tl.float32)   # but BLOCK_SIZE is the number of threads? We need to store 900 elements.\n\n          We can set BLOCK_SIZE to the next power of two >= 900? 1024.\n\n          Then:\n\n              offs = tl.arange(0, BLOCK_SIZE)\n              mask = offs < n_elements_per_channel\n              data = tl.load(x_ptr + base + offs, mask=mask, other=0.0)\n              # store in shared memory? Actually, we don't need to store? We can use the loaded data.\n\n              # Compute the mean by reducing the data array? We can use tl.sum? But we can do a reduction tree in shared memory.\n\n          We can use tl.sum(data) to get the sum? But the data is in registers? We want to do a reduction.\n\n          Alternatively, we can do:\n\n              total = tl.sum(data)\n              mean = total / n_elements_per_channel\n\n              centered = data - mean\n              variance = tl.sum(centered * centered) / n_elements_per_channel\n              std = tl.sqrt(variance + eps)\n\n              # Then write: (centered / std / divide_by) to the same location.\n\n          But note: the centered array is computed from data and mean. We have the data in registers? We can do:\n\n              out = (data - mean) / std / divide_by\n\n          Then store.\n\n          But the data is in a vector of BLOCK_SIZE? And we are going to write it back? We can do:\n\n              tl.store(x_ptr + base + offs, out, mask=mask)\n\n      This is one pass for loading and one pass for storing? And the reductions are done in registers? But the tl.sum might be efficient.\n\n      But note: the tl.sum on a vector of 1024 elements might be implemented with a reduction tree? That is efficient.\n\n      So we design:\n\n          @triton.jit\n          def instance_norm_div_kernel(\n              x_ptr,\n              n_elements_per_channel,  # = height * width\n              eps,\n              divide_by,\n              BLOCK_SIZE: tl.constexpr,\n          ):\n              pid = tl.program_id(0)\n              # We assume grid is (batch * channels,)\n              base = pid * n_elements_per_channel   # but wait: the tensor is [batch, channels, height, width] and contiguous? The original is contiguous? We made it contiguous.\n\n              # We are going to load n_elements_per_channel elements, but we allocated BLOCK_SIZE which is >= n_elements_per_channel.\n              offs = tl.arange(0, BLOCK_SIZE)\n              mask = offs < n_elements_per_channel\n              data = tl.load(x_ptr + base + offs, mask=mask, other=0.0)\n\n              # Compute mean\n              total = tl.sum(data, axis=0)\n              mean = total / n_elements_per_channel\n\n              centered = data - mean\n              variance = tl.sum(centered * centered, axis=0) / n_elements_per_channel\n              std = tl.sqrt(variance + eps)\n\n              out = centered / std / divide_by\n              tl.store(x_ptr + base + offs, out, mask=mask)\n\n      But note: the base offset: the tensor is [batch, channels, height, width] and contiguous. Then the base for a given (batch, channel) is:\n\n          base = batch * (channels * height * width) + channel * (height * width)\n\n      But in the kernel, we are given pid = batch * channels + channel_idx? So:\n\n          base = pid * (height*width)\n\n      That is correct? Only if the tensor is contiguous and the channels are the second dimension? And the next channel is contiguous? Yes.\n\n      But note: the tensor after convolution might be contiguous? We can ensure that by calling .contiguous().\n\n  So in the forward of the model:\n\n      x = self.conv(x)   # [batch, out_channels, 30,30]\n      x = x.contiguous()\n      # Now x has shape [batch, out_channels, 30,30] and is contiguous? Then the elements for one channel are contiguous? Yes.\n\n      n_elements_per_channel = 30*30\n      grid = (x.shape[0] * x.shape[1],)\n\n      # call the kernel\n      instance_norm_div_kernel[grid](x, n_elements_per_channel, eps, divide_by, BLOCK_SIZE=1024)\n\n  But what about the eps? The nn.InstanceNorm2d uses a default eps of 1e-5.\n\n  And the divide_by is self.divide_by.\n\n  We must set eps=1e-5.\n\n  Now, let's replace the ModelNew.forward with this new version.\n\n  We remove the instance_norm and the division, and replace with the kernel call.\n\n  We also need to define the kernel.\n\n  We put the kernel inside the ModelNew? Or outside? We can put it as a static method? But Triton kernels are compiled. We can define it at the top level? But the problem says to put everything in the EVOLVE-BLOCK.\n\n  We'll define the kernel inside the class? Or outside? We can define it at the top level within the block.\n\n  But note: the block must contain the entire program? Yes.\n\n  So we do:\n\n      import triton and tl.\n\n      Then define the kernel.\n\n      Then in the forward, call the kernel.\n\n  We also note: we must not change the initialization parameters? We still need the conv and the divide_by? But we remove the instance_norm? Because we are doing it in the kernel.\n\n  But wait: the original instance_norm had parameters? The original nn.InstanceNorm2d(out_channels) has no affine parameters by default? So it's just normalization.\n\n  So we can remove the instance_norm layer? And replace it with the kernel.\n\n  Let's restructure:\n\n      class ModelNew(nn.Module):\n          def __init__(self, in_channels, out_channels, kernel_size, divide_by):\n              super().__init__()\n              self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)   # padding=0, so output is 30x30\n              self.divide_by = divide_by\n              # remove instance_norm\n\n          def forward(self, x):\n              x = self.conv(x)\n              # Now x is [batch, out_channels, 30,30]\n              # We want to apply instance norm and division\n\n              # Ensure contiguous\n              x = x.contiguous()\n\n              batch, channels, height, width = x.shape\n              n_elements_per_channel = height * width\n\n              # Grid: one per channel per batch: batch * channels\n              grid = (batch * channels,)\n\n              # We must define the kernel? But we are going to define it outside? But within the block.\n\n              # We define the kernel at the top level? But then we must be in the same scope.\n\n              # We can define the kernel at the top level of the module.\n\n              # But note: the block is the entire code.\n\n              # We'll define the kernel at the top level.\n\n              # Call the kernel\n              instance_norm_div_kernel[grid](\n                  x,\n                  n_elements_per_channel,\n                  1e-5,   # eps\n                  self.divide_by,\n                  BLOCK_SIZE=1024,\n              )\n\n              return x\n\n      Then define the kernel at the top level? But we are in the same block.\n\n  But note: the kernel must be defined before it is called.\n\n  We'll define the kernel before the class? Or inside the class? We can define it as a static method? But Triton doesn't allow that? We can define it at the top level.\n\n  So the structure:\n\n      import ... (including triton and tl)\n\n      @triton.jit\n      def instance_norm_div_kernel(...):\n          ...\n\n      class ModelNew(nn.Module):\n          ...\n\n  But the problem says: we must not change the function signatures? We are creating a new one.\n\n  We are allowed.\n\n  However, note: the constraints say \"NEVER change @triton.jit function signatures or parameter names\" for existing kernels? We are creating a new one.\n\n  So it's okay.\n\n  But note: the kernel might be called with different BLOCK_SIZE? We set it to 1024, but we can make it a configurable? We can use autotune? But for simplicity, we use 1024.\n\n  But what if the spatial size is larger than 1024? We are currently setting n_elements_per_channel and then BLOCK_SIZE=1024, but we are using a loop? In our kernel above, we did not use a loop. We assumed BLOCK_SIZE>=n_elements_per_channel.\n\n  But the current n_elements_per_channel is 900, so 1024 is safe.\n\n  But to be general, we should use:\n\n      BLOCK_SIZE: tl.constexpr that is the next power of two? But we can set it to the next power of two? Or we can use a loop if n_elements_per_channel > BLOCK_SIZE.\n\n  We'll change the kernel to handle any size by looping? Or we can require that BLOCK_SIZE >= n_elements_per_channel? But the grid is set to batch*channels, and we set BLOCK_SIZE to the next power of two >= n_elements_per_channel? But we don't know n_elements_per_channel at compile time.\n\n  Alternatively, we can do:\n\n      Let the kernel be designed for a fixed maximum per block? And we require that the spatial size is <= BLOCK_SIZE? Then we set BLOCK_SIZE to 1024 and hope that the spatial size is <=1024.\n\n  For this problem, the spatial size is 30x30=900, so it is safe.\n\n  But to be safe, we can use:\n\n      if n_elements_per_channel > BLOCK_SIZE:\n          # we cannot handle\n          ... \n\n  But we don't have conditionals in the kernel? We can have, but we can also design to loop.\n\n  We'll do the loop version? But then we have to do multiple loads? And reductions? We did that in the three-pass version.\n\n  Given the time, we decide to use the three-pass version without shared memory? But we are not using shared memory, so it works for any size.\n\n  We redesign the kernel to handle any size by looping:\n\n      @triton.jit\n      def instance_norm_div_kernel(\n          x_ptr,\n          n_elements_per_channel,  # = height * width\n          eps,\n          divide_by,\n          BLOCK_SIZE: tl.constexpr,   # this is the block size for the loop\n      ):\n          pid = tl.program_id(0)\n          base = pid * n_elements_per_channel\n\n          # Step 1: compute mean\n          mean = 0.0\n          for off in range(0, n_elements_per_channel, BLOCK_SIZE):\n              idx = off + tl.arange(0, BLOCK_SIZE)\n              mask = idx < n_elements_per_channel\n              data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n              mean += tl.sum(data, axis=0)\n\n          mean = mean / n_elements_per_channel\n\n          # Step 2: compute variance\n          var = 0.0\n          for off in range(, n_elements_per_channel, BLOCK_SIZE):\n              idx = off + tl.arange(0, BLOCK_SIZE)\n              mask = idx < n_elements_per_channel\n              data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n              centered = data - mean\n              var += tl.sum(centered * centered, axis=0)\n\n          var = var / n_elements_per_channel\n          std = tl.sqrt(var + eps)\n\n          # Step 3: write back\n          for off in range(0, n_elements_per_channel, BLOCK_SIZE):\n              idx = off + tl.arange(0, BLOCK_SIZE)\n              mask = idx < n_elements_per_channel\n              data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n              centered = data - mean\n              normed = centered / std\n              out = normed / divide_by\n              tl.store(x_ptr + base + idx, out, mask=mask)\n\n  This kernel works for any size.\n\n  We set BLOCK_SIZE to 1024? But then for 900, it will do one iteration per loop? But we can set a smaller block? It doesn't matter.\n\n  We call the kernel with BLOCK_SIZE=1024.\n\n  Now, we must also consider the data type? The convolution output is float32? The original model: the conv output is float32, then instance norm float32, then division float32.\n\n  The kernel uses float32.\n\n  Let's code accordingly.\n\n  We'll also add the imports.\n\n  But note: the original code did not import triton? We must add the imports.\n\n  We are allowed to change the entire block.\n\n  So the new block:\n\n      import math\n      import torch\n      import torch.nn as nn\n      import triton\n      import triton.language as tl\n\n      @triton.jit\n      def instance_norm_div_kernel(\n          x_ptr,\n          n_elements_per_channel,\n          eps,\n          divide_by,\n          BLOCK_SIZE: tl.constexpr,\n      ):\n          pid = tl.program_id(0)\n          base = pid * n_elements_per_channel\n\n          # Step 1: compute mean\n          mean = 0.0\n          for off in range(0, n_elements_per_channel, BLOCK_SIZE):\n              idx = off + tl.arange(0, BLOCK_SIZE)\n              mask = idx < n_elements_per_channel\n              data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n              mean += tl.sum(data, axis=0)\n\n          mean = mean / n_elements_per_channel\n\n          # Step 2: compute variance\n          var = 0.0\n          for off in range(0, n_elements_per_channel, BLOCK_SIZE):\n              idx = off + tl.arange(0, BLOCK_SIZE)\n              mask = idx < n_elements_per_channel\n              data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n              centered = data - mean\n              var += tl.sum(centered * centered, axis=0)\n\n          var = var / n_elements_per_channel\n          std = tl.sqrt(var + eps)\n\n          # Step 3: write back\n          for off in range(0, n_elements_per_channel, BLOCK_SIZE):\n              idx = off + tl.arange(0, BLOCK_SIZE)\n              mask = idx < n_elements_per_channel\n              data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n              centered = data - mean\n              normed = centered / std\n              out = normed / divide_by\n              tl.store(x_ptr + base + idx, out, mask=mask)\n\n      class ModelNew(nn.Module):\n          def __init__(self, in_channels, out_channels, kernel_size, divide_by):\n              super().__init__()\n              self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n              self.divide_by = divide_by\n\n          def forward(self, x):\n              x = self.conv(x)\n              # Now x is [batch, out_channels, height, width]\n              # We need to be contiguous and have the shape\n              x = x.contiguous()\n              batch, channels, height, width = x.shape\n              n_elements_per_channel = height * width\n\n              grid = (batch * channels,)\n              # We use a BLOCK_SIZE of 1024 for the reduction and loading\n              instance_norm_div_kernel[grid](x, n_elements_per_channel, 1e-5, self.divide_by, BLOCK_SIZE=1024)\n\n              return x\n\n      batch_size = 128\n      in_channels = 3\n      out_channels = 16\n      height, width = 32, 32\n      kernel_size = 3\n      divide_by = 2.0\n\n      def get_inputs():\n          return [torch.randn(batch_size, in_channels, height, width)]\n\n      def get_init_inputs():\n          return [in_channels, out_channels, kernel_size, divide_by]\n\n  But note: the conv output spatial size is 30x30? Because (32-3+1)=30? So we set height=30, width=30? But the get_inputs returns 32x32.\n\n  The input to the model is 32x32, and the conv reduces to 30x30.\n\n  In the kernel, we get the shape from x.shape after conv.\n\n  So it should be 30x30.\n\n  We are done.\n\n  However, note: the kernel might be slow because it does three passes? But we are fusing two operations and avoiding the overhead of PyTorch's instance norm and the division.\n\n  We hope for a speedup.\n\n  Let's run and see.\n\n  But note: the current reference time is 0.103 ms, and our custom time is 0.089 ms? That was for the reference code? We are replacing the reference code with this custom code.\n\n  We hope to get a speedup.\n\n  We output the code accordingly.\n\n  IMPORTANT: We must not change the get_inputs and get_init_inputs? They are the same.\n\n  We are ready.\n</think>\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef instance_norm_div_kernel(\n    x_ptr,\n    n_elements_per_channel,\n    eps,\n    divide_by,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    base = pid * n_elements_per_channel\n    \n    # Step 1: Compute mean\n    mean = 0.0\n    for off in range(0, n_elements_per_channel, BLOCK_SIZE):\n        idx = off + tl.arange(0, BLOCK_SIZE)\n        mask = idx < n_elements_per_channel\n        data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n        mean += tl.sum(data, axis=0)\n    mean = mean / n_elements_per_channel\n\n    # Step 2: Compute variance\n    var = 0.0\n    for off in range(0, n_elements_per_channel, BLOCK_SIZE):\n        idx = off + tl.arange(0, BLOCK_SIZE)\n        mask = idx < n_elements_per_channel\n        data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n        centered = data - mean\n        var += tl.sum(centered * centered, axis=0)\n    var = var / n_elements_per_channel\n    std = tl.sqrt(var + eps)\n\n    # Step 3: Apply normalization and scaling\n    for off in range(0, n_elements_per_channel, BLOCK_SIZE):\n        idx = off + tl.arange(0, BLOCK_SIZE)\n        mask = idx < n_elements_per_channel\n        data = tl.load(x_ptr + base + idx, mask=mask, other=0.0)\n        centered = data - mean\n        normed = centered / std\n        out = normed / divide_by\n        tl.store(x_ptr + base + idx, out, mask=mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, divide_by):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.divide_by = divide_by\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.contiguous()\n        batch, channels, height, width = x.shape\n        n_elements_per_channel = height * width\n        grid = (batch * channels,)\n        instance_norm_div_kernel[grid](\n            x, \n            n_elements_per_channel,\n            1e-5,\n            self.divide_by,\n            BLOCK_SIZE=1024\n        )\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\nheight, width = 32, 32\nkernel_size = 3\ndivide_by = 2.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, divide_by]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "5f7a5910-80e8-416b-89b0-1e691a0d306e", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=8, num_stages=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4, num_stages=5),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4, num_stages=5),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4, num_stages=5),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=8, num_stages=3),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 64}, num_warps=8, num_stages=3),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _matmul_kernel(\n    # Pointers to matrices\n    a_ptr, b_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # Stride parameters (in number of elements, not bytes)\n    stride_am, stride_ak,  # for A: MxK\n    stride_bk, stride_bn,   # for B: KxN\n    stride_cm, stride_cn,\n    # Tile sizes\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    # Create initial pointers for the first block of A and B.\n    a_block_ptr = tl.make_block_ptr(\n        base=a_ptr,\n        shape=(M, K),\n        strides=(stride_am, stride_ak),\n        offsets=(pid_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_K),\n        order=(1, 0)\n    )\n    b_block_ptr = tl.make_block_ptr(\n        base=b_ptr,\n        shape=(K, N),\n        strides=(stride_bk, stride_bn),\n        offsets=(0, pid_n * BLOCK_N),\n        block_shape=(BLOCK_K, BLOCK_N),\n        order=(1, 0)\n    )\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    # Prefetch first blocks for double buffering\n    a0 = tl.load(a_block_ptr)\n    b0 = tl.load(b_block_ptr)\n    a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_K))\n    b_block_ptr = tl.advance(b_block_ptr, (BLOCK_K, 0))\n    \n    # Double buffered loop\n    num_blocks = tl.cdiv(K, BLOCK_K)\n    for k in range(1, num_blocks):\n        # Prefetch next blocks\n        a1 = tl.load(a_block_ptr)\n        b1 = tl.load(b_block_ptr)\n        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_K))\n        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_K, 0))\n        \n        # Compute on current blocks\n        accumulator += tl.dot(a0, b0, allow_tf32=True)\n        \n        # Swap buffers\n        a0 = a1\n        b0 = b1\n    \n    # Compute last block\n    accumulator += tl.dot(a0, b0, allow_tf32=True)\n    \n    # Create a block pointer for the output\n    c_block_ptr = tl.make_block_ptr(\n        base=c_ptr,\n        shape=(M, N),\n        strides=(stride_cm, stride_cn),\n        offsets=(pid_m * BLOCK_M, pid_n * BLOCK_N),\n        block_shape=(BLOCK_M, BLOCK_N),\n        order=(1, 0)\n    )\n    tl.store(c_block_ptr, accumulator)\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "d01b4ab4-4005-433e-84e7-a9782a769ab7", "generation": 5, "timestamp": 1754592526.0097678, "iteration_found": 29, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.596, "runtime_ms": 0.505, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.301, "speed_up": 0.596, "custom_timing": 0.505}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.593, "runtime_ms": 0.508, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.301, "speed_up": 0.593, "custom_timing": 0.508}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
{"id": "b205791f-68e6-4205-b371-1fc30ccefa27", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_IC': 32, 'BLOCK_SIZE_OC': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_IC': 64, 'BLOCK_SIZE_OC': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_IC': 128, 'BLOCK_SIZE_OC': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_IC': 32, 'BLOCK_SIZE_OC': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_IC': 64, 'BLOCK_SIZE_OC': 256}, num_warps=8),\n    ],\n    key=['in_channels', 'out_channels', 'height', 'width']\n)\n@triton.jit\ndef conv_pool_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    batch, in_channels, height, width, out_channels,\n    stride_b, stride_c, stride_h, stride_w,\n    out_stride_b, out_stride_c, out_stride_h, out_stride_w,\n    BLOCK_SIZE_IC: tl.constexpr, BLOCK_SIZE_OC: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pixels = height * width\n    num_output_pixels = (height // 2) * (width // 2)\n    batch_id = pid // num_output_pixels\n    pixel_id = pid % num_output_pixels\n    \n    h_out = pixel_id // (width // 2)\n    w_out = pixel_id % (width // 2)\n    h_in = h_out * 2\n    w_in = w_out * 2\n\n    pool_acc = tl.zeros((BLOCK_SIZE_OC,), dtype=tl.float32)\n    pool_count = 0.0\n\n    for dh in range(2):\n        for dw in range(2):\n            h = h_in + dh\n            w = w_in + dw\n            if h < height and w < width:\n                pool_count += 1.0\n                for oc_block in range(0, out_channels, BLOCK_SIZE_OC):\n                    oc_offsets = oc_block + tl.arange(0, BLOCK_SIZE_OC)\n                    oc_mask = oc_offsets < out_channels\n                    acc = tl.zeros((BLOCK_SIZE_OC,), dtype=tl.float32)\n                    \n                    for ic_block in range(0, in_channels, BLOCK_SIZE_IC):\n                        ic_offsets = ic_block + tl.arange(0, BLOCK_SIZE_IC)\n                        ic_mask = ic_offsets < in_channels\n                        \n                        input_offset = batch_id * stride_b + h * stride_h + w * stride_w\n                        input_val = tl.load(\n                            input_ptr + input_offset + ic_offsets * stride_c,\n                            mask=ic_mask\n                        )\n                        \n                        weight_offset = oc_offsets[:, None] * in_channels + ic_offsets[None, :]\n                        weight_val = tl.load(\n                            weight_ptr + weight_offset,\n                            mask=oc_mask[:, None] & ic_mask[None, :]\n                        )\n                        \n                        acc += tl.sum(weight_val * input_val[None, :], axis=1)\n                    \n                    pool_acc += tl.where(oc_mask, acc, 0.0)\n    \n    pool_acc /= pool_count\n    output_offset = batch_id * out_stride_b + h_out * out_stride_h + w_out * out_stride_w\n    for oc_block in range(0, out_channels, BLOCK_SIZE_OC):\n        oc_offsets = oc_block + tl.arange(0, BLOCK_SIZE_OC)\n        oc_mask = oc_offsets < out_channels\n        tl.store(\n            output_ptr + output_offset + oc_offsets * out_stride_c,\n            pool_acc,\n            mask=oc_mask\n        )\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        super(ModelNew, self).__init__()\n        self.bn = nn.BatchNorm2d(num_input_features)\n        self.conv = nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.num_input_features = num_input_features\n        self.num_output_features = num_output_features\n\n    def forward(self, x):\n        if x.is_cuda and not self.training:\n            x = self.bn(x)\n            x = self.relu(x)\n            x = x.contiguous()\n            output = torch.empty(\n                (x.size(0), self.num_output_features, x.size(2)//2, x.size(3)//2),\n                device=x.device, dtype=x.dtype\n            )\n            batch, in_channels, height, width = x.shape\n            out_channels = self.num_output_features\n            grid = (batch * (height//2) * (width//2),)\n            conv_pool_kernel[grid](\n                x, self.conv.weight, output,\n                batch, in_channels, height, width, out_channels,\n                x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n                output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n                BLOCK_SIZE_IC=32, BLOCK_SIZE_OC=128\n            )\n            return output\n        else:\n            return self.pool(self.conv(self.relu(self.bn(x))))\n\nbatch_size = 10\nnum_input_features = 32\nnum_output_features = 64\nheight, width = 224, 224\n\ndef get_inputs():\n    return [torch.randn(batch_size, num_input_features, height, width)]\n\ndef get_init_inputs():\n    return [num_input_features, num_output_features]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "a93f6ff3-fa4b-41d4-8bf4-c87470b430f7", "generation": 1, "timestamp": 1754587893.3602943, "iteration_found": 7, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.595, "runtime_ms": 0.506, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.301, "speed_up": 0.595, "custom_timing": 0.506}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.505, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.301, "speed_up": 0.596, "custom_timing": 0.505}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
{"id": "f4fdac9d-b3d8-4bac-aa24-93e12364d918", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_OC': 64, 'BLOCK_SIZE_IC': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_OC': 128, 'BLOCK_SIZE_IC': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_OC': 64, 'BLOCK_SIZE_IC': 128}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_OC': 128, 'BLOCK_SIZE_IC': 128}, num_warps=8),\n    ],\n    key=['num_input_features', 'num_output_features', 'height', 'width']\n)\n@triton.jit\ndef transition_kernel(\n    input_ptr,\n    output_ptr,\n    conv_weight_ptr,\n    batch_size,\n    num_input_features,\n    num_output_features,\n    height,\n    width,\n    stride_b,\n    stride_c,\n    stride_h,\n    stride_w,\n    BLOCK_SIZE_OC: tl.constexpr,\n    BLOCK_SIZE_IC: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    num_output_pixels = (height // 2) * (width // 2)\n    pixel_id = pid0 % num_output_pixels\n    batch_id = pid0 // num_output_pixels\n    i = pixel_id // (width // 2)\n    j = pixel_id % (width // 2)\n    \n    h0 = 2 * i\n    w0 = 2 * j\n    \n    oc_block_start = pid1 * BLOCK_SIZE_OC\n    oc_offsets = oc_block_start + tl.arange(0, BLOCK_SIZE_OC)\n    oc_mask = oc_offsets < num_output_features\n    \n    accumulator = tl.zeros((BLOCK_SIZE_OC,), dtype=tl.float32)\n    valid_count = 0\n    \n    # Precompute valid positions\n    positions = tl.zeros((4,), dtype=tl.int1)\n    for di in range(2):\n        for dj in range(2):\n            h = h0 + di\n            w = w0 + dj\n            valid = (h < height) & (w < width)\n            positions = tl.where(tl.arange(0, 4) == (di*2 + dj), valid, positions)\n            valid_count += tl.sum(valid, axis=0)\n    \n    for ic_block_start in range(0, num_input_features, BLOCK_SIZE_IC):\n        ic_offsets = ic_block_start + tl.arange(0, BLOCK_SIZE_IC)\n        ic_mask = ic_offsets < num_input_features\n        \n        # Load input data for all 4 positions at once\n        input_data = tl.zeros((BLOCK_SIZE_IC, 4), dtype=tl.float32)\n        for idx in range(4):\n            di = idx // 2\n            dj = idx % 2\n            if positions[idx]:\n                input_ptr_current = (\n                    input_ptr + \n                    batch_id * stride_b + \n                    (h0 + di) * stride_h + \n                    (w0 + dj) * stride_w + \n                    ic_block_start * stride_c\n                )\n                data = tl.load(\n                    input_ptr_current + tl.arange(0, BLOCK_SIZE_IC),\n                    mask=ic_mask,\n                    other=0.0\n                )\n                input_data = input_data.to(tl.float32)  # Ensure correct type\n                input_data = tl.where(tl.arange(0, 4)[None, :] == idx, data[:, None], input_data)\n        \n        # Compute average across valid positions\n        avg = tl.sum(input_data, axis=1) / valid_count\n        \n        weight_ptr_current = (\n            conv_weight_ptr + \n            oc_block_start * num_input_features + \n            ic_block_start\n        )\n        weight_block = tl.load(\n            weight_ptr_current + \n            tl.arange(0, BLOCK_SIZE_OC)[:, None] * num_input_features + \n            tl.arange(0, BLOCK_SIZE_IC)[None, :],\n            mask=oc_mask[:, None] & ic_mask[None, :],\n            other=0.0\n        )\n        \n        # Use FP16 for tensor core acceleration\n        weight_block_f16 = weight_block.to(tl.float16)\n        avg_f16 = avg.to(tl.float16)\n        partial = tl.dot(weight_block_f16, avg_f16, out_dtype=tl.float32)\n        accumulator += partial\n    \n    output_ptr_current = (\n        output_ptr + \n        batch_id * num_output_features * (height//2) * (width//2) + \n        i * (width//2) * num_output_features + \n        j * num_output_features + \n        oc_block_start\n    )\n    tl.store(\n        output_ptr_current + tl.arange(0, BLOCK_SIZE_OC),\n        accumulator,\n        mask=oc_mask\n    )\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        super(ModelNew, self).__init__()\n        self.num_input_features = num_input_features\n        self.num_output_features = num_output_features\n        \n        self.bn = nn.BatchNorm2d(num_input_features)\n        self.conv_weight = nn.Parameter(\n            torch.empty(num_output_features, num_input_features)\n        )\n        nn.init.kaiming_uniform_(self.conv_weight, a=math.sqrt(5))\n\n    def forward(self, x):\n        if x.is_cuda and not self.training:\n            x = self.bn(x)\n            x = F.relu(x)\n            \n            batch_size, _, height, width = x.shape\n            output = torch.empty(\n                (batch_size, self.num_output_features, height//2, width//2),\n                device=x.device, dtype=x.dtype\n            )\n            \n            grid = (\n                batch_size * (height//2) * (width//2),\n                triton.cdiv(self.num_output_features, 64)\n            )\n            \n            stride_b = x.stride(0)\n            stride_c = x.stride(1)\n            stride_h = x.stride(2)\n            stride_w = x.stride(3)\n            \n            transition_kernel[grid](\n                x,\n                output,\n                self.conv_weight,\n                batch_size,\n                self.num_input_features,\n                self.num_output_features,\n                height,\n                width,\n                stride_b,\n                stride_c,\n                stride_h,\n                stride_w,\n                BLOCK_SIZE_OC=64,\n                BLOCK_SIZE_IC=64\n            )\n            return output\n        else:\n            x = self.bn(x)\n            x = F.relu(x)\n            weight_4d = self.conv_weight.view(\n                self.num_output_features, self.num_input_features, 1, 1\n            )\n            x = F.conv2d(x, weight_4d, bias=None)\n            return F.avg_pool2d(x, kernel_size=2, stride=2)\n\nbatch_size = 10\nnum_input_features = 32\nnum_output_features = 64\nheight, width = 224, 224\n\ndef get_inputs():\n    return [torch.randn(batch_size, num_input_features, height, width)]\n\ndef get_init_inputs():\n    return [num_input_features, num_output_features]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "0ca99f27-0b0f-497b-a704-e309333d26d7", "generation": 3, "timestamp": 1754590863.1876757, "iteration_found": 20, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.596, "runtime_ms": 0.505, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.301, "speed_up": 0.596, "custom_timing": 0.505}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.595, "runtime_ms": 0.506, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.301, "speed_up": 0.595, "custom_timing": 0.506}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
{"id": "298f140c-b347-4134-bd55-d524b5bde7d9", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef transition_kernel(\n    input_ptr,\n    output_ptr,\n    folded_scale_ptr,\n    folded_bias_ptr,\n    folded_weight_ptr,\n    num_input_features,\n    num_output_features,\n    H, W,\n    stride_b, stride_c, stride_h, stride_w,\n    BLOCK_C: tl.constexpr,\n    BLOCK_OC: tl.constexpr\n):\n    pid = tl.program_id(0)\n    H_out = H // 2\n    W_out = W // 2\n    elements_per_batch = H_out * W_out\n    total_elements = tl.num_programs(0)\n    \n    # Decompose PID into components\n    batch_idx = pid // elements_per_batch\n    spatial_idx = pid % elements_per_batch\n    i = spatial_idx // W_out\n    j = spatial_idx % W_out\n\n    # Initialize accumulators for output channel blocks\n    accs = tl.zeros((BLOCK_OC,), dtype=tl.float32)\n    \n    # Loop over output channel blocks\n    for oc_block_idx in range(0, num_output_features, BLOCK_OC):\n        oc_offsets = oc_block_idx + tl.arange(0, BLOCK_OC)\n        oc_mask = oc_offsets < num_output_features\n        \n        # Loop over input channels in blocks\n        for c_start in range(0, num_input_features, BLOCK_C):\n            c_offsets = c_start + tl.arange(0, BLOCK_C)\n            c_mask = c_offsets < num_input_features\n            \n            # Load parameters\n            scale_block = tl.load(folded_scale_ptr + c_offsets, mask=c_mask)\n            bias_block = tl.load(folded_bias_ptr + c_offsets, mask=c_mask)\n            \n            # Load weight block for current input/output channels\n            weight_ptr = folded_weight_ptr + (oc_offsets[:, None] * num_input_features + c_offsets[None, :])\n            weight_block = tl.load(weight_ptr, mask=oc_mask[:, None] & c_mask[None, :], other=0.0)\n            \n            # Process 2x2 window\n            for di in range(2):\n                for dj in range(2):\n                    h = 2*i + di\n                    w = 2*j + dj\n                    if h < H and w < W:\n                        # Compute input offset\n                        offset = (batch_idx * stride_b + \n                                 c_offsets * stride_c + \n                                 h * stride_h + \n                                 w * stride_w)\n                        # Load input\n                        x_val = tl.load(input_ptr + offset, mask=c_mask, other=0.0)\n                        # BN + ReLU\n                        norm_val = x_val * scale_block + bias_block\n                        activated = tl.where(norm_val > 0, norm_val, 0.0)\n                        # Weighted sum (matrix-vector product)\n                        accs += tl.sum(weight_block * activated[None, :], axis=1)\n        \n        # Store results for current output channel block\n        out_idx = (batch_idx * num_output_features * H_out * W_out + \n                   oc_offsets * H_out * W_out + \n                   i * W_out + j)\n        tl.store(output_ptr + out_idx, accs / 4.0, mask=oc_mask)\n        accs *= 0  # Reset for next block\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_input_features: int, num_output_features: int):\n        super(ModelNew, self).__init__()\n        self.bn = nn.BatchNorm2d(num_input_features)\n        self.conv = nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False)\n        self.eps = self.bn.eps\n        \n    def forward(self, x):\n        # Precompute folded BN parameters\n        with torch.no_grad():\n            bn_scale = self.bn.weight / torch.sqrt(self.bn.running_var + self.bn.eps)\n            bn_bias = self.bn.bias - self.bn.running_mean * bn_scale\n        \n        # Prepare output tensor\n        batch_size, _, H, W = x.shape\n        H_out = H // 2\n        W_out = W // 2\n        output = torch.empty(\n            batch_size, \n            self.conv.out_channels, \n            H_out, \n            W_out, \n            device=x.device, \n            dtype=x.dtype\n        )\n        \n        # Reshape convolution weights to 2D\n        conv_weight_2d = self.conv.weight.view(self.conv.out_channels, -1)\n        \n        # Launch kernel\n        total_elements = batch_size * H_out * W_out\n        grid = (total_elements,)\n        transition_kernel[grid](\n            x.contiguous(),\n            output.view(-1),\n            bn_scale,\n            bn_bias,\n            conv_weight_2d,\n            self.conv.in_channels,\n            self.conv.out_channels,\n            H, W,\n            x.stride(0),\n            x.stride(1),\n            x.stride(2),\n            x.stride(3),\n            BLOCK_C=32,\n            BLOCK_OC=4\n        )\n        return output\n\nbatch_size = 10\nnum_input_features = 32\nnum_output_features = 64\nheight, width = 224, 224\n\ndef get_inputs():\n    return [torch.randn(batch_size, num_input_features, height, width)]\n\ndef get_init_inputs():\n    return [num_input_features, num_output_features]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "6282ec43-4570-4b49-ba35-34bbf02fdfd9", "generation": 3, "timestamp": 1754592647.224301, "iteration_found": 28, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.06, "runtime_ms": 5.0, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.301, "speed_up": 0.06, "custom_timing": 5.0}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.016, "runtime_ms": 19.1, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.301, "speed_up": 0.016, "custom_timing": 19.1}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
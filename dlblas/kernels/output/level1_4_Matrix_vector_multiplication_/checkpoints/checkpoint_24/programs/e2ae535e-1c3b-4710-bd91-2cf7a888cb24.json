{"id": "e2ae535e-1c3b-4710-bd91-2cf7a888cb24", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit(num_warps=8)\ndef _matvec_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, K,\n    stride_am, stride_ak,\n    stride_bk, stride_b1,\n    stride_cm, stride_c1,\n    BLOCK_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    offs_m = pid\n    c_ptr = C_ptr + offs_m * stride_cm\n\n    accumulator = 0.0\n    for k in range(0, K, BLOCK_K):\n        offs_k = k + tl.arange(0, BLOCK_K)\n        mask = offs_k < K\n\n        a_ptr = A_ptr + offs_m * stride_am + offs_k * stride_ak\n        a = tl.load(a_ptr, mask=mask, other=0.0)\n\n        b_ptr = B_ptr + offs_k * stride_bk\n        b = tl.load(b_ptr, mask=mask, other=0.0)\n\n        product = a * b\n        accumulator += tl.sum(product, axis=0)\n\n    tl.store(c_ptr, accumulator)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        M, K = A.shape\n        assert B.shape == (K, 1), f\"B must be of shape ({K}, 1)\"\n        C = torch.empty((M, 1), device=A.device, dtype=A.dtype)\n        \n        if M == 0:\n            return C\n\n        BLOCK_K = 2048\n        grid = (M,)\n        stride_am, stride_ak = A.stride()\n        stride_bk, stride_b1 = B.stride()\n        stride_cm, stride_c1 = C.stride()\n\n        _matvec_kernel[grid](\n            A, B, C,\n            M, K,\n            stride_am, stride_ak,\n            stride_bk, stride_b1,\n            stride_cm, stride_c1,\n            BLOCK_K=BLOCK_K\n        )\n        return C\n\nM = 256\nK = 131072\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "b7e606d0-573c-4a2d-a6d8-247e209c5c79", "generation": 2, "timestamp": 1754639956.6243844, "iteration_found": 10, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "", "exec_log": "[Child] \u7ed1\u5b9a GPU 7, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 7, in <module>\nTypeError: jit() got an unexpected keyword argument 'num_warps'\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.463, "runtime_ms": 0.126, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0584, "speed_up": 0.463, "custom_timing": 0.126}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.4630\n- runtime_ms: 0.1260\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0584, 'speed_up': 0.463, 'custom_timing': 0.126}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level1_4_Matrix_vector_multiplication_', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2045, in dot\\n    return _semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1497, in dot\\n    assert lhs_rank == rhs_rank == 2 or lhs_rank == rhs_rank == 3, f\"Both inputs must be either 2D or 3D; (lhs: {lhs.shape} vs rhs: {rhs.shape})\"\\nAssertionError: Both inputs must be either 2D or 3D; (lhs: [\\'constexpr[2048]\\'] vs rhs: [\\'constexpr[2048]\\'])\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpct4icvns.py\", line 56, in forward\\n    _matvec_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 27:23:\\n    accumulator = 0.0\\n    for k in range(0, K, BLOCK_K):\\n        offs_k = k + tl.arange(0, BLOCK_K)\\n        mask = offs_k < K\\n\\n        a_ptr = A_ptr + offs_m * stride_am + offs_k * stride_ak\\n        a = tl.load(a_ptr, mask=mask, other=0.0)\\n\\n        b_ptr = B_ptr + offs_k * stride_bk\\n        b = tl.load(b_ptr, mask=mask, other=0.0)\\n\\n        accumulator += tl.dot(a, b)\\n                       ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2045, in dot\\n    return _semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1497, in dot\\n    assert lhs_rank == rhs_rank == 2 or lhs_rank == rhs_rank == 3, f\"Both inputs must be either 2D or 3D; (lhs: {lhs.shape} vs rhs: {rhs.shape})\"\\nAssertionError: Both inputs must be either 2D or 3D; (lhs: [\\'constexpr[2048]\\'] vs rhs: [\\'constexpr[2048]\\'])\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.0584, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.4630, runtime_ms: 0.1260, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0584, 'speed_up': 0.463, 'custom_timing': 0.126}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _matvec_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, K,\n    stride_am, stride_ak,\n    stride_bk, stride_b1,\n    stride_cm, stride_c1,\n    BLOCK_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    offs_m = pid\n    c_ptr = C_ptr + offs_m * stride_cm\n\n    accumulator = 0.0\n    for k in range(0, K, BLOCK_K):\n        offs_k = k + tl.arange(0, BLOCK_K)\n        mask = offs_k < K\n\n        a_ptr = A_ptr + offs_m * stride_am + offs_k * stride_ak\n        a = tl.load(a_ptr, mask=mask, other=0.0)\n\n        b_ptr = B_ptr + offs_k * stride_bk\n        b = tl.load(b_ptr, mask=mask, other=0.0)\n\n        product = a * b\n        accumulator += tl.sum(product, axis=0)\n\n    tl.store(c_ptr, accumulator)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        M, K = A.shape\n        assert B.shape == (K, 1), f\"B must be of shape ({K}, 1)\"\n        C = torch.empty((M, 1), device=A.device, dtype=A.dtype)\n        \n        if M == 0:\n            return C\n\n        BLOCK_K = 1024\n        grid = (M,)\n        stride_am, stride_ak = A.stride()\n        stride_bk, stride_b1 = B.stride()\n        stride_cm, stride_c1 = C.stride()\n\n        _matvec_kernel[grid](\n            A, B, C,\n            M, K,\n            stride_am, stride_ak,\n            stride_bk, stride_b1,\n            stride_cm, stride_c1,\n            BLOCK_K=BLOCK_K\n        )\n        return C\n\nM = 256\nK = 131072\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.4630), Performs well on runtime_ms (0.1260), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0584, 'speed_up': 0.463, 'custom_timing': 0.126})\n\n\n### Program 2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _matvec_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, K,\n    stride_am, stride_ak,\n    stride_bk, stride_b1,\n    stride_cm, stride_c1,\n    BLOCK_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    offs_m = pid\n    c_ptr = C_ptr + offs_m * stride_cm\n\n    accumulator = 0.0\n    for k in range(0, K, BLOCK_K):\n        offs_k = k + tl.arange(0, BLOCK_K)\n        mask = offs_k < K\n\n        a_ptr = A_ptr + offs_m * stride_am + offs_k * stride_ak\n        a = tl.load(a_ptr, mask=mask, other=0.0)\n\n        b_ptr = B_ptr + offs_k * stride_bk\n        b = tl.load(b_ptr, mask=mask, other=0.0)\n\n        accumulator += tl.dot(a, b)\n\n    tl.store(c_ptr, accumulator)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        M, K = A.shape\n        assert B.shape == (K, 1), f\"B must be of shape ({K}, 1)\"\n        C = torch.empty((M, 1), device=A.device, dtype=A.dtype)\n        \n        if M == 0:\n            return C\n\n        BLOCK_K = 2048\n        grid = (M,)\n        stride_am, stride_ak = A.stride()\n        stride_bk, stride_b1 = B.stride()\n        stride_cm, stride_c1 = C.stride()\n\n        _matvec_kernel[grid](\n            A, B, C,\n            M, K,\n            stride_am, stride_ak,\n            stride_bk, stride_b1,\n            stride_cm, stride_c1,\n            BLOCK_K=BLOCK_K\n        )\n        return C\n\nM = 256\nK = 131072\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level1_4_Matrix_vector_multiplication_', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2045, in dot\\n    return _semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1497, in dot\\n    assert lhs_rank == rhs_rank == 2 or lhs_rank == rhs_rank == 3, f\"Both inputs must be either 2D or 3D; (lhs: {lhs.shape} vs rhs: {rhs.shape})\"\\nAssertionError: Both inputs must be either 2D or 3D; (lhs: [\\'constexpr[2048]\\'] vs rhs: [\\'constexpr[2048]\\'])\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpct4icvns.py\", line 56, in forward\\n    _matvec_kernel[grid](\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\\n    kernel = self.compile(src, target=target, options=options.__dict__)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\\n    module = src.make_ir(options, codegen_fns, module_map, context)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\\ntriton.compiler.errors.CompilationError: at 27:23:\\n    accumulator = 0.0\\n    for k in range(0, K, BLOCK_K):\\n        offs_k = k + tl.arange(0, BLOCK_K)\\n        mask = offs_k < K\\n\\n        a_ptr = A_ptr + offs_m * stride_am + offs_k * stride_ak\\n        a = tl.load(a_ptr, mask=mask, other=0.0)\\n\\n        b_ptr = B_ptr + offs_k * stride_bk\\n        b = tl.load(b_ptr, mask=mask, other=0.0)\\n\\n        accumulator += tl.dot(a, b)\\n                       ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\\n    return fn(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 2045, in dot\\n    return _semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 1497, in dot\\n    assert lhs_rank == rhs_rank == 2 or lhs_rank == rhs_rank == 3, f\"Both inputs must be either 2D or 3D; (lhs: {lhs.shape} vs rhs: {rhs.shape})\"\\nAssertionError: Both inputs must be either 2D or 3D; (lhs: [\\'constexpr[2048]\\'] vs rhs: [\\'constexpr[2048]\\'])\\n', 'runtime_error_name': 'triton.compiler.errors.CompilationError', 'reference_timing_ms': 0.0584, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _matvec_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, K,\n    stride_am, stride_ak,\n    stride_bk, stride_b1,\n    stride_cm, stride_c1,\n    BLOCK_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    offs_m = pid\n    c_ptr = C_ptr + offs_m * stride_cm\n\n    accumulator = 0.0\n    for k in range(0, K, BLOCK_K):\n        offs_k = k + tl.arange(0, BLOCK_K)\n        mask = offs_k < K\n\n        a_ptr = A_ptr + offs_m * stride_am + offs_k * stride_ak\n        a = tl.load(a_ptr, mask=mask, other=0.0)\n\n        b_ptr = B_ptr + offs_k * stride_bk\n        b = tl.load(b_ptr, mask=mask, other=0.0)\n\n        accumulator += tl.dot(a, b)\n\n    tl.store(c_ptr, accumulator)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        M, K = A.shape\n        assert B.shape == (K, 1), f\"B must be of shape ({K}, 1)\"\n        C = torch.empty((M, 1), device=A.device, dtype=A.dtype)\n        \n        if M == 0:\n            return C\n\n        BLOCK_K = 2048\n        grid = (M,)\n        stride_am, stride_ak = A.stride()\n        stride_bk, stride_b1 = B.stride()\n        stride_cm, stride_c1 = C.stride()\n\n        _matvec_kernel[grid](\n            A, B, C,\n            M, K,\n            stride_am, stride_ak,\n            stride_bk, stride_b1,\n            stride_cm, stride_c1,\n            BLOCK_K=BLOCK_K\n        )\n        return C\n\nM = 256\nK = 131072\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _matvec_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, K,\n    stride_am, stride_ak,\n    stride_bk, stride_b1,\n    stride_cm, stride_c1,\n    BLOCK_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    offs_m = pid\n    c_ptr = C_ptr + offs_m * stride_cm\n\n    accumulator = 0.0\n    for k in range(0, K, BLOCK_K):\n        offs_k = k + tl.arange(0, BLOCK_K)\n        mask = offs_k < K\n\n        a_ptr = A_ptr + offs_m * stride_am + offs_k * stride_ak\n        a = tl.load(a_ptr, mask=mask, other=0.0)\n\n        b_ptr = B_ptr + offs_k * stride_bk\n        b = tl.load(b_ptr, mask=mask, other=0.0)\n\n        product = a * b\n        accumulator += tl.sum(product, axis=0)\n\n    tl.store(c_ptr, accumulator)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        M, K = A.shape\n        assert B.shape == (K, 1), f\"B must be of shape ({K}, 1)\"\n        C = torch.empty((M, 1), device=A.device, dtype=A.dtype)\n        \n        if M == 0:\n            return C\n\n        BLOCK_K = 1024\n        grid = (M,)\n        stride_am, stride_ak = A.stride()\n        stride_bk, stride_b1 = B.stride()\n        stride_cm, stride_c1 = C.stride()\n\n        _matvec_kernel[grid](\n            A, B, C,\n            M, K,\n            stride_am, stride_ak,\n            stride_bk, stride_b1,\n            stride_cm, stride_c1,\n            BLOCK_K=BLOCK_K\n        )\n        return C\n\nM = 256\nK = 131072\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a Triton kernel for matrix-vector multiplication (A of shape [M, K] and B of shape [K, 1]).\n The current implementation uses a loop over blocks of K (size BLOCK_K) and for each row (M) it:\n   - Loads a block of row of A and the corresponding block of B (as a vector)\n   - Multiplies element-wise and then sums the products to accumulate.\n\n However, the current speedup is 0.463 (which is slower than the reference). We need to make it faster.\n\n Observations:\n   - The current BLOCK_K is set to 1024.\n   - The kernel is launched with one program per row (grid = (M,)).\n   - The inner loop does a simple element-wise multiply and then a sum.\n\n The problem: The reference time is 0.0584 ms and the custom kernel takes 0.126 ms (so about 2.15x slower).\n\n Why is it slow?\n   - The memory access pattern for A is good (each row is contiguous) and for B we are loading a contiguous block.\n   - However, note that B is a vector of length K. We are loading the same block of B for every row (so it would be beneficial to cache it in shared memory? But note: the same block of B is reused for all rows? Actually, yes: the entire B vector is reused for every row).\n\n Proposed optimization:\n   - We can use tiling and load a block of B into shared memory so that it can be reused by multiple rows? But note: our grid is over rows (M). We are processing one row per program. So we cannot share between rows unless we change the grid.\n\n Alternative approach:\n   - Instead of having one program per row, we can have multiple programs per row? That might not help.\n\n Another idea: use vectorized loads and reduce the number of operations.\n\n However, note that the inner loop is already vectorized: we are loading a block of BLOCK_K elements and then doing a vector multiply and then a sum.\n\n What if we increase BLOCK_K? Currently it is 1024. The problem is that K is 131072, so we are doing 131072/1024 = 128 iterations. This might cause a lot of overhead.\n\n Alternatively, we can use a larger BLOCK_K? But note that the maximum BLOCK_K is limited by the available registers and shared memory? Actually, we are not using shared memory.\n\n Let's check the constraints of the H100: \n   - We are loading two vectors of length BLOCK_K (each element is float32, so 4 bytes). So we need 2 * BLOCK_K * 4 bytes of registers? Actually, the accumulator is a scalar and the product is a vector of BLOCK_K? But note: we are doing `tl.sum` over the vector. So we only need one accumulator per thread? And the vectors are loaded and then multiplied and then summed immediately? So we don't need to keep the entire vector in registers? Actually, the Triton compiler might break the loop into chunks if necessary? \n\n But note: the current implementation does:\n   a = tl.load(...)   # vector of BLOCK_K\n   b = tl.load(...)   # vector of BLOCK_K\n   product = a * b    # vector of BLOCK_K\n   accumulator += tl.sum(product, axis=0)\n\n This requires storing the entire product vector in registers? Not necessarily: the sum reduction can be done in a tree-like fashion. However, the Triton compiler might use multiple threads for the reduction? Actually, no: the entire block is processed by one thread? Because we are in a loop over k and the program is for one row.\n\n Therefore, the entire BLOCK_K must be processed by one thread. The number of registers per thread is limited (256 on H100). We can have up to 256 float32 values (1024 bytes) in registers? But we are loading two vectors (a and b) of BLOCK_K and then we have the product. So that's 3 * BLOCK_K registers? That would be 3 * 1024 = 3072 registers? That is too many.\n\n Actually, Triton uses a different approach: it breaks the program into warps and then into threads. The BLOCK_K is split among threads? No, in the current kernel, the program is one-dimensional over M. The inner loop over k is sequential? And the tl.arange(0, BLOCK_K) creates a vector of BLOCK_K, which is then processed by the thread? Actually, no: the Triton compiler will automatically parallelize the operations within the thread? But note: the tl.sum will be parallelized within the thread? \n\n However, the key is that the current BLOCK_K=1024 might be too large for the compiler to handle efficiently? We might get register spilling.\n\n We see that in the inspiration program (Attempt 2) they tried to use `tl.dot` but that failed because the vectors are 1D and the dot product expects 2D or 3D. \n\n What if we change the kernel to process multiple rows per block? We can have a 2D grid: \n   - Grid: (triton.cdiv(M, BLOCK_M), ) and then within the block, we have BLOCK_M rows and BLOCK_K columns? \n\n But note: the problem says we cannot change the grid configuration? The constraint says: \"\u274c NEVER modify grid configuration, output tensor shapes, or PID logic\"\n\n So we must keep the grid as (M,). We cannot change the grid.\n\n Another idea: use tensor cores. The H100 has tensor cores for FP16. But the current dtype is float32? The example input is torch.randn which is float32. We cannot change the dtype.\n\n However, we can check the input dtypes? The problem does not specify, but the reference code is in float32? Actually, the example uses torch.randn which by default is float32.\n\n But note: the problem says we are allowed to use Tensor Core alignment (FP16/bf16/tf32). So we might use tf32? However, we must be cautious because tf32 has lower precision.\n\n Alternatively, we can try to break the inner loop into chunks that are more efficient? Or use vectorized loads? \n\n How about we change the inner loop to use a two-dimensional block? But note: we cannot change the grid. \n\n However, we can change the inner loop to use a two-dimensional tiling within the thread? We are allowed to change the BLOCK_K and also the way we load and compute.\n\n Proposed change:\n   - Instead of having BLOCK_K as the entire chunk, we can break the inner loop into two levels? But that might not help.\n\n Alternatively, we can use the fact that the vector B is the same for all rows? We can cache it in shared memory? But note: we are processing one row per program. So the same program might not have access to the same shared memory? And we cannot share between different programs.\n\n However, we can change the kernel to process multiple rows per program? But that would change the grid. We are not allowed.\n\n Another idea: use a different BLOCK_K. We see that in the inspiration program they used 2048 and it compiled but failed because of the dot product. We are not using dot product. We are using element-wise multiply and then sum. \n\n Why not try a larger BLOCK_K? The current BLOCK_K=1024, and we have 131072 columns. 131072/1024=128 iterations. We can try to increase BLOCK_K to 2048? Then we have 64 iterations. But note: the register pressure? We are loading two vectors of 2048 elements? That might cause register spilling.\n\n Alternatively, we can try a smaller BLOCK_K? But that would increase the number of iterations.\n\n How about we change the kernel to use vectorized loads? Currently, we are loading BLOCK_K elements. We can load the same amount but in a vectorized way? But Triton's tl.load already does vectorization if the pointer is aligned and the mask is contiguous? \n\n Alternatively, we can change the inner loop to do the reduction in a more efficient way? Currently, we do:\n   accumulator += tl.sum(product, axis=0)\n\n We can use a tree reduction? But Triton's tl.sum already does that.\n\n Another idea: use the fact that the vector B is constant? We could pre-fetch the entire vector B? But that would require too much memory.\n\n Considering the constraints, we might try to increase BLOCK_K to 2048 and see if it improves performance? But we must be cautious of the register pressure.\n\n Alternatively, we can try to break the inner loop into two levels: \n   - We break BLOCK_K into chunks of, say, 256, and then within each chunk we do the multiply and then a partial sum? Then we accumulate the partial sums? This might help with register pressure? But it might not.\n\n Given that the current implementation is slow, and we are not using the tensor cores, we might try to use tensor cores by converting the data to FP16? But note: the problem says we cannot change the output, and the reference is in float32. We must use the same dtype.\n\n However, we can do the accumulation in FP32 and use FP16 for the tensor cores? The tensor cores support mixed precision.\n\n How to use tensor cores for matrix-vector multiplication?\n   - We are doing: for each row i: C[i] = sum_j A[i,j] * B[j]\n   - This is a matrix (M, K) times a vector (K, 1). We can view it as a matrix multiplication of A (M, K) and B (K, 1). So it's a matrix multiplication with a very small second dimension (1) in B. Tensor cores are designed for larger matrices.\n\n But note: we are allowed to use the dot product if we reshape the vectors? However, the previous attempt failed because the dot product requires 2D or 3D. We can reshape the vectors to 2D? \n\n Let me see: the current a is a vector of shape [BLOCK_K] and b is a vector of shape [BLOCK_K]. We want to compute the dot product of these two vectors. The tl.dot function requires two 2D matrices? Actually, the documentation says: \n   - tl.dot(a, b) where a and b are 2D tensors.\n\n We can reshape the vectors to 2D? \n   - We can do: a_2d = a.reshape((1, BLOCK_K)) and b_2d = b.reshape((BLOCK_K, 1))? Then dot would be a (1, BLOCK_K) * (BLOCK_K, 1) -> (1,1). \n\n But note: the Triton dot function does not support arbitrary reshapes? We would have to create a new tensor? \n\n Alternatively, we can use the fact that the dot product of two vectors is the same as tl.sum(a * b). So we are already doing the equivalent.\n\n Therefore, using tl.dot for two vectors is not going to help.\n\n Another idea: we can use the tensor cores by tiling the matrix A into blocks and then doing a matrix multiplication with a block of B? But note that B has only one column. So we would be multiplying a block of A (BLOCK_M, BLOCK_K) by a block of B (BLOCK_K, 1) to get (BLOCK_M, 1). This would be efficient if we use tensor cores? But we are constrained by the grid: we cannot change the grid.\n\n How about we change the grid? We are not allowed. So we must stick to one row per program.\n\n Given the above, we need to optimize the current kernel without changing the grid.\n\n Proposed optimizations:\n\n 1. Increase BLOCK_K to reduce the number of iterations. We try 2048? But note that the previous attempt with 2048 and tl.dot failed. We are not using tl.dot, so it might work? But the register pressure might be too high.\n\n 2. Use vectorized loads? We can load the data in chunks of 2 or 4? But Triton might already be doing that.\n\n 3. Pre-fetch the next block of B while processing the current one? But note: the same block of B is used by all rows? But we are processing one row at a time. So we cannot share B between rows? And we don't have shared memory.\n\n 4. Use the fact that B is contiguous? We are loading it with stride_bk? It is contiguous? Because B is (K,1) and we are loading a contiguous block.\n\n 5. We can try to change the number of warps? Currently, we are not setting any. We can try to set num_warps and num_stages? We can autotune? But the problem says we are allowed to use adaptive tuning.\n\n However, we cannot change the function signature? But we can add autotune? The constraint says: \"\u274c NEVER change @triton.jit function signatures or parameter names\". So we cannot add autotune if it changes the signature? Actually, the autotune decorator goes above the jit.\n\n Let me look at the example: the example I provided earlier had an autotune decorator. We can do:\n\n   @triton.autotune(...)\n\n But note: the constraint says we cannot change the function signature? The autotune doesn't change the signature? It just adds configurations.\n\n So we can try to autotune the BLOCK_K and also the number of warps and stages.\n\n But note: the kernel currently has only one parameter: BLOCK_K. We can autotune over BLOCK_K and also over the number of warps and stages.\n\n However, the grid is fixed: one program per row. So the number of threads per block? We are currently using the default? We can set the number of threads per block? Actually, the grid is 1D of size M, and each program is independent. We can have multiple threads per block? But note: we are not using a block of threads? The kernel is written for one row per program. So each program is a single thread? That would be inefficient.\n\n Actually, in Triton, the program is executed by a block of threads. Even though we have one program per row, the program might be executed by multiple threads? We are using tl.arange(0, BLOCK_K) which is a vector operation. The Triton compiler will map that to multiple threads? \n\n Therefore, we can set the number of warps per block? We can try to set num_warps=8? \n\n How about we do:\n\n   @triton.autotune(\n        configs=[\n            triton.Config({'BLOCK_K': 256}, num_warps=4),\n            triton.Config({'BLOCK_K': 512}, num_warps=4),\n            triton.Config({'BLOCK_K': 1024}, num_warps=4),\n            triton.Config({'BLOCK_K': 2048}, num_warps=4),\n            triton.Config({'BLOCK_K': 256}, num_warps=8),\n            triton.Config({'BLOCK_K': 512}, num_warps=8),\n            triton.Config({'BLOCK_K': 1024}, num_warps=8),\n            triton.Config({'BLOCK_K': 2048}, num_warps=8),\n        ],\n        key=['M', 'K'],\n   )\n\n But note: the constraint says we cannot change the function signature? The autotune decorator doesn't change the signature? It is just a decorator.\n\n However, we must be cautious: the kernel must still be called with the same parameters. We are only changing the BLOCK_K and the number of warps? And we are adding an autotune config.\n\n But note: the kernel call in the forward function currently passes BLOCK_K=1024. With autotune, we cannot pass it as a constant? We have to remove the BLOCK_K from the call? Actually, the autotune will override the BLOCK_K? And we have to let the autotuner choose the value.\n\n So we have to remove the BLOCK_K from the call? But then the kernel expects BLOCK_K? We can pass it as a variable? We can predefine a set of possible BLOCK_K? \n\n Alternatively, we can let the autotuner choose the BLOCK_K and then we pass it as a triton constant? \n\n How about we change the kernel to:\n\n   @triton.autotune(...)\n   @triton.jit\n   def _matvec_kernel(..., BLOCK_K: tl.constexpr, ...):\n        ...\n\n Then in the forward function, we do not set BLOCK_K? We let the autotuner choose? But the kernel call must provide BLOCK_K? \n\n Actually, the autotuner will compile multiple versions and then choose the best one at runtime? And we must not specify BLOCK_K? We can pass it as a keyword argument? \n\n But note: the constraint says we cannot change the function signature? We are adding the autotune decorator and removing the BLOCK_K from the call? That might be acceptable? \n\n However, the constraint says: \"NEVER change @triton.jit function signatures or parameter names\". The autotune decorator does not change the signature? It just adds a layer. The function still has the same parameters.\n\n But the call site must change? We must not pass BLOCK_K? The autotuner will handle it? Actually, no: the autotuner requires that we pass the parameters that are being tuned? \n\n Alternatively, we can keep the call and let the autotuner override? I think we can do:\n\n   _matvec_kernel[grid](\n        ...,\n        BLOCK_K=BLOCK_K,   # but now we are not defining BLOCK_K? We have to define it as a variable that the autotuner will set? \n   )\n\n How about we do:\n\n   # in the forward function, we define:\n   BLOCK_K = 1024   # default, but autotuner will override\n\n   # and then we call with BLOCK_K=BLOCK_K\n\n But the autotuner will try different BLOCK_K? And we are passing a variable? Actually, the autotuner will compile multiple versions and then at runtime, it will choose the version that matches the BLOCK_K we passed? But we are passing a fixed BLOCK_K? That doesn't make sense.\n\n Alternatively, we can remove the BLOCK_K from the call and let the autotuner choose? \n\n The autotuner example in the documentation shows:\n\n   @triton.autotune(...)\n   @triton.jit\n   def kernel(...):\n        ...\n\n   kernel[grid](...)\n\n And the autotuner will automatically choose the best config? But note: the kernel must have the parameters that are being tuned? And we are not passing them? \n\n Actually, the autotuner will pass the parameters as constants? \n\n Given the complexity and the fact that we are not allowed to change the grid, I propose a simpler change: try a larger BLOCK_K and also set the number of warps.\n\n We'll set BLOCK_K to 2048 and num_warps=8? We can set num_warps by a decorator:\n\n   @triton.jit\n   def ...   -> we can set it by a config? But without autotune, we have to set it statically.\n\n We can do:\n\n   @triton.jit(num_warps=8)\n\n Let's try that.\n\n Steps:\n\n   - Change BLOCK_K from 1024 to 2048 in the forward function.\n   - Add num_warps=8 to the jit decorator.\n\n Why 8 warps? Because 8 warps is 256 threads, which is a common block size.\n\n Alternatively, we can try 4 warps? But let's try 8.\n\n Also, note that the kernel must be efficient in memory access. We are loading two vectors: \n   - A: for a given row and a block of columns -> contiguous? Yes, because the row is contiguous.\n   - B: a contiguous block of the vector.\n\n So the loads are coalesced? For A: each thread is loading a contiguous block? But note: the entire block is loaded by one thread? Actually, no: the thread block (with multiple threads) will load the vectors? \n\n How is the vector load distributed? The tl.arange(0, BLOCK_K) is split among the threads? \n\n So with more threads (num_warps=8, which is 256 threads), we can load BLOCK_K=2048 in 2048/256=8 elements per thread? That is efficient.\n\n Therefore, we change:\n\n   In the kernel decorator: @triton.jit -> @triton.jit(num_warps=8)\n\n   In the forward function: BLOCK_K = 2048\n\n But note: what if K is not divisible by 2048? The kernel already has a mask.\n\n However, we must be cautious: the previous attempt with BLOCK_K=2048 and tl.dot failed, but we are not using tl.dot. We are using the same element-wise multiply and sum.\n\n Let's try.\n\n But note: the constraint says we must not remove boundary checks? We are keeping the mask.\n\n Alternatively, we can also try to use a different BLOCK_K? But we start with 2048.\n\n However, we must be cautious of the maximum number of threads? The kernel is launched with grid=(M,). Each program is a block of 256 threads? And we have M blocks? That is acceptable.\n\n But note: M=256, so 256 blocks? Each block has 256 threads? That is 256*256=65536 threads? Which is acceptable.\n\n Therefore, we change:\n\n   @triton.jit(num_warps=8)\n   ... rest of kernel ...\n\n   and in forward: BLOCK_K = 2048\n\n We hope that this reduces the number of iterations and improves the performance.\n\n Let's write the code accordingly.\n\n If it doesn't work, we can try autotune in the next iteration.\n\n But note: the constraint says we should do small iterative steps? This change is small (only two lines).\n\n We'll do:\n\n   - Change the jit decorator to set num_warps=8.\n   - Change the BLOCK_K in the forward function to 2048.\n\n Let me write the code.\n\n Important: We must not change anything else.\n\n Note: We are not allowed to change the grid, and we are not changing it.\n\n Let's output the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit(num_warps=8)\ndef _matvec_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, K,\n    stride_am, stride_ak,\n    stride_bk, stride_b1,\n    stride_cm, stride_c1,\n    BLOCK_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    offs_m = pid\n    c_ptr = C_ptr + offs_m * stride_cm\n\n    accumulator = 0.0\n    for k in range(0, K, BLOCK_K):\n        offs_k = k + tl.arange(0, BLOCK_K)\n        mask = offs_k < K\n\n        a_ptr = A_ptr + offs_m * stride_am + offs_k * stride_ak\n        a = tl.load(a_ptr, mask=mask, other=0.0)\n\n        b_ptr = B_ptr + offs_k * stride_bk\n        b = tl.load(b_ptr, mask=mask, other=0.0)\n\n        product = a * b\n        accumulator += tl.sum(product, axis=0)\n\n    tl.store(c_ptr, accumulator)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        M, K = A.shape\n        assert B.shape == (K, 1), f\"B must be of shape ({K}, 1)\"\n        C = torch.empty((M, 1), device=A.device, dtype=A.dtype)\n        \n        if M == 0:\n            return C\n\n        BLOCK_K = 2048\n        grid = (M,)\n        stride_am, stride_ak = A.stride()\n        stride_bk, stride_b1 = B.stride()\n        stride_cm, stride_c1 = C.stride()\n\n        _matvec_kernel[grid](\n            A, B, C,\n            M, K,\n            stride_am, stride_ak,\n            stride_bk, stride_b1,\n            stride_cm, stride_c1,\n            BLOCK_K=BLOCK_K\n        )\n        return C\n\nM = 256\nK = 131072\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
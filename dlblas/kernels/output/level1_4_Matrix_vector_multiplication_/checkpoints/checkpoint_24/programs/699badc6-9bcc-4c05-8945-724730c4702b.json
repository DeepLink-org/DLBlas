{"id": "699badc6-9bcc-4c05-8945-724730c4702b", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef matvec_kernel(\n    a_ptr,\n    b_ptr,\n    output_ptr,\n    stride_am,\n    K,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    accumulator = tl.zeros(tl.float32, (1,))\n    for j in range(0, K, BLOCK_SIZE_K):\n        offs_k = j + tl.arange(0, BLOCK_SIZE_K)\n        mask = offs_k < K\n        a_ptrs = a_ptr + pid * stride_am + offs_k\n        b_ptrs = b_ptr + offs_k\n        a_val = tl.load(a_ptrs, mask=mask, other=0.0)\n        b_val = tl.load(b_ptrs, mask=mask, other=0.0)\n        product = a_val * b_val\n        block_sum = tl.sum(product, axis=0)\n        accumulator += block_sum\n    tl.store(output_ptr + pid, accumulator)\n\nclass ModelNew(nn.Module):\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        M, K = A.shape\n        B_vec = B.reshape(-1)\n        output = torch.empty(M, 1, device=A.device, dtype=A.dtype)\n        A_cont = A.contiguous()\n        B_cont = B_vec.contiguous()\n        grid = (M,)\n        BLOCK_SIZE_K = 1024\n        stride_am = A_cont.stride(0)\n        matvec_kernel[grid](A_cont, B_cont, output, stride_am, K, BLOCK_SIZE_K=BLOCK_SIZE_K)\n        return output\n\nM = 256\nK = 131072\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "a168af0f-8e69-4fc7-a336-ef8b04fd739b", "generation": 1, "timestamp": 1754638535.962938, "iteration_found": 3, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level1_4_Matrix_vector_multiplication_", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1693, in full\n    shape = _shape_check_impl(shape)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1676, in _shape_check_impl\n    shape = _unwrap_shape(shape)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1672, in _unwrap_shape\n    return [_unwrap_if_constexpr(s) for s in shape]\nTypeError: 'dtype' object is not iterable\n\nThe above exception was the direct cause of the following exception:\n\ntriton.compiler.errors.CompilationError: at 10:11:\ndef zeros(shape, dtype):\n    \"\"\"\n    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n\n    :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n    :type shape: tuple of ints\n    :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n    :type dtype: DType\n    \"\"\"\n    return core.full(shape, 0, dtype)\n           ^\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpu61clgjc.py\", line 41, in forward\n    matvec_kernel[grid](A_cont, B_cont, output, stride_am, K, BLOCK_SIZE_K=BLOCK_SIZE_K)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 10:18:\ndef matvec_kernel(\n    a_ptr,\n    b_ptr,\n    output_ptr,\n    stride_am,\n    K,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    accumulator = tl.zeros(tl.float32, (1,))\n                  ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1693, in full\n    shape = _shape_check_impl(shape)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1676, in _shape_check_impl\n    shape = _unwrap_shape(shape)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1672, in _unwrap_shape\n    return [_unwrap_if_constexpr(s) for s in shape]\nTypeError: 'dtype' object is not iterable\n\nThe above exception was the direct cause of the following exception:\n\ntriton.compiler.errors.CompilationError: at 10:11:\ndef zeros(shape, dtype):\n    \"\"\"\n    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n\n    :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n    :type shape: tuple of ints\n    :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n    :type dtype: DType\n    \"\"\"\n    return core.full(shape, 0, dtype)\n           ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1693, in full\n    shape = _shape_check_impl(shape)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1676, in _shape_check_impl\n    shape = _unwrap_shape(shape)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1672, in _unwrap_shape\n    return [_unwrap_if_constexpr(s) for s in shape]\nTypeError: 'dtype' object is not iterable\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 0.0584, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0596, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0584, "speed_up": 0.98, "custom_timing": 0.0596}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0596\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0584, 'speed_up': 0.98, 'custom_timing': 0.0596}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0596, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0584, 'speed_up': 0.98, 'custom_timing': 0.0596}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs matrix-vector multiplication (C = A * B).\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-vector multiplication.\n\n        Args:\n            A: Input matrix of shape (M, K).\n            B: Input vector of shape (K, 1).\n\n        Returns:\n            Output vector of shape (M, 1).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256\nK = 131072\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0596), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0584, 'speed_up': 0.98, 'custom_timing': 0.0596})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs matrix-vector multiplication (C = A * B).\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-vector multiplication.\n\n        Args:\n            A: Input matrix of shape (M, K).\n            B: Input vector of shape (K, 1).\n\n        Returns:\n            Output vector of shape (M, 1).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256\nK = 131072\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a simple matrix-vector multiplication (A: [M, K] and B: [K, 1] -> [M, 1]).\n The current implementation uses `torch.matmul` which is efficient but we are asked to rewrite for Triton to get a speedup.\n\n Steps:\n 1. We are going to write a Triton kernel for matrix-vector multiplication.\n 2. The kernel will have one program per row of A (since each row of A multiplied by vector B gives one element of output).\n 3. We note that the vector B is of size K, and we have to compute the dot product of each row of A with B.\n\n Considerations:\n - The matrix A is of shape (M, K) and vector B is (K, 1). Output is (M, 1).\n - We can use a 1D grid of M blocks (each block computes one output element).\n - Each block will load a row of A (length K) and the entire vector B (length K). However, note that K is very large (131072) and we cannot load the entire row at once.\n\n Plan:\n - We will split the row of A into multiple chunks. We'll use a block size (BLOCK_SIZE) and then have inner loops to accumulate the dot product.\n - We need to be careful about the memory access: we want to load contiguous chunks of A and the corresponding chunk of B.\n\n Kernel design:\n   pid = tl.program_id(0)  # which row we are processing (0..M-1)\n   We will have a loop over the columns of A in steps of BLOCK_SIZE.\n   For each step, we load a block of the row of A and a block of the vector B (both of size BLOCK_SIZE) and compute a partial dot product.\n\n Steps in the kernel:\n   Initialize accumulator to zero.\n   For start in range(0, K, BLOCK_SIZE):\n        offs_am = pid * stride_am + start + tl.arange(0, BLOCK_SIZE)\n        mask = (start + tl.arange(0, BLOCK_SIZE)) < K\n        a = tl.load(a_ptr + offs_am, mask=mask, other=0.0)\n        b = tl.load(b_ptr + start + tl.arange(0, BLOCK_SIZE), mask=mask, other=0.0)\n        accumulator += tl.sum(a * b)\n\n   Then store the accumulator at the output location for row pid.\n\n However, note: the entire vector B might not fit in shared memory and we are loading it in chunks. This is acceptable because each block is independent and we are reading B from global memory.\n\n But note: each block will read the entire vector B. Since there are M blocks (256) and the vector B has K elements (131072), the total global memory read would be:\n   A: M * K (each row of A is read once)\n   B: M * K (each block reads the entire B)\n\n This is inefficient because we are reading B M times (256 times). We can do better by caching B in shared memory? However, the size of B is 131072 * 4 (if float32) = 524288 bytes (512KB). The shared memory per SM is limited (max 227KB for H100? but we have L20Y which is similar to H100). Also, we cannot have the entire vector in shared memory because the shared memory per block is limited (max 227KB). And we have 256 blocks? They would each need 512KB -> not possible.\n\n Alternative approach: we can use a different kernel that loads the entire vector B once per SM and then shares it among multiple blocks? But note: Triton does not have inter-block communication and shared memory is per block.\n\n Therefore, we must avoid reading the entire vector B for each block. Instead, we can use a different kernel structure: use multiple program ids per row? Or use a 2D grid? Actually, we can restructure the kernel to have multiple blocks per row? But then we have to reduce across blocks.\n\n We can do:\n   Let's set a block size (say 1024). Then we have grid of (M, num_blocks) where num_blocks = ceil(K / BLOCK_SIZE).\n   Each block (i, j) computes a partial dot product for row i and the j-th chunk of the row and vector B.\n   Then we do a second kernel to reduce the partial results.\n\n However, the problem says: \"NEVER change grid configuration, output tensor shapes, or PID logic\". We are allowed to change the grid? Actually, the grid configuration is set by the user in the kernel launch. But note the problem says: \"NEVER change grid configuration\" meaning we cannot change the grid that the user expects? The original grid was not set because it was a torch.matmul.\n\n Since the problem says: \"NEVER modify grid configuration\", we must stick to a 1D grid of M? Then we cannot split the row into multiple blocks? \n\n So we have to do the entire row in one block? But K=131072 is too large to do in one block without multiple passes? Actually, we can have a loop inside the block. We are allowed to have loops.\n\n We'll do:\n   BLOCK_SIZE = 1024 (or 2048, etc.) and then loop over the row in chunks of BLOCK_SIZE.\n\n However, note: the maximum number of threads per block is 1024 (or 2048 on some GPUs, but Triton supports up to 1024 per block? Actually, Triton uses block in a different way: the block is a group of threads that run together, but we can have a 1D block with up to 1024 threads? and then we use a loop to handle the entire row?).\n\n Implementation:\n\n   We will have a kernel that is launched with grid = (M,). Each block has BLOCK_SIZE threads? But note: we are going to process the row in chunks of BLOCK_SIZE. So the number of threads per block can be set to, say, 256 or 512, and then we set BLOCK_SIZE to the number of threads? Actually, we can set the block of threads to be 1D of size BLOCK. Then we have:\n\n   pid = tl.program_id(0)  # row index\n   num_pid_m = tl.num_programs(0)  # should be M\n\n   We want to compute the dot product for row pid: \n        output[pid] = 0\n        for j in range(0, K, BLOCK_SIZE_K):\n            offs_k = j + tl.arange(0, BLOCK_SIZE_K)\n            mask = offs_k < K\n            a_ptrs = a_ptr + pid * stride_am + offs_k\n            b_ptrs = b_ptr + offs_k   # because B is (K,1) so we use offs_k to index the first dimension\n            a = tl.load(a_ptrs, mask=mask, other=0.0)\n            b = tl.load(b_ptrs, mask=mask, other=0.0)\n            product = a * b\n            # Now we need to sum the products in the current chunk? But note: we are in a loop and we have multiple chunks. We can do a local sum and then accumulate.\n\n            # We can use tl.sum to sum the entire chunk? But note: tl.sum is over the entire block and we are in a loop. We want to accumulate the entire row.\n\n            accumulator += tl.sum(product, axis=0)\n\n        tl.store(output_ptr + pid, accumulator)\n\n However, note: the tl.sum function reduces the entire chunk to a scalar. Then we accumulate that scalar in the outer loop.\n\n But note: the vector B is stored as a column vector. We can treat it as a 1D vector. The stride for B is 1? Actually, if B is (K,1) and contiguous, then stride for the first dimension is 1? But note: in PyTorch, a tensor of shape (K,1) is stored as [0,0], [1,0], ... [K-1,0] so the stride for the first dimension is 1? Actually, the stride for the first dimension is 1 and the second dimension is K? But that doesn't make sense. Actually, the memory layout for (K,1) is contiguous: the elements are stored in order. So the element at index (i,0) is at position i.\n\n Therefore, we can set:\n   b_ptr = B.data_ptr() and then we index with offs_k.\n\n But note: the matrix A: it is of shape (M, K). If it is row-major, then the stride for the first dimension is K? Actually, the stride for the first dimension is K (if contiguous) because to get to the next row we skip K elements. So the element at (i,j) is at i * K + j.\n\n We are given:\n   A: [M, K] (row-major contiguous) -> stride_am = K\n   B: [K, 1] (column-major? but actually it's contiguous in the first dimension: the elements are stored in order of the first index) -> so we can consider it as a 1D vector of length K.\n\n Steps for the kernel:\n\n   Let BLOCK_SIZE_K = block size for the inner loop (number of columns we process at a time). We set this as a tunable parameter.\n\n   We'll have:\n      pid = tl.program_id(0)   # row index in [0, M-1]\n      # We are going to loop over the columns in chunks of BLOCK_SIZE_K.\n\n      accumulator = 0.0\n      for j in range(0, K, BLOCK_SIZE_K):\n          # current column index\n          col_offsets = j + tl.arange(0, BLOCK_SIZE_K)\n          mask = col_offsets < K\n\n          # pointers for A and B\n          a_ptr_j = a_ptr + pid * stride_am + col_offsets\n          b_ptr_j = b_ptr + col_offsets   # because B is (K,1) and contiguous, so we can use the same offset\n\n          a_val = tl.load(a_ptr_j, mask=mask, other=0.0)\n          b_val = tl.load(b_ptr_j, mask=mask, other=0.0)\n\n          product = a_val * b_val\n          # sum the products in this chunk\n          chunk_sum = tl.sum(product, axis=0)\n          accumulator += chunk_sum\n\n      # store the result for this row\n      output_ptr_j = output_ptr + pid\n      tl.store(output_ptr_j, accumulator)\n\n However, note: the output is a vector of shape (M,1). We are storing at offset pid.\n\n But what about the data types? We assume A and B are float32? The problem does not specify. But the reference uses torch.matmul which by default for randn is float32.\n\n We must set the kernel to use float32.\n\n But note: the kernel must be generic? Actually, we are going to write the kernel for float32.\n\n Also note: we are using tl.sum which returns a scalar of the same type as the input (float32).\n\n We must also consider the grid and block configuration for the kernel launch.\n\n We are launching M blocks (one per row). The number of threads per block? We are using a 1D block of size BLOCK_SIZE_K? Actually, in Triton, the block of threads is used to parallelize the inner loop. We are processing BLOCK_SIZE_K elements at a time? But note: we are using tl.arange(0, BLOCK_SIZE_K) and that must be the same as the number of threads? Actually, in Triton, the block of threads is independent of the loop chunk size? We can set the block of threads to be 1D of size BLOCK_SIZE_K? But note: the loop is over chunks of size BLOCK_SIZE_K, and we process each chunk with one block? Actually, we are in a block and we are processing one chunk per iteration? And we are looping? And each iteration we use the entire block to load and compute the product? Then we reduce within the chunk? \n\n How do we reduce within the chunk? The tl.sum is a reduction over the entire chunk. This is efficient because it uses the hardware reduction.\n\n However, note: the block of threads in Triton is used to parallelize the chunk? Actually, we are not explicitly using the thread id. We are using the tl.arange to generate the indices and then we do a vector operation. The Triton compiler will map that to the threads.\n\n Therefore, we set the kernel to have a 1D block of size BLOCK_SIZE_K. And we launch grid = (M,). But note: the block size must be a power of two? And we must set BLOCK_SIZE_K to a value that is <= the maximum block size (1024). Also, we must ensure that the loop step is BLOCK_SIZE_K.\n\n We must also consider the maximum number of blocks: M=256 is small, so it's acceptable.\n\n Steps in the Python function:\n\n   We are going to replace the forward function of the model with a call to our Triton kernel.\n\n   We must:\n        - Make sure the tensors are contiguous.\n        - Allocate the output tensor: (M, 1) of type float32.\n        - Define the grid and the kernel.\n\n   We will set the BLOCK_SIZE_K as a tunable parameter? We can set it to 1024? But note K=131072, which is 128 * 1024, so we have 128 iterations per row.\n\n   We can autotune the block size? But the problem says we can do adaptive tuning. However, let's set it to 1024 for now.\n\n   Also note: the problem says we must not change the grid configuration? We are launching grid=(M,) and the original grid was not set? Actually, the original was a single call to torch.matmul. We are changing the grid to M blocks? But the problem says: \"NEVER change grid configuration, output tensor shapes, or PID logic\". We are not changing the output shape. The grid configuration is being set for the kernel, but the original didn't have a grid? We are allowed to write a Triton kernel and set the grid as we wish? The constraint is about the Triton kernel: we cannot change the function signature? We cannot change the grid that the kernel expects? Actually, the constraint says: \"NEVER change grid configuration\" meaning the way the grid is set? But the original program did not have a Triton kernel. So we are introducing one.\n\n   However, the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\" and we are allowed to change the forward function? Actually, the entire block is given and we can change it arbitrarily? But note the output format: we must return the complete updated code containing the EVOLVE-BLOCK comment marker.\n\n   So we are going to rewrite the ModelNew class and the functions? Actually, the entire block is given and we can change it.\n\n   We'll write:\n        class ModelNew(nn.Module):\n            def forward(...):\n                ... call the Triton kernel ...\n\n        And then define the kernel at the top.\n\n   But note: the problem says \"Name the output structure class as ModelNew\", which we are keeping.\n\n Implementation:\n\n   We are going to write:\n        import triton\n        import triton.language as tl\n\n        @triton.jit\n        def _matvec_kernel(...):\n            ... as above ...\n\n        Then in the forward function:\n            # Check contiguous and data types?\n            # A and B are expected to be float32? The reference uses float32.\n\n            M, K = A.shape\n            # Allocate output\n            output = torch.empty(M, 1, device=A.device, dtype=A.dtype)\n            # Launch kernel\n            grid = (M,)\n            BLOCK_SIZE_K = 1024   # tunable? we can set to 1024\n\n            # Note: the kernel uses the stride for A: we can compute it as A.stride(0) if non-contiguous? But we are making contiguous.\n            # We must make sure A is contiguous in row-major? and B is contiguous?\n            A_cont = A.contiguous()\n            B_cont = B.contiguous().view(-1)   # because we want a 1D vector\n\n            _matvec_kernel[grid](A_cont, B_cont, output, \n                                 stride_am = A_cont.stride(0),\n                                 K=K,\n                                 BLOCK_SIZE_K: tl.constexpr)\n\n            return output\n\n   But note: the output is expected to be (M,1). We are storing the result in a tensor of shape (M,1) but in the kernel we are storing at offset pid? So the output tensor should be of shape (M,) or (M,1)? Actually, we are storing at output_ptr + pid. If output is of shape (M,1) and contiguous, then the element at (pid,0) is at offset pid? Actually, the stride for the first dimension is 1? No, because it's (M,1) and contiguous, then the stride for the first dimension is 1? Actually, no: a tensor of shape (M,1) is stored as [0,0], [1,0], ... [M-1,0]. So the element at (pid,0) is at offset pid? Actually, the stride for the first dimension is 1? That would be if it were a row vector? Actually, in PyTorch, a 2D tensor is row-major by default. So the tensor of shape (M,1) is stored as:\n        [0,0], [1,0], ... [M-1,0]\n        and the stride is (1, M) ? That doesn't make sense because the next element in the same row would be at offset M? \n\n   Actually, the stride for a contiguous tensor (M,1) is (1,1) because the next element in the same column is the next in memory? But that would be for column-major. \n\n   Let me check: \n        x = torch.randn(M,1)\n        x.stride() -> (1, M) ? Actually, no: \n        Example: \n            M=3, then the memory: [0,0], [1,0], [2,0] -> so the stride for the first dimension is 1? and the second dimension is 3? because to get to the next column we jump 3? but there is only one column.\n\n   Actually, the stride for (M,1) is (1, M) but the second dimension is not used. We are storing only at (pid,0). So we can set output as a 1D tensor of size M? Then we can reshape to (M,1). The problem: the reference returns (M,1). So we can do:\n\n        output_1d = torch.empty(M, device=A.device, dtype=A.dtype)\n        ... kernel that writes to output_1d ...\n        then output = output_1d.view(-1,1)\n\n   But note: the kernel writes to output_1d: at offset pid we write the accumulator.\n\n   Alternatively, we can set the output tensor to be (M,1) and then in the kernel we do:\n        output_ptr + pid * stride_out\n        and set stride_out = output.stride(0) if we are using the first dimension? \n\n   However, if we make the output contiguous, then the stride for the first dimension would be 1? Actually, for a (M,1) contiguous tensor, the stride is (1, M) but that doesn't make sense because the next row is at +1? Actually, no: the memory layout for a contiguous (M,1) is:\n        [0,0], [1,0], [2,0], ... [M-1,0]\n        so the element at (i,0) is at i.\n\n   Therefore, we can set the output tensor as (M,1) and then in the kernel we use:\n        tl.store(output_ptr + pid, accumulator)\n\n   But wait: if the output tensor is (M,1) and contiguous, then the element at (i,0) is at offset i? Yes, because the tensor is contiguous and the total length is M, and the first dimension has stride 1? Actually, the stride for the first dimension is 1 and the second dimension is M? But M is the first dimension size? Actually, the stride for the first dimension is 1 and the second dimension is 1? That doesn't add up.\n\n   Let me compute: \n        element (i, j) = i * stride[0] + j * stride[1]\n        for j=0, it's i * stride[0]. If stride[0] is 1, then it's i.\n\n   How do we get the stride? \n        output = torch.empty(M,1) -> contiguous, then output.stride(0)=1? and output.stride(1)=M? \n        Actually: \n            >>> a = torch.randn(5,1)\n            >>> a.stride()\n            (1, 5)\n\n        So to access (i,0) we do i * 1 + 0 * 5 = i.\n\n        Therefore, we can set:\n            output_ptr + pid   -> which is the same as pid.\n\n   So we are safe.\n\n   Let's code:\n\n        @triton.jit\n        def _matvec_kernel(\n            a_ptr,\n            b_ptr,\n            output_ptr,\n            stride_am,   # stride in elements for the row of A (should be K)\n            K,           # total columns\n            M,           # total rows? we don't need it because we have pid in [0, M-1] and grid is M\n            BLOCK_SIZE_K: tl.constexpr,\n        ):\n            pid = tl.program_id(0)\n            # Initialize accumulator\n            acc = 0.0\n            # Loop over the columns\n            for j in range(0, K, BLOCK_SIZE_K):\n                # Create offsets for the current block\n                offs_k = j + tl.arange(0, BLOCK_SIZE_K)\n                mask = offs_k < K\n                # Load A: [pid, j:j+BLOCK_SIZE_K]\n                a_ptrs = a_ptr + pid * stride_am + offs_k\n                a_val = tl.load(a_ptrs, mask=mask, other=0.0)\n                # Load B: [j:j+BLOCK_SIZE_K] (as a 1D vector)\n                b_ptrs = b_ptr + offs_k\n                b_val = tl.load(b_ptrs, mask=mask, other=0.0)\n                # Compute product and accumulate\n                product = a_val * b_val\n                # Sum the product in this block\n                block_sum = tl.sum(product, axis=0)\n                acc += block_sum\n            # Store the result at output[pid,0]\n            tl.store(output_ptr + pid, acc)\n\n   In the forward function:\n\n        def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n            # Check shapes: A: [M, K], B: [K,1] -> we will reshape B to [K]\n            M, K = A.shape\n            # Check B: must be [K,1] -> then we can reshape to [K]\n            assert B.shape == (K,1) or B.shape == (K,), f\"B must be of shape ({K},) or ({K},1), but got {B.shape}\"\n            B_vec = B.reshape(-1)   # makes it 1D\n\n            # Allocate output: [M,1] but we will fill the first dimension and then reshape?\n            output = torch.empty(M, 1, device=A.device, dtype=A.dtype)\n\n            # We need to make sure A and B_vec are contiguous\n            A_cont = A.contiguous()\n            B_cont = B_vec.contiguous()\n\n            # Launch kernel\n            grid = (M,)\n            # We set BLOCK_SIZE_K to 1024 (or 2048? but 1024 is the maximum number of threads per block? and we are using 1D block of BLOCK_SIZE_K threads)\n            BLOCK_SIZE_K = 1024\n\n            # Note: the kernel requires the stride for A: which is the stride in elements for the first dimension (row stride)\n            stride_am = A_cont.stride(0)   # in number of elements? yes, because A_cont is float32 and stride is in elements.\n\n            _matvec_kernel[grid](\n                A_cont, B_cont, output,\n                stride_am, K, M,\n                BLOCK_SIZE_K=BLOCK_SIZE_K\n            )\n\n            return output\n\n   But note: the kernel signature: we are passing M but not using it? We can remove M.\n\n   Also note: the kernel must be compiled for BLOCK_SIZE_K. We set it as a constant.\n\n   However, we must handle the case when K is not a multiple of BLOCK_SIZE_K? The loop and mask take care of that.\n\n   We are using tl.sum which is efficient? Yes.\n\n   But note: the entire vector B is read by each block? That means we are reading B M times (256 times) from global memory. This is 256 * 128KB (if K=131072, 4 bytes per float -> 512KB per vector) = 128 MB. The H100 has high bandwidth, but we can try to cache? \n\n   However, the problem says: we are allowed to use shared memory? But note: the entire vector B is 512KB and shared memory per block is only 227KB? So we cannot cache the entire B in shared memory per block.\n\n   Alternatively, we could use a different kernel that caches chunks of B in shared memory and then uses multiple blocks? But we are constrained to a grid of M blocks? Then we cannot.\n\n   We must hope that the L2 cache will capture the repeated reads? Because each block will read the entire vector B, and the blocks are executed in sequence? Actually, the blocks are executed in parallel? The GPU has multiple SMs and blocks are distributed. So the same B might be read by multiple blocks at the same time? And if the entire B does not fit in L2, then we get a lot of global memory traffic.\n\n   We can try to use the L2 cache by making the access to B contiguous? Actually, the access to B is contiguous: we are reading contiguous chunks. And we are reading the entire vector in order? So it should be efficient? And because we are reading the same B for every row, the L2 cache might help? But the entire B is 512KB and the L2 cache is 50MB? So it might stay in cache? \n\n   However, 256 blocks each reading 512KB of B would be 256 * 512KB = 128 MB. The L2 cache is 50MB? So it might not fit? But note: the blocks are executed in waves? And the L2 cache might hold a portion of B that is reused by multiple blocks? Actually, the blocks are independent and there is no guarantee of reuse.\n\n   We might need to restructure the kernel to use a different grid? But the problem says: \"NEVER change grid configuration, output tensor shapes, or PID logic\". We are using the grid of M blocks? And we are not changing the output shape. So we are constrained.\n\n   We can try to use a 2D grid? But the constraint says we cannot change the grid? Actually, the constraint says \"NEVER change grid configuration\" meaning we cannot change the grid that the kernel expects? We are writing the kernel so we can design it as we wish? But the problem says: \"NEVER change grid configuration\" in the constraints? The exact constraint is: \"NEVER change grid configuration, output tensor shapes, or PID logic\". We are setting the grid to (M,) and the PID logic is that each block handles one row. We are not changing that.\n\n   However, the problem also says: \"NEVER modify grid configuration\" in the context of the Triton kernel? We are setting it to (M,). So that's acceptable.\n\n   We'll proceed.\n\n   Let's write the complete code.\n\n   We'll also note: we are using the same class and functions.\n\n   We must set M and K as before? They are set as global variables? Actually, in the original code, M and K are set as global variables? And get_inputs uses them. We are not changing that.\n\n   We'll write the kernel and the forward function.\n\n   We'll also note: the kernel might be slow because of the repeated reads of B. But we hope that the L2 cache will help? And we are using a large block size to get contiguous reads.\n\n   We are using BLOCK_SIZE_K=1024? That means we are reading 1024 elements at a time? 4KB per block per iteration? And we have 128 iterations per block? And 256 blocks? The total global memory read for B is 256 blocks * 128 iterations * 1024 elements * 4 bytes = 128 MB? Plus A: 256*131072*4 = 128 MB? Total 256 MB? The H100 has 3 TB/s bandwidth -> 256 MB in 0.085 ms? But we are doing computation? And the kernel has to do the loop and the reductions? \n\n   We'll try to tune the block size? But for now, we set 1024.\n\n   We can also try to use vectorized loads? The problem says we can do vectorized tl.load/tl.store. We are already doing contiguous loads? The tl.load for a contiguous block is vectorized.\n\n   We'll write the code and hope for a speedup over the reference? The reference is torch.matmul. The torch.matmul for matrix-vector is highly optimized? It uses cuBLAS? So we might not beat it? But we are using a simple kernel? \n\n   However, note: the current reference timing is 0.0584 ms? And our kernel? We are doing 256 blocks, each doing 128 iterations? And each iteration we do a load of A and B, a multiplication, a reduction? The reduction might be expensive? \n\n   We can try to optimize the reduction by using a tree reduction? But the tl.sum already does that? \n\n   Alternatively, we can do the reduction within the block without tl.sum? We could do:\n\n        acc_chunk = tl.zeros((BLOCK_SIZE_K,), dtype=tl.float32)\n        ... then we do:\n            a_val = tl.load(...)\n            b_val = tl.load(...)\n            acc_chunk += a_val * b_val\n        then at the end of the chunk, we do a reduction of acc_chunk to a scalar? and then add to the global accumulator? \n\n   But that would require more registers? And the tl.sum is efficient? So we stick to tl.sum.\n\n   We'll write the kernel as above.\n\n   Let's code.\n\n   Note: we are adding the Triton imports? They must be inside the block.\n\n   Also note: the original block had M and K as global variables? We are not changing that.\n\n   We are also keeping the get_inputs and get_init_inputs.\n\n   We are replacing the forward function and adding the kernel.\n\n   We must be cautious: the kernel might be slower? But we are asked to rewrite for Triton.\n\n   We are also going to autotune the block size? The problem says we can do adaptive tuning? But for now, we set 1024.\n\n   We can make BLOCK_SIZE_K a triton.autotune parameter? But the problem says: \"small iterative steps\". We can do that in a future step.\n\n   We'll write:\n\n        import math\n        import triton\n        import triton.language as tl\n\n        @triton.jit\n        def _matvec_kernel(\n            a_ptr,\n            b_ptr,\n            output_ptr,\n            stride_am,\n            K,\n            BLOCK_SIZE_K: tl.constexpr,\n        ):\n            pid = tl.program_id(0)   # row index\n            # Initialize accumulator\n            acc = 0.0\n            # Loop over the columns\n            for j in range(0, K, BLOCK_SIZE_K):\n                offs_k = j + tl.arange(0, BLOCK_SIZE_K)\n                mask = offs_k < K\n                a_ptrs = a_ptr + pid * stride_am + offs_k\n                a_val = tl.load(a_ptrs, mask=mask, other=0.0)\n                b_ptrs = b_ptr + offs_k\n                b_val = tl.load(b_ptrs, mask=mask, other=0.0)\n                product = a_val * b_val\n                block_sum = tl.sum(product, axis=0)\n                acc += block_sum\n            tl.store(output_ptr + pid, acc)\n\n   Then in the forward function:\n\n        def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n            M, K = A.shape\n            # We require B to be (K,1) or (K,)\n            assert B.shape == (K,1) or B.shape == (K,), f\"B must be of shape ({K},) or ({K},1), but got {B.shape}\"\n            B_vec = B.reshape(-1)\n            output = torch.empty(M, 1, device=A.device, dtype=A.dtype)\n            A_cont = A.contiguous()\n            B_cont = B_vec.contiguous()\n            grid = (M,)\n            # We choose a block size that is a power of two and at most 1024\n            BLOCK_SIZE_K = 1024\n            # If K is less than 1024, we can set BLOCK_SIZE_K to the next power of two? But we can just set to min(1024, K) but then we don't need to change the kernel? The kernel handles masking.\n            # However, we set to 1024 and if K is small we still use 1024? It's okay.\n\n            # Note: the stride_am is the number of elements between rows? For A_cont, stride(0) is K? Because the next row starts at +K.\n            stride_am = A_cont.stride(0)\n\n            _matvec_kernel[grid](A_cont, B_cont, output, stride_am, K, BLOCK_SIZE_K=BLOCK_SIZE_K)\n\n            return output\n\n   But note: the output is (M,1) and the kernel writes to the first M elements? That's correct.\n\n   We must test? But we are in an evolution system.\n\n   We'll output the code.\n\n   IMPORTANT: We are replacing the entire block? We must keep the M and K and get_inputs? Yes.\n\n   We'll write:\n\n        import math\n        import torch\n        import torch.nn as nn\n        import triton\n        import triton.language as tl\n\n        # Triton kernel for matrix-vector multiplication\n        @triton.jit\n        def _matvec_kernel(\n            a_ptr,\n            b_ptr,\n            output_ptr,\n            stride_am,\n            K,\n            BLOCK_SIZE_K: tl.constexpr,\n        ):\n            pid = tl.program_id(0)   # row index\n            acc = tl.zeros(tl.float32, (1,))\n            # Loop over the columns in chunks of BLOCK_SIZE_K\n            for j in range(0, K, BLOCK_SIZE_K):\n                offs_k = j + tl.arange(0, BLOCK_SIZE_K)\n                mask = offs_k < K\n                a_ptrs = a_ptr + pid * stride_am + offs_k\n                a_val = tl.load(a_ptrs, mask=mask, other=0.0)\n                b_ptrs = b_ptr + offs_k\n                b_val = tl.load(b_ptrs, mask=mask, other=0.0)\n                product = a_val * b_val\n                block_sum = tl.sum(product, axis=0)\n                acc += block_sum\n            tl.store(output_ptr + pid, acc)\n\n        class ModelNew(nn.Module):\n            def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n                M, K = A.shape\n                # Check and reshape B\n                assert B.shape == (K,1) or B.shape == (K,), f\"B must be of shape ({K},) or ({K},1), but got {B.shape}\"\n                B_vec = B.reshape(-1)\n                output = torch.empty(M, 1, device=A.device, dtype=A.dtype)\n                A_cont = A.contiguous()\n                B_cont = B_vec.contiguous()\n                grid = (M,)\n                BLOCK_SIZE_K = 1024\n                stride_am = A_cont.stride(0)\n                _matvec_kernel[grid](A_cont, B_cont, output, stride_am, K, BLOCK_SIZE_K=BLOCK_SIZE_K)\n                return output\n\n        M = 256\n        K = 131072\n\n        def get_inputs():\n            A = torch.randn(M, K)\n            B = torch.randn(K, 1)\n            return [A, B]\n\n        def get_init_inputs():\n            return []\n\n   This is our new code.\n\n   We hope it compiles and runs and gives a speedup.\n\n   Note: the original reference timing was 0.0584 ms? And we are doing 256 blocks? The GPU has many SMs? So it might be fast.\n\n   Let's output.\n</think>\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef matvec_kernel(\n    a_ptr,\n    b_ptr,\n    output_ptr,\n    stride_am,\n    K,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    accumulator = tl.zeros(tl.float32, (1,))\n    for j in range(0, K, BLOCK_SIZE_K):\n        offs_k = j + tl.arange(0, BLOCK_SIZE_K)\n        mask = offs_k < K\n        a_ptrs = a_ptr + pid * stride_am + offs_k\n        b_ptrs = b_ptr + offs_k\n        a_val = tl.load(a_ptrs, mask=mask, other=0.0)\n        b_val = tl.load(b_ptrs, mask=mask, other=0.0)\n        product = a_val * b_val\n        block_sum = tl.sum(product, axis=0)\n        accumulator += block_sum\n    tl.store(output_ptr + pid, accumulator)\n\nclass ModelNew(nn.Module):\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        M, K = A.shape\n        B_vec = B.reshape(-1)\n        output = torch.empty(M, 1, device=A.device, dtype=A.dtype)\n        A_cont = A.contiguous()\n        B_cont = B_vec.contiguous()\n        grid = (M,)\n        BLOCK_SIZE_K = 1024\n        stride_am = A_cont.stride(0)\n        matvec_kernel[grid](A_cont, B_cont, output, stride_am, K, BLOCK_SIZE_K=BLOCK_SIZE_K)\n        return output\n\nM = 256\nK = 131072\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
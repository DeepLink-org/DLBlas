{"id": "4ed174f3-71e9-4f48-bad9-92c9aa796c41", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _matvec_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, K,\n    stride_am, stride_ak,\n    stride_bk, stride_b1,\n    stride_cm, stride_c1,\n    BLOCK_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    offs_m = pid\n    c_ptr = C_ptr + offs_m * stride_cm\n\n    accumulator = 0.0\n    for k in range(0, K, BLOCK_K):\n        offs_k = k + tl.arange(0, BLOCK_K)\n        mask = offs_k < K\n\n        a_ptr = A_ptr + offs_m * stride_am + offs_k * stride_ak\n        a_vals = tl.load(a_ptr, mask=mask, other=0.0)\n\n        b_ptr = B_ptr + offs_k * stride_bk\n        b_vals = tl.load(b_ptr, mask=mask, other=0.0)\n\n        product = a_vals * b_vals\n        accumulator += tl.sum(product, axis=0)\n\n    tl.store(c_ptr, accumulator)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        M, K = A.shape\n        assert B.shape == (K, 1), f\"B must be of shape ({K}, 1)\"\n        C = torch.empty((M, 1), device=A.device, dtype=A.dtype)\n        \n        if M == 0:\n            return C\n\n        BLOCK_K = 4096\n        grid = (M,)\n        stride_am, stride_ak = A.stride()\n        stride_bk, stride_b1 = B.stride()\n        stride_cm, stride_c1 = C.stride()\n\n        _matvec_kernel[grid](\n            A, B, C,\n            M, K,\n            stride_am, stride_ak,\n            stride_bk, stride_b1,\n            stride_cm, stride_c1,\n            BLOCK_K=BLOCK_K\n        )\n        return C\n\nM = 256\nK = 131072\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "f6d5fd6e-05df-4c54-b792-d42f6dd80982", "generation": 3, "timestamp": 1754640751.893094, "iteration_found": 17, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.471, "runtime_ms": 0.124, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0584, "speed_up": 0.471, "custom_timing": 0.124}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "", "exec_log": "[Child] \u7ed1\u5b9a GPU 4, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 7, in <module>\nTypeError: jit() got an unexpected keyword argument 'num_warps'\n", "exit_code": "1"}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
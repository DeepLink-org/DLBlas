{"id": "f6d5fd6e-05df-4c54-b792-d42f6dd80982", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit(num_warps=4, num_stages=4)\ndef _matvec_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, K,\n    stride_am, stride_ak,\n    stride_bk, stride_b1,\n    stride_cm, stride_c1,\n    BLOCK_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    if pid >= M:\n        return\n\n    offs_m = pid\n    c_ptr = C_ptr + offs_m * stride_cm\n\n    accumulator = 0.0\n    VEC_SIZE = 4\n    for k in range(0, K, BLOCK_K):\n        for i in range(0, BLOCK_K, VEC_SIZE):\n            offs = k + i\n            vec_offsets = tl.arange(0, VEC_SIZE)\n            offs_vec = offs + vec_offsets\n            vec_mask = offs_vec < K\n\n            a_ptrs = A_ptr + offs_m * stride_am + offs_vec * stride_ak\n            a_vals = tl.load(a_ptrs, mask=vec_mask, other=0.0)\n\n            b_ptrs = B_ptr + offs_vec * stride_bk\n            b_vals = tl.load(b_ptrs, mask=vec_mask, other=0.0)\n\n            product = a_vals * b_vals\n            accumulator += tl.sum(product, axis=0)\n\n    tl.store(c_ptr, accumulator)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        M, K = A.shape\n        assert B.shape == (K, 1), f\"B must be of shape ({K}, 1)\"\n        C = torch.empty((M, 1), device=A.device, dtype=A.dtype)\n        \n        if M == 0:\n            return C\n\n        BLOCK_K = 256\n        grid = (M,)\n        stride_am, stride_ak = A.stride()\n        stride_bk, stride_b1 = B.stride()\n        stride_cm, stride_c1 = C.stride()\n\n        _matvec_kernel[grid](\n            A, B, C,\n            M, K,\n            stride_am, stride_ak,\n            stride_bk, stride_b1,\n            stride_cm, stride_c1,\n            BLOCK_K=BLOCK_K\n        )\n        return C\n\nM = 256\nK = 131072\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "b7e606d0-573c-4a2d-a6d8-247e209c5c79", "generation": 2, "timestamp": 1754639672.710503, "iteration_found": 7, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "", "exec_log": "[Child] \u7ed1\u5b9a GPU 4, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 7, in <module>\nTypeError: jit() got an unexpected keyword argument 'num_warps'\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.463, "runtime_ms": 0.126, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0584, "speed_up": 0.463, "custom_timing": 0.126}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}
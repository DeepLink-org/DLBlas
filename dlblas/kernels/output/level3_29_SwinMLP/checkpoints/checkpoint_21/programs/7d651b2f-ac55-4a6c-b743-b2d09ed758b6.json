{"id": "7d651b2f-ac55-4a6c-b743-b2d09ed758b6", "code": "import math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"BLOCK_SIZE\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask, other=0.0)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    max_val = tl.maximum(max_val, 1e-4)\n    scale = 127.0 / max_val\n    scaled = x * scale\n    clipped = tl.minimum(tl.maximum(scaled, -127.0), 127.0)\n    integer_value = libdevice.llrint(clipped)\n    tl.store(output_ptr + offsets, integer_value.to(tl.int8), mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127.0)\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    row_length = x.shape[1]\n    P2_val = 1 << (row_length - 1).bit_length()\n    grid = (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=row_length, P2=P2_val)\n    return output, output_scale", "language": "python", "parent_id": "8802fe55-ac8c-4284-ba38-79143d61cac4", "generation": 3, "timestamp": 1754595962.3737838, "iteration_found": 10, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.95, "runtime_ms": 4.81, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:0", "correctness_trials": "(5 / 5)", "reference_timing_ms": 4.57, "speed_up": 0.95, "custom_timing": 4.81}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": false, "correctness": false, "score": -100.0, "stage": "no_valid_triton_code", "feedback": "You need to generate real and valid triton code for the core function. The key word 'import triton' is not found."}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 0.0000\n- correctness: 0.0000\n- score: -100.0000\n- stage: no_valid_triton_code\n- feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Areas identified for improvement: - Focus on optimizing the code for better performance on the target metrics\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 0.0000, correctness: 0.0000, score: -100.0000, stage: no_valid_triton_code, feedback: You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: -100.0000)\n```python\n@triton.jit\ndef _quantize_rowwise(\n    ...\n```\nKey features: Performs well on compiled (0.0000), Performs well on correctness (0.0000), Performs well on score (-100.0000), Performs well on stage (no_valid_triton_code), Performs well on feedback (You need to generate real and valid triton code for the core function. The key word 'import triton' is not found.)\n\n\n\n\n# Current Program\n```python\n@triton.jit\ndef _quantize_rowwise(\n    ...\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a task to rewrite the Triton kernel for per_row_quantize_int8.\n The previous attempt failed because the code did not import triton and had other issues.\n\n Steps to fix:\n 1. We must include the necessary imports: triton, triton.language, and math.\n 2. We note that the function signature of the Triton kernel must remain the same.\n 3. We must ensure the kernel is launched with the correct grid and block configuration.\n\n However, note that the example provided in the problem statement has a Triton implementation that we can use as a base.\n\n Let's break down the example:\n   - The kernel uses autotune with several configurations.\n   - The kernel function `_quantize_rowwise` takes pointers and two constants (BLOCK_SIZE and P2).\n   - The kernel computes the absolute value, then the max per row, then scales and rounds to int8.\n\n But note: the example kernel uses `libdevice.llrint` for rounding, but we must be cautious because the example also stores the scale per row.\n\n In the current task, the feedback says: \"The key word 'import triton' is not found.\"\n\n Therefore, we must include the import statements.\n\n Additionally, the example uses a power-of-two block size (P2) to avoid bank conflicts and to allow efficient reduction.\n\n We also note that the original PyTorch function returns:\n   output: the quantized tensor (int8)\n   output_scale: the scale per row (float32) which is max_val / 127\n\n However, in the example, the kernel writes the output_scale at the position `pid` (one per row).\n\n We must design the kernel to handle any row length. The example uses a variable `P2` which is the next power of two of the row length.\n\n How the example works:\n   - The grid is set to the number of rows (x.shape[0]).\n   - Each program (block) handles one row.\n   - The block loads a row of length `x.shape[1]` but uses a mask for the actual data (to avoid out-of-bound).\n   - Then it computes the max absolute value in the row (using a reduction).\n   - Then it scales the row by (127 / max_val) and rounds to the nearest integer (using llrint).\n   - Then it stores the quantized row and the scale (max_val/127) for that row.\n\n However, note that the example uses `tl.where(row_mask, abs_x, 0)` to set the out-of-bound elements to 0 so they don't affect the max.\n\n But there is a problem: what if the entire row is zeros? Then max_val would be 0, and we would have division by zero. The original PyTorch code uses `.clamp(min=1e-4)`.\n\n We must replicate that. So in the kernel, after computing `max_val`, we clamp it to a minimum of 1e-4.\n\n Also note: the example uses `libdevice.llrint` but we are rounding to the nearest integer. Alternatively, we can use `tl.round` and then convert to int8.\n\n However, note that the example stores the result as `output` which is then stored as int8. The `llrint` returns an integer, but we must ensure it is stored as int8.\n\n Let's adjust the example to include the clamp and also use a safe division.\n\n Steps in the kernel:\n  1. Load the row with mask.\n  2. Compute absolute value.\n  3. Compute max_val = max(absolute values in the row, but set out-of-bound to 0) and clamp to 1e-4.\n  4. Scale the original x (not the absolute) by (127.0 / max_val).\n  5. Round and clamp to the range [-127, 127] (note: the example only rounds and then clamps? Actually the example does: \n        output = libdevice.llrint(127.0 * (x / max_val))\n     This is equivalent to: round(127 * (x / max_val)) but without clamping? That might cause overflow.\n\n So we must clamp the scaled value to [-127, 127] before rounding? Actually, the original PyTorch does:\n        output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n\n Therefore, we should do:\n        scaled = x * (127.0 / max_val)\n        clipped = tl.minimum(tl.maximum(scaled, -127.0), 127.0)\n        rounded = tl.round(clipped)\n        and then convert to int8.\n\n However, note: the `tl.round` returns a float. We can then convert to int8 by casting? But Triton does not have a direct int8 type in the language? Actually, we can use `tl.int8`?\n\n Alternatively, we can use `tl.where` to avoid branching and then use `tl.store` with the int8 pointer.\n\n But note: the example uses `libdevice.llrint` which returns an integer. We can do the same and then clamp? Actually, the clamping must be done before rounding to avoid out-of-range.\n\n Let me reexamine: if we have a value that is 128.3, then rounding would give 128 which is out of range for int8 (which is -127 to 127). So we must clamp before rounding.\n\n However, note that the example does not clamp? That might be a bug.\n\n So we will do:\n   scaled = x * (127.0 / max_val)\n   clipped = tl.minimum(tl.maximum(scaled, -127.0), 127.0)\n   rounded = tl.round(clipped)\n   then convert to int8? But the rounded value is float. We can use `tl.int8(rounded)`? But Triton does not support casting to int8? Actually, we can use `rounded.to(tl.int8)`? \n\n Alternatively, we can use `tl.store(..., rounded.to(tl.int8))`? But note: the output_ptr is of type int8.\n\n However, the Triton documentation says: when storing, the value is converted to the type of the pointer.\n\n So we can just store the float rounded value to an int8 pointer? It will be converted. But we must be cautious: it might saturate? \n\n Actually, the Triton language does not define the behavior of storing a float to an int8 pointer. We should cast the value to int8 explicitly.\n\n We can do: `tl.store(output_ptr + offsets, rounded.to(tl.int8), mask=row_mask)`\n\n But note: the rounding mode of `tl.round` is to the nearest even? That is acceptable.\n\n Alternatively, we can use the `libdevice.llrint` which returns an integer in the same type as the input? Actually, it returns a long integer. Then we can cast to int8.\n\n However, the problem is that we must clamp to avoid values beyond [-127,127] because if we have a value that is 128, then storing 128 to int8 becomes -128? That would be an error.\n\n So we must clamp the scaled value to [-127, 127] before rounding and converting.\n\n Therefore, we change the example to:\n\n   scaled = x * (127.0 / max_val)\n   clipped = tl.minimum(127.0, tl.maximum(scaled, -127.0))\n   rounded = tl.math.llrint(clipped)   # This returns an int64? But we want int8.\n\n   However, note: we are storing to an int8 tensor. We can do:\n\n        int8_value = rounded.astype(tl.int8)\n\n   But that might truncate the 64-bit integer? We don't need the high bits.\n\n   Alternatively, we can use `tl.where` to avoid the intermediate float? Actually, we can just do:\n\n        clipped = tl.minimum(127.0, tl.maximum(scaled, -127.0))\n        # Then round to integer and convert to int8.\n\n   But note: the rounding function in Triton (llrint) returns an int64. We can cast to int8 with:\n\n        int8_value = (rounded & 0xFF).to(tl.int8)   # This is not safe because negative?\n\n   Alternatively, we can avoid the rounding function and use `tl.round` and then cast to int8? The cast will convert the float to int8 by truncation? But we want rounding.\n\n   Actually, the Triton language does not have a direct rounding to int8. We can do:\n\n        rounded = tl.round(clipped).to(tl.int8)\n\n   But note: the rounding mode of `tl.round` is to the nearest even, and then converting to int8 will take the low 8 bits? But that might not be the same as the PyTorch round then clamp.\n\n   However, the PyTorch does: \n        (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n\n   So the rounding is done in float, then clamped, then converted to int8. The conversion to int8 in PyTorch is by truncating the float to an integer and then clamping? Actually, the `.to(torch.int8)` does a conversion that is equivalent to casting? But note: the rounding is already done and clamped to integers in the range [-127,127] so the conversion is safe.\n\n   Therefore, in Triton, we can do:\n\n        rounded = tl.round(clipped)\n        as_int = rounded.to(tl.int8)   # This is safe because rounded is between -127 and 127.\n\n   However, note: the rounded value might be 127.5? Then rounding to even would become 128? But we have already clipped to 127.0, so the maximum value is 127.0. Therefore, the rounded value will be at most 127.0 and at least -127.0, and they are integers? Actually, no: because of the clip, the value is between -127 and 127, but the rounding might produce non-integer? Wait, no: the clip is done on the scaled value, but then we round to the nearest integer. So the rounded value is an integer? But stored as float. Then converting to int8 is safe.\n\n   But note: the rounded value is a float that represents an integer. When we convert to int8, it will truncate? Or round? The Triton documentation says: when converting float to integer, it truncates. So we must use `tl.round` and then we can convert by truncation? But that would be wrong because 0.5 would become 0? \n\n   Actually, the `tl.round` rounds to the nearest even. Then we want to store that integer. We can use:\n\n        as_int = rounded.to(tl.int8, bitcast=False)   # This will convert by rounding to the nearest integer? Actually, no: the conversion from float to integer in Triton is truncation? \n\n   We must check: the Triton language does not specify. But we have already rounded to the nearest integer, so the fractional part is zero? Not exactly: because of rounding, the value is an integer but represented as a float. Then when we convert to int8, we are essentially converting an integer in float form to an integer. We can do:\n\n        as_int = tl.math.llrint(rounded).to(tl.int8)\n\n   But that is inefficient.\n\n   Alternatively, we can use the `tl.store` to an int8 pointer: it will convert the float to int8 by truncation? We want rounding to the nearest integer? But we already rounded to the nearest integer, so the fractional part is zero? Actually, no: because the rounding might have produced a non-integer? Wait, we did:\n\n        rounded = tl.round(clipped)\n\n   This produces a float that is the rounded integer. Then we want to store that integer as int8. We can use:\n\n        tl.store(output_ptr + offsets, rounded, mask=row_mask)\n\n   But the store to an int8 pointer will convert the float to int8. How? The Triton documentation doesn't specify. We must assume it truncates? That would be problematic.\n\n   Instead, we can do:\n\n        integer_value = rounded.to(tl.int32).to(tl.int8)   # But this is two steps and might be inefficient.\n\n   Alternatively, we can avoid the rounding and use:\n\n        integer_value = (tl.sign(scaled) * tl.minimum(127.0, tl.abs(scaled) + 0.5)).to(tl.int8)\n\n   But that is not standard.\n\n   Given the complexity, let's stick to the example's approach: using `libdevice.llrint` but with clamping.\n\n   We can do:\n\n        scaled = 127.0 * (x / max_val)   # same as x * (127.0 / max_val)\n        # Clamp the scaled value to [-127, 127] to avoid overflow in rounding\n        clamped = tl.minimum(tl.maximum(scaled, -127.0), 127.0)\n        # Now round to the nearest integer using llrint, which returns an int64\n        integer_value = libdevice.llrint(clamped)\n        # Now we can convert to int8 by casting? But note: integer_value is int64. We want to store the low 8 bits? But we know the value is between -127 and 127, so we can cast to int8 without loss.\n        # However, Triton doesn't have a direct cast from int64 to int8? We can do:\n        #   as_int8 = integer_value.to(tl.int8)\n        # But note: the Triton language does not support casting between integer types? Actually, it does.\n\n   But let me check: the `llrint` returns a 64-bit integer. We are storing to an int8 pointer. We can store the integer_value directly? The store will truncate the 64-bit integer to 8 bits? That is acceptable because we clamped.\n\n   So:\n\n        tl.store(output_ptr + offsets, integer_value, mask=row_mask)\n\n   But note: the Triton store operation for an int64 to an int8 pointer will truncate. That is what we want.\n\n   Alternatively, we can use:\n\n        tl.store(output_ptr + offsets, integer_value.to(tl.int8), mask=row_mask)\n\n   But the type of integer_value is int64, and we are converting to int8. That is safe.\n\n   However, the example did not clamp and that might cause overflow. We are adding the clamp.\n\n   Therefore, the kernel code:\n\n        abs_x = tl.abs(x)\n        max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n        max_val = tl.maximum(max_val, 1e-4)   # clamp to avoid division by zero\n        scaled = x * (127.0 / max_val)\n        clamped = tl.minimum(127.0, tl.maximum(scaled, -127.0))\n        integer_value = libdevice.llrint(clamped)\n        tl.store(output_ptr + offsets, integer_value.to(tl.int8), mask=row_mask)\n        tl.store(output_scale + pid, max_val / 127.0)\n\n   But note: the output_scale is stored as max_val / 127.0, which is the same as the original PyTorch.\n\n   However, the original PyTorch stored `x_amax / 127` and `x_amax` is the max_val. So we are consistent.\n\n   Also note: the example used `libdevice.llrint(127.0 * (x / max_val))` without clamping? That might have been a bug.\n\n   We are fixing that by clamping.\n\n   Now, let's code accordingly.\n\n   Also note: the reduction for max_val must be done carefully. We are using `tl.max` with the row_mask.\n\n   We are using:\n        max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n\n   But what if the row has negative zeros? It doesn't matter. Also, what if the row has NaN? We don't handle that.\n\n   We assume the input does not have NaN.\n\n   We also clamp the max_val to 1e-4.\n\n   We can do the clamp after the reduction.\n\n   Now, let's write the complete kernel.\n\n   However, note: the example used a variable `P2` which is the next power of two of the row length. This is used to set the block size for the reduction. The kernel uses `P2` as the block size? Actually, the kernel uses `BLOCK_SIZE` as the actual row length and `P2` as the power-of-two block size for the reduction.\n\n   We are using a reduction across the row. The reduction algorithm in Triton for a row of arbitrary length requires that we use a power-of-two block size? Yes, to avoid bank conflicts and to allow tree reduction.\n\n   How the reduction works in the example?\n        It sets the block size to P2 (the next power of two) and then masks out the extra elements.\n\n   We are doing the same.\n\n   Therefore, we keep the same structure.\n\n   We also note that the grid is set to the number of rows.\n\n   The function `per_row_quantize_int8` in the host code:\n        - Makes the input contiguous.\n        - Allocates output and output_scale tensors.\n        - Computes P2 as the next power of two of the row length (x.shape[1]).\n        - Defines the grid as (x.shape[0],).\n        - Launches the kernel.\n\n   We will do the same.\n\n   But note: the kernel uses `BLOCK_SIZE` as the actual row length? Actually, the kernel uses `BLOCK_SIZE` to compute the mask? And the reduction uses `P2` as the block size for the reduction? Actually, the kernel uses `P2` to set the arange for the block.\n\n   The kernel code:\n\n        pid = tl.program_id(0)\n        block_start = pid * BLOCK_SIZE\n        arange = tl.arange(0, P2)\n        offsets = block_start + arange\n        row_mask = arange < BLOCK_SIZE   # because BLOCK_SIZE is the actual row length\n\n   So we are using `BLOCK_SIZE` as the row length and `P2` as the next power of two (>= row length).\n\n   This is acceptable.\n\n   Now, let's write the code accordingly.\n\n   We must include the import for libdevice? The example did: `from triton.language.extra import libdevice`\n\n   Alternatively, we can use `tl.math` for rounding? But `llrint` is in libdevice.\n\n   We'll use libdevice.\n\n   Let's code accordingly.\n\n   We'll also autotune the kernel? The example did. We can use the same autotune configurations.\n\n   However, note: the kernel is memory bound? And the reduction might be the bottleneck. The autotune might help.\n\n   But the problem says: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\"\n\n   We are allowed to change the kernel and the host function? Actually, the entire code must be rewritten? But the problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\n   However, the example provided a complete function. We are going to write the entire function.\n\n   But note: the previous program had a kernel and a host function. We are going to write both.\n\n   However, the problem says: the EVOLVE-BLOCK. We are going to put the entire code.\n\n   But wait: the problem says \"Name the output structure class as ModelNew.\"? Actually, in this example, there is no class. The function is named `per_row_quantize_int8`.\n\n   We are going to write the function.\n\n   Since the problem says: \"Provide the complete new program code\", we write:\n\n        import math\n        import torch\n        import triton\n        import triton.language as tl\n        from triton.language.extra import libdevice\n\n        ... kernel and host function ...\n\n   We must be cautious: the Triton version might matter. We assume the latest.\n\n   Now, let's code.\n\n   We'll write the kernel with the clamping of max_val and the clamping of the scaled value.\n\n   We also note: the reduction for max_val: we are using `tl.max` with an axis. This is a reduction operation that Triton handles.\n\n   We are done.\n\n   Let's write the code.\n\n   Note: the example used `tl.where(row_mask, abs_x, 0)` to set the out-of-bound to 0. Why 0? Because we are taking the max. If we set to a very small number, then the max would ignore them. But 0 is safe because the absolute value is non-negative. And if the entire row is masked (which shouldn't happen because row_mask is at least the first BLOCK_SIZE elements) then the max would be 0? Then we clamp to 1e-4. So that row would be clamped to 1e-4? Then the scaling becomes 127/(1e-4) which is very large? But that row is all zeros? Then x is zero? Then scaled becomes 0? So it's safe.\n\n   Therefore, we keep the 0.\n\n   Code:\n\n        import math\n        import torch\n        import triton\n        import triton.language as tl\n        from triton.language.extra import libdevice\n\n        @triton.autotune(\n            configs=[\n                triton.Config({}, num_stages=1, num_warps=8),\n                triton.Config({}, num_stages=2, num_warps=8),\n                triton.Config({}, num_stages=4, num_warps=8),\n                triton.Config({}, num_stages=8, num_warps=8),\n                triton.Config({}, num_stages=1),\n                triton.Config({}, num_stages=2),\n                triton.Config({}, num_stages=4),\n                triton.Config({}, num_stages=8),\n                triton.Config({}, num_warps=1),\n                triton.Config({}, num_warps=2),\n                triton.Config({}, num_warps=4),\n                triton.Config({}, num_warps=8),\n            ],\n            key=[\"n_elements\"],\n        )\n        @triton.jit\n        def _quantize_rowwise(\n            x_ptr,\n            output_ptr,\n            output_scale,\n            BLOCK_SIZE: tl.constexpr,\n            P2: tl.constexpr,\n        ):\n            pid = tl.program_id(axis=0)\n            block_start = pid * BLOCK_SIZE\n            arange = tl.arange(0, P2)\n            offsets = block_start + arange\n            row_mask = arange < BLOCK_SIZE\n            x = tl.load(x_ptr + offsets, mask=row_mask, other=0.0)\n\n            abs_x = tl.abs(x)\n            # We set out-of-bound to 0 for the max reduction.\n            # Then we do a max reduction along the row.\n            max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n            # Avoid division by zero: clamp max_val to at least 1e-4\n            max_val = tl.maximum(max_val, 1e-4)\n            # Scale the row\n            scale = 127.0 / max_val\n            scaled = x * scale\n            # Clip to the range [-127, 127]\n            clipped = tl.minimum(tl.maximum(scaled, -127.0), 127.0)\n            # Round to the nearest integer\n            integer_value = libdevice.llrint(clipped)\n            # Convert to int8 by casting (truncate the 64-bit integer to 8 bits, but we know it's in range)\n            tl.store(output_ptr + offsets, integer_value.to(tl.int8), mask=row_mask)\n            # Store the scale: max_val / 127.0\n            tl.store(output_scale + pid, max_val / 127.0)\n\n        def per_row_quantize_int8(x: torch.Tensor):\n            # Make sure the input is contiguous\n            x = x.contiguous()\n            # Allocate output tensor: same shape as x, but int8\n            output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n            # Allocate output_scale: one per row, so shape (x.shape[0],)\n            output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n            # Compute the next power of two for the row length\n            row_length = x.shape[1]\n            P2 = 1\n            while P2 < row_length:\n                P2 *= 2\n            # Alternatively: P2 = triton.next_power_of_2(row_length) but that function is not available? We can use math.\n            # But note: we are using math, so we did: P2 = int(2 ** (math.ceil(math.log2(row_length)))) but that might be off for exact powers?\n            # We can do: P2 = 1 << (row_length - 1).bit_length()   # This gives the next power of two, but if row_length is 0?\n            # Since row_length is at least 1, we can do:\n            #   P2 = 1 << (row_length - 1).bit_length()\n            # But let's use the while loop for clarity? Actually, we can use:\n            #   P2 = triton.next_power_of_2(row_length)   # This is available in triton? Actually, no.\n\n            # We'll use the math method: \n            #   P2 = 1 if row_length==0 else 2**math.ceil(math.log2(row_length))\n            # But row_length cannot be 0? We assume at least 1.\n            P2_val = 1 << (row_length - 1).bit_length()   # This gives the next power of two that is >= row_length.\n            # But note: if row_length is a power of two, then (row_length-1).bit_length() is the exponent of the next power? Actually, no.\n\n            # Example: row_length=128 -> (127).bit_length()=7 -> 1<<7 = 128 -> correct.\n            # row_length=129 -> (128).bit_length()=8 -> 1<<8=256 -> correct.\n\n            # However, the example in the problem used: P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n\n            # We'll do the same for consistency.\n\n            # But note: the problem says to avoid changing the grid configuration? We are not changing the grid, we are just setting a constant.\n\n            # We can do:\n            P2_val = int(2 ** math.ceil(math.log2(row_length))) if row_length > 0 else 1\n\n            # But we know row_length>=1.\n\n            # Now, set the grid and launch\n            grid = (x.shape[0],)\n            _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=row_length, P2=P2_val)\n\n            return output, output_scale\n\n   However, note: the autotune key is \"n_elements\". But in the kernel, we are not using n_elements? The example autotune key was \"n_elements\". We are using the same.\n\n   But the example kernel did not have n_elements? Actually, the autotune key can be any string that is in the kernel's arguments? We are passing row_length and P2_val? But the autotune key must be one of the arguments? Actually, the autotune key is a list of argument names that are used to select the best configuration. We don't have n_elements? We should change the key to something that is passed? Or remove autotune? \n\n   Alternatively, we can change the key to \"BLOCK_SIZE\" and \"P2\"? But the autotune doesn't tune over these because they are constexpr? Actually, the autotune in Triton can use constexpr? But note: the autotune will generate multiple versions for different values of the key? \n\n   We don't have a variable that changes per call? The row_length might change per call? But the autotune caches the best configuration per key. The key \"n_elements\" is not in the kernel parameters? We can change the key to \"BLOCK_SIZE\" and \"P2\"? But they are passed as constants? \n\n   Alternatively, we can remove autotune? But the problem says to optimize for performance. We can leave it as in the example? The example used \"n_elements\", which is not defined. That might be an error in the example.\n\n   Actually, the example autotune key was \"n_elements\", but we are not passing n_elements to the kernel. We should change the key to a parameter that is passed? We don't have such a parameter.\n\n   We can remove the autotune? But the problem says to optimize. Alternatively, we can change the key to \"P2\" because the block size for the reduction is P2? But P2 is a constexpr. The autotune can be done per constexpr value? \n\n   Let me check: the Triton autotune documentation says that the key is a list of argument names that are used to index the cache. The arguments must be provided at launch time? \n\n   We are passing BLOCK_SIZE and P2 at launch time? They are constexpr. So we can use:\n\n        key = [\"BLOCK_SIZE\", \"P2\"]\n\n   But note: the autotune will then generate a configuration for each (BLOCK_SIZE, P2) pair? That might be too many? \n\n   Alternatively, we can use a simpler autotune? Or remove it for now to make it compile? \n\n   The problem says: if compilation fails, fix that first.\n\n   We decide to remove the autotune for now? Or we can set the key to a fixed value? \n\n   Alternatively, we can note that the kernel is memory bound and the reduction is over a row. The performance is determined by the row length? We can use the row length (BLOCK_SIZE) as the key? \n\n   We change the autotune key to \"BLOCK_SIZE\".\n\n   So:\n\n        @triton.autotune(\n            configs=[ ... ],\n            key=[\"BLOCK_SIZE\"],\n        )\n\n   Now, the autotune will cache the best configuration for each BLOCK_SIZE (row length) that it encounters.\n\n   This is acceptable.\n\n   We'll do that.\n\n   Let's write the code accordingly.\n\n   We are done.\n\n   Note: the example in the problem had the kernel named `_quantize_rowwise` and the host function `per_row_quantize_int8`. We use the same.\n\n   Now, we output the code.\n\n   IMPORTANT: The problem says to return ONLY the complete updated code. We do not include any explanation.\n\n   We write the code.\n</think>\n```python\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"BLOCK_SIZE\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask, other=0.0)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    max_val = tl.maximum(max_val, 1e-4)\n    scale = 127.0 / max_val\n    scaled = x * scale\n    clipped = tl.minimum(tl.maximum(scaled, -127.0), 127.0)\n    integer_value = libdevice.llrint(clipped)\n    tl.store(output_ptr + offsets, integer_value.to(tl.int8), mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127.0)\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    row_length = x.shape[1]\n    P2_val = 1 << (row_length - 1).bit_length()\n    grid = (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=row_length, P2=P2_val)\n    return output, output_scale\n```"]}}}
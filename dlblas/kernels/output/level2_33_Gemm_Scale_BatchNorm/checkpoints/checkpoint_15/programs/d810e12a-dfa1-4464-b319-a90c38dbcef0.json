{"id": "d810e12a-dfa1-4464-b319-a90c38dbcef0", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n# Triton GEMM kernel for optimized matrix multiplication\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % num_pid_m\n    pid_n = pid // num_pid_m\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        \n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    offs_m = tl.max_contiguous(tl.multiple_of(offs_m, BLOCK_SIZE_M), BLOCK_SIZE_M)\n    offs_n = tl.max_contiguous(tl.multiple_of(offs_n, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\n# Custom Triton GEMM function\ndef triton_gemm(x, weight, bias=None):\n    assert x.dim() == 2\n    M, K = x.shape\n    N, _ = weight.shape\n    x = x.contiguous()\n    weight = weight.contiguous()\n    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    gemm_kernel[grid](\n        x, weight, output, bias,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        output.stride(0), output.stride(1),\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        x = triton_gemm(x, self.weight, self.bias)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "0bdcc392-ce4b-4cbd-b8b7-e6c1aea7799a", "generation": 1, "timestamp": 1754566008.3401725, "iteration_found": 1, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_33_Gemm_Scale_BatchNorm", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "max_difference": ["5.936826", "7.339652", "6.298962", "5.846116", "6.035876"], "avg_difference": ["1.128108", "1.125815", "1.120703", "1.124913", "1.121710"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0732, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 0.0944, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:1", "correctness_trials": "(5 / 5)", "reference_timing_ms": 0.0732, "speed_up": 0.775, "custom_timing": 0.0944}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 0.0944\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0732, 'speed_up': 0.775, 'custom_timing': 0.0944}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 0.0944, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0732, 'speed_up': 0.775, 'custom_timing': 0.0944}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a GEMM (general matrix multiplication), applies scaling, \n    and then batch normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (0.0944), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:1', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 0.0732, 'speed_up': 0.775, 'custom_timing': 0.0944})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Simple model that performs a GEMM (general matrix multiplication), applies scaling, \n    and then batch normalization.\n    \"\"\"\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.gemm = nn.Linear(in_features, out_features)\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.gemm(x)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model that does:\n  1. Linear layer (GEMM)\n  2. Element-wise scaling by a parameter vector\n  3. Batch normalization\n\nThe current implementation uses standard nn.Linear and nn.BatchNorm1d.\n\nWe are to rewrite the model to use Triton for the GEMM and possibly other operations to improve speed.\n\nHowever, note the constraints:\n  - We cannot change the function signatures (forward, __init__) or the input/output shapes.\n  - We must maintain the same functionality.\n\nThe current performance shows a slowdown (speed_up = 0.775) meaning our custom code is slower than the reference (which is the original PyTorch code?).\n\nWe are to focus on:\n  - Improving the GEMM operation by writing a Triton kernel for the matrix multiplication.\n  - The scaling and batch normalization might be left in PyTorch for simplicity, but if they are bottlenecks we might consider fusing.\n\nBut note: the entire forward pass is:\n   x = self.gemm(x)   -> [batch_size, out_features]\n   x = x * self.scale  -> [batch_size, out_features] * [out_features] (broadcasted)\n   x = self.bn(x)      -> [batch_size, out_features]\n\nWe can try to write a Triton kernel for the GEMM step. However, note that the nn.Linear also has a bias. Our current model does not show a bias? Actually, nn.Linear has a bias by default. We must account for that.\n\nPlan:\n  1. Replace the nn.Linear with a custom Triton GEMM that does: x @ W.t() + bias\n  2. We will have to initialize the weights and bias in the __init__ and make them parameters.\n\nBut note: the original nn.Linear has parameters that are stored in the module. We are currently using self.gemm which is an nn.Linear. We are going to remove that and create our own parameters.\n\nWe must also note that the input to the forward is of shape [batch_size, in_features] and the output should be [batch_size, out_features].\n\nWe can write a Triton kernel for the matrix multiplication. We note that the matrix multiplication is:\n   output = input @ weight.t() + bias   [if bias is not None]\n\nWe will write a Triton kernel for this GEMM.\n\nHowever, the current implementation also does scaling and batch norm. We are not going to fuse them in this step because:\n  - The batch norm has running mean and variance that are updated during training and used during inference. It is stateful and non-trivial to fuse with GEMM in Triton for the general case.\n\nBut note: the batch norm in this model is applied on the output of the scaling. However, we might consider fusing the scaling into the GEMM? Actually, the scaling is a per-feature (out_features) scaling. It is applied after the GEMM.\n\nWe can break down the operations:\n\n  Step 1: GEMM: [batch, in] @ [out, in].t() -> [batch, out]   (plus bias)\n  Step 2: Multiply by scale: [batch, out] * [out] -> [batch, out]\n  Step 3: BatchNorm: which is (x - running_mean) * (gamma / sqrt(running_var + eps)) + beta\n\nWe are going to write a Triton kernel for the GEMM step. We leave the scaling and batch norm in PyTorch for now because:\n  - The scaling is a simple element-wise operation that is memory bound, and the batch norm is also a simple operation that might be memory bound. They are not as compute-intensive as GEMM.\n\nBut note: the current performance shows that our custom code (which is the entire model) is slower than the reference. The reference is the original PyTorch code? Actually, the problem states that the current program is the reference code? The meta says: 'stage': 'reference code'. So we are currently at the reference code.\n\nWe are to write a custom implementation that replaces the reference. We are allowed to change only the Triton kernel part? Actually, the entire block is replaceable.\n\nWe are going to:\n  - Remove the nn.Linear and replace it with a Triton GEMM kernel.\n  - We will keep the scaling and batch norm as they are.\n\nWe note that the GEMM is the most expensive part, so optimizing it should help.\n\nSteps for the Triton GEMM kernel:\n\n  We are going to write a kernel that does:\n    C = A @ B.t()   [where A is [M, K], B is [N, K], and C is [M, N]]\n\n  In our case:\n    A = input (batch_size, in_features) -> M = batch_size, K = in_features\n    B = weight (out_features, in_features) -> N = out_features, K = in_features\n    Then add bias (out_features) to each row.\n\n  We will use a block tiling approach for the matrix multiplication.\n\n  We will have:\n    - BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K as tunable parameters.\n\n  The grid will be (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n\n  Each program will compute a block of C of size [BLOCK_SIZE_M, BLOCK_SIZE_N].\n\n  We will load a block of A and a block of B, and then compute the dot product.\n\n  We will also use vectorized loads/stores for efficiency.\n\n  We must handle boundaries.\n\n  We will accumulate in float32 for numerical stability.\n\n  We will add the bias at the end.\n\nWe are going to write:\n\n  @triton.autotune(...)\n  @triton.jit\n  def _gemm_kernel(...)\n\n  Then we will call this kernel in the forward pass.\n\nWe must also note that the weights and bias are parameters, so we have to store them as such.\n\nWe change the __init__:\n\n  Instead of:\n      self.gemm = nn.Linear(in_features, out_features)\n\n  We do:\n      self.weight = nn.Parameter(torch.empty(out_features, in_features))\n      self.bias = nn.Parameter(torch.empty(out_features))\n      # initialize as in nn.Linear: kaiming uniform for weight, zeros for bias? But note the original nn.Linear uses init.kaiming_uniform_ and bound = 1/sqrt(fan_in) for uniform. We can do the same.\n\n  Then in forward, we call our triton gemm function.\n\nWe must also note that the input might not be contiguous. We can make it contiguous.\n\nLet's write the kernel:\n\n  We assume row-major ordering.\n\n  A: [M, K] -> stride_am, stride_ak\n  B: [N, K] -> stride_bn, stride_bk (but note: we are using B.t()? Actually, we are not transposing B in the kernel because we are accessing B in a transposed way? We can change the kernel to do A @ B.t() by having B in row-major without transposing. Alternatively, we can have the kernel that does A @ B^T by having B in row-major (so that we are accessing columns of B as rows of B^T).\n\n  Actually, we want to compute: C[i, j] = sum_k A[i, k] * B[j, k]\n\n  So we can have:\n      A: [M, K] -> row i, column k: A + i * stride_am + k * stride_ak\n      B: [N, K] -> row j, column k: B + j * stride_bn + k * stride_bk\n\n  Then we can load blocks of A and blocks of B.\n\n  We will use:\n      pid = (pid_m, pid_n)  for the grid.\n\n  Then:\n      i = pid_m * BLOCK_SIZE_M + range(0, BLOCK_SIZE_M)\n      j = pid_n * BLOCK_SIZE_N + range(0, BLOCK_SIZE_N)\n\n  We will need to load a block of A of size [BLOCK_SIZE_M, BLOCK_SIZE_K] and a block of B of size [BLOCK_SIZE_N, BLOCK_SIZE_K] (but note: we are iterating over K).\n\n  We will break K into blocks of size BLOCK_SIZE_K.\n\n  We accumulate in a local variable.\n\n  Then we add the bias: for each column j, we add bias[j] to the entire row of the block.\n\n  We store the result.\n\nWe will write the kernel accordingly.\n\nHowever, note that the current program does not show any Triton code. We are going to introduce Triton.\n\nWe are going to restructure the code:\n\n  We will define a function `triton_gemm` that does the operation.\n\n  We will then use that in the forward.\n\nWe are going to write the kernel and the wrapper.\n\nWe note that the input to the forward is x of shape [batch_size, in_features].\n\nWe are going to make sure that the weights are stored as [out_features, in_features].\n\nWe will write:\n\n  def triton_gemm(x, weight, bias):\n      # x: [M, K], weight: [N, K], bias: [N]\n      M, K = x.shape\n      N, _ = weight.shape\n      # output: [M, N]\n      output = torch.empty((M, N), device=x.device, dtype=torch.float32)\n      # grid and kernel call\n      ...\n\n  Then in forward:\n      x = triton_gemm(x, self.weight, self.bias)\n\nBut note: the original nn.Linear uses the weight as [out_features, in_features] and does x @ weight.t() + bias.\n\nWe are doing the same.\n\nNow, we write the kernel.\n\nWe will use the following template for the kernel:\n\n  @triton.autotune(...)\n  @triton.jit\n  def _gemm_kernel(\n      x_ptr, weight_ptr, bias_ptr, output_ptr,\n      M, N, K,\n      stride_xm, stride_xk,\n      stride_wn, stride_wk,\n      stride_om, stride_on,\n      BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n  ):\n      pid = tl.program_id(0)\n      pid_m = pid // (tl.cdiv(N, BLOCK_SIZE_N))\n      pid_n = pid % (tl.cdiv(N, BLOCK_SIZE_N))\n\n      # ... rest of the kernel\n\nBut note: we are using 2D grid? Actually, we can use 2D grid for better efficiency.\n\n  grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n\n  Then in the kernel:\n      pid_m = tl.program_id(0)\n      pid_n = tl.program_id(1)\n\n  This is simpler.\n\nWe will do:\n\n  pid_m = tl.program_id(0)\n  pid_n = tl.program_id(1)\n\n  Then:\n      offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n      offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n      offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n      # pointers to the first elements in the blocks\n      x_ptr_block = x_ptr + offs_am[:, None] * stride_xm + offs_k[None, :] * stride_xk\n      weight_ptr_block = weight_ptr + offs_bn[:, None] * stride_wn + offs_k[None, :] * stride_wk\n\n      # We will accumulate\n      acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n      # loop over K\n      for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n          # load a block of x and weight\n          # Note: we must check boundaries for k\n          k_remaining = K - k * BLOCK_SIZE_K\n          k_size = tl.minimum(BLOCK_SIZE_K, k_remaining)\n          mask_x = (offs_am[:, None] < M) & (offs_k[None, :] < k_size)\n          mask_w = (offs_bn[:, None] < N) & (offs_k[None, :] < k_size)\n          a = tl.load(x_ptr_block, mask=mask_x, other=0.0)\n          b = tl.load(weight_ptr_block, mask=mask_w, other=0.0)\n          # We are going to do: a [BLOCK_SIZE_M, BLOCK_SIZE_K] @ b [BLOCK_SIZE_N, BLOCK_SIZE_K]^T\n          # But note: we have to transpose b? Actually, we want to do a @ b^T -> [BLOCK_SIZE_M, BLOCK_SIZE_N]\n          # We can do: tl.dot(a, b, trans_b=True) -> but wait, our b is [BLOCK_SIZE_N, BLOCK_SIZE_K] and we want to transpose to [BLOCK_SIZE_K, BLOCK_SIZE_N]? Actually, the dot product in Triton is:\n          #   tl.dot(a, b) -> [M, N] = [M, K] @ [K, N]\n          # But we have a [M, K] and b [N, K] -> we want to do a @ b^T -> which is [M, K] @ [K, N] = [M, N]. So we can transpose b to get [K, N]? Alternatively, we can change the order.\n\n          # Actually, we can do: a @ b.t() -> but Triton does not have a direct way to do that? We can transpose b first? But that is expensive.\n\n          # Alternatively, we can note that:\n          #   a @ b.t() = (b @ a.t()).t()\n          # But that is not efficient.\n\n          # Instead, we can change the way we load the weight: we can load it in a transposed manner? Actually, we are loading a block of weight as [BLOCK_SIZE_N, BLOCK_SIZE_K]. We want to do a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] @ [BLOCK_SIZE_K, BLOCK_SIZE_N] = [BLOCK_SIZE_M, BLOCK_SIZE_N]. So we can use tl.dot(a, b, trans_b=False) if we had b as [BLOCK_SIZE_K, BLOCK_SIZE_N]? But we loaded it as [BLOCK_SIZE_N, BLOCK_SIZE_K].\n\n          # We can transpose b by doing:\n          #   b = tl.trans(b)\n          # But that might be expensive.\n\n          # Alternatively, we can change the pointer for weight to load a block of [BLOCK_SIZE_K, BLOCK_SIZE_N]? That would require a different memory layout. But our weight is stored as row-major: [N, K]. We are loading a block of [BLOCK_SIZE_N, BLOCK_SIZE_K]. We want to do the dot product as [M, K] @ [K, N] -> so we need the second matrix to be [K, N]. We can load the weight block in a transposed way? Actually, we can use a different stride.\n\n          # We can change the pointer for weight to be:\n          #   weight_ptr_block = weight_ptr + offs_k[:, None] * stride_wk + offs_bn[None, :] * stride_wn\n          # Then we get a block of [BLOCK_SIZE_K, BLOCK_SIZE_N]. Then we can do:\n          #   acc += tl.dot(a, b)\n\n          # So let's do that.\n\n          # We change the weight pointer:\n          weight_ptr_block = weight_ptr + offs_k[:, None] * stride_wk + offs_bn[None, :] * stride_wn\n\n          # Then we load b as [BLOCK_SIZE_K, BLOCK_SIZE_N] but note the mask must be adjusted.\n\n          mask_w = (offs_k[:, None] < k_size) & (offs_bn[None, :] < N)\n          b = tl.load(weight_ptr_block, mask=mask_w, other=0.0)\n\n          # Now we can do:\n          acc += tl.dot(a, b)\n\n          # Then update the pointers for the next block of K?\n          x_ptr_block += BLOCK_SIZE_K * stride_xk\n          weight_ptr_block += BLOCK_SIZE_K * stride_wk\n\n      # Now, after the loop, we have the accumulated result in acc for the block [BLOCK_SIZE_M, BLOCK_SIZE_N]\n      # Then we add the bias: for each column j (which is offs_bn) we add bias[offs_bn] to every row in the block.\n      bias_ptrs = bias_ptr + offs_bn\n      bias = tl.load(bias_ptrs, mask=offs_bn < N, other=0.0)  # [BLOCK_SIZE_N]\n      # Now we want to add bias to every row of the block: [BLOCK_SIZE_M, BLOCK_SIZE_N] + [1, BLOCK_SIZE_N]\n      acc = acc + bias[None, :]\n\n      # Now we store the result to output.\n      offs_om = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n      offs_on = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n      output_ptrs = output_ptr + offs_om[:, None] * stride_om + offs_on[None, :] * stride_on\n      mask = (offs_om[:, None] < M) & (offs_on[None, :] < N)\n      tl.store(output_ptrs, acc, mask=mask)\n\nBut note: the above loading for weight has changed. We are now loading a block of [BLOCK_SIZE_K, BLOCK_SIZE_N]. This is a column-major block? Actually, it's a block that is contiguous in the K dimension and then the N dimension. This is efficient for the dot product with A (which is [M, K]) because we are doing [M, K] @ [K, N] = [M, N].\n\nHowever, note that the original weight tensor is stored as row-major [N, K]. The stride for weight is (stride_wn, stride_wk). We are using:\n   weight_ptr + offs_k * stride_wk + offs_bn * stride_wn\n\nBut if the tensor is row-major, then stride_wn = K, and stride_wk = 1? Actually, for a row-major tensor [N, K], the stride for the first dimension (n) is K, and the second dimension (k) is 1.\n\nSo the above pointer arithmetic is:\n   weight_ptr + offs_k * 1 + offs_bn * K\n\nThat is: we are accessing the element at [offs_bn, offs_k]? But wait, we want to load a block of [BLOCK_SIZE_K, BLOCK_SIZE_N]. The element at (k, n) in the block would be at the original index [n, k]. So the pointer for (k, n) is:\n   base + n * stride_wn + k * stride_wk = base + n * K + k * 1.\n\nThis is the same as base + k + n*K. This is contiguous in k? Actually, if we fix n and vary k, we are getting contiguous memory. But in our block, we have:\n   offs_k: [0, 1, ... , BLOCK_SIZE_K-1] (along rows)\n   offs_bn: [0, 1, ... , BLOCK_SIZE_N-1] (along columns)\n\nWe are then doing:\n   weight_ptr_block = weight_ptr + offs_k[:, None] * stride_wk + offs_bn[None, :] * stride_wn\n                = weight_ptr + offs_k[:, None] * 1 + offs_bn[None, :] * K\n\nThis gives a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] in memory? The memory layout of the block is:\n   row0: [n0, k0], [n0, k1], ... [n0, k_{BLOCK_SIZE_K-1}], [n1, k0], ... -> no, wait: the memory is not contiguous because the next element in the same row (same k, next n) is at base + (k) + (n+1)*K -> which is K elements away.\n\nSo we are loading a block that is not contiguous. This will cause many memory transactions.\n\nAlternatively, we can use a different approach: we can load the weight block in a transposed manner without changing the memory layout? Actually, we can use the original row-major and then do the dot product without transposing by using a different order.\n\nBut note: the dot product in Triton requires the second matrix to be in column-major? Actually, the tl.dot operation expects both matrices to be in row-major? The documentation says:\n\n   tl.dot(a, b) -> a and b are 2D tensors, and the operation is a matrix multiplication: [M, K] @ [K, N] -> [M, N]\n\n   But for that, b must be of shape [K, N] and stored in row-major? Actually, the dot product in Triton does not require a specific layout, it just does the math.\n\n   However, the efficiency comes from the memory access pattern. We want to load contiguous memory.\n\n   For the weight matrix, we want to load a block that is contiguous in the K dimension? That is, we want to load a block of [BLOCK_SIZE_N, BLOCK_SIZE_K] in row-major: each row is contiguous. Then we can use tl.dot(a, b, trans_b=True) to compute a @ b^T? But Triton does not have trans_b argument.\n\n   Actually, the tl.dot operation does not support transposition. We have to pre-transpose the block? We can use tl.trans(b) but that is expensive.\n\n   Alternatively, we can note that we can change the loop order and do the dot product by hand? That would be inefficient.\n\n   Another approach: we can pre-transpose the weight matrix? But that would be done once and then stored. But then we are changing the parameter layout. We are not allowed to change the interface? The original model expects the weight to be [out_features, in_features]. We cannot change that because the user might expect that.\n\n   We can transpose the weight once in the forward? But that would be an extra operation.\n\n   We decide to load the weight in the original row-major and then use a different kernel that does:\n\n        acc = tl.zeros(...)\n        for k in range(...):\n            a = tl.load(x_ptr_block)   # [BLOCK_SIZE_M, BLOCK_SIZE_K] (contiguous in K)\n            b = tl.load(weight_ptr_block)  # [BLOCK_SIZE_N, BLOCK_SIZE_K] (contiguous in K, because each row is contiguous)\n\n            # We want to compute: for each i, j: acc[i,j] += sum_k a[i,k] * b[j,k]\n            # We can do: \n            #   acc += tl.sum(a[:, :, None] * b[None, :, :], axis=1)   -> but that is not efficient.\n\n        Instead, we can do:\n\n            a = a[:, None, :]   # [BLOCK_SIZE_M, 1, BLOCK_SIZE_K]\n            b = b[None, :, :]   # [1, BLOCK_SIZE_N, BLOCK_SIZE_K]\n            product = a * b      # [BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K]\n            acc += tl.sum(product, axis=2)\n\n        But that requires a lot of registers and is O(BLOCK_SIZE_M * BLOCK_SIZE_N * BLOCK_SIZE_K) per step.\n\n        This is not efficient.\n\n   We can use tl.dot if we can get the weight block as [BLOCK_SIZE_K, BLOCK_SIZE_N]. We can do that by loading the weight block in a transposed manner? But we cannot change the memory layout of the weight.\n\n   Another idea: we can preload the weight block into shared memory and then transpose it? But Triton doesn't have shared memory? Actually, it does, but it's called \"static memory\" and is limited.\n\n   Alternatively, we can change the kernel to do the dot product without tl.dot by using loop over K and accumulate in parallel for M and N. We can use:\n\n        for k in range(0, BLOCK_SIZE_K):\n            a_k = a[:, k]   # [BLOCK_SIZE_M]\n            b_k = b[:, k]   # [BLOCK_SIZE_N]\n            # outer product: [BLOCK_SIZE_M, BLOCK_SIZE_N] += a_k[:, None] * b_k[None, :]\n\n        We can do this with broadcasting? But Triton doesn't support that.\n\n   We can do nested loops? That would be inefficient.\n\nGiven the complexity, we decide to use the original approach of loading the weight block in a transposed layout by changing the pointer. Even though it might not be contiguous, we are still loading the entire block and then doing the dot product. The tl.dot function in Triton is optimized for any layout? Actually, it might not be.\n\nAlternatively, we can use the following approach:\n\n   We note that the weight matrix is stored in row-major, so we can treat it as a matrix of [K, N] in column-major? Actually, no.\n\n   We decide to pre-transpose the weight matrix in the forward pass? That would be an extra O(N*K) operation, which we cannot afford.\n\n   We can store the weight in two layouts: the original and a transposed one? That would use extra memory.\n\n   We are going to try to load the weight block as [BLOCK_SIZE_K, BLOCK_SIZE_N] by using a different pointer and stride. We already did that above. We hope that the memory access pattern is acceptable.\n\n   We note that the access to weight is:\n        weight_ptr + offs_k * 1 + offs_bn * K\n\n   For a fixed offs_bn, as offs_k increases by 1, we move to the next element in the same row -> contiguous. But for a fixed offs_k, as offs_bn increases by 1, we jump by K. So when we load a block of [BLOCK_SIZE_K, BLOCK_SIZE_N], we are loading BLOCK_SIZE_K contiguous segments of length BLOCK_SIZE_N, each separated by K elements. This is not contiguous, but it is strided.\n\n   We can use a vectorized load? We can set the vector size to 1? Or we can use a vector size that matches the contiguous dimension.\n\n   Actually, we are using tl.load with a mask. The Triton compiler will try to optimize the memory access.\n\n   We hope that for large BLOCK_SIZE_K and small BLOCK_SIZE_N, or vice versa, the access is efficient.\n\n   Alternatively, we can change the blocking strategy: we can make BLOCK_SIZE_K as large as possible to amortize the cost of the strided load.\n\n   We will proceed and then tune the block sizes.\n\nWe will write the kernel as described above.\n\nWe must also note that we are accumulating in float32, which is good.\n\nWe will now write the complete code.\n\nWe also note that we are using autotune for the block sizes.\n\nWe will define several configs for the block sizes.\n\nWe must also note that the kernel must handle boundaries.\n\nWe will write the wrapper function for the gemm.\n\nWe then replace the nn.Linear with our custom gemm.\n\nWe leave the scaling and batch norm as is.\n\nWe hope that the speedup in the GEMM will overcome the overhead of the extra kernel launch.\n\nLet's code accordingly.\n\nNote: the original model uses the nn.Linear which might be using cuBLAS. Our Triton kernel must be competitive.\n\nWe will write the kernel and then the rest of the model.\n\nWe must also initialize the weights and biases as in the original nn.Linear.\n\nWe will change the __init__ to initialize the weight and bias.\n\nWe note: the original nn.Linear uses:\n   init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n   if bias is not None:\n        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        init.uniform_(self.bias, -bound, bound)\n\nBut we are not given the original init of the nn.Linear? We can do the same.\n\nAlternatively, we can use:\n   nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n   if self.bias is not None:\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in)\n        nn.init.uniform_(self.bias, -bound, bound)\n\nBut note: the original nn.Linear uses:\n   stdv = 1. / math.sqrt(self.weight.size(1))\n   self.weight.data.uniform_(-stdv, stdv)\n   if self.bias is not None:\n        self.bias.data.uniform_(-stdv, stdv)\n\nActually, the modern nn.Linear uses kaiming uniform. We'll use kaiming uniform.\n\nNow, we write the complete code.\n\nNote: We must be cautious of the data types. We assume float32.\n\nWe are going to write a lot of code. We must make sure it fits within the EVOLVE-BLOCK.\n\nWe will write:\n\n   import triton\n   import triton.language as tl\n\n   @triton.autotune(...)\n   @triton.jit\n   def _gemm_kernel(...):\n        ...\n\n   def triton_gemm(x, weight, bias):\n        ...\n\n   class ModelNew(nn.Module):\n        def __init__(...):\n            # remove nn.Linear and define weight and bias\n            ...\n        def forward(...):\n            x = triton_gemm(x, self.weight, self.bias)\n            x = x * self.scale\n            x = self.bn(x)\n            return x\n\n   And then the batch_size, etc.\n\nWe must note that the original model used nn.Linear, which by default uses bias. We are including bias.\n\nWe are also going to set the grid and call the kernel.\n\nWe hope this will be faster.\n\nLet's write the code.\n\nNote: The current program has a batch_size=128, in_features=1024, out_features=512. This is a small to medium size GEMM.\n\nWe choose block sizes that are powers of two and divisors of the dimensions? We can try BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32.\n\nWe will provide several configs for autotune.\n\nWe are going to write the complete code in the EVOLVE-BLOCK.\n\nWe must be cautious: the kernel must be correct.\n\nGiven the complexity, we might make mistakes. We will focus on getting it to compile and run correctly first, then optimize.\n\nWe are also going to note that the kernel might be slow at first, but we can then tune.\n\nBut note: we are only allowed to change the EVOLVE-BLOCK. We must output the entire block.\n\nLet's do it.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n# Triton GEMM kernel for optimized matrix multiplication\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef gemm_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % num_pid_m\n    pid_n = pid // num_pid_m\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        \n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)\n        accumulator += bias[None, :]\n    \n    offs_m = tl.max_contiguous(tl.multiple_of(offs_m, BLOCK_SIZE_M), BLOCK_SIZE_M)\n    offs_n = tl.max_contiguous(tl.multiple_of(offs_n, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=mask)\n\n# Custom Triton GEMM function\ndef triton_gemm(x, weight, bias=None):\n    assert x.dim() == 2\n    M, K = x.shape\n    N, _ = weight.shape\n    x = x.contiguous()\n    weight = weight.contiguous()\n    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n    \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    gemm_kernel[grid](\n        x, weight, output, bias,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        output.stride(0), output.stride(1),\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        x = triton_gemm(x, self.weight, self.bias)\n        x = x * self.scale\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "ee062af5-c297-4340-bfa3-abb3ea5f1b7a", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_warps=4),\n    ],\n    key=['batch_size', 'in_features', 'out_features'],\n)\n@triton.jit\ndef fused_gemm_scale_kernel(\n    x_ptr, weight_ptr, bias_ptr, scale_ptr, output_ptr,\n    batch_size, in_features, out_features,\n    stride_xb, stride_xf,\n    stride_wf, stride_wo,\n    stride_ob, stride_of,\n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr, \n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    \n    batch_offsets = pid0 * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    feat_offsets = pid1 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    batch_mask = batch_offsets < batch_size\n    feat_mask = feat_offsets < out_features\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, in_features, BLOCK_SIZE_K):\n        k_offsets = k + tl.arange(0, BLOCK_SIZE_K)\n        k_mask = k_offsets < in_features\n        \n        x_vals = tl.load(\n            x_ptr + batch_offsets[:, None] * stride_xb + k_offsets[None, :] * stride_xf,\n            mask=batch_mask[:, None] & k_mask[None, :],\n            other=0.0\n        ).to(tl.float16)\n        \n        w_vals = tl.load(\n            weight_ptr + k_offsets[:, None] * stride_wf + feat_offsets[None, :] * stride_wo,\n            mask=k_mask[:, None] & feat_mask[None, :],\n            other=0.0\n        ).to(tl.float16)\n        \n        acc += tl.dot(x_vals, w_vals, out_dtype=tl.float32)\n    \n    bias_vals = tl.load(bias_ptr + feat_offsets, mask=feat_mask, other=0.0)\n    acc += bias_vals[None, :]\n    \n    scale_vals = tl.load(scale_ptr + feat_offsets, mask=feat_mask, other=0.0)\n    acc *= scale_vals[None, :]\n    \n    output_ptrs = (\n        output_ptr + \n        batch_offsets[:, None] * stride_ob + \n        feat_offsets[None, :] * stride_of\n    )\n    tl.store(output_ptrs, acc, mask=batch_mask[:, None] & feat_mask[None, :])\n\ndef fused_gemm_scale(x, weight_t, bias, scale):\n    batch_size, in_features = x.shape\n    out_features = bias.shape[0]\n    output = torch.empty((batch_size, out_features), device=x.device, dtype=x.dtype)\n    \n    grid = lambda meta: (\n        triton.cdiv(batch_size, meta['BLOCK_SIZE_M']),\n        triton.cdiv(out_features, meta['BLOCK_SIZE_N']),\n    )\n    \n    fused_gemm_scale_kernel[grid](\n        x, weight_t, bias, scale, output,\n        batch_size, in_features, out_features,\n        x.stride(0), x.stride(1),\n        weight_t.stride(0), weight_t.stride(1),\n        output.stride(0), output.stride(1),\n    )\n    return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.scale = nn.Parameter(torch.randn(scale_shape))\n        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, x):\n        weight_t = self.weight.t().contiguous()\n        x = fused_gemm_scale(x, weight_t, self.bias, self.scale)\n        x = self.bn(x)\n        return x\n\nbatch_size = 128\nin_features = 1024\nout_features = 512\nscale_shape = (out_features,)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_features)]\n\ndef get_init_inputs():\n    return [in_features, out_features, scale_shape]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "688d8f32-b6de-4e60-8830-f6fb9e4d29b6", "generation": 3, "timestamp": 1754570963.6790884, "iteration_found": 25, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_33_Gemm_Scale_BatchNorm", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "max_difference": ["5.979198", "5.950242", "5.924732", "5.982923", "5.906321"], "avg_difference": ["1.126785", "1.130793", "1.121469", "1.126780", "1.127120"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0732, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level2_33_Gemm_Scale_BatchNorm", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "max_difference": ["5.977910", "5.950563", "5.924537", "5.983512", "5.906163"], "avg_difference": ["1.126786", "1.130792", "1.121465", "1.126778", "1.127119"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 0.0732, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
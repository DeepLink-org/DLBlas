{"id": "86dd4216-8eec-4b17-ba31-b42e4d637f9e", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr   # Now represents number of threads per block\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    tid = tl.arange(0, BLOCK_SIZE)  # Thread ID within block\n\n    spatial_size = height * width\n    num_chunks = tl.cdiv(spatial_size, BLOCK_SIZE)\n    \n    # Each thread accumulates partial sum\n    partial = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for chunk_idx in range(num_chunks):\n        idx = chunk_idx * BLOCK_SIZE + tid\n        mask = idx < spatial_size\n        h_idx = idx // width\n        w_idx = idx % width\n        input_offset = pid_batch * input_batch_stride + \\\n                       pid_channel * input_channel_stride + \\\n                       h_idx * input_height_stride + \\\n                       w_idx * input_width_stride\n        values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n        partial += values\n\n    # Parallel reduction across threads in block\n    total = tl.sum(partial, axis=0)\n    \n    # First thread writes final result\n    if tid == 0:\n        avg = total / spatial_size\n        output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n        tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Launch kernel with 256 threads per block (8 warps)\n    adaptive_avg_pool_kernel[grid, num_warps=8](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=256\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "c1eddd7b-6e2f-4357-9cd5-7e92294f8e43", "generation": 2, "timestamp": 1754591748.1694708, "iteration_found": 22, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "", "exec_log": "[Child] \u7ed1\u5b9a GPU 2, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 58\n    adaptive_avg_pool_kernel[grid, num_warps=8](\n                                   ^^^^^^^^^^^\nSyntaxError: invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.958, "runtime_ms": 1.2, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.15, "speed_up": 0.958, "custom_timing": 1.2}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.9580\n- runtime_ms: 1.2000\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.958, 'custom_timing': 1.2}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9580, runtime_ms: 1.2000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.958, 'custom_timing': 1.2}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.8390, runtime_ms: 1.3700, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.839, 'custom_timing': 1.37}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9350, runtime_ms: 1.2300, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.935, 'custom_timing': 1.23}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    COL_BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    # Process rows sequentially for better cache locality\n    for h in range(height):\n        # Process columns in contiguous blocks\n        for w_base in range(0, width, COL_BLOCK_SIZE):\n            w_offsets = w_base + tl.arange(0, COL_BLOCK_SIZE)\n            mask = w_offsets < width\n            \n            input_offset = pid_batch * input_batch_stride + \\\n                           pid_channel * input_channel_stride + \\\n                           h * input_height_stride + \\\n                           w_offsets * input_width_stride\n            values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n            total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Optimize column block size for contiguous memory access\n    col_block_size = triton.next_power_of_2(width)\n    if col_block_size > 1024:\n        col_block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        COL_BLOCK_SIZE=col_block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9350), Performs well on runtime_ms (1.2300), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.935, 'custom_timing': 1.23})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_relu_concat(\n    x1_ptr, x2_ptr, output_ptr,\n    x1_batch_stride, x1_channel_stride, x1_h_stride, x1_w_stride,\n    x2_batch_stride, x2_channel_stride, x2_h_stride, x2_w_stride,\n    output_batch_stride, output_channel_stride, output_h_stride, output_w_stride,\n    c1, c2, h, w,\n    total_elements: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < total_elements\n\n    # Calculate 4D indices\n    offsets_4d = offsets\n    b_idx = offsets_4d // ( (c1 + c2) * h * w )\n    remainder = offsets_4d % ( (c1 + c2) * h * w )\n    c_all_idx = remainder // (h * w)\n    remainder2 = remainder % (h * w)\n    h_idx = remainder2 // w\n    w_idx = remainder2 % w\n\n    # Initialize values\n    val = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Load from x1 or x2 based on channel index\n    is_x1 = c_all_idx < c1\n    x1_channel = c_all_idx\n    x2_channel = c_all_idx - c1\n\n    # Compute pointers and load data\n    x1_offsets = b_idx * x1_batch_stride + x1_channel * x1_channel_stride + h_idx * x1_h_stride + w_idx * x1_w_stride\n    x2_offsets = b_idx * x2_batch_stride + x2_channel * x2_channel_stride + h_idx * x2_h_stride + w_idx * x2_w_stride\n\n    x1_val = tl.load(x1_ptr + x1_offsets, mask=mask & is_x1, other=0.0)\n    x2_val = tl.load(x2_ptr + x2_offsets, mask=mask & (~is_x1), other=0.0)\n    \n    val = tl.where(is_x1, x1_val, x2_val)\n    val = tl.maximum(val, 0.0)  # ReLU activation\n\n    # Compute output pointer\n    output_offsets = b_idx * output_batch_stride + c_all_idx * output_channel_stride + h_idx * output_h_stride + w_idx * output_w_stride\n    tl.store(output_ptr + output_offsets, val, mask=mask)\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        out1 = self.expand1x1(x)\n        out2 = self.expand3x3(x)\n        \n        batch, c1, h, w = out1.shape\n        c2 = out2.shape[1]\n        output = torch.empty(batch, c1 + c2, h, w, device=x.device, dtype=x.dtype)\n        \n        total_elements = batch * (c1 + c2) * h * w\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        fused_relu_concat[grid](\n            out1, out2, output,\n            out1.stride(0), out1.stride(1), out1.stride(2), out1.stride(3),\n            out2.stride(0), out2.stride(1), out2.stride(2), out2.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            c1, c2, h, w,\n            total_elements,\n            BLOCK_SIZE=1024\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.8390), Performs well on runtime_ms (1.3700), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.839, 'custom_timing': 1.37})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        h_offsets = offsets // width\n        w_offsets = offsets % width\n        \n        mask = offsets < num_elements\n        h_mask = h_offsets < height\n        w_mask = w_offsets < width\n        mask = mask & h_mask & w_mask\n        \n        input_offset = pid_batch * input_batch_stride + \\\n                       pid_channel * input_channel_stride + \\\n                       h_offsets * input_height_stride + \\\n                       w_offsets * input_width_stride\n        values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n        total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Use power-of-two block size for efficient reduction\n    block_size = triton.next_power_of_2(height * width)\n    if block_size > 1024:  # Cap block size for register efficiency\n        block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9580), Performs well on runtime_ms (1.2000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.958, 'custom_timing': 1.2})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef linear_relu_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < k_remaining)\n        b_mask = (offs_k[:, None] < k_remaining) & (offs_bn[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        acc += bias[None, :]\n\n    acc = tl.where(acc < 0, 0, acc)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask)\n\ndef linear_relu(a, weight, bias):\n    M, K = a.shape\n    weight_t = weight.t().contiguous()\n    N = weight_t.shape[1]\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    linear_relu_kernel[grid](\n        a, weight_t, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        weight_t.stride(0), weight_t.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64\n    )\n    return c\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        h_offsets = offsets // width\n        w_offsets = offsets % width\n        \n        mask = offsets < num_elements\n        h_mask = h_offsets < height\n        w_mask = w_offsets < width\n        mask = mask & h_mask & w_mask\n        \n        input_offset = pid_batch * input_batch_stride + \\\n                       pid_channel * input_channel_stride + \\\n                       h_offsets * input_height_stride + \\\n                       w_offsets * input_width_stride\n        values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n        total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef triton_adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    block_size = triton.next_power_of_2(height * width)\n    if block_size > 1024:\n        block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass TritonLinearReLU(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n        \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        batch, c, h, w = x.shape\n        x_flat = x.permute(0,2,3,1).reshape(-1, c).contiguous()\n        output_flat = linear_relu(x_flat, self.weight, self.bias)\n        return output_flat.reshape(batch, h, w, self.out_features).permute(0,3,1,2).contiguous()\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = TritonLinearReLU(in_channels, squeeze_channels)\n        self.expand1x1 = TritonLinearReLU(squeeze_channels, expand1x1_channels)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze(x)\n        return torch.cat([\n            self.expand1x1(x),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            TritonLinearReLU(512, num_classes),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Linearize spatial dimensions for efficient reduction\n    spatial_size = height * width\n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    \n    # Parallel reduction initialization\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for idx in range(0, spatial_size, BLOCK_SIZE):\n        offsets = idx + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < spatial_size\n        \n        # Convert linear index to 2D coordinates\n        h_idx = offsets // width\n        w_idx = offsets % width\n        input_offsets = base_offset + h_idx * input_height_stride + w_idx * input_width_stride\n        \n        # Load and accumulate values\n        values = tl.load(input_ptr + input_offsets, mask=mask, other=0.0)\n        acc += values\n\n    # Final reduction and averaging\n    total = tl.sum(acc, axis=0)\n    avg = total / spatial_size\n    \n    # Store result\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Optimize block size for efficient parallel reduction\n    spatial_size = height * width\n    block_size = min(triton.next_power_of_2(spatial_size), 1024)\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_relu_concat(\n    x1_ptr, x2_ptr, output_ptr,\n    x1_batch_stride, x1_channel_stride, x1_h_stride, x1_w_stride,\n    x2_batch_stride, x2_channel_stride, x2_h_stride, x2_w_stride,\n    output_batch_stride, output_channel_stride, output_h_stride, output_w_stride,\n    c1, c2, h, w,\n    total_elements: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < total_elements\n\n    # Calculate 4D indices\n    offsets_4d = offsets\n    b_idx = offsets_4d // ( (c1 + c2) * h * w )\n    remainder = offsets_4d % ( (c1 + c2) * h * w )\n    c_all_idx = remainder // (h * w)\n    remainder2 = remainder % (h * w)\n    h_idx = remainder2 // w\n    w_idx = remainder2 % w\n\n    # Initialize values\n    val = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Load from x1 or x2 based on channel index\n    is_x1 = c_all_idx < c1\n    x1_channel = c_all_idx\n    x2_channel = c_all_idx - c1\n\n    # Compute pointers and load data\n    x1_offsets = b_idx * x1_batch_stride + x1_channel * x1_channel_stride + h_idx * x1_h_stride + w_idx * x1_w_stride\n    x2_offsets = b_idx * x2_batch_stride + x2_channel * x2_channel_stride + h_idx * x2_h_stride + w_idx * x2_w_stride\n\n    x1_val = tl.load(x1_ptr + x1_offsets, mask=mask & is_x1, other=0.0)\n    x2_val = tl.load(x2_ptr + x2_offsets, mask=mask & (~is_x1), other=0.0)\n    \n    val = tl.where(is_x1, x1_val, x2_val)\n    val = tl.maximum(val, 0.0)  # ReLU activation\n\n    # Compute output pointer\n    output_offsets = b_idx * output_batch_stride + c_all_idx * output_channel_stride + h_idx * output_h_stride + w_idx * output_w_stride\n    tl.store(output_ptr + output_offsets, val, mask=mask)\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        out1 = self.expand1x1(x)\n        out2 = self.expand3x3(x)\n        \n        batch, c1, h, w = out1.shape\n        c2 = out2.shape[1]\n        output = torch.empty(batch, c1 + c2, h, w, device=x.device, dtype=x.dtype)\n        \n        total_elements = batch * (c1 + c2) * h * w\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        fused_relu_concat[grid](\n            out1, out2, output,\n            out1.stride(0), out1.stride(1), out1.stride(2), out1.stride(3),\n            out2.stride(0), out2.stride(1), out2.stride(2), out2.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            c1, c2, h, w,\n            total_elements,\n            BLOCK_SIZE=1024\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    COL_BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    # Process rows sequentially for better cache locality\n    for h in range(height):\n        # Process columns in contiguous blocks\n        for w_base in range(0, width, COL_BLOCK_SIZE):\n            w_offsets = w_base + tl.arange(0, COL_BLOCK_SIZE)\n            mask = w_offsets < width\n            \n            input_offset = pid_batch * input_batch_stride + \\\n                           pid_channel * input_channel_stride + \\\n                           h * input_height_stride + \\\n                           w_offsets * input_width_stride\n            values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n            total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Optimize column block size for contiguous memory access\n    col_block_size = triton.next_power_of_2(width)\n    if col_block_size > 1024:\n        col_block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        COL_BLOCK_SIZE=col_block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 3 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Linearize spatial dimensions for efficient reduction\n    spatial_size = height * width\n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    \n    # Parallel reduction initialization\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for idx in range(0, spatial_size, BLOCK_SIZE):\n        offsets = idx + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < spatial_size\n        \n        # Convert linear index to 2D coordinates\n        h_idx = offsets // width\n        w_idx = offsets % width\n        input_offsets = base_offset + h_idx * input_height_stride + w_idx * input_width_stride\n        \n        # Load and accumulate values\n        values = tl.load(input_ptr + input_offsets, mask=mask, other=0.0)\n        acc += values\n\n    # Final reduction and averaging\n    total = tl.sum(acc, axis=0)\n    avg = total / spatial_size\n    \n    # Store result\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Optimize block size for efficient parallel reduction\n    spatial_size = height * width\n    block_size = min(triton.next_power_of_2(spatial_size), 1024)\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 4 (Score: -100.0000, Type: Exploratory)\n```python\ndef linear_relu(a, weight, bias):\n    M, K = a.shape\n    # Transpose the weight to get [K, N] where N is out_features\n    weight_t = weight.t().contiguous()\n    N = weight_t.shape[1]   # out_features\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    linear_relu_kernel[grid](\n        a, weight_t, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        weight_t.stride(0), weight_t.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64,\n    )\n    return c\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 5 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef linear_relu_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < k_remaining)\n        b_mask = (offs_k[:, None] < k_remaining) & (offs_bn[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        acc += bias[None, :]\n\n    acc = tl.where(acc < 0, 0, acc)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask)\n\ndef linear_relu(a, weight, bias):\n    M, K = a.shape\n    weight_t = weight.t().contiguous()\n    N = weight_t.shape[1]\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    linear_relu_kernel[grid](\n        a, weight_t, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        weight_t.stride(0), weight_t.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64\n    )\n    return c\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        h_offsets = offsets // width\n        w_offsets = offsets % width\n        \n        mask = offsets < num_elements\n        h_mask = h_offsets < height\n        w_mask = w_offsets < width\n        mask = mask & h_mask & w_mask\n        \n        input_offset = pid_batch * input_batch_stride + \\\n                       pid_channel * input_channel_stride + \\\n                       h_offsets * input_height_stride + \\\n                       w_offsets * input_width_stride\n        values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n        total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef triton_adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    block_size = triton.next_power_of_2(height * width)\n    if block_size > 1024:\n        block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass TritonLinearReLU(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n        \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        batch, c, h, w = x.shape\n        x_flat = x.permute(0,2,3,1).reshape(-1, c).contiguous()\n        output_flat = linear_relu(x_flat, self.weight, self.bias)\n        return output_flat.reshape(batch, h, w, self.out_features).permute(0,3,1,2).contiguous()\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = TritonLinearReLU(in_channels, squeeze_channels)\n        self.expand1x1 = TritonLinearReLU(squeeze_channels, expand1x1_channels)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze(x)\n        return torch.cat([\n            self.expand1x1(x),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            TritonLinearReLU(512, num_classes),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        h_offsets = offsets // width\n        w_offsets = offsets % width\n        \n        mask = offsets < num_elements\n        h_mask = h_offsets < height\n        w_mask = w_offsets < width\n        mask = mask & h_mask & w_mask\n        \n        input_offset = pid_batch * input_batch_stride + \\\n                       pid_channel * input_channel_stride + \\\n                       h_offsets * input_height_stride + \\\n                       w_offsets * input_width_stride\n        values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n        total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Use power-of-two block size for efficient reduction\n    block_size = triton.next_power_of_2(height * width)\n    if block_size > 1024:  # Cap block size for register efficiency\n        block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a current program that has a Triton kernel for adaptive average pooling (1x1) and a SqueezeNet-like model.\n The current performance shows a speed_up of 0.958 (which is less than 1, meaning it's slower than the reference).\n We are to improve the Triton kernel to get at least 5% speedup.\n\n Analysis of the current Triton kernel for adaptive_avg_pool_kernel:\n  - It uses a grid of (batch, channels)\n  - For each (batch, channel), it iterates over the spatial dimensions (height * width) in blocks of size `BLOCK_SIZE`.\n  - For each block, it computes the linear indices, then converts to 2D (h, w) indices, and then loads the values with a mask.\n  - It accumulates the sum and then divides by the total number of elements.\n\n The current approach might be inefficient because:\n  - The conversion from linear index to 2D indices (h_offsets = offsets // width, w_offsets = offsets % width) can be expensive.\n  - The memory access pattern might not be optimal.\n\n We have seen in the \"Inspiration Programs\" that there are two main approaches:\n  1. Process the spatial dimensions by rows and then within a row by contiguous columns (as in Program 1 and Inspiration 2).\n  2. Linearize the spatial dimensions and process in one contiguous block (as in the current program and Program D2).\n\n However, note that the current program is already using the linearized approach.\n\n Another idea: Instead of having two nested loops (over the entire spatial dimension in one loop), we can use a two-level reduction.\n But note: the current kernel uses a single loop over the entire spatial dimension. This might be acceptable for small spatial sizes, but for large ones (like 224x224) the block size is capped at 1024, so we have multiple iterations.\n\n We note that the current kernel does:\n   total = 0.0\n   for i in range(0, num_elements, BLOCK_SIZE):\n        ... load a block of values and add to total\n\n Then it does a scalar division at the end.\n\n However, the reduction is done by a single thread for the entire spatial dimension per channel. This is sequential in the spatial dimension and does not take advantage of parallel reduction within a thread block.\n\n We are constrained by the grid being (batch, channels). Each program is handling one channel.\n\n How can we improve?\n  - We can try to use a parallel reduction within the thread for the spatial dimension? But note: each program is a single thread? Actually, no: Triton programs are executed by a block of threads. But the current kernel is written for a 1D grid of programs (each program is a (batch, channel)) and within the program we are using a loop.\n\n  - Alternatively, we can change the kernel to use a 2D grid for the spatial reduction? But note the problem: we are reducing the entire spatial dimension to a single value per channel. The grid is already (batch, channels). We cannot add more grid dimensions without changing the grid.\n\n  - We can use the parallel reduction capabilities within a Triton program (using the fact that we have multiple threads per block). Currently, the kernel is written as if it's a single thread (it uses a single accumulator and loops). We can instead use multiple threads in the program and then do a reduction across the threads.\n\n Proposed redesign for the kernel:\n\n  Let's change the kernel to use:\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    pid_reduction = tl.program_id(2)   # for splitting the spatial dimension\n\n  But wait, the grid is currently (batch, channels). We don't have a third dimension.\n\n  Alternatively, we can use a 1D grid and then have each block handle one (batch, channel) and use the threads in the block to parallelize the reduction over the spatial dimension.\n\n  However, the current program does not specify the number of threads per block (it's using a 1D grid and the kernel is written for a single thread per block?).\n\n  How Triton works: by default, the kernel is launched with one block per grid element, and the block has 128 threads? But we are not using any of the threads in the block for parallelism.\n\n  We can restructure the kernel to use the threads in the block to parallelize the reduction.\n\n  Steps for a parallel reduction within a block for one (batch, channel):\n    - We assign to each thread in the block a segment of the spatial dimension.\n    - Each thread computes a partial sum for its segment.\n    - Then we reduce the partial sums across the block to get the total sum.\n\n  This way, we can process the entire spatial dimension in one step (with multiple threads) and then do a fast reduction.\n\n  Let's change the kernel to use a 1D grid of (batch * channels) and then within the block, we use multiple threads to split the spatial dimension.\n\n  But note: the grid is currently (batch, channels). We can change the grid to be (batch, channels, num_blocks) but that would change the grid structure.\n\n  Alternatively, we can keep the grid as (batch, channels) and then use the block of threads to parallelize the reduction.\n\n  How to do:\n    - We set the grid as (batch, channels) and then we use a block of threads (say, BLOCK_SIZE_REDUCTION threads) for each program.\n    - The kernel will have an additional argument: BLOCK_SIZE_REDUCTION (which is the number of threads per block for reduction).\n\n  Then, each thread in the block will handle a portion of the spatial dimension.\n\n  We can do:\n    spatial_size = height * width\n    tid = tl.program_id(axis=2)   # but note: triton.jit only supports 1D or 2D grid? Actually, Triton supports up to 3D grid.\n\n  However, the grid is currently 2D (batch, channels). We can change to 3D: (batch, channels, reduction_blocks). But that would require changing the grid function.\n\n  Alternatively, we can flatten the grid: \n      grid = (batch, channels, reduction_blocks)\n  but note that the problem requires that we do not change the grid configuration? Actually, the constraint says:\n      \"\u274c NEVER modify grid configuration, output tensor shapes, or PID logic\"\n\n  So we cannot change the grid? But wait, the grid is set by the function `adaptive_avg_pool2d_1x1`. We are allowed to change the grid as long as the output is the same? However, the constraint says \"NEVER modify grid configuration\". So we must keep the grid as (batch, channels).\n\n  Therefore, we cannot use a 3D grid. We are stuck with 2D.\n\n  How to use multiple threads per block without a 3D grid?\n    - We note that each program (which is for one (batch, channel)) is executed by a block of threads. We can use the threads in the block to parallelize the reduction.\n\n  We can do:\n\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n\n    # We are in a block with BLOCK_SIZE_REDUCTION threads.\n    tid = tl.arange(0, BLOCK_SIZE_REDUCTION)\n\n    # We split the spatial dimension (height * width) into chunks of size BLOCK_SIZE_REDUCTION.\n    spatial_size = height * width\n    num_chunks = tl.cdiv(spatial_size, BLOCK_SIZE_REDUCTION)\n\n    # Each thread will accumulate its own partial sum.\n    partial = 0.0\n    for chunk_idx in range(num_chunks):\n        idx = chunk_idx * BLOCK_SIZE_REDUCTION + tid\n        if idx < spatial_size:\n            h_idx = idx // width\n            w_idx = idx % width\n            offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride + h_idx * input_height_stride + w_idx * input_width_stride\n            value = tl.load(input_ptr + offset)\n            partial += value\n\n    # Now, we reduce the partial sums across the block.\n    # We use a tree reduction.\n    total = tl.sum(partial, axis=0)   # This is a built-in that does a reduction across the block.\n\n    # Then, we have one thread (the first) write the result.\n    if tl.program_id(axis=0) == 0:   # Actually, we want the first thread in the block to write? But note: we are storing per (batch, channel). We want every block to write one value.\n        # But note: the entire block is for one (batch, channel). So we can let the first thread write.\n        # However, we can use the store instruction by all threads but with the same address? That would be inefficient. Instead, we let the first thread write.\n\n        # How to let the first thread write? We can use:\n        #   if tid == 0:\n        avg = total / spatial_size\n        output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n        tl.store(output_ptr + output_offset, avg)\n\n  However, note that the reduction `tl.sum` already aggregates across the entire block. So we only need one thread to write.\n\n  But wait: what if we have multiple warps? The `tl.sum` is a collective operation that synchronizes the block. Then we can let the first thread write.\n\n  However, we must note: the output is one value per (batch, channel). So we only write once per block.\n\n  This approach should be more efficient because we are using multiple threads to parallelize the loading and the reduction.\n\n  But note: the current kernel uses a loop that loads a contiguous block of memory. The new kernel loads non-contiguous memory? Actually, the indices are linear in the spatial dimension, so they are contiguous in the linearized space. But the original tensor is 4D. The spatial dimensions are contiguous? Yes, in a 4D tensor (N, C, H, W), the spatial dimensions are contiguous: the stride for H is stride[2] and for W is stride[3]. And the linear index we are using (h_idx, w_idx) is contiguous in the spatial part.\n\n  However, note that the entire tensor might be stored in row-major, so the linearized spatial index (h * width + w) is contiguous.\n\n  But the original tensor might not be contiguous? We are loading with strides. We assume the tensor is contiguous (or at least the strides are provided). The function `adaptive_avg_pool2d_1x1` is called with a contiguous tensor? We see in the function: the output is created as empty, and the input is passed as is. We don't enforce contiguous in the function? But note: the input tensor x is from the previous layer. We should make sure it's contiguous.\n\n  However, the current code does not make the input contiguous. Let's change the function to make the input contiguous? But that might be a separate optimization.\n\n  Alternatively, we can change the kernel to use a 2D access pattern that is more cache friendly? We are currently linearizing the spatial dimension. It might be better to process by rows? But the linearization is just an index calculation.\n\n  Let's try the parallel reduction within the block.\n\n  Steps for the new kernel:\n\n    @triton.jit\n    def adaptive_avg_pool_kernel(\n        input_ptr,\n        output_ptr,\n        input_batch_stride,\n        input_channel_stride,\n        input_height_stride,\n        input_width_stride,\n        output_batch_stride,\n        output_channel_stride,\n        height,\n        width,\n        BLOCK_SIZE_REDUCTION: tl.constexpr   # This is the number of threads per block for reduction.\n    ):\n        pid_batch = tl.program_id(0)\n        pid_channel = tl.program_id(1)\n        tid = tl.arange(0, BLOCK_SIZE_REDUCTION)\n\n        spatial_size = height * width\n        num_chunks = tl.cdiv(spatial_size, BLOCK_SIZE_REDUCTION)\n\n        # Each thread accumulates a partial sum.\n        partial = 0.0\n        for chunk_idx in range(num_chunks):\n            idx = chunk_idx * BLOCK_SIZE_REDUCTION + tid\n            if idx < spatial_size:\n                h_idx = idx // width\n                w_idx = idx % width\n                offset = pid_batch * input_batch_stride + \\\n                         pid_channel * input_channel_stride + \\\n                         h_idx * input_height_stride + \\\n                         w_idx * input_width_stride\n                partial += tl.load(input_ptr + offset)\n\n        # Reduce the partial sums in the block to get the total for this (batch, channel)\n        total = tl.sum(partial, axis=0)\n\n        # Only the first thread in the block writes the result.\n        if tid == 0:\n            avg = total / spatial_size\n            output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n            tl.store(output_ptr + output_offset, avg)\n\n  Then, in the function adaptive_avg_pool2d_1x1, we set the grid as (batch, channels) and we set the block size (number of threads per block) to a constant (say 128, 256, or 512). We can make it configurable.\n\n  However, note: the constraint says \"NEVER change @triton.jit function signatures or parameter names\". We are adding a new parameter BLOCK_SIZE_REDUCTION. We are allowed to change the kernel as long as we don't change the existing parameters? The constraint says \"NEVER change ... parameter names\" meaning we cannot rename the existing ones. We are adding a new one.\n\n  Also, the grid configuration: we are still using (batch, channels) for the grid, but we are now using a block of threads (BLOCK_SIZE_REDUCTION) for each program. So the grid configuration in terms of the number of blocks is the same.\n\n  We are also not changing the output tensor.\n\n  But note: the kernel now uses a 2D grid and a 1D block of threads. The previous kernel did not use any threads (it was scalar). This is an allowed optimization.\n\n  We must also note: the current kernel had a parameter BLOCK_SIZE (which was the block size for the loop). We are replacing that with BLOCK_SIZE_REDUCTION (the number of threads per block). We can remove the old BLOCK_SIZE.\n\n  However, the constraint says: \"NEVER change @triton.jit function signatures or parameter names\". We are changing the parameters (removing BLOCK_SIZE and adding BLOCK_SIZE_REDUCTION). But note the constraint does not say we cannot change the parameters? It says \"NEVER change ... parameter names\" meaning we cannot change the names of existing parameters? We are removing one and adding another. This might be considered a change in the signature.\n\n  Alternatively, we can keep the same parameter name? We can call it BLOCK_SIZE, but now it means the number of threads per block.\n\n  Let me re-read: \"NEVER change @triton.jit function signatures or parameter names\". So we cannot change the names of the existing parameters. We are allowed to add new ones? The example optimizations include \"Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions\", so adding new block size parameters is allowed.\n\n  But note: the constraint also says: \"NEVER remove boundary checks or out-of-bound masks\". We are adding a boundary check: `if idx < spatial_size`.\n\n  We must be cautious: the current kernel has a mask in the load. We are replacing it with a condition. But note: the condition `if idx < spatial_size` is equivalent to a mask.\n\n  However, we are loading without a mask? We do:\n        if idx < spatial_size:\n            ... load\n        else:\n            skip\n\n  But note: the Triton load operation requires a mask for out-of-bound. We are using a conditional. This is safe because the condition avoids the load for out-of-bound.\n\n  Alternatively, we can use a mask? We can do:\n\n        mask = idx < spatial_size\n        value = tl.load(input_ptr + offset, mask=mask, other=0.0)\n\n        Then we can do: partial += value\n\n  This avoids the branch and might be more efficient.\n\n  Let's change to:\n\n        idx = chunk_idx * BLOCK_SIZE_REDUCTION + tid\n        mask = idx < spatial_size\n        h_idx = idx // width\n        w_idx = idx % width\n        offset = pid_batch * input_batch_stride + \\\n                 pid_channel * input_channel_stride + \\\n                 h_idx * input_height_stride + \\\n                 w_idx * input_width_stride\n        value = tl.load(input_ptr + offset, mask=mask, other=0.0)\n        partial += value\n\n  This way, we avoid the branch and use a masked load.\n\n  Also, note: we are doing the same index calculation (h_idx, w_idx) for every thread. This might be expensive. We can precompute the base pointer for the (batch, channel) and then add the spatial part? But the calculation is necessary.\n\n  We are also doing a reduction across the block with `tl.sum(partial, axis=0)`. This is a fast operation.\n\n  Now, we must change the function adaptive_avg_pool2d_1x1 to set the block of threads (the number of threads per block). We choose a block size, say 128, 256, or 512.\n\n  We note: the spatial size for the last layer might be 13x13 (if we follow the SqueezeNet architecture) which is 169. So we don't need a very large block. But for the early layers, the spatial size is large.\n\n  We can set the block size to 256. We can also make it configurable? But for simplicity, we can set it to 256.\n\n  The grid remains the same: (batch, channels). But now we are launching each grid element with a block of 256 threads.\n\n  Let's write the new kernel and the function.\n\n  We remove the old BLOCK_SIZE and replace it with a new constant for the reduction block size (number of threads per block). We call it BLOCK_SIZE_REDUCTION.\n\n  We also note: the kernel now requires the block size to be a power of two? Because the reduction `tl.sum` might require it? Actually, Triton's `tl.sum` for a 1D array of length BLOCK_SIZE_REDUCTION should work for any size, but it's more efficient for power of two.\n\n  We can set BLOCK_SIZE_REDUCTION to 128, 256, or 512. We can choose 256.\n\n  We also note: the current kernel did a loop over the spatial dimension. The new kernel does a loop over chunks. The number of chunks is ceil(spatial_size / BLOCK_SIZE_REDUCTION). For a spatial size of 224*224=50176 and BLOCK_SIZE_REDUCTION=256, that would be 50176 / 256 = 196 chunks. This is acceptable.\n\n  We hope that the parallel reduction within the block will speed up the kernel.\n\n  Let's write the new code.\n\n  IMPORTANT: We must not change the rest of the model. We are only changing the adaptive_avg_pool2d_1x1 function and the kernel.\n\n  Steps:\n    - Rewrite the kernel to use a block of threads for reduction.\n    - Change the function adaptive_avg_pool2d_1x1 to set the number of threads per block to 256 (or a tunable value).\n\n  We can also try to tune the block size? But we are not using autotune. We can set it to 256.\n\n  We note: the constraint says \"Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions\" is allowed. But we are not using autotune here.\n\n  We'll set it to 256 for now.\n\n  We also note: the kernel now has a different signature. We are replacing the parameter BLOCK_SIZE with BLOCK_SIZE_REDUCTION. But the constraint says we cannot change the parameter names? We are removing one and adding another. However, the constraint says \"NEVER change ... parameter names\", meaning we cannot change the names of existing parameters? We are removing the existing BLOCK_SIZE and adding a new one. This might be considered a change. But note: the constraint does not explicitly forbid adding new parameters. It forbids changing the names of existing ones.\n\n  However, the existing BLOCK_SIZE is no longer used. We are removing it. But the constraint says \"NEVER change @triton.jit function signatures\", which might include the set of parameters? We must be cautious.\n\n  Alternatively, we can keep the parameter name as BLOCK_SIZE? But now it means the number of threads per block. Then we are not changing the name. We are just redefining its meaning.\n\n  But the constraint says \"NEVER change ... parameter names\". We are not changing the name, so that's allowed? However, the meaning changes. But the constraint does not specify that the meaning must remain the same.\n\n  To be safe, we can keep the same parameter name: BLOCK_SIZE, but now it represents the number of threads per block.\n\n  So we do:\n\n    @triton.jit\n    def adaptive_avg_pool_kernel(\n        ...,\n        BLOCK_SIZE: tl.constexpr   # now meaning the number of threads per block\n    ):\n\n  And then in the kernel, we use:\n\n        tid = tl.arange(0, BLOCK_SIZE)\n\n  And then the loop:\n\n        num_chunks = tl.cdiv(spatial_size, BLOCK_SIZE)\n\n  And then:\n\n        idx = chunk_idx * BLOCK_SIZE + tid\n\n  This way, the parameter name is the same.\n\n  But note: the function call in adaptive_avg_pool2d_1x1 currently passes:\n\n        BLOCK_SIZE=block_size   # which was set to the next power of two of the spatial size, capped at 1024.\n\n  Now, we are changing the meaning: we want BLOCK_SIZE to be the number of threads per block (which we fix to 256). So we set:\n\n        adaptive_avg_pool_kernel[grid](\n            ...,\n            BLOCK_SIZE=256   # fixed number of threads per block\n        )\n\n  But note: the kernel now expects BLOCK_SIZE to be the number of threads per block, not the spatial block size.\n\n  We are removing the old block_size computation.\n\n  We change the function adaptive_avg_pool2d_1x1 to:\n\n        def adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n            batch, channels, height, width = x.shape\n            output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n            grid = (batch, channels)\n\n            # We set the number of threads per block to 256.\n            # We don't need to compute the old block_size.\n            adaptive_avg_pool_kernel[grid](\n                x,\n                output,\n                x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n                output.stride(0), output.stride(1),\n                height,\n                width,\n                BLOCK_SIZE=256   # number of threads per block\n            )\n            return output\n\n  However, we must note: the kernel now uses a block of 256 threads. We must set the grid to be (batch, channels) and the block size to 256? Actually, in Triton, we set the grid (number of blocks) and the block size (number of threads per block) separately.\n\n  How do we set the block size? In the kernel launch, we can specify the number of threads per block with the `num_warps` and `num_stages`? Actually, the block size in Triton is set by the `num_warps` and the number of threads per warp (which is fixed to 32). So the total threads per block is num_warps * 32.\n\n  We cannot set the block size arbitrarily? We must set it by num_warps.\n\n  Therefore, we must set the number of warps such that the total threads per block is 256? That would be 256 / 32 = 8 warps.\n\n  We can set the kernel launch to use 8 warps.\n\n  How? In the kernel decorator, we cannot set it per launch. We set it in the grid function? Actually, the grid function returns the grid, and we can also pass the number of warps and the number of stages as metadata.\n\n  But the current kernel does not have autotune. We can set it in the launch:\n\n        adaptive_avg_pool_kernel[grid, num_warps=8](...)\n\n  So we change the launch to:\n\n        adaptive_avg_pool_kernel[grid, num_warps=8](\n            ...,\n            BLOCK_SIZE=256   # we are passing 256 as the BLOCK_SIZE, but note: the kernel uses BLOCK_SIZE as the number of threads. We must set the number of warps so that the block has 256 threads? Actually, 8 warps * 32 = 256 threads.\n\n        )\n\n  However, note: the kernel uses `tl.arange(0, BLOCK_SIZE)` and we are setting BLOCK_SIZE=256. And we are setting num_warps=8 which gives 256 threads. So it matches.\n\n  Alternatively, we can compute the number of warps from the BLOCK_SIZE? But we are setting BLOCK_SIZE=256 and then setting num_warps=8. It's fixed.\n\n  But what if we want to change the block size? We can make it a variable? For now, we fix it.\n\n  Let's write the new kernel and function.\n\n  We also note: the kernel must be launched with the correct number of threads per block. We are setting num_warps=8 for 256 threads.\n\n  We hope this will be faster.\n\n  Let's code accordingly.\n\n  Important: We must also note that the kernel now uses a reduction across the block. We are using `tl.sum(partial, axis=0)`. This is a collective operation that requires all threads to participate. So the entire block must be active.\n\n  We also note: the condition `if tid == 0` for the store is safe because only one thread writes.\n\n  We are ready to write the new code.\n\n  We are changing the kernel and the function adaptive_avg_pool2d_1x1.\n\n  We note: the rest of the model remains the same.\n\n  We are also removing the old block_size computation (the power-of-two and cap at 1024) because now we are setting BLOCK_SIZE=256 (the number of threads) and the kernel uses it as the block size for the reduction.\n\n  We must test the code for correctness and performance.\n\n  Given the constraints of the problem, we are making a small iterative change (only the kernel and the function launch).\n\n  We hope to achieve a speedup.\n\n  If we don't, we might try a different block size or a different approach.\n\n  But note: the current kernel is slow because it is sequential in the spatial dimension per thread. The new kernel uses parallelism.\n\n  We are optimistic.\n\n  Code:\n\n    @triton.jit\n    def adaptive_avg_pool_kernel(\n        input_ptr,\n        output_ptr,\n        input_batch_stride,\n        input_channel_stride,\n        input_height_stride,\n        input_width_stride,\n        output_batch_stride,\n        output_channel_stride,\n        height,\n        width,\n        BLOCK_SIZE: tl.constexpr   # Now: number of threads per block (for reduction)\n    ):\n        pid_batch = tl.program_id(0)\n        pid_channel = tl.program_id(1)\n        tid = tl.arange(0, BLOCK_SIZE)   # thread ID within the block\n\n        spatial_size = height * width\n        num_chunks = tl.cdiv(spatial_size, BLOCK_SIZE)\n\n        # Each thread will accumulate a partial sum.\n        partial = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        for chunk_idx in range(num_chunks):\n            idx = chunk_idx * BLOCK_SIZE + tid\n            mask = idx < spatial_size\n            h_idx = idx // width\n            w_idx = idx % width\n            input_offset = pid_batch * input_batch_stride + \\\n                           pid_channel * input_channel_stride + \\\n                           h_idx * input_height_stride + \\\n                           w_idx * input_width_stride\n            values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n            partial += values\n\n        # Reduce the partial sums in the block to get the total for this (batch, channel)\n        total = tl.sum(partial, axis=0)\n\n        # Only the first thread in the block writes the result.\n        if tid == 0:\n            avg = total / spatial_size\n            output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n            tl.store(output_ptr + output_offset, avg)\n\n    def adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n        batch, channels, height, width = x.shape\n        output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n        grid = (batch, channels)\n\n        # We set the kernel launch to use 8 warps (which is 256 threads) and BLOCK_SIZE=256.\n        adaptive_avg_pool_kernel[grid, num_warps=8](\n            x,\n            output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            height,\n            width,\n            BLOCK_SIZE=256\n        )\n        return output\n\n  We note: the kernel uses a loop over chunks. We hope that the Triton compiler will optimize this.\n\n  Let's hope for a speedup.\n\n  If we see that the kernel is still slow, we might consider other optimizations (like processing by rows to improve cache locality).\n\n  But for now, we try this.\n\n  We output the entire code block with the new kernel and function.\n\n  Note: We are not changing the rest of the model.\n\n  IMPORTANT: The class name must remain ModelNew.\n\n  We output the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr   # Now represents number of threads per block\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    tid = tl.arange(0, BLOCK_SIZE)  # Thread ID within block\n\n    spatial_size = height * width\n    num_chunks = tl.cdiv(spatial_size, BLOCK_SIZE)\n    \n    # Each thread accumulates partial sum\n    partial = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for chunk_idx in range(num_chunks):\n        idx = chunk_idx * BLOCK_SIZE + tid\n        mask = idx < spatial_size\n        h_idx = idx // width\n        w_idx = idx % width\n        input_offset = pid_batch * input_batch_stride + \\\n                       pid_channel * input_channel_stride + \\\n                       h_idx * input_height_stride + \\\n                       w_idx * input_width_stride\n        values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n        partial += values\n\n    # Parallel reduction across threads in block\n    total = tl.sum(partial, axis=0)\n    \n    # First thread writes final result\n    if tid == 0:\n        avg = total / spatial_size\n        output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n        tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Launch kernel with 256 threads per block (8 warps)\n    adaptive_avg_pool_kernel[grid, num_warps=8](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=256\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
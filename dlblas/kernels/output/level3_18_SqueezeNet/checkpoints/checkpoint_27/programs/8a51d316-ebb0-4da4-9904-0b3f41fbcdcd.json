{"id": "8a51d316-ebb0-4da4-9904-0b3f41fbcdcd", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 192, 'BLOCK_SIZE_N': 192, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n        # New configurations optimized for H100 Tensor Cores\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=1, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=1, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=1, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    grid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    grid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * grid_n\n    group_id = pid // num_pid_in_group\n    first_pid_in_group = group_id * num_pid_in_group\n    group_size = min(grid_m - group_id * GROUP_SIZE_M, GROUP_SIZE_M) * grid_n\n    pid_in_group = pid - first_pid_in_group\n    if pid_in_group >= group_size:\n        return\n\n    block_m = pid_in_group // grid_n\n    block_n = pid_in_group % grid_n\n    m = group_id * GROUP_SIZE_M + block_m\n    n = block_n\n\n    offs_m = (m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_n = (n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n    )\n    return c\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "08571523-66b4-4ea0-9803-6140b5228841", "generation": 3, "timestamp": 1754592732.7193818, "iteration_found": 27, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.927, "runtime_ms": 1.24, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.15, "speed_up": 0.927, "custom_timing": 1.24}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.975, "runtime_ms": 1.18, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.15, "speed_up": 0.975, "custom_timing": 1.18}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.9750\n- runtime_ms: 1.1800\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.975, 'custom_timing': 1.18}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9750, runtime_ms: 1.1800, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.975, 'custom_timing': 1.18}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9750, runtime_ms: 1.1800, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.975, 'custom_timing': 1.18}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9580, runtime_ms: 1.2000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.958, 'custom_timing': 1.2}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        h_offsets = offsets // width\n        w_offsets = offsets % width\n        \n        mask = offsets < num_elements\n        h_mask = h_offsets < height\n        w_mask = w_offsets < width\n        mask = mask & h_mask & w_mask\n        \n        input_offset = pid_batch * input_batch_stride + \\\n                       pid_channel * input_channel_stride + \\\n                       h_offsets * input_height_stride + \\\n                       w_offsets * input_width_stride\n        values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n        total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Use power-of-two block size for efficient reduction\n    block_size = triton.next_power_of_2(height * width)\n    if block_size > 1024:  # Cap block size for register efficiency\n        block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9580), Performs well on runtime_ms (1.2000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.958, 'custom_timing': 1.2})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 192, 'BLOCK_SIZE_N': 192, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    grid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    grid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * grid_n\n    group_id = pid // num_pid_in_group\n    first_pid_in_group = group_id * num_pid_in_group\n    group_size = min(grid_m - group_id * GROUP_SIZE_M, GROUP_SIZE_M) * grid_n\n    pid_in_group = pid - first_pid_in_group\n    if pid_in_group >= group_size:\n        return\n\n    block_m = pid_in_group // grid_n\n    block_n = pid_in_group % grid_n\n    m = group_id * GROUP_SIZE_M + block_m\n    n = block_n\n\n    offs_m = (m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_n = (n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n    )\n    return c\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9750), Performs well on runtime_ms (1.1800), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:3', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.975, 'custom_timing': 1.18})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 192, 'BLOCK_SIZE_N': 192, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n        # New optimized configurations\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    grid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    grid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * grid_n\n    group_id = pid // num_pid_in_group\n    first_pid_in_group = group_id * num_pid_in_group\n    group_size = min(grid_m - group_id * GROUP_SIZE_M, GROUP_SIZE_M) * grid_n\n    pid_in_group = pid - first_pid_in_group\n    if pid_in_group >= group_size:\n        return\n\n    block_m = pid_in_group // grid_n\n    block_n = pid_in_group % grid_n\n    m = group_id * GROUP_SIZE_M + block_m\n    n = block_n\n\n    offs_m = (m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_n = (n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n    )\n    return c\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9750), Performs well on runtime_ms (1.1800), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.975, 'custom_timing': 1.18})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    \n    # Process elements in chunks using vectorized loads\n    VEC_SIZE = 4\n    num_vecs = num_elements // VEC_SIZE\n    for i in range(0, num_vecs * VEC_SIZE, VEC_SIZE):\n        offsets = i + tl.arange(0, VEC_SIZE)\n        values = tl.load(input_ptr + input_offset + offsets)\n        total += tl.sum(values)\n    \n    # Process remaining elements\n    remainder = num_elements % VEC_SIZE\n    if remainder > 0:\n        offsets = num_vecs * VEC_SIZE + tl.arange(0, remainder)\n        mask = tl.arange(0, VEC_SIZE) < remainder\n        values = tl.load(input_ptr + input_offset + offsets, mask=mask, other=0.0)\n        total += tl.sum(values)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Use exact spatial size for block processing\n    block_size = min(height * width, 1024)\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr,\n    REDUCE_BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    pid_reduce = tl.program_id(2)\n    \n    # Calculate offsets for this reduction block\n    reduce_start = pid_reduce * REDUCE_BLOCK_SIZE\n    offsets = reduce_start + tl.arange(0, REDUCE_BLOCK_SIZE)\n    h_offsets = offsets // width\n    w_offsets = offsets % width\n    \n    # Create masks for valid elements\n    spatial_mask = offsets < (height * width)\n    h_mask = h_offsets < height\n    w_mask = w_offsets < width\n    mask = spatial_mask & h_mask & w_mask\n    \n    # Compute input pointer offsets\n    input_offset = pid_batch * input_batch_stride + \\\n                   pid_channel * input_channel_stride + \\\n                   h_offsets * input_height_stride + \\\n                   w_offsets * input_width_stride\n                   \n    # Load values with masking\n    values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n    \n    # Efficient reduction within the block\n    partial_sum = tl.sum(values, axis=0)\n    \n    # Store partial sum in global memory for final reduction\n    partial_ptr = output_ptr + pid_batch * output_batch_stride + \\\n                  pid_channel * output_channel_stride + \\\n                  pid_reduce\n    tl.store(partial_ptr, partial_sum)\n\n@triton.jit\ndef reduce_partial_sums(\n    partial_ptr,\n    output_ptr,\n    num_partials,\n    output_batch_stride,\n    output_channel_stride,\n    total_elements: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Pointer to partial sums for this channel\n    partials_ptr = partial_ptr + pid_batch * output_batch_stride + \\\n                   pid_channel * output_channel_stride\n    \n    # Accumulate partial sums\n    accumulator = 0.0\n    for i in range(0, num_partials, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < num_partials\n        partials = tl.load(partials_ptr + offsets, mask=mask, other=0.0)\n        accumulator += tl.sum(partials, axis=0)\n    \n    # Compute average and store final result\n    avg = accumulator / total_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    total_elements = height * width\n    \n    # Tuned parameters\n    REDUCE_BLOCK_SIZE = 2048\n    num_partials = triton.cdiv(total_elements, REDUCE_BLOCK_SIZE)\n    \n    # Allocate intermediate buffer for partial sums\n    partials = torch.empty((batch, channels, num_partials), device=x.device, dtype=x.dtype)\n    \n    # Configure kernel grids\n    reduce_grid = (batch, channels, num_partials)\n    reduce_block_size = min(REDUCE_BLOCK_SIZE, total_elements)\n    \n    # First kernel: compute partial sums\n    adaptive_avg_pool_kernel[reduce_grid](\n        x,\n        partials,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        partials.stride(0), partials.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=1024,\n        REDUCE_BLOCK_SIZE=reduce_block_size,\n        num_warps=8\n    )\n    \n    # Second kernel: reduce partial sums\n    reduce_grid = (batch, channels)\n    reduce_partial_sums[reduce_grid](\n        partials,\n        output,\n        num_partials,\n        output.stride(0), output.stride(1),\n        total_elements,\n        BLOCK_SIZE=1024,\n        num_warps=4\n    )\n    \n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        h_offsets = offsets // width\n        w_offsets = offsets % width\n        \n        mask = offsets < num_elements\n        h_mask = h_offsets < height\n        w_mask = w_offsets < width\n        mask = mask & h_mask & w_mask\n        \n        input_offset = pid_batch * input_batch_stride + \\\n                       pid_channel * input_channel_stride + \\\n                       h_offsets * input_height_stride + \\\n                       w_offsets * input_width_stride\n        values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n        total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Use power-of-two block size for efficient reduction\n    block_size = triton.next_power_of_2(height * width)\n    if block_size > 1024:  # Cap block size for register efficiency\n        block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    # Process entire rows at once for contiguous memory access\n    for h in range(height):\n        # Load entire row in one operation\n        w_offsets = tl.arange(0, width)\n        input_offset = pid_batch * input_batch_stride + \\\n                       pid_channel * input_channel_stride + \\\n                       h * input_height_stride + \\\n                       w_offsets * input_width_stride\n        row = tl.load(input_ptr + input_offset, mask=None)\n        total += tl.sum(row, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Use power-of-two block size for efficient reduction\n    block_size = triton.next_power_of_2(height * width)\n    if block_size > 1024:  # Cap block size for register efficiency\n        block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 192, 'BLOCK_SIZE_N': 192, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n        # New optimized configurations\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    grid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    grid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * grid_n\n    group_id = pid // num_pid_in_group\n    first_pid_in_group = group_id * num_pid_in_group\n    group_size = min(grid_m - group_id * GROUP_SIZE_M, GROUP_SIZE_M) * grid_n\n    pid_in_group = pid - first_pid_in_group\n    if pid_in_group >= group_size:\n        return\n\n    block_m = pid_in_group // grid_n\n    block_n = pid_in_group % grid_n\n    m = group_id * GROUP_SIZE_M + block_m\n    n = block_n\n\n    offs_m = (m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_n = (n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n    )\n    return c\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr,\n    REDUCE_BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    pid_reduce = tl.program_id(2)\n    \n    # Calculate offsets for this reduction block\n    reduce_start = pid_reduce * REDUCE_BLOCK_SIZE\n    offsets = reduce_start + tl.arange(0, REDUCE_BLOCK_SIZE)\n    h_offsets = offsets // width\n    w_offsets = offsets % width\n    \n    # Create masks for valid elements\n    spatial_mask = offsets < (height * width)\n    h_mask = h_offsets < height\n    w_mask = w_offsets < width\n    mask = spatial_mask & h_mask & w_mask\n    \n    # Compute input pointer offsets\n    input_offset = pid_batch * input_batch_stride + \\\n                   pid_channel * input_channel_stride + \\\n                   h_offsets * input_height_stride + \\\n                   w_offsets * input_width_stride\n                   \n    # Load values with masking\n    values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n    \n    # Efficient reduction within the block\n    partial_sum = tl.sum(values, axis=0)\n    \n    # Store partial sum in global memory for final reduction\n    partial_ptr = output_ptr + pid_batch * output_batch_stride + \\\n                  pid_channel * output_channel_stride + \\\n                  pid_reduce\n    tl.store(partial_ptr, partial_sum)\n\n@triton.jit\ndef reduce_partial_sums(\n    partial_ptr,\n    output_ptr,\n    num_partials,\n    output_batch_stride,\n    output_channel_stride,\n    total_elements: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Pointer to partial sums for this channel\n    partials_ptr = partial_ptr + pid_batch * output_batch_stride + \\\n                   pid_channel * output_channel_stride\n    \n    # Accumulate partial sums\n    accumulator = 0.0\n    for i in range(0, num_partials, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < num_partials\n        partials = tl.load(partials_ptr + offsets, mask=mask, other=0.0)\n        accumulator += tl.sum(partials, axis=0)\n    \n    # Compute average and store final result\n    avg = accumulator / total_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    total_elements = height * width\n    \n    # Tuned parameters\n    REDUCE_BLOCK_SIZE = 2048\n    num_partials = triton.cdiv(total_elements, REDUCE_BLOCK_SIZE)\n    \n    # Allocate intermediate buffer for partial sums\n    partials = torch.empty((batch, channels, num_partials), device=x.device, dtype=x.dtype)\n    \n    # Configure kernel grids\n    reduce_grid = (batch, channels, num_partials)\n    reduce_block_size = min(REDUCE_BLOCK_SIZE, total_elements)\n    \n    # First kernel: compute partial sums\n    adaptive_avg_pool_kernel[reduce_grid](\n        x,\n        partials,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        partials.stride(0), partials.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=1024,\n        REDUCE_BLOCK_SIZE=reduce_block_size,\n        num_warps=8\n    )\n    \n    # Second kernel: reduce partial sums\n    reduce_grid = (batch, channels)\n    reduce_partial_sums[reduce_grid](\n        partials,\n        output,\n        num_partials,\n        output.stride(0), output.stride(1),\n        total_elements,\n        BLOCK_SIZE=1024,\n        num_warps=4\n    )\n    \n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    \n    # Process elements in chunks using vectorized loads\n    VEC_SIZE = 4\n    num_vecs = num_elements // VEC_SIZE\n    for i in range(0, num_vecs * VEC_SIZE, VEC_SIZE):\n        offsets = i + tl.arange(0, VEC_SIZE)\n        values = tl.load(input_ptr + input_offset + offsets)\n        total += tl.sum(values)\n    \n    # Process remaining elements\n    remainder = num_elements % VEC_SIZE\n    if remainder > 0:\n        offsets = num_vecs * VEC_SIZE + tl.arange(0, remainder)\n        mask = tl.arange(0, VEC_SIZE) < remainder\n        values = tl.load(input_ptr + input_offset + offsets, mask=mask, other=0.0)\n        total += tl.sum(values)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Use exact spatial size for block processing\n    block_size = min(height * width, 1024)\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 192, 'BLOCK_SIZE_N': 192, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    grid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    grid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * grid_n\n    group_id = pid // num_pid_in_group\n    first_pid_in_group = group_id * num_pid_in_group\n    group_size = min(grid_m - group_id * GROUP_SIZE_M, GROUP_SIZE_M) * grid_n\n    pid_in_group = pid - first_pid_in_group\n    if pid_in_group >= group_size:\n        return\n\n    block_m = pid_in_group // grid_n\n    block_n = pid_in_group % grid_n\n    m = group_id * GROUP_SIZE_M + block_m\n    n = block_n\n\n    offs_m = (m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_n = (n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n    )\n    return c\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a Triton matmul kernel that is currently achieving a speedup of 0.975 (i.e., slower than the reference PyTorch implementation).\n The goal is to improve the performance (speed_up) to at least 1.05 (5% improvement) and ideally beyond.\n\n Analysis of the current kernel:\n  - The kernel uses an autotuner with several configurations.\n  - The kernel uses a grouped program ID mapping to balance work among SMs.\n  - The kernel loads blocks of A and B and computes a block dot product.\n\n Potential areas for improvement:\n  1. The current kernel uses a loop over K with a fixed step of BLOCK_SIZE_K. We are loading A and B in blocks and using tl.dot.\n  2. The current kernel uses tl.dot with allow_tf32=True. We are using FP32 accumulators.\n\n However, note that the hardware is NVIDIA H100 (SM8.9) which has Tensor Cores for FP16 (and TF32). The reference PyTorch implementation might be using cuBLAS which uses Tensor Cores effectively.\n\n The current kernel is using:\n   accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n   and then:\n        accumulator += tl.dot(a, b, allow_tf32=True)\n\n But note: the inputs `a` and `b` are in the same dtype as the input tensors (which is not specified in the code, but typically for matmul we expect float32).\n\n However, the problem says the hardware has \"FP16 Tensor Core 32\u00d716\u00d716\". We might consider converting the inputs to FP16 to use Tensor Cores more efficiently?\n\n But note: the reference PyTorch code (not shown) might be in float32. We must maintain numerical correctness.\n\n Alternatively, we can use the tf32 data type which is designed for Tensor Cores on Ampere and later. The current kernel uses `allow_tf32=True` which should use tf32 if the inputs are float32.\n\n However, the autotuner configurations we have might not be optimal.\n\n Another idea: the current kernel does not use any shared memory or software pipelining. The Triton dot operation internally uses Tensor Cores when possible, so that might be handled.\n\n But note: the performance is currently worse than PyTorch (0.975x). We must improve.\n\n Steps to consider:\n\n 1. Adjust the autotuner configurations to include block sizes that are multiples of 16 (for Tensor Core efficiency) and also consider larger blocks to improve occupancy.\n\n 2. We note that the current kernel uses a 2D grid for the reduction in K. We might try to increase the BLOCK_SIZE_K to reduce the number of steps and improve cache reuse? However, the current BLOCK_SIZE_K is at most 64. We might try 128 or 256?\n\n 3. We can also try to use the following:\n      - Use FP16 if the inputs are FP16? But the problem does not specify. However, the reference PyTorch code is in the dataset and we are not told the dtype.\n\n    But note: the current kernel uses `tl.float32` for accumulator and the inputs are not converted. We must assume the input tensors are float32? Because the reference PyTorch matmul is typically in float32.\n\n 4. Another idea: use a different tiling strategy. The current kernel uses a 1D grid of program IDs and then groups by GROUP_SIZE_M. This is standard.\n\n 5. We can also try to optimize the memory access by vectorizing loads. The current kernel uses scalar loads? Actually, the tl.dot operation internally uses vectorized loads.\n\n 6. We can try to adjust the number of warps and stages in the autotuner configurations.\n\n 7. We can also try to use the `num_ctas` (number of CTAs per cluster) for H100 to improve.\n\n However, note the constraints: \n   - We cannot change the function signature of the kernel.\n\n Since we are allowed to change the autotuner configurations and the kernel body (as long as we don't change the signature and maintain correctness), we can try:\n\n   a) Adding more configurations that have larger block sizes and are multiples of 16 (for Tensor Cores). For example, BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=64 is a multiple of 16? 128 and 64 are multiples of 16, so that should be okay.\n\n   b) We can also try to use the warp specialization for Tensor Cores? But Triton does not currently expose that.\n\n   c) We can try to increase the BLOCK_SIZE_K to 128 or 256 to reduce the number of steps and improve cache locality.\n\n   d) We can also try to use the `EVENLY_DIVISIBLE` optimization? The current kernel uses masking in the inner loop. If we know that K is a multiple of BLOCK_SIZE_K, we can avoid masking? But we don't know that. However, we can add an option to the kernel to handle even divisibility? But note the constraint: we cannot change the function signature. So we must avoid.\n\n   e) We can try to pre-fetch the next block of A and B to hide memory latency.\n\n 8. Another idea: use the dot product with accumulation in FP32 but with FP16 inputs? But we are not sure about the input dtype.\n\n Since the problem does not specify the dtype, we must assume the input tensors are float32. But we can convert them to float16 in the kernel? That would require changing the dtype of the accumulator? And we might lose precision.\n\n However, note the reference PyTorch code is in float32. We must match. So we cannot change the dtype arbitrarily.\n\n 9. We can try to use the `out_dtype` of tl.dot to be float32 and set the input a and b to float16? But that would require converting the loaded data to float16. We can do:\n\n        a = tl.load(a_ptrs, mask=...).to(tl.float16)\n        b = tl.load(b_ptrs, mask=...).to(tl.float16)\n\n    and then do the dot product in float16 but accumulate in float32? Actually, tl.dot can take two float16 and output float32. But note: we are using `allow_tf32=True` which is for float32 inputs.\n\n    Instead, we can use:\n\n        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n\n    and set the a and b to float16. But note: converting to float16 might cause precision loss.\n\n 10. Alternatively, we can use the tf32 which is a 19-bit format. We are already using `allow_tf32=True` for the dot product when the inputs are float32. So that should be using Tensor Cores.\n\n Given the above, I think the main issue might be the autotuner configurations and the block sizes.\n\n Let's try to add more configurations that are known to be efficient on H100 for tensor cores.\n\n According to the Tensor Core requirements, the dimensions should be multiples of 16 (or 8 for some). Also, the BLOCK_SIZE_K should be at least 32.\n\n We can add configurations such as:\n   - BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64 (this is a multiple of 16 and 32, and 64 is a common size)\n   - BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=64\n   - BLOCK_SIZE_M=256, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64\n   - BLOCK_SIZE_M=64, BLOCK_SIZE_N=256, BLOCK_SIZE_K=64\n   - BLOCK_SIZE_M=128, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64\n   - BLOCK_SIZE_M=64, BLOCK_SIZE_N=128, BLOCK_SIZE_K=64\n\n Also, note that the GROUP_SIZE_M might affect the load balancing. We can try to adjust that as well.\n\n Additionally, we can try to set the `num_warps` to 4 or 8 and the `num_stages` to 3 or 4.\n\n We are going to add a few more configurations and also change the existing ones to include more that are multiples of 16.\n\n However, note that the current kernel has a loop over K. We can also try to unroll the loop? Triton might do that automatically.\n\n Another idea: use the `EVENLY_DIVISIBLE` trick by having a separate kernel for when K is a multiple of BLOCK_SIZE_K? But we cannot change the kernel signature. We can instead compute the number of steps and if the last block is not full, we mask. That is what we are already doing.\n\n Alternatively, we can try to use a different memory order? The current kernel uses row-major for A and column-major for B? Actually, we are using the strides provided.\n\n Since we are allowed to change the kernel body, we can try to pre-fetch the next block of A and B to overlap memory and computation.\n\n However, the current Triton kernel does not have explicit async copies. We can use the `tl.advance` to move the pointers and then do:\n\n        a_next = tl.load(a_ptrs_next, ...)\n        b_next = tl.load(b_ptrs_next, ...)\n\n    and then while computing the current dot product, we pre-fetch the next.\n\n But note: Triton does not have async copies at the language level. The compiler might handle it.\n\n Given the complexity, we start by adjusting the autotuner and increasing the BLOCK_SIZE_K to 128 for some configurations.\n\n Let's update the autotuner configs to include:\n\n   - Configurations with BLOCK_SIZE_K=128 (if K is large, it will reduce the number of steps by half compared to 64)\n   - Configurations with BLOCK_SIZE_M and BLOCK_SIZE_N that are multiples of 64 or 128 (to increase the work per block and improve Tensor Core utilization)\n\n Also, we note that the current kernel does not use any shared memory. The Triton dot operation uses shared memory internally? Probably.\n\n We are going to add the following configurations:\n\n   triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n   triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n   triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n   triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n   triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n   triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n\n But note: we must be cautious about the shared memory usage. The block size in K is 128, and we have M and N blocks. The shared memory required per block is (BLOCK_SIZE_M * BLOCK_SIZE_K + BLOCK_SIZE_K * BLOCK_SIZE_N) * sizeof(float). For BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=128, that would be (128*128 + 128*128)*4 = 128KB per block. The H100 has 228KB of shared memory per SM? So it might be too big if we have multiple blocks per SM.\n\n Therefore, we must reduce the number of blocks per SM? The `num_stages` parameter in Triton controls the number of blocks that are simultaneously scheduled per SM. For the 128x128x128 block, we set num_stages=3 -> 3 * 128KB = 384KB which exceeds the shared memory. So we must set num_stages=1 or 2.\n\n Alternatively, we can set num_stages=1 for that configuration.\n\n Let's adjust the configs accordingly.\n\n However, note that the kernel might not use shared memory for the entire block? The Triton dot operation might use registers and Tensor Cores without shared memory? I'm not sure.\n\n Given the complexity and the fact that we cannot test immediately, we will add a few configurations with BLOCK_SIZE_K=128 and set num_stages=1 or 2 to avoid excessive shared memory usage.\n\n We'll also keep the existing configs and add the new ones.\n\n Additionally, we can try to change the inner loop to use a larger step? We are already stepping by BLOCK_SIZE_K.\n\n Since we are allowed to change the kernel body, we can try to use a double buffering technique to preload the next block while computing the current one. This might hide the memory latency.\n\n How to do double buffering in Triton?\n\n   We can split the loop into two parts: one for the first iteration and then the rest with a preload.\n\n   Example:\n\n        # Load first block\n        a = tl.load(a_ptrs, mask=mask_k, other=0.0)\n        b = tl.load(b_ptrs, mask=mask_k, other=0.0)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n        for k in range(BLOCK_SIZE_K, K, BLOCK_SIZE_K):\n            # Prefetch next block\n            a_next = tl.load(a_ptrs, mask=mask_k_next, other=0.0)\n            b_next = tl.load(b_ptrs, mask=mask_k_next, other=0.0)\n\n            accumulator += tl.dot(a, b, allow_tf32=True)\n\n            a = a_next\n            b = b_next\n\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n\n        # Last iteration\n        accumulator += tl.dot(a, b, allow_tf32=True)\n\n   However, note that the mask for the next block must be computed. We can compute the mask for the next step as:\n\n        mask_k_next = (offs_k[None, :] < K - k - BLOCK_SIZE_K)   # for a\n        # But note: we are using the same offs_k? Actually, we are moving the pointers.\n\n   Alternatively, we can avoid masking by ensuring that we are not going out of bounds? But we cannot because K might not be divisible.\n\n   This double buffering would require more registers.\n\n Given the complexity and the fact that we are limited by the number of lines we can change, we decide to focus on the autotuner and block sizes.\n\n We will:\n\n   - Add new configurations with BLOCK_SIZE_K=128 and adjust num_stages to 1 or 2 to avoid shared memory pressure.\n   - Also, we will try to use larger block sizes for M and N to improve Tensor Core utilization.\n\n   - We will also try to set the GROUP_SIZE_M to 16 for some configurations to improve load balancing.\n\n   - We also note that the H100 has 144 SMs, so we want to have enough blocks to fill the SMs.\n\n We are going to update the autotuner configs as follows:\n\n   We keep the existing ones and add:\n\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=1, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=1, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=1, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n\n   We also add a configuration with very large blocks but only 2 stages and 4 warps to avoid resource issues.\n\n   We also note that the current kernel has a loop:\n\n        for k in range(0, K, BLOCK_SIZE_K):\n\n   We change this loop to:\n\n        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K) * BLOCK_SIZE_K, BLOCK_SIZE_K):\n\n   to avoid the subtraction in the mask? Actually, the current mask is:\n\n        mask_a = offs_k[None, :] < K - k   --> This can be replaced by: offs_k[None, :] < K - k, but we can precompute the remaining K.\n\n   Alternatively, we can precompute the number of steps and avoid the subtraction? But the subtraction is cheap.\n\n   We leave the loop and masking as is.\n\n Let's update the autotuner and also add the new configurations.\n\n We hope that the new configurations with larger BLOCK_SIZE_K will reduce the number of steps and improve performance.\n\n We also note that the current kernel does not use vectorized loads. We can try to vectorize the loads? But the tl.dot operation might already be using vectorized memory access.\n\n Given the constraints (change <=20 lines), we focus on the autotuner and the kernel loop structure.\n\n However, we are also allowed to change the kernel body. We will try to implement the double buffering to hide memory latency.\n\n Since we are limited by the number of lines, we will implement a simple double buffering that preloads the next block at the beginning of the loop.\n\n Steps:\n\n   We preload the first block outside the loop, then in the loop we preload the next block and then compute the current block.\n\n   We do:\n\n        k0 = 0\n        a = tl.load(a_ptrs, mask=mask_a0, other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b0, other=0.0)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n        for k in range(BLOCK_SIZE_K, K, BLOCK_SIZE_K):\n            # Prefetch next\n            a_next = tl.load(a_ptrs, mask=(offs_k[None, :] < K - k), other=0.0)\n            b_next = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k), other=0.0)\n\n            accumulator += tl.dot(a, b, allow_tf32=True)\n\n            a = a_next\n            b = b_next\n\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n\n        accumulator += tl.dot(a, b, allow_tf32=True)\n\n   But note: the mask for the next block is computed using `k` which is the next starting index? Actually, we are at step `k`, and we are going to load for the next step `k + BLOCK_SIZE_K`. The remaining elements are `K - k - BLOCK_SIZE_K`? Actually, the mask for the next block should be:\n\n        mask_a_next = (offs_k[None, :] < K - k)   # because k is the current start of the next block? Actually, in the loop, k is the next block start.\n\n   But wait, in the loop we are at step `k` (which is the start of the next block) and we are about to load the block starting at `k`. The remaining elements from `k` is `K - k`. So the mask is the same as the current mask.\n\n   However, note that the mask condition in the current kernel is:\n\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n\n   In the double buffering, we want to preload the next block at the beginning of the loop. So at the start of the loop, we are at step `k` (which is the current block we are about to compute, but we have already loaded it in the previous iteration). Then we preload the next block (for the next iteration) and then compute the current block.\n\n   Actually, we are going to restructure the loop:\n\n        Step 1: Load the first block (for k0=0)\n        Step 2: For the next block (starting at k1 = BLOCK_SIZE_K) until the end, in steps of BLOCK_SIZE_K:\n                   Preload the block at k1 (which will be used in the next iteration)\n                   Compute the current block (which we loaded in the previous iteration or initially)\n                   Update pointers\n        Step 3: Compute the last block.\n\n   The mask for the preload at step k1 is: (offs_k[None, :] < K - k1) for A and similarly for B.\n\n   We must compute the mask for the next block. We can do:\n\n        next_k = k + BLOCK_SIZE_K\n        mask_a_next = (offs_k[None, :] < K - next_k)   # ??? This doesn't look right.\n\n   Actually, the mask condition is independent of the step index? We are using:\n\n        mask = (offs_k[None, :] < K - k)   for A, but note that k is the start of the current block.\n\n   For the next block, we are at k_next = k + BLOCK_SIZE_K, so the mask would be (offs_k[None, :] < K - (k+BLOCK_SIZE_K))\n\n   But note: the current mask condition in the kernel is:\n\n        mask = (offs_k[None, :] < K - k)   for A? Actually, the current mask condition is:\n\n            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n\n        Here, `k` is the current start index in the loop. The condition is: the offset in K dimension (which is from 0 to BLOCK_SIZE_K) should be less than the remaining elements (K - k). So the condition is: for each element in the block, if the offset in the block (which is from 0 to BLOCK_SIZE_K-1) is less than (K - k) then it is valid.\n\n   For the next block, the start index is `k + BLOCK_SIZE_K`, so the condition would be: (offs_k[None, :] < K - (k+BLOCK_SIZE_K))\n\n   We can precompute that mask.\n\n   However, note that the inner loop might become more complex and we are limited by the number of lines we can change.\n\n Given the complexity and the line limit, we decide to only change the autotuner configurations in this step and hope that the larger block sizes in K and the new configurations will help.\n\n If we have room, we might add one or two configurations that are known to be good on H100.\n\n Let's write the updated autotuner configs and also change the kernel to use the double buffering? We are allowed up to 20 lines.\n\n We decide to try the double buffering because it is a known optimization for memory-bound kernels.\n\n We will change the inner loop to double buffering. We note that we must compute two masks: one for the current block and one for the next? Actually, the current block was loaded in the previous iteration, so we don't need to mask again? We already have the values.\n\n But the computation of the mask for the next block is the same as the current mask computation in the original kernel, just at the next index.\n\n We will do:\n\n    # inside the kernel, after initializing the pointers and offsets:\n\n    # Preload the first block\n    k = 0\n    mask_a = offs_k[None, :] < K - k\n    mask_b = offs_k[:, None] < K - k   # Note: the original kernel uses two different masks? Actually, for A it's [BLOCK_SIZE_M, BLOCK_SIZE_K] and for B it's [BLOCK_SIZE_K, BLOCK_SIZE_N]\n    a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n    b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Loop over K\n    for k in range(BLOCK_SIZE_K, K, BLOCK_SIZE_K):\n        # Prefetch next block\n        mask_a_next = offs_k[None, :] < K - k\n        mask_b_next = offs_k[:, None] < K - k\n        a_next = tl.load(a_ptrs, mask=mask_a_next, other=0.0)\n        b_next = tl.load(b_ptrs, mask=mask_b_next, other=0.0)\n\n        accumulator += tl.dot(a, b, allow_tf32=True)\n\n        a = a_next\n        b = b_next\n\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    accumulator += tl.dot(a, b, allow_tf32=True)\n\n We replace the original loop with this double buffered loop.\n\n But note: the original loop was:\n\n        for k in range(0, K, BLOCK_SIZE_K):\n            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n            accumulator += tl.dot(a, b, allow_tf32=True)\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n\n We are replacing that with the double buffered version.\n\n However, note that the mask for the next block uses `k` which is the start of the next block? In the loop, `k` runs from BLOCK_SIZE_K to K in steps of BLOCK_SIZE_K. The condition is: `offs_k[None, :] < K - k` which is the same condition as the original.\n\n This change might help by overlapping the load of the next block with the computation of the current block? But Triton's compiler might already be doing that.\n\n We are going to do both: update the autotuner and change the inner loop to double buffering.\n\n Given the line constraint, we count the lines:\n\n   Original loop: 5 lines\n   Double buffered: 15 lines? We must be cautious.\n\n Let's write the double buffered loop in a compact form.\n\n We do:\n\n    # Preload the first block\n    k0 = 0\n    mask_a = offs_k[None, :] < K - k0\n    mask_b = offs_k[:, None] < K - k0\n    a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n    b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Inner loop\n    num_blocks = tl.cdiv(K, BLOCK_SIZE_K)\n    for kb in range(1, num_blocks):\n        k = kb * BLOCK_SIZE_K\n        # Prefetch next block\n        mask_a_next = offs_k[None, :] < K - k\n        mask_b_next = offs_k[:, None] < K - k\n        a_next = tl.load(a_ptrs, mask=mask_a_next, other=0.0)\n        b_next = tl.load(b_ptrs, mask=mask_b_next, other=0.0)\n\n        # Compute current block\n        accumulator += tl.dot(a, b, allow_tf32=True)\n\n        # Advance pointers\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n        # Swap\n        a = a_next\n        b = b_next\n\n    # Last block\n    accumulator += tl.dot(a, b, allow_tf32=True)\n\n But note: the condition `offs_k[None, :] < K - k` might be for the entire block? It is the same as the original.\n\n We are replacing the original loop with this. We hope that the Triton compiler can schedule the loads earlier.\n\n Given the complexity and the line limit, and since we are only allowed 20 lines, we note that the autotuner update might take 10 lines (adding new configs) and the double buffering might take 15 lines. That would be 25 lines. We must choose.\n\n We decide to do:\n\n   - Update the autotuner with 7 new configurations (we remove some of the existing ones to make room? But we are allowed to change up to 20 lines, and the autotuner configs are within the 20 lines? Actually, the autotuner configs are 20 lines? We are going to count.\n\n   Alternatively, we can do the autotuner update and the double buffering in two separate evolutions? But the problem says we can do up to 20 lines per revision.\n\n   We are going to do only the autotuner update in this step and see if we get the speedup.\n\n   We will add the new configurations and keep the double buffering for the next step if needed.\n\n   We will also try to increase the number of warps for some configurations to 8 warps (which is 64 threads) because the H100 SM is large.\n\n   We are going to change the autotuner configs to:\n\n        configs=[\n            # Existing configs (7)\n            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n            triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n            triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n            triton.Config({'BLOCK_SIZE_M': 192, 'BLOCK_SIZE_N': 192, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n            # New configs (6)\n            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=1, num_warps=8),\n            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=1, num_warps=8),\n            triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=1, num_warps=8),\n            triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n            # Also add a config with large blocks and num_stages=2 for shared memory capacity\n            triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=8),\n        ]\n\n   This is 13 configs. We replaced the existing 7 with 7 and added 6 new ones? Actually, we are adding 6 new and one more (the last one) so total 14.\n\n   We also change the key to include K? But it is already ['M','N','K'].\n\n   We hope that the autotuner will pick a better configuration.\n\n   We also note that the current kernel has a line:\n\n        group_size = min(grid_m - group_id * GROUP_SIZE_M, GROUP_SIZE_M) * grid_n\n\n   This is correct.\n\n   We leave the kernel body unchanged.\n\n We will output the entire kernel with the updated autotuner configs.\n\n If the performance does not improve in the next step, we can try the double buffering.\n\n Let's do this step.\n\n Note: We are not changing the kernel body, only the autotuner configs.\n\n We are also not changing the function signature.\n\n We output the entire code block.\n</think>\nTo improve the performance of the Triton matrix multiplication kernel, several key optimizations have been implemented. The autotuner configurations have been expanded to include larger block sizes (particularly for the K dimension) that better utilize NVIDIA H100 Tensor Cores. New configurations with BLOCK_SIZE_K=128 reduce the number of memory loads and better exploit data reuse. The number of stages and warps have been carefully balanced to maximize occupancy while respecting shared memory constraints. Additionally, tensor core utilization is improved by ensuring block dimensions are multiples of 16 where possible.\n\nHere is the optimized implementation:\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 192, 'BLOCK_SIZE_N': 192, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n        # New configurations optimized for H100 Tensor Cores\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=1, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=1, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=1, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    grid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    grid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * grid_n\n    group_id = pid // num_pid_in_group\n    first_pid_in_group = group_id * num_pid_in_group\n    group_size = min(grid_m - group_id * GROUP_SIZE_M, GROUP_SIZE_M) * grid_n\n    pid_in_group = pid - first_pid_in_group\n    if pid_in_group >= group_size:\n        return\n\n    block_m = pid_in_group // grid_n\n    block_n = pid_in_group % grid_n\n    m = group_id * GROUP_SIZE_M + block_m\n    n = block_n\n\n    offs_m = (m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_n = (n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)\n        accumulator += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    tl.store(c_ptrs, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\ndef matmul(a, b):\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n    )\n    return c\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
{"id": "9eba8364-6d82-41f3-b92b-7d345e595195", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    \n    # Process elements in chunks using vectorized loads\n    VEC_SIZE = 4\n    num_vecs = num_elements // VEC_SIZE\n    for i in range(0, num_vecs * VEC_SIZE, VEC_SIZE):\n        offsets = i + tl.arange(0, VEC_SIZE)\n        values = tl.load(input_ptr + input_offset + offsets)\n        total += tl.sum(values)\n    \n    # Process remaining elements\n    remainder = num_elements % VEC_SIZE\n    if remainder > 0:\n        offsets = num_vecs * VEC_SIZE + tl.arange(0, remainder)\n        mask = tl.arange(0, VEC_SIZE) < remainder\n        values = tl.load(input_ptr + input_offset + offsets, mask=mask, other=0.0)\n        total += tl.sum(values)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Use exact spatial size for block processing\n    block_size = min(height * width, 1024)\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "c1eddd7b-6e2f-4357-9cd5-7e92294f8e43", "generation": 2, "timestamp": 1754591029.029832, "iteration_found": 19, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_18_SqueezeNet", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "runtime_error": "Traceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 574, in arange\n    raise ValueError(\"arange's arguments must be of type tl.constexpr\")\nValueError: arange's arguments must be of type tl.constexpr\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp74ibdk96.py\", line 117, in forward\n    x = adaptive_avg_pool2d_1x1(x)\n  File \"/tmp/tmp74ibdk96.py\", line 57, in adaptive_avg_pool2d_1x1\n    adaptive_avg_pool_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 339, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\ntriton.compiler.errors.CompilationError: at 25:22:\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n\n    total = 0.0\n    num_elements = height * width\n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n    # Process elements in chunks using vectorized loads\n    VEC_SIZE = 4\n    num_vecs = num_elements // VEC_SIZE\n    for i in range(0, num_vecs * VEC_SIZE, VEC_SIZE):\n        offsets = i + tl.arange(0, VEC_SIZE)\n                      ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 42, in wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/core.py\", line 1654, in arange\n    return _semantic.arange(start, end)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/language/semantic.py\", line 574, in arange\n    raise ValueError(\"arange's arguments must be of type tl.constexpr\")\nValueError: arange's arguments must be of type tl.constexpr\n", "runtime_error_name": "triton.compiler.errors.CompilationError", "reference_timing_ms": 1.15, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.958, "runtime_ms": 1.2, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.15, "speed_up": 0.958, "custom_timing": 1.2}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.9580\n- runtime_ms: 1.2000\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.958, 'custom_timing': 1.2}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9580, runtime_ms: 1.2000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.958, 'custom_timing': 1.2}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.8390, runtime_ms: 1.3700, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.839, 'custom_timing': 1.37}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9350, runtime_ms: 1.2300, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.935, 'custom_timing': 1.23}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    COL_BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    # Process rows sequentially for better cache locality\n    for h in range(height):\n        # Process columns in contiguous blocks\n        for w_base in range(0, width, COL_BLOCK_SIZE):\n            w_offsets = w_base + tl.arange(0, COL_BLOCK_SIZE)\n            mask = w_offsets < width\n            \n            input_offset = pid_batch * input_batch_stride + \\\n                           pid_channel * input_channel_stride + \\\n                           h * input_height_stride + \\\n                           w_offsets * input_width_stride\n            values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n            total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Optimize column block size for contiguous memory access\n    col_block_size = triton.next_power_of_2(width)\n    if col_block_size > 1024:\n        col_block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        COL_BLOCK_SIZE=col_block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9350), Performs well on runtime_ms (1.2300), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.935, 'custom_timing': 1.23})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_relu_concat(\n    x1_ptr, x2_ptr, output_ptr,\n    x1_batch_stride, x1_channel_stride, x1_h_stride, x1_w_stride,\n    x2_batch_stride, x2_channel_stride, x2_h_stride, x2_w_stride,\n    output_batch_stride, output_channel_stride, output_h_stride, output_w_stride,\n    c1, c2, h, w,\n    total_elements: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < total_elements\n\n    # Calculate 4D indices\n    offsets_4d = offsets\n    b_idx = offsets_4d // ( (c1 + c2) * h * w )\n    remainder = offsets_4d % ( (c1 + c2) * h * w )\n    c_all_idx = remainder // (h * w)\n    remainder2 = remainder % (h * w)\n    h_idx = remainder2 // w\n    w_idx = remainder2 % w\n\n    # Initialize values\n    val = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Load from x1 or x2 based on channel index\n    is_x1 = c_all_idx < c1\n    x1_channel = c_all_idx\n    x2_channel = c_all_idx - c1\n\n    # Compute pointers and load data\n    x1_offsets = b_idx * x1_batch_stride + x1_channel * x1_channel_stride + h_idx * x1_h_stride + w_idx * x1_w_stride\n    x2_offsets = b_idx * x2_batch_stride + x2_channel * x2_channel_stride + h_idx * x2_h_stride + w_idx * x2_w_stride\n\n    x1_val = tl.load(x1_ptr + x1_offsets, mask=mask & is_x1, other=0.0)\n    x2_val = tl.load(x2_ptr + x2_offsets, mask=mask & (~is_x1), other=0.0)\n    \n    val = tl.where(is_x1, x1_val, x2_val)\n    val = tl.maximum(val, 0.0)  # ReLU activation\n\n    # Compute output pointer\n    output_offsets = b_idx * output_batch_stride + c_all_idx * output_channel_stride + h_idx * output_h_stride + w_idx * output_w_stride\n    tl.store(output_ptr + output_offsets, val, mask=mask)\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        out1 = self.expand1x1(x)\n        out2 = self.expand3x3(x)\n        \n        batch, c1, h, w = out1.shape\n        c2 = out2.shape[1]\n        output = torch.empty(batch, c1 + c2, h, w, device=x.device, dtype=x.dtype)\n        \n        total_elements = batch * (c1 + c2) * h * w\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        fused_relu_concat[grid](\n            out1, out2, output,\n            out1.stride(0), out1.stride(1), out1.stride(2), out1.stride(3),\n            out2.stride(0), out2.stride(1), out2.stride(2), out2.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            c1, c2, h, w,\n            total_elements,\n            BLOCK_SIZE=1024\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.8390), Performs well on runtime_ms (1.3700), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.839, 'custom_timing': 1.37})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        h_offsets = offsets // width\n        w_offsets = offsets % width\n        \n        mask = offsets < num_elements\n        h_mask = h_offsets < height\n        w_mask = w_offsets < width\n        mask = mask & h_mask & w_mask\n        \n        input_offset = pid_batch * input_batch_stride + \\\n                       pid_channel * input_channel_stride + \\\n                       h_offsets * input_height_stride + \\\n                       w_offsets * input_width_stride\n        values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n        total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Use power-of-two block size for efficient reduction\n    block_size = triton.next_power_of_2(height * width)\n    if block_size > 1024:  # Cap block size for register efficiency\n        block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9580), Performs well on runtime_ms (1.2000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.958, 'custom_timing': 1.2})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    # Linearize spatial dimensions for efficient reduction\n    spatial_size = height * width\n    base_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    \n    # Parallel reduction initialization\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for idx in range(0, spatial_size, BLOCK_SIZE):\n        offsets = idx + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < spatial_size\n        \n        # Convert linear index to 2D coordinates\n        h_idx = offsets // width\n        w_idx = offsets % width\n        input_offsets = base_offset + h_idx * input_height_stride + w_idx * input_width_stride\n        \n        # Load and accumulate values\n        values = tl.load(input_ptr + input_offsets, mask=mask, other=0.0)\n        acc += values\n\n    # Final reduction and averaging\n    total = tl.sum(acc, axis=0)\n    avg = total / spatial_size\n    \n    # Store result\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Optimize block size for efficient parallel reduction\n    spatial_size = height * width\n    block_size = min(triton.next_power_of_2(spatial_size), 1024)\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef linear_relu_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < k_remaining)\n        b_mask = (offs_k[:, None] < k_remaining) & (offs_bn[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        acc += bias[None, :]\n\n    acc = tl.where(acc < 0, 0, acc)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask)\n\ndef linear_relu(a, weight, bias):\n    M, K = a.shape\n    weight_t = weight.t().contiguous()\n    N = weight_t.shape[1]\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    linear_relu_kernel[grid](\n        a, weight_t, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        weight_t.stride(0), weight_t.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64\n    )\n    return c\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        h_offsets = offsets // width\n        w_offsets = offsets % width\n        \n        mask = offsets < num_elements\n        h_mask = h_offsets < height\n        w_mask = w_offsets < width\n        mask = mask & h_mask & w_mask\n        \n        input_offset = pid_batch * input_batch_stride + \\\n                       pid_channel * input_channel_stride + \\\n                       h_offsets * input_height_stride + \\\n                       w_offsets * input_width_stride\n        values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n        total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef triton_adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    block_size = triton.next_power_of_2(height * width)\n    if block_size > 1024:\n        block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass TritonLinearReLU(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n        \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        batch, c, h, w = x.shape\n        x_flat = x.permute(0,2,3,1).reshape(-1, c).contiguous()\n        output_flat = linear_relu(x_flat, self.weight, self.bias)\n        return output_flat.reshape(batch, h, w, self.out_features).permute(0,3,1,2).contiguous()\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = TritonLinearReLU(in_channels, squeeze_channels)\n        self.expand1x1 = TritonLinearReLU(squeeze_channels, expand1x1_channels)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze(x)\n        return torch.cat([\n            self.expand1x1(x),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            TritonLinearReLU(512, num_classes),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_relu_concat(\n    x1_ptr, x2_ptr, output_ptr,\n    x1_batch_stride, x1_channel_stride, x1_h_stride, x1_w_stride,\n    x2_batch_stride, x2_channel_stride, x2_h_stride, x2_w_stride,\n    output_batch_stride, output_channel_stride, output_h_stride, output_w_stride,\n    c1, c2, h, w,\n    total_elements: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < total_elements\n\n    # Calculate 4D indices\n    offsets_4d = offsets\n    b_idx = offsets_4d // ( (c1 + c2) * h * w )\n    remainder = offsets_4d % ( (c1 + c2) * h * w )\n    c_all_idx = remainder // (h * w)\n    remainder2 = remainder % (h * w)\n    h_idx = remainder2 // w\n    w_idx = remainder2 % w\n\n    # Initialize values\n    val = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Load from x1 or x2 based on channel index\n    is_x1 = c_all_idx < c1\n    x1_channel = c_all_idx\n    x2_channel = c_all_idx - c1\n\n    # Compute pointers and load data\n    x1_offsets = b_idx * x1_batch_stride + x1_channel * x1_channel_stride + h_idx * x1_h_stride + w_idx * x1_w_stride\n    x2_offsets = b_idx * x2_batch_stride + x2_channel * x2_channel_stride + h_idx * x2_h_stride + w_idx * x2_w_stride\n\n    x1_val = tl.load(x1_ptr + x1_offsets, mask=mask & is_x1, other=0.0)\n    x2_val = tl.load(x2_ptr + x2_offsets, mask=mask & (~is_x1), other=0.0)\n    \n    val = tl.where(is_x1, x1_val, x2_val)\n    val = tl.maximum(val, 0.0)  # ReLU activation\n\n    # Compute output pointer\n    output_offsets = b_idx * output_batch_stride + c_all_idx * output_channel_stride + h_idx * output_h_stride + w_idx * output_w_stride\n    tl.store(output_ptr + output_offsets, val, mask=mask)\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        out1 = self.expand1x1(x)\n        out2 = self.expand3x3(x)\n        \n        batch, c1, h, w = out1.shape\n        c2 = out2.shape[1]\n        output = torch.empty(batch, c1 + c2, h, w, device=x.device, dtype=x.dtype)\n        \n        total_elements = batch * (c1 + c2) * h * w\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        fused_relu_concat[grid](\n            out1, out2, output,\n            out1.stride(0), out1.stride(1), out1.stride(2), out1.stride(3),\n            out2.stride(0), out2.stride(1), out2.stride(2), out2.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            c1, c2, h, w,\n            total_elements,\n            BLOCK_SIZE=1024\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    COL_BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    # Process rows sequentially for better cache locality\n    for h in range(height):\n        # Process columns in contiguous blocks\n        for w_base in range(0, width, COL_BLOCK_SIZE):\n            w_offsets = w_base + tl.arange(0, COL_BLOCK_SIZE)\n            mask = w_offsets < width\n            \n            input_offset = pid_batch * input_batch_stride + \\\n                           pid_channel * input_channel_stride + \\\n                           h * input_height_stride + \\\n                           w_offsets * input_width_stride\n            values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n            total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Optimize column block size for contiguous memory access\n    col_block_size = triton.next_power_of_2(width)\n    if col_block_size > 1024:\n        col_block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        COL_BLOCK_SIZE=col_block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        h_offsets = offsets // width\n        w_offsets = offsets % width\n        \n        mask = offsets < num_elements\n        h_mask = h_offsets < height\n        w_mask = w_offsets < width\n        mask = mask & h_mask & w_mask\n        \n        input_offset = pid_batch * input_batch_stride + \\\n                       pid_channel * input_channel_stride + \\\n                       h_offsets * input_height_stride + \\\n                       w_offsets * input_width_stride\n        values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n        total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Use power-of-two block size for efficient reduction\n    block_size = triton.next_power_of_2(height * width)\n    if block_size > 1024:  # Cap block size for register efficiency\n        block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program and we want to improve the performance of the adaptive_avg_pool2d_1x1 function.\n The current implementation uses a Triton kernel that processes the input tensor by iterating over blocks of spatial elements (height * width).\n However, the current kernel has a speed_up of 0.958 (which is less than 1, meaning it's slower than the reference).\n\n We note that the current kernel does a reduction over the entire spatial dimension (height * width) for each channel and batch.\n The kernel uses a loop over blocks of the spatial dimension and accumulates the sum, then divides by the number of elements.\n\n The problem is that the current kernel is not efficiently utilizing the GPU memory bandwidth and compute resources.\n\n Let's analyze the current kernel:\n   - The kernel uses two program IDs: batch and channel.\n   - Each instance (batch, channel) iterates over the entire spatial dimension (height * width) in blocks.\n   - For each block, it loads a contiguous set of values (if the block is aligned) and sums them.\n\n However, the spatial dimension is contiguous in memory for a given channel and batch. Therefore, we can try to optimize by:\n   - Increasing the block size to process more elements per iteration, but note we are already using a block size that is the next power of two (capped at 1024).\n   - Alternatively, we can change the kernel to use a more efficient reduction pattern.\n\n Another idea: Instead of having each program reduce an entire channel, we can use multiple programs per channel and do a parallel reduction.\n\n However, note that the spatial dimension might be large (e.g., 224x224=50176) and we are limited by the block size (max 1024). The current kernel does a sequential reduction over the blocks.\n\n We can try to use a hierarchical reduction: \n   Step 1: Each block of the spatial dimension is reduced by a group of threads (within the block) to a single value.\n   Step 2: Then we combine the results of these blocks.\n\n But note: The Triton kernel is launched with a grid of (batch, channels). We cannot launch a two-dimensional grid for the reduction (because we don't have a third dimension for the reduction blocks).\n\n Alternatively, we can change the kernel to use a different grid structure.\n\n Inspiration from the top-performing programs and diverse programs:\n   - Program 1 (Top Program 1) uses a different approach: it processes the columns in contiguous blocks for each row. This might be more cache-friendly.\n\n Let's try to use a two-dimensional reduction within the kernel? Actually, we are constrained by the grid being (batch, channels). We have to reduce the entire spatial dimension for each (batch, channel) independently.\n\n Another idea: We can use the fact that the spatial dimension is contiguous in memory and use vectorized loads. The current kernel does not use vectorization.\n\n We can change the kernel to use vectorized loads. For example, we can load multiple elements at once if the block size is a multiple of the vector size.\n\n Steps for vectorization:\n   - We can set a vector size (e.g., 4 or 8) and then adjust the block size to be a multiple of that vector size.\n   - Then, we can use `tl.arange(0, BLOCK_SIZE)[:, None]` and then load a vector per thread? Actually, we are using a 1D block.\n\n Alternatively, we can note that the spatial dimension might be contiguous and we can load multiple contiguous elements at once.\n\n How about:\n   - We break the spatial dimension into chunks of size `BLOCK_SIZE * VEC_SIZE`.\n   - Then, each thread processes `VEC_SIZE` elements.\n\n We can modify the kernel to use vectorized loads.\n\n Proposed changes:\n\n   Let VEC_SIZE be a constant (e.g., 4 or 8). Then:\n\n   ```\n   for i in range(0, num_elements, BLOCK_SIZE * VEC_SIZE):\n        # Within the block, we have BLOCK_SIZE threads, each processing VEC_SIZE elements.\n        vec_offsets = i + tl.arange(0, BLOCK_SIZE)[:, None] * VEC_SIZE + tl.arange(0, VEC_SIZE)[None, :]\n        # But note: vec_offsets is a matrix of [BLOCK_SIZE, VEC_SIZE]. We need to flatten it and mask appropriately.\n\n        # Alternatively, we can use:\n        #   base = i\n        #   offsets = base + tl.arange(0, BLOCK_SIZE * VEC_SIZE)\n        #   and then reshape to [BLOCK_SIZE, VEC_SIZE]?\n\n        # Actually, we can do:\n        offsets = i + tl.arange(0, BLOCK_SIZE * VEC_SIZE)\n        # Then we can split the offsets into the spatial indices.\n\n        # But note: we are limited by the maximum number of elements we can process at once (register pressure).\n\n   ```\n\n However, we must be cautious because the spatial dimension might not be a multiple of the vector size.\n\n Alternatively, we can change the kernel to use:\n\n   ```\n   total = 0.0\n   num_vectors = tl.cdiv(num_elements, VEC_SIZE)\n   for i in range(0, num_vectors, BLOCK_SIZE):\n        vector_offsets = i * VEC_SIZE + tl.arange(0, VEC_SIZE)\n        # But note: we are iterating by block of vectors? Actually, we want each thread to process a vector.\n\n        # We can have:\n        idx = i + tl.arange(0, BLOCK_SIZE)\n        mask = idx < num_vectors\n        base_offsets = tl.where(mask, idx * VEC_SIZE, 0)\n\n        # Then for each vector element, compute the spatial index and load.\n        # But this would require a nested loop? We want to load a vector at once.\n\n        # Instead, we can compute the linear offsets for the entire vector block and then load with a mask.\n\n        # We can do:\n        #   base = idx * VEC_SIZE   [BLOCK_SIZE]\n        #   Then for each vector element, we have base + j for j in [0, VEC_SIZE-1]\n        #   But then we have a 2D index: [BLOCK_SIZE, VEC_SIZE]. We can use a single pointer and broadcast?\n\n        # Alternatively, we can use `tl.load` with a 2D mask.\n\n        # We compute the 2D offsets for the entire block and vectors:\n        #   offsets = base_offsets[:, None] + tl.arange(0, VEC_SIZE)[None, :]   -> [BLOCK_SIZE, VEC_SIZE]\n\n        # Then we need to convert these linear offsets to 2D spatial indices (h, w).\n\n        h_offsets = offsets // width\n        w_offsets = offsets % width\n\n        # Then we compute the input pointer offsets for each element.\n\n        # This approach might be complex and might not be efficient because of the division and modulo.\n\n   ```\n\n Considering the complexity and the fact that the spatial data is contiguous, we can instead use:\n\n   ```\n   input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n   for i in range(0, width * height, BLOCK_SIZE * VEC_SIZE):\n        # We are going to load a contiguous block of BLOCK_SIZE * VEC_SIZE elements\n        offsets = i + tl.arange(0, BLOCK_SIZE * VEC_SIZE)\n        mask = offsets < (width * height)\n\n        # Now, we can load a contiguous block of memory at input_offset + offsets\n        values = tl.load(input_ptr + input_offset + offsets, mask=mask, other=0.0)\n\n        # Then we need to sum these values. We can reshape to [BLOCK_SIZE, VEC_SIZE] and then reduce.\n        # But note: we are accumulating into a scalar per program.\n\n        # We can do:\n        #   total += tl.sum(values)\n\n        # However, this might not be efficient because the sum of a large vector is done serially in the kernel.\n\n        # Instead, we can do a reduction within the thread block? But note: we are not using shared memory and we are in a single thread? \n        # Actually, each kernel instance is a single thread? No, in Triton, the kernel is executed by a block of threads.\n\n        # But wait: in our kernel, we are only using one thread per (batch, channel) because the grid is (batch, channels) and the kernel doesn't use intra-block threads for reduction.\n\n        # This is the problem: we are not parallelizing the reduction within the spatial dimension.\n\n   ```\n\n Therefore, we must change the kernel to use multiple threads per channel and do a parallel reduction.\n\n Let's restructure the kernel:\n\n   We change the grid to be three-dimensional: (batch, channels, num_blocks), where num_blocks is the number of reduction blocks per channel.\n\n   However, note that the original grid is (batch, channels). We cannot change the grid without changing the function signature and the way the function is called.\n\n   Alternatively, we can use the block of threads for reduction. Currently, the kernel is launched with a 1D block? Actually, we are not specifying the block dimensions. Triton uses a default block of 1024 threads.\n\n   We can use the threads in the block to parallelize the reduction over the spatial dimension.\n\n   Proposed plan:\n\n     - We change the kernel to use a 1D grid of (batch * channels) and then use the block of threads to reduce the spatial dimension.\n\n     - But note: the grid in the current function is (batch, channels). We would have to change the grid to (batch * channels) and then use the block of threads.\n\n     However, we are constrained by the requirement: \"\u274c NEVER change @triton.jit function signatures or parameter names\"\n     and \"\u274c NEVER modify grid configuration, output tensor shapes, or PID logic\"\n\n     But note: the requirement says \"NEVER change @triton.jit function signatures or parameter names\", meaning the parameters of the kernel. The grid configuration is set by the caller.\n\n     However, the problem says: we are allowed to change the grid configuration? Actually, the constraint says: \"NEVER modify grid configuration, output tensor shapes, or PID logic\".\n\n     So we cannot change the grid.\n\n     Therefore, we are stuck with a grid of (batch, channels). Each kernel instance must reduce the entire spatial dimension.\n\n     Given that, we have to optimize the reduction within a single thread.\n\n     We can try to use vectorized loads to reduce the number of memory transactions.\n\n     Let's try to load multiple elements at a time (using the fact that the memory is contiguous) and then sum them.\n\n     We can do:\n\n        total_vec = tl.zeros((VEC_SIZE,), dtype=tl.float32)\n        for i in range(0, num_elements, BLOCK_SIZE * VEC_SIZE):\n            offsets = i + tl.arange(0, BLOCK_SIZE * VEC_SIZE)\n            mask = offsets < num_elements\n            values = tl.load(input_ptr + input_offset + offsets, mask=mask, other=0.0)\n            # Now, we want to sum these values. We can break the values into VEC_SIZE segments and then do a vector sum?\n            # Actually, we can reshape the values to [BLOCK_SIZE, VEC_SIZE] and then sum along the rows and then accumulate to a vector?\n            # But note: we are in a single thread.\n\n            # Alternatively, we can use tl.sum(values) to get the total of the entire block? That is what we are doing now.\n\n        total = tl.sum(total_vec)\n\n     But this doesn't change much.\n\n     Alternatively, we can use the dot product to accumulate multiple values at once? But we are just summing.\n\n     Given the constraints, we try the following:\n\n        We change the kernel to use vectorized loads of size VEC_SIZE. We process the spatial dimension in chunks of VEC_SIZE.\n\n        We set:\n            BLOCK_SIZE: the number of vectors we process per iteration? Actually, we are not using the block of threads for parallel reduction.\n\n        But note: the kernel is executed by a block of threads? Actually, no: the grid is (batch, channels) and the block is by default 1 thread? \n\n        Wait, in Triton, the kernel is launched with a block of threads. The default block size is 1024. But in our kernel, we are not using the thread index. We are using the entire block as a single thread? \n\n        Actually, the kernel is written as if it is a single thread because we are not using `tl.program_id(2)` or `tl.arange` for the threads in the block.\n\n        We can use the threads in the block to parallelize the reduction.\n\n        How:\n\n          Let the block have 1024 threads. We split the spatial dimension (num_elements) into 1024 chunks. Each thread sums a chunk. Then we do a reduction across the threads.\n\n        Steps:\n\n          - We compute the chunk size per thread: chunk_size = tl.cdiv(num_elements, num_threads)\n          - Each thread j (from 0 to 1023) will process elements from j * chunk_size to min((j+1)*chunk_size, num_elements)\n          - Then we do a reduction across the threads in the block.\n\n        This way, we parallelize the reduction.\n\n        We can change the kernel to:\n\n          @triton.jit\n          def adaptive_avg_pool_kernel(\n              ...,\n              BLOCK_SIZE: tl.constexpr,   # This is the number of threads per block, which we set to 1024\n          ):\n              pid_batch = tl.program_id(0)\n              pid_channel = tl.program_id(1)\n              pid_thread = tl.program_id(2)   # But wait, we are using a 1D block? Actually, the grid is 2D and the block is 1D? \n\n          However, we cannot change the grid to 3D. But note: we are using the block of threads. The grid remains (batch, channels) and the block is (BLOCK_SIZE,). \n\n          We can do:\n\n              total = 0.0\n              num_threads = BLOCK_SIZE   # number of threads in the block\n              tid = tl.program_id(axis=0)   # but wait, the program_id for the block? Actually, we are in a block.\n\n          Correction: inside the kernel, we can get the thread index within the block with `tl.program_id(axis=0)`? \n\n          Actually, in Triton, the kernel uses a 3-level nested parallelism: grid, block, and thread. But the model is more like CUDA: we have a grid of blocks, and each block has multiple threads.\n\n          How to get the thread index:\n\n              pid = tl.program_id(0)   # grid x index\n              block_id = tl.program_id(1)   # grid y index? Actually, Triton uses a 1D grid and 1D blocks? \n\n          Actually, Triton uses a 1D grid and 1D blocks. The kernel is launched with:\n\n              kernel[grid](..., BLOCK_SIZE=...)\n\n          where grid is a tuple of integers (the grid dimension) and BLOCK_SIZE is the number of threads per block.\n\n          Inside the kernel, we can get:\n\n              pid = tl.program_id(0)   # which gives the index in the grid (from 0 to grid-1)\n\n          But in our case, the grid is (batch, channels). So we have two dimensions.\n\n          We can get:\n\n              pid_batch = tl.program_id(0)\n              pid_channel = tl.program_id(1)\n\n          And the thread index within the block is:\n\n              tid = tl.arange(0, BLOCK_SIZE)   # This gives a vector of thread indices? Actually, no: we are in a single thread? \n\n          Actually, each thread in the block will execute the kernel independently? \n\n          We need to use the thread index to split the work.\n\n          How to do:\n\n              We set the block size to, say, 1024. Then we have 1024 threads per block.\n\n              We launch the grid as (batch, channels) and the block as (1024,). Then the total number of threads is batch * channels * 1024.\n\n              For each (batch, channel), we have 1024 threads that will reduce the spatial dimension.\n\n          Steps:\n\n              Let num_elements = height * width.\n              We split num_elements into 1024 chunks.\n\n              chunk_size = tl.cdiv(num_elements, 1024)\n              start = tid * chunk_size\n              end = min((tid+1)*chunk_size, num_elements)\n\n              Then each thread sums the elements from start to end.\n\n              Then we do a reduction across the 1024 threads in the block.\n\n          We can use `tl.reduce` or `tl.sum` with a dimension? We can use shared memory to do a block reduction.\n\n          We must allocate shared memory for the partial sums.\n\n          Code:\n\n              total = 0.0\n              input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n              for i in range(start, end, VEC_SIZE):\n                  # We can use vector loads for efficiency\n                  offsets = i + tl.arange(0, VEC_SIZE)\n                  mask = (offsets < end) & (offsets < num_elements)\n                  values = tl.load(input_ptr + input_offset + offsets, mask=mask, other=0.0)\n                  total += tl.sum(values)\n\n              # Now, we need to reduce across the block\n              # Allocate shared memory\n              partials = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n              # But how to index by thread? We can use:\n              #   partials[tid] = total\n              # But we don't have a way to index by thread? Actually, we can use tl.arange to get the thread index and then use it as an offset.\n\n              # Instead, we can use:\n              #   total = tl.reshape(total, (1,))   # not necessary\n\n              # We can use a reduction operator that works across the block.\n\n              # Triton provides reduction across a block of threads via `tl.reduce` or by using the `tl.sum` with the axis.\n\n              # But note: we are in a block of threads, and we want to sum the `total` of each thread.\n\n              # We can do:\n              total = tl.sum(total, axis=0)   # but total is a scalar per thread.\n\n              # Actually, we need to use a communication between threads. We can use:\n\n              from triton.language import reduce\n\n              # Or we can use:\n\n              #   total = tl.sum(total, axis=None)   # but this would be within the thread? \n\n              # Instead, we can do:\n\n              #   total = tl.reshape(total, (1,))   # make it a tensor of 1 element\n              #   reduced = tl.reduce(total, 0, tl.sum)   # but this is for a tensor dimension.\n\n              # Alternatively, we can use:\n\n              #   total = tl.sum(total)   # this is the same as before.\n\n              # We must use the `tl.reduce` across the block.\n\n              # We can use:\n\n              #   total = tl.reshape(total, (1,))\n              #   reduced = tl.sum(total, axis=0)   # doesn't work across threads.\n\n              # The correct way is to use `tl.atomic_add` to shared memory? Or use a reduction function that works across threads.\n\n              # Triton has `tl.reduce` for this purpose? Actually, the documentation says we can use `tl.sum` with a tensor that is shared among threads.\n\n              # We can declare a shared memory array:\n\n              shared = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n              # Then each thread stores its partial sum:\n              shared = tl.where(tl.arange(0, BLOCK_SIZE) == tid, total, shared)\n              # Then we do a barrier and then a reduction?\n\n              # But we don't have shared memory in Triton? Actually, we do: using `tl.static_shared`.\n\n          However, note that the current kernel does not use the thread index. We are going to change the kernel to use multiple threads per block and do a reduction.\n\n          This is a significant change.\n\n          Let's restructure the kernel:\n\n            @triton.jit\n            def adaptive_avg_pool_kernel(\n                ...,\n                BLOCK_SIZE: tl.constexpr,   # number of threads per block\n            ):\n                pid_batch = tl.program_id(0)\n                pid_channel = tl.program_id(1)\n                tid = tl.program_id(2)   # no, the program_id is 2D? Actually, the grid is 2D and the block is 1D.\n\n                # We are in a block of BLOCK_SIZE threads. The grid is (batch, channels) and the block is (BLOCK_SIZE,).\n                # How to get the thread id within the block? We use:\n                #   tid = tl.arange(0, BLOCK_SIZE)   # no, that gives a vector of indices? We want a scalar for the current thread.\n\n                # Actually, we are writing a block-level kernel. Each thread has its own index.\n\n                # We can get the linear index of the thread within the block by:\n                #   tid = tl.program_id(0)   # but that is the batch index.\n\n                # Correction: in Triton, the kernel is launched with a grid and a block. The program_id is the index in the grid (which is 2D in our case). The thread index is obtained by `tl.arange(0, BLOCK_SIZE)` but that gives a vector of indices for the entire block? \n\n                # This is confusing.\n\n                # Actually, in Triton, the kernel is written from the perspective of a single thread. We use operations on vectors that are the size of the block? \n\n                # Triton uses a Single Program Multiple Threads (SPMT) model, where each thread executes the same program but on different data.\n\n                # We can use:\n\n                #   tid = tl.arange(0, BLOCK_SIZE)   # this gives a vector of [0, 1, ..., BLOCK_SIZE-1] for the entire block? \n\n                # But then how do we split the work? We would have to use this vector to compute the chunk for each thread.\n\n                # We can do:\n\n                num_elements = height * width\n                chunk_size = tl.cdiv(num_elements, BLOCK_SIZE)\n                start = tid * chunk_size\n                end = min((tid+1)*chunk_size, num_elements)\n\n                # But `tid` is a vector? We want each thread to have a scalar start and end.\n\n                # This is not possible in the current Triton model.\n\n Given the complexity and the time, and since we are constrained by the grid structure, I propose a simpler solution:\n\n   We change the kernel to use vectorized loads of size 4 or 8. We hope that this will reduce the number of load instructions and improve memory bandwidth utilization.\n\n   We do:\n\n        total = 0.0\n        VEC_SIZE = 4\n        num_vectors = num_elements // VEC_SIZE\n        remainder = num_elements % VEC_SIZE\n\n        input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n        # Vectorized part\n        for i in range(0, num_vectors * VEC_SIZE, VEC_SIZE):\n            offsets = i + tl.arange(0, VEC_SIZE)\n            # We know that these offsets are within [0, num_vectors*VEC_SIZE) which is <= num_elements\n            values = tl.load(input_ptr + input_offset + offsets)\n            total += tl.sum(values)\n\n        # Remainder\n        if remainder > 0:\n            offsets = num_vectors * VEC_SIZE + tl.arange(0, remainder)\n            values = tl.load(input_ptr + input_offset + offsets, mask=offsets < num_elements, other=0.0)\n            total += tl.sum(values)\n\n        avg = total / num_elements\n\n   But note: the kernel is currently written to use a loop over blocks of size BLOCK_SIZE. We are removing the BLOCK_SIZE loop and replacing it with a vectorized loop.\n\n   However, the current kernel also uses a block of BLOCK_SIZE in the loop. We are now not using that block size? \n\n   We are going to remove the BLOCK_SIZE parameter and replace it with VEC_SIZE? But the function signature must remain.\n\n   Alternatively, we can reinterpret the BLOCK_SIZE as the vector size? But then we would lose the ability to control the block size.\n\n   Or we can keep the block loop and do vectorized loads within each block.\n\n   We try:\n\n        for i in range(0, num_elements, BLOCK_SIZE * VEC_SIZE):\n            base = i\n            offsets = base + tl.arange(0, BLOCK_SIZE * VEC_SIZE)\n            mask = offsets < num_elements\n            values = tl.load(input_ptr + input_offset + offsets, mask=mask, other=0.0)\n            # Sum the entire block of values at once\n            total += tl.sum(values)\n\n   This is similar to what we had before, but now we are loading a block of size BLOCK_SIZE * VEC_SIZE at once. But the summation is still done on a large vector.\n\n   However, the summation of a large vector might be expensive. We can try to break it into vectors and then sum the vectors.\n\n        values_vec = values.reshape((BLOCK_SIZE, VEC_SIZE))\n        vec_sum = tl.sum(values_vec, axis=1)   # then we have a vector of length BLOCK_SIZE\n        total += tl.sum(vec_sum)\n\n   But this doesn't change the total number of operations.\n\n   Given the time, we choose to try the vectorized loads without breaking the summation.\n\n   We change the kernel to use a fixed vector size (e.g., 4) and set the block size to be the number of vectors per iteration.\n\n   But note: the block size in the kernel is passed as a constant. We can set it to the next power of two of the spatial dimension, but that might be too large for vectorized loads.\n\n   Alternatively, we can remove the block size from the loop and use a fixed vector size. But then we lose the ability to tune.\n\n   We decide to use a vector size of 4 and keep the block size as before, but now the block size will be in terms of vectors.\n\n   However, we must maintain the same function signature. We will keep the BLOCK_SIZE parameter but use it as the number of vectors to process per iteration.\n\n   We can do:\n\n        VEC_SIZE = 4\n        for i in range(0, num_elements, BLOCK_SIZE * VEC_SIZE):\n            # ...\n\n   This means we are loading BLOCK_SIZE * VEC_SIZE elements per iteration.\n\n   We hope that by loading in vectors of 4, the memory transactions will be more efficient.\n\n   We also note that the current kernel does not use the vectorized load/store instructions.\n\n   Therefore, we change the kernel as follows:\n\n        total = 0.0\n        num_elements = height * width\n        input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n        VEC_SIZE = 4   # fixed vector size\n        for i in range(0, num_elements, BLOCK_SIZE * VEC_SIZE):\n            offsets = i + tl.arange(0, BLOCK_SIZE * VEC_SIZE)\n            mask = offsets < num_elements\n            values = tl.load(input_ptr + input_offset + offsets, mask=mask, other=0.0)\n            total += tl.sum(values)\n\n        avg = total / num_elements\n\n   But note: the indices for the spatial dimension are linear. The input tensor for a given (batch, channel) is contiguous, so this vectorized load is valid.\n\n   We also change the block size computation in the host function to be:\n\n        block_size = min(triton.next_power_of_2(height * width) // 4, 1024)   # but note: we are using BLOCK_SIZE as the number of vectors per iteration.\n\n   However, we want to process as many elements as possible per iteration. We can keep the block_size as the next power of two of the spatial dimension, but then we might be processing a very large number of elements per iteration (which might cause register spill).\n\n   Alternatively, we can set the block_size to 256 (so that 256*4=1024 elements per iteration) and then let the loop run.\n\n   But note: the block_size in the kernel is a constant and we are setting it in the host. We can leave the host block_size computation as before (next power of two, capped at 1024), but then we are using it as the number of vectors.\n\n   This might not be optimal.\n\n   Given the complexity, and since we are only allowed to change <=20 lines, we try a simpler approach: remove the block_size loop and do one big vectorized load? \n\n   But the spatial dimension can be very large.\n\n   Alternatively, we can try to use the vectorized load in the inner loop without changing the loop structure much.\n\n   We change the inner loop to process VEC_SIZE elements at a time, but keep the block_size as the number of elements per iteration.\n\n   We do:\n\n        for i in range(0, num_elements, BLOCK_SIZE):\n            # Now, within the block, we process in vectors of VEC_SIZE\n            for j in range(0, min(BLOCK_SIZE, num_elements - i), VEC_SIZE):\n                vec_offsets = i + j + tl.arange(0, VEC_SIZE)\n                mask = vec_offsets < num_elements\n                values = tl.load(input_ptr + input_offset + vec_offsets, mask=mask, other=0.0)\n                total += tl.sum(values)\n\n   But this introduces an inner loop and might be slower.\n\n   Given the time constraints, we choose to try the following:\n\n        total = 0.0\n        num_elements = height * width\n        input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n\n        VEC_SIZE = 4\n        for i in range(0, num_elements, VEC_SIZE):\n            offsets = i + tl.arange(0, VEC_SIZE)\n            mask = offsets < num_elements\n            values = tl.load(input_ptr + input_offset + offsets, mask=mask, other=0.0)\n            total += tl.sum(values)\n\n        avg = total / num_elements\n\n   This processes 4 elements per iteration. We hope that the compiler will generate vectorized loads.\n\n   But the number of iterations is num_elements / 4, which might be too many and cause register pressure.\n\n   We can also change the kernel to use the block size to split the work among iterations of a vectorized strip of size VEC_SIZE * BLOCK_SIZE? \n\n   Given the complexity and the fact that we are only allowed to change <=20 lines, we will try the following:\n\n        We keep the current loop over blocks of size BLOCK_SIZE, but within each block, we use vectorized loads of size 4.\n\n        total = 0.0\n        num_elements = height * width\n        input_offset = ...   # as before\n\n        VEC_SIZE = 4\n        for i in range(0, num_elements, BLOCK_SIZE):\n            for j in range(0, min(BLOCK_SIZE, num_elements - i), VEC_SIZE):\n                vec_offsets = i + j + tl.arange(0, VEC_SIZE)\n                mask = vec_offsets < num_elements\n                values = tl.load(input_ptr + input_offset + vec_offsets, mask=mask, other=0.0)\n                total += tl.sum(values)\n\n   But note: this is two nested loops, and it might be slow.\n\n   Given the above, and since the current speedup is 0.958 (which is close to 1), we might try a different optimization.\n\n   We note that the reference implementation in PyTorch might be using a highly optimized kernel for adaptive_avg_pool2d. \n\n   In the current model, the adaptive_avg_pool2d_1x1 is called on a tensor of shape [batch, 1000, 1, 1] -> wait, no, the classifier output is [batch, num_classes, 1, 1] and then we do adaptive_avg_pool2d to 1x1, which is a no-op? \n\n   Actually, the adaptive_avg_pool2d_1x1 is called on the output of the classifier, which is a convolution with kernel size 1, so the spatial size is the same as the input to the classifier.\n\n   Looking at the model:\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n        # Then we call adaptive_avg_pool2d_1x1(x)\n\n   The input to the classifier comes from the features, which at the end have been downsampled several times. The spatial size at the classifier input is not 1x1. For example, for an input of 224x224, the spatial size after the last FireModule might be 13x13 (or something).\n\n   So the adaptive_avg_pool2d_1x1 is indeed reducing the spatial dimensions to 1x1.\n\n   But note: the current kernel does a reduction over the spatial dimensions. The number of elements is height * width, which might be 13*13=169.\n\n   With such a small number, the reduction is not expensive. The current kernel might be slow because of the overhead of launching the kernel.\n\n   However, the benchmark shows that the reference time is 1.15 ms and the custom time is 1.2 ms, which is very close.\n\n   Given the small spatial size, it might be better to use a single block of 169 elements and do the reduction in one iteration.\n\n   We see that the current block_size is set to the next power of two of 169, which is 256, and then capped at 1024.\n\n   In the kernel, we are doing:\n\n        for i in range(0, 169, 256):   # only one iteration\n\n   and then in that iteration, we load 256 elements but mask out the ones beyond 169.\n\n   This is inefficient because we are loading 256 elements but only 169 are valid.\n\n   We can change the block_size to be exactly the number of elements (or the next power of two) but without exceeding the number of elements.\n\n   But note: the kernel already does:\n\n        block_size = triton.next_power_of_2(height * width)\n        if block_size > 1024:\n            block_size = 1024\n\n   For 169, next_power_of_two is 256, which is <=1024, so we use 256.\n\n   We can try to set the block_size to the actual number of elements (169) if it is <= 1024, but that might not be a power of two.\n\n   The kernel should work with any block_size.\n\n   Alternatively, we can avoid the loop by setting the block_size to the number of elements exactly.\n\n   But the kernel uses a loop to be able to handle any size. \n\n   Given the small number of elements (169), we can change the kernel to not use a loop and set the block_size to the spatial size.\n\n   However, the kernel must handle any size.\n\n   We can do:\n\n        if height * width <= 1024:\n            block_size = height * width\n        else:\n            block_size = 1024\n\n   Then in the kernel, we do only one iteration.\n\n   This way, for small spatial sizes, we only do one load.\n\n   Let's change the host code:\n\n        block_size = height * width\n        if block_size > 1024:\n            block_size = 1024\n        else:\n            block_size = triton.next_power_of_2(block_size)   # not necessary, we can use the exact size.\n\n   But using the exact size might be more efficient.\n\n   We change the host code to:\n\n        block_size = min(height * width, 1024)\n\n   Then in the kernel, the loop will have only one iteration for spatial sizes <=1024.\n\n   For sizes >1024, we will have multiple iterations.\n\n   Since in our case the spatial size is 169, we will have one iteration.\n\n   This will reduce the overhead of the loop and the masking for the out-of-bounds elements.\n\n   We change in the host function:\n\n        block_size = min(height * width, 1024)\n\n   And then in the kernel, we hope that the one iteration will be faster.\n\n   This is a small change.\n\n   Let's do it.\n\n   We change the host code from:\n\n        block_size = triton.next_power_of_2(height * width)\n        if block_size > 1024:\n            block_size = 1024\n\n   to:\n\n        block_size = min(height * width, 1024)\n\n   This might help for small spatial sizes.\n\n   Given the above, we will make two changes:\n\n      1. Change the host code to set block_size = min(height * width, 1024)\n      2. In the kernel, we hope that for small spatial sizes, we only do one iteration.\n\n   This is within the <=20 lines change.\n\n   Let's write the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    \n    # Process elements in chunks using vectorized loads\n    VEC_SIZE = 4\n    num_vecs = num_elements // VEC_SIZE\n    for i in range(0, num_vecs * VEC_SIZE, VEC_SIZE):\n        offsets = i + tl.arange(0, VEC_SIZE)\n        values = tl.load(input_ptr + input_offset + offsets)\n        total += tl.sum(values)\n    \n    # Process remaining elements\n    remainder = num_elements % VEC_SIZE\n    if remainder > 0:\n        offsets = num_vecs * VEC_SIZE + tl.arange(0, remainder)\n        mask = tl.arange(0, VEC_SIZE) < remainder\n        values = tl.load(input_ptr + input_offset + offsets, mask=mask, other=0.0)\n        total += tl.sum(values)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Use exact spatial size for block processing\n    block_size = min(height * width, 1024)\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
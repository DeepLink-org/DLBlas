{"id": "9ffac628-2b35-4369-8fc8-98d2af0745c1", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        # Additional configs optimized for small K dimensions\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 16}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 16}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 16}, num_stages=3, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef linear_relu_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    # Precompute constant masks for rows and columns\n    a_mask_const = offs_am[:, None] < M\n    b_mask_const = offs_bn[None, :] < N\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        # Compute variable part of masks\n        a_mask_var = offs_k[None, :] < k_remaining\n        b_mask_var = offs_k[:, None] < k_remaining\n        \n        # Combine masks\n        a_mask = a_mask_const & a_mask_var\n        b_mask = b_mask_const & b_mask_var\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        acc += bias[None, :]\n\n    acc = tl.where(acc < 0, 0, acc)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask)\n\ndef linear_relu(a, weight, bias):\n    M, K = a.shape\n    weight_t = weight.t().contiguous()\n    N = weight_t.shape[1]\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    linear_relu_kernel[grid](\n        a, weight_t, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        weight_t.stride(0), weight_t.stride(1),\n        c.stride(0), c.stride(1),\n    )\n    return c\n\nclass TritonLinearReLU(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n        \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        batch, c, h, w = x.shape\n        x_flat = x.permute(0,2,3,1).reshape(-1, c).contiguous()\n        output_flat = linear_relu(x_flat, self.weight, self.bias)\n        return output_flat.reshape(batch, h, w, self.out_features).permute(0,3,1,2)\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = TritonLinearReLU(in_channels, squeeze_channels)\n        self.expand1x1 = TritonLinearReLU(squeeze_channels, expand1x1_channels)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze(x)\n        return torch.cat([\n            self.expand1x1(x),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            TritonLinearReLU(512, num_classes),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "e23708f7-fe4f-4739-9944-78e4c95ba1dd", "generation": 3, "timestamp": 1754590940.5602179, "iteration_found": 20, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.528, "runtime_ms": 2.18, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.15, "speed_up": 0.528, "custom_timing": 2.18}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.489, "runtime_ms": 2.35, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.15, "speed_up": 0.489, "custom_timing": 2.35}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.4890\n- runtime_ms: 2.3500\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.489, 'custom_timing': 2.35}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.6180, runtime_ms: 1.8600, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.618, 'custom_timing': 1.86}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.4890, runtime_ms: 2.3500, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.489, 'custom_timing': 2.35}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.9580, runtime_ms: 1.2000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.958, 'custom_timing': 1.2}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        h_offsets = offsets // width\n        w_offsets = offsets % width\n        \n        mask = offsets < num_elements\n        h_mask = h_offsets < height\n        w_mask = w_offsets < width\n        mask = mask & h_mask & w_mask\n        \n        input_offset = pid_batch * input_batch_stride + \\\n                       pid_channel * input_channel_stride + \\\n                       h_offsets * input_height_stride + \\\n                       w_offsets * input_width_stride\n        values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n        total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Use power-of-two block size for efficient reduction\n    block_size = triton.next_power_of_2(height * width)\n    if block_size > 1024:  # Cap block size for register efficiency\n        block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.9580), Performs well on runtime_ms (1.2000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.958, 'custom_timing': 1.2})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef linear_relu_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < k_remaining)\n        b_mask = (offs_k[:, None] < k_remaining) & (offs_bn[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        acc += bias[None, :]\n\n    acc = tl.where(acc < 0, 0, acc)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask)\n\ndef linear_relu(a, weight, bias):\n    M, K = a.shape\n    weight_t = weight.t().contiguous()\n    N = weight_t.shape[1]\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    linear_relu_kernel[grid](\n        a, weight_t, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        weight_t.stride(0), weight_t.stride(1),\n        c.stride(0), c.stride(1),\n    )\n    return c\n\nclass TritonLinearReLU(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n        \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        batch, c, h, w = x.shape\n        x_flat = x.permute(0,2,3,1).reshape(-1, c).contiguous()\n        output_flat = linear_relu(x_flat, self.weight, self.bias)\n        return output_flat.reshape(batch, h, w, self.out_features).permute(0,3,1,2)\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = TritonLinearReLU(in_channels, squeeze_channels)\n        self.expand1x1 = TritonLinearReLU(squeeze_channels, expand1x1_channels)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze(x)\n        return torch.cat([\n            self.expand1x1(x),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            TritonLinearReLU(512, num_classes),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.4890), Performs well on runtime_ms (2.3500), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.489, 'custom_timing': 2.35})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef linear_relu_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < k_remaining)\n        b_mask = (offs_k[:, None] < k_remaining) & (offs_bn[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        acc += bias[None, :]\n\n    acc = tl.where(acc < 0, 0, acc)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask)\n\ndef linear_relu(a, weight, bias):\n    M, K = a.shape\n    weight_t = weight.t().contiguous()\n    N = weight_t.shape[1]\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    linear_relu_kernel[grid](\n        a, weight_t, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        weight_t.stride(0), weight_t.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64\n    )\n    return c\n\nclass TritonLinearReLU(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n        \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        batch, c, h, w = x.shape\n        x_flat = x.permute(0,2,3,1).reshape(-1, c).contiguous()\n        output_flat = linear_relu(x_flat, self.weight, self.bias)\n        return output_flat.reshape(batch, h, w, self.out_features).permute(0,3,1,2)\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = TritonLinearReLU(in_channels, squeeze_channels)\n        self.expand1x1 = TritonLinearReLU(squeeze_channels, expand1x1_channels)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze(x)\n        return torch.cat([\n            self.expand1x1(x),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            TritonLinearReLU(512, num_classes),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.6180), Performs well on runtime_ms (1.8600), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.15, 'speed_up': 0.618, 'custom_timing': 1.86})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef linear_relu_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_ak = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_ak[None, :] * stride_ak\n\n    offs_bk = tl.arange(0, BLOCK_SIZE_K)\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    b_ptrs = b_ptr + offs_bk[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_ak[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_bk[:, None] < K - k, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        acc += bias[None, :]\n\n    acc = tl.where(acc < 0, 0, acc)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask)\n\ndef linear_relu(a, weight, bias):\n    M, K = a.shape\n    N = weight.shape[1]\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    linear_relu_kernel[grid](\n        a, weight, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        weight.stride(0), weight.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64,\n    )\n    return c\n\nclass TritonLinearReLU(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n        \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        batch, c, h, w = x.shape\n        x_flat = x.permute(0,2,3,1).reshape(-1, c)\n        output_flat = linear_relu(x_flat, self.weight, self.bias)\n        return output_flat.reshape(batch, h, w, self.out_features).permute(0,3,1,2)\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = TritonLinearReLU(in_channels, squeeze_channels)\n        self.expand1x1 = TritonLinearReLU(squeeze_channels, expand1x1_channels)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze(x)\n        return torch.cat([\n            self.expand1x1(x),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            TritonLinearReLU(512, num_classes),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    # Base pointer for current channel\n    base_ptr = input_ptr + pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < num_elements\n        values = tl.load(base_ptr + offsets, mask=mask, other=0.0)\n        total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    x = x.contiguous()  # Ensure contiguous memory access\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Use maximum block size for efficient reduction\n    block_size = min(1024, triton.next_power_of_2(height * width))\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef adaptive_avg_pool_kernel(\n    input_ptr,\n    output_ptr,\n    input_batch_stride,\n    input_channel_stride,\n    input_height_stride,\n    input_width_stride,\n    output_batch_stride,\n    output_channel_stride,\n    height,\n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    total = 0.0\n    num_elements = height * width\n    \n    for i in range(0, num_elements, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        h_offsets = offsets // width\n        w_offsets = offsets % width\n        \n        mask = offsets < num_elements\n        h_mask = h_offsets < height\n        w_mask = w_offsets < width\n        mask = mask & h_mask & w_mask\n        \n        input_offset = pid_batch * input_batch_stride + \\\n                       pid_channel * input_channel_stride + \\\n                       h_offsets * input_height_stride + \\\n                       w_offsets * input_width_stride\n        values = tl.load(input_ptr + input_offset, mask=mask, other=0.0)\n        total += tl.sum(values, axis=0)\n    \n    avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, avg)\n\ndef adaptive_avg_pool2d_1x1(x: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = x.shape\n    output = torch.empty((batch, channels, 1, 1), device=x.device, dtype=x.dtype)\n    grid = (batch, channels)\n    \n    # Use power-of-two block size for efficient reduction\n    block_size = triton.next_power_of_2(height * width)\n    if block_size > 1024:  # Cap block size for register efficiency\n        block_size = 1024\n    \n    adaptive_avg_pool_kernel[grid](\n        x,\n        output,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        output.stride(0), output.stride(1),\n        height,\n        width,\n        BLOCK_SIZE=block_size\n    )\n    return output\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = adaptive_avg_pool2d_1x1(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef linear_relu_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        k_remaining = K - k * BLOCK_SIZE_K\n        a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < k_remaining)\n        b_mask = (offs_k[:, None] < k_remaining) & (offs_bn[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        acc += bias[None, :]\n\n    acc = tl.where(acc < 0, 0, acc)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask)\n\ndef linear_relu(a, weight, bias):\n    M, K = a.shape\n    weight_t = weight.t().contiguous()\n    N = weight_t.shape[1]\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    linear_relu_kernel[grid](\n        a, weight_t, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        weight_t.stride(0), weight_t.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64\n    )\n    return c\n\nclass TritonLinearReLU(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n        \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        batch, c, h, w = x.shape\n        x_flat = x.permute(0,2,3,1).reshape(-1, c).contiguous()\n        output_flat = linear_relu(x_flat, self.weight, self.bias)\n        return output_flat.reshape(batch, h, w, self.out_features).permute(0,3,1,2)\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = TritonLinearReLU(in_channels, squeeze_channels)\n        self.expand1x1 = TritonLinearReLU(squeeze_channels, expand1x1_channels)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze(x)\n        return torch.cat([\n            self.expand1x1(x),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            TritonLinearReLU(512, num_classes),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 3 (Score: 1.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_relu_concat(\n    x1_ptr, x2_ptr, output_ptr,\n    x1_batch_stride, x1_channel_stride, x1_h_stride, x1_w_stride,\n    x2_batch_stride, x2_channel_stride, x2_h_stride, x2_w_stride,\n    output_batch_stride, output_channel_stride, output_h_stride, output_w_stride,\n    c1, c2, h, w,\n    total_elements: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < total_elements\n\n    # Use linear indexing for contiguous tensors\n    output_offset = offsets\n    tl.store(output_ptr + output_offset, 0.0, mask=mask)\n\n    # Precompute channel boundary\n    channel_boundary = c1 * h * w\n\n    # Check if element belongs to x1 or x2\n    is_x1 = offsets < channel_boundary\n    x1_offset = offsets\n    x2_offset = offsets - channel_boundary\n\n    # Compute batch index for x2\n    batch_idx_x2 = x2_offset // (c2 * h * w)\n    remainder_x2 = x2_offset % (c2 * h * w)\n    channel_idx_x2 = remainder_x2 // (h * w)\n    spatial_idx_x2 = remainder_x2 % (h * w)\n    h_idx_x2 = spatial_idx_x2 // w\n    w_idx_x2 = spatial_idx_x2 % w\n\n    # Load and process x1 values\n    x1_val = tl.where(is_x1, tl.load(x1_ptr + x1_offset, mask=mask, other=0.0), 0.0)\n    \n    # Load and process x2 values\n    x2_ptr_offset = batch_idx_x2 * x2_batch_stride + \\\n                    channel_idx_x2 * x2_channel_stride + \\\n                    h_idx_x2 * x2_h_stride + \\\n                    w_idx_x2 * x2_w_stride\n    x2_val = tl.where(~is_x1, tl.load(x2_ptr + x2_ptr_offset, mask=mask, other=0.0), 0.0)\n    \n    # Combine values and apply ReLU\n    result = tl.maximum(x1_val + x2_val, 0.0)\n    tl.store(output_ptr + output_offset, result, mask=mask)\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        \n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        out1 = self.expand1x1(x)\n        out2 = self.expand3x3(x)\n        \n        batch, c1, h, w = out1.shape\n        c2 = out2.shape[1]\n        output = torch.empty(batch, c1 + c2, h, w, device=x.device, dtype=x.dtype)\n        \n        total_elements = batch * (c1 + c2) * h * w\n        grid = lambda meta: (triton.cdiv(total_elements, meta['BLOCK_SIZE']),)\n        fused_relu_concat[grid](\n            out1, out2, output,\n            out1.stride(0), out1.stride(1), out1.stride(2), out1.stride(3),\n            out2.stride(0), out2.stride(1), out2.stride(2), out2.stride(3),\n            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n            c1, c2, h, w,\n            total_elements,\n            BLOCK_SIZE=1024\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef linear_relu_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_ak = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_ak[None, :] * stride_ak\n\n    offs_bk = tl.arange(0, BLOCK_SIZE_K)\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    b_ptrs = b_ptr + offs_bk[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_ak[None, :] < K - k, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_bk[:, None] < K - k, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        acc += bias[None, :]\n\n    acc = tl.where(acc < 0, 0, acc)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask)\n\ndef linear_relu(a, weight, bias):\n    M, K = a.shape\n    N = weight.shape[1]\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    linear_relu_kernel[grid](\n        a, weight, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        weight.stride(0), weight.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64,\n    )\n    return c\n\nclass TritonLinearReLU(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n        \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        batch, c, h, w = x.shape\n        x_flat = x.permute(0,2,3,1).reshape(-1, c)\n        output_flat = linear_relu(x_flat, self.weight, self.bias)\n        return output_flat.reshape(batch, h, w, self.out_features).permute(0,3,1,2)\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = TritonLinearReLU(in_channels, squeeze_channels)\n        self.expand1x1 = TritonLinearReLU(squeeze_channels, expand1x1_channels)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze(x)\n        return torch.cat([\n            self.expand1x1(x),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            TritonLinearReLU(512, num_classes),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        \"\"\"\n        :param in_channels: Number of input channels\n        :param squeeze_channels: Number of output channels for the squeeze layer\n        :param expand1x1_channels: Number of output channels for the 1x1 expand layer\n        :param expand3x3_channels: Number of output channels for the 3x3 expand layer\n        \"\"\"\n        super(FireModule, self).__init__()\n        \n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        \n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, in_channels, height, width)\n        :return: Output tensor, shape (batch_size, expand1x1_channels + expand3x3_channels, height, width)\n        \"\"\"\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        \"\"\"\n        :param num_classes: Number of output classes\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: Input tensor, shape (batch_size, 3, height, width)\n        :return: Output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nUnique approach: Excellent compiled (1.000), Excellent correctness (1.000), Alternative passed approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef linear_relu_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < k_remaining)\n        b_mask = (offs_k[:, None] < k_remaining) & (offs_bn[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        acc += bias[None, :]\n\n    acc = tl.where(acc < 0, 0, acc)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask)\n\ndef linear_relu(a, weight, bias):\n    M, K = a.shape\n    weight_t = weight.t().contiguous()\n    N = weight_t.shape[1]\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    linear_relu_kernel[grid](\n        a, weight_t, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        weight_t.stride(0), weight_t.stride(1),\n        c.stride(0), c.stride(1),\n    )\n    return c\n\nclass TritonLinearReLU(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n        \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        batch, c, h, w = x.shape\n        x_flat = x.permute(0,2,3,1).reshape(-1, c).contiguous()\n        output_flat = linear_relu(x_flat, self.weight, self.bias)\n        return output_flat.reshape(batch, h, w, self.out_features).permute(0,3,1,2)\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = TritonLinearReLU(in_channels, squeeze_channels)\n        self.expand1x1 = TritonLinearReLU(squeeze_channels, expand1x1_channels)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze(x)\n        return torch.cat([\n            self.expand1x1(x),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            TritonLinearReLU(512, num_classes),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a current program that uses Triton for the linear+relu operation in a SqueezeNet-like model.\n The current program has a speed_up of 0.489 (which is less than 1, meaning it's slower than the reference) and a runtime of 2.35 ms (reference is 1.15 ms).\n\n The problem: The model uses Triton for the linear layers (which are actually 1x1 convolutions in the Fire modules) and then uses a standard conv for the 3x3 part.\n\n However, note that the current Triton kernel is for a linear layer (matrix multiplication) and is applied on the flattened spatial dimensions of the feature maps.\n\n The main issue is that the current Triton kernel for linear+relu might not be optimal for the specific sizes encountered in the model.\n\n Steps for improvement:\n\n 1. We note that the Triton kernel is already autotuned, but the performance is still suboptimal.\n\n 2. The kernel uses a block approach for matrix multiplication. We might try to optimize the kernel by:\n    a. Ensuring that the kernel is using the tensor cores effectively (by setting appropriate block sizes and using the right data types).\n    b. We are using float32, but the H100 supports TF32 for matrix multiplications. The kernel already uses `allow_tf32=True`.\n\n 3. However, the kernel might be suffering from inefficient memory access patterns. We are loading blocks of A and B, but the way we iterate over K might be improved.\n\n 4. Another idea: the input to the linear layer in the Fire module is of shape [batch, squeeze_channels, h, w]. We are flattening the spatial dimensions and then doing a matrix multiplication of size [batch * h * w, squeeze_channels] with a weight of size [expand1x1_channels, squeeze_channels] (transposed to [squeeze_channels, expand1x1_channels]). The output is then reshaped.\n\n 5. The problem might be that the matrix sizes (M = batch * h * w, N = expand1x1_channels, K = squeeze_channels) are not large enough to saturate the GPU, or the shapes are not favorable.\n\n 6. Alternatively, we might consider that the current implementation uses two separate TritonLinearReLU layers for the 1x1 expansions and a standard conv for 3x3. Then we concatenate. We could try to fuse these operations?\n\n 7. However, the task is to rewrite the Triton kernel for the linear+relu to be faster.\n\n 8. Let's look at the kernel:\n\n    - We are using a 2D grid: each block handles a block of M (rows of A) and a block of N (columns of B).\n    - We load a block of A (BLOCK_SIZE_M x BLOCK_SIZE_K) and a block of B (BLOCK_SIZE_K x BLOCK_SIZE_N) and accumulate the product.\n\n    The current autotune configs are:\n\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n\n    We might try to add more configs that are known to be good on H100, especially with larger block sizes for K to increase arithmetic intensity.\n\n 9. Another idea: use fp16 if the input data type is half? But the problem states the reference is in float32, so we must maintain accuracy.\n\n    However, note that the Triton kernel uses `tl.float32` for accumulation. We are using `allow_tf32` which uses tensor cores for float32 with reduced precision (but acceptable for deep learning).\n\n10. We note that the current kernel does not use shared memory for caching. The Triton compiler might automatically use shared memory, but we can try to explicitly manage it.\n\n    Actually, in Triton we don't manage shared memory explicitly; the compiler does.\n\n11. We might try to adjust the number of warps and stages to better suit the H100.\n\n12. Alternatively, we might consider that the matrix sizes are small. For example, in the Fire modules, the squeeze layer reduces the channels to 16, 32, etc. Then the expand layers are 64, 128, etc. The spatial size might be large (e.g., 55x55 in the first Fire module). So M = batch * h * w can be 1*55*55=3025, which is acceptable.\n\n    But the kernel might not be optimal for the specific sizes.\n\n13. We can try to add more autotune configs that are tailored for the specific matrix sizes we encounter.\n\n    However, we don't know the exact sizes at kernel definition time.\n\n14. Another idea: change the kernel to use a different tiling strategy? For example, we could tile the K dimension more aggressively to reduce the number of loads.\n\n15. We note that the kernel already uses a loop over K in blocks. We are using BLOCK_SIZE_K for that.\n\n16. We might try to increase the block sizes for M and N to reduce the grid size and increase the work per block.\n\n17. Let's look at the top performing programs for inspiration:\n\n    - In the top programs, one uses an adaptive average pool with Triton and gets a speedup of 0.958 (which is better, but still less than 1). \n    - Another uses a Triton linear kernel with fixed block sizes and gets 0.618.\n\n    But note: the current program we are to rewrite is the one with the autotuned linear kernel and gets 0.489.\n\n    The best Triton linear kernel in the top programs (Program 1) got 0.958, but that was for a different part (adaptive pooling). \n\n    The linear kernel in Program 2 (score 20, speed_up 0.489) is the same as our current one? Actually, the current program is Program 2.\n\n    Program 1 (with adaptive pooling) had a speed_up of 0.958, but that was without Triton for the linear layers? Actually, Program 1 used standard convolutions and only Triton for the adaptive pooling.\n\n    So we are focusing on the linear kernel.\n\n18. We might try to use a different kernel design. For example, we could use a kernel that is specialized for small K (which is the case here: the squeeze_channels are 16, 32, etc.).\n\n    However, note that in the first Fire module, the squeeze layer has 16 channels, so K=16. This is very small. The matrix multiplication for small K can be done by loading the entire row of A and column of B in one go.\n\n    We might write a kernel that uses a block size for M and N, and for K we load the entire vector at once? But the current kernel already uses a loop over K in blocks, and when K is 16, it will do one iteration.\n\n    The problem might be that the block sizes we have in the autotune configs are too large for small K. For example, if K=16, then we set BLOCK_SIZE_K to 64, but then we are only using 16 of it. Also, the mask for the remaining 48 elements would be false, so we are loading only 16 elements. But the block sizes for M and N might be too large for the small matrices.\n\n    We might need to add autotune configs that are for small K.\n\n    How about we add:\n\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, ...)\n\n    and similar for even smaller K.\n\n    But note: the kernel must work for any K, and we have Fire modules with different squeeze_channels (16, 32, 48, 64). So we need to cover these.\n\n19. We can extend the autotune configs to include:\n\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 16}, num_stages=3, num_warps=4),\n        ... etc.\n\n    But note: the number of configs is limited and we don't want to slow down autotuning too much.\n\n20. Alternatively, we can try to use a fixed block size for K that is the next power of two of the actual K? But we don't know K at compile time.\n\n21. Another idea: use the kernel from the Triton matmul tutorial that is optimized for small K? \n\n    The tutorial has an example for matmul that uses a different approach: https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html\n\n    But our kernel is similar.\n\n22. We might try to change the kernel to use a different arrangement of the loops? Or use vectorized loads?\n\n    We are already using tl.dot, which is optimized.\n\n23. We note that the input tensor `a` is in row-major, and the weight is in column-major (because we transpose it). We are loading the weight in a transposed manner? Actually, we are doing:\n\n        a = [M, K] -> row-major\n        weight_t = [K, N] -> row-major (because we transposed the weight which was [out_features, in_features] to [in_features, out_features])\n\n    So we are doing A @ B, where B is the weight_t.\n\n    This is standard.\n\n24. We might try to change the kernel to use a different tiling for the accumulation? For example, we could use a block size for K that is larger to reduce the outer loop, but that might not be possible because K is small.\n\n25. We decide to try the following:\n\n    - Add more autotune configurations that are tailored for small K (16, 32, 64).\n    - Also, adjust the number of warps and stages for these small K.\n\n    We add:\n\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 16}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=8),\n        ... (we already have some for 32 and 64)\n\n    But note: we already have a config for 32 and 64.\n\n    We are particularly concerned about the case when K=16.\n\n26. Additionally, we might try to use a smaller number of warps for small matrices to avoid underutilization.\n\n27. We change the autotune configs to:\n\n    configs = [\n        # Existing ones\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        # New ones for small K\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 16}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 16}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 16}, num_stages=3, num_warps=8),\n    ]\n\n    We hope that when K is small (like 16) one of these configs will be chosen and perform better.\n\n28. We also note that the kernel might be doing unnecessary work in the mask calculation. We are recalculating the mask at every iteration of the k-loop. But the mask for the rows of A (offs_am) and columns of B (offs_bn) are constant. We could precompute these masks.\n\n    However, the mask for k_remaining changes per iteration. We are doing:\n\n        a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < k_remaining)\n        b_mask = (offs_k[:, None] < k_remaining) & (offs_bn[None, :] < N)\n\n    We can split the mask into a constant part and a variable part:\n\n        a_mask_const = (offs_am[:, None] < M)   # [BLOCK_SIZE_M, 1] -> broadcastable to [BLOCK_SIZE_M, BLOCK_SIZE_K]\n        a_mask_var = (offs_k[None, :] < k_remaining)   # [1, BLOCK_SIZE_K] -> same\n\n        a_mask = a_mask_const & a_mask_var\n\n    Similarly for b_mask.\n\n    But the Triton compiler might optimize this. We are not sure.\n\n29. We might try to precompute the constant mask and then combine it with the variable one. However, Triton does not support storing masks in variables for the entire kernel. We can recompute the constant mask once outside the loop and then reuse it.\n\n    We change:\n\n        # Precompute the constant part of the masks\n        a_mask_const = offs_am[:, None] < M   # [BLOCK_SIZE_M, 1]\n        b_mask_const = offs_bn[None, :] < N   # [1, BLOCK_SIZE_N]\n\n        for k in range(0, K, BLOCK_SIZE_K):\n            k_remaining = K - k\n            a_mask_var = offs_k[None, :] < k_remaining   # [1, BLOCK_SIZE_K]\n            b_mask_var = offs_k[:, None] < k_remaining   # [BLOCK_SIZE_K, 1]\n\n            a_mask = a_mask_const & a_mask_var\n            b_mask = b_mask_const & b_mask_var\n\n            ... rest ...\n\n    This might save some computation.\n\n30. However, note that the mask for b in the k dimension is the same as for a? Actually, in the b_mask we have:\n\n        b_mask = (offs_k[:, None] < k_remaining) & (offs_bn[None, :] < N)\n\n    We can precompute the constant part for b as:\n\n        b_mask_const = (offs_bn[None, :] < N)   # [1, BLOCK_SIZE_N]\n\n    and then for the variable part we have (offs_k[:, None] < k_remaining) which is [BLOCK_SIZE_K, 1].\n\n    Then we do:\n\n        b_mask = b_mask_var & b_mask_const\n\n    This is valid.\n\n31. We make that change to the kernel to precompute the constant masks.\n\n32. We also note that the kernel currently uses:\n\n        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    and then we use these to compute the masks. We are not clamping the offs_am and offs_bn to the bounds? We are using masks for that.\n\n    The precomputation of the constant masks is exactly as before.\n\n33. We try to change the kernel accordingly.\n\n34. We also note that the kernel might be slowed down by the conditional for the bias. We do:\n\n        if bias_ptr is not None:\n            ... \n\n    This is a host-side condition? Actually, it is inside the Triton kernel, so it is a device-side condition. We are branching on a constant for the entire grid? No, the bias_ptr might be null, but in our case it is always provided.\n\n    However, the Triton compiler might be able to optimize away the branch if the bias_ptr is always non-null? We don't know. We might try to remove the condition and always load the bias? But then if the bias_ptr is null we would crash.\n\n    We leave it as is because the model always uses bias.\n\n35. We will change the autotune configs and the mask precomputation.\n\n    We hope that these changes improve the performance.\n\n36. We also note that the kernel might not be using the tensor cores effectively for small matrices. But we are using `allow_tf32=True` and the block sizes are multiples of 16? Not exactly, but 16, 32, 64 are.\n\n    The H100 tensor cores require the dimensions to be multiples of 16 for FP16, but for TF32 they might be more flexible.\n\n    We cannot do much more.\n\n37. We write the new kernel code.\n\n    Steps:\n\n        a. Add the new autotune configs.\n        b. Precompute the constant masks outside the k-loop.\n\n38. We must be cautious: the indices offs_am, offs_bn are used in the pointer arithmetic and in the masks. We are not changing that.\n\n39. We change the kernel as follows:\n\n    Inside the kernel:\n\n        # Precompute constant masks for rows of A and columns of B\n        a_mask_const = offs_am[:, None] < M   # [BLOCK_SIZE_M, 1]\n        b_mask_const = offs_bn[None, :] < N   # [1, BLOCK_SIZE_N]\n\n        for k in range(0, K, BLOCK_SIZE_K):\n            k_remaining = K - k\n            a_mask_var = offs_k[None, :] < k_remaining   # [1, BLOCK_SIZE_K]\n            b_mask_var = offs_k[:, None] < k_remaining   # [BLOCK_SIZE_K, 1]\n\n            a_mask = a_mask_const & a_mask_var\n            b_mask = b_mask_const & b_mask_var\n\n            ... rest ...\n\n40. We leave the rest of the kernel unchanged.\n\n41. We run the code and hope for a speedup.\n\n42. Given that the current speedup is 0.489, we aim to get at least 0.6 or higher.\n\n43. We output the entire code block with the changes.\n\nNote: We must not change the function signature or the grid configuration.\n\nLet's write the new code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        # Additional configs optimized for small K dimensions\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 16}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 16}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 16}, num_stages=3, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef linear_relu_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    # Precompute constant masks for rows and columns\n    a_mask_const = offs_am[:, None] < M\n    b_mask_const = offs_bn[None, :] < N\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        k_remaining = K - k\n        # Compute variable part of masks\n        a_mask_var = offs_k[None, :] < k_remaining\n        b_mask_var = offs_k[:, None] < k_remaining\n        \n        # Combine masks\n        a_mask = a_mask_const & a_mask_var\n        b_mask = b_mask_const & b_mask_var\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        acc += tl.dot(a, b, allow_tf32=True)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N, other=0.0)\n        acc += bias[None, :]\n\n    acc = tl.where(acc < 0, 0, acc)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=mask)\n\ndef linear_relu(a, weight, bias):\n    M, K = a.shape\n    weight_t = weight.t().contiguous()\n    N = weight_t.shape[1]\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    \n    linear_relu_kernel[grid](\n        a, weight_t, c, bias,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        weight_t.stride(0), weight_t.stride(1),\n        c.stride(0), c.stride(1),\n    )\n    return c\n\nclass TritonLinearReLU(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.empty(out_features))\n        self.reset_parameters()\n        \n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n        nn.init.uniform_(self.bias, -bound, bound)\n        \n    def forward(self, x):\n        batch, c, h, w = x.shape\n        x_flat = x.permute(0,2,3,1).reshape(-1, c).contiguous()\n        output_flat = linear_relu(x_flat, self.weight, self.bias)\n        return output_flat.reshape(batch, h, w, self.out_features).permute(0,3,1,2)\n\nclass FireModule(nn.Module):\n    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n        super(FireModule, self).__init__()\n        self.squeeze = TritonLinearReLU(in_channels, squeeze_channels)\n        self.expand1x1 = TritonLinearReLU(squeeze_channels, expand1x1_channels)\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.squeeze(x)\n        return torch.cat([\n            self.expand1x1(x),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(ModelNew, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(96, 16, 64, 64),\n            FireModule(128, 16, 64, 64),\n            FireModule(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(256, 32, 128, 128),\n            FireModule(256, 48, 192, 192),\n            FireModule(384, 48, 192, 192),\n            FireModule(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            FireModule(512, 64, 256, 256),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.0),\n            TritonLinearReLU(512, num_classes),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n# Test code\nbatch_size = 1\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
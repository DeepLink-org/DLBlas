{"id": "5a1e3536-6044-470a-bf47-7b892ed57500", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\nclass TritonConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_channels))\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        if self.kernel_size != 1 or self.stride != 1 or self.padding != 0:\n            return F.conv2d(x, self.weight, self.bias, self.stride, self.padding)\n        \n        n, c, h, w = x.shape\n        x_reshaped = x.permute(0, 2, 3, 1).contiguous().view(n * h * w, c)\n        weight_reshaped = self.weight.view(self.weight.size(0), -1).t().contiguous()\n        output = torch.empty(n * h * w, self.weight.size(0), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(x_reshaped.size(0), META['BLOCK_M']), \n                            triton.cdiv(weight_reshaped.size(1), META['BLOCK_N']))\n        \n        _matmul_kernel[grid](x_reshaped, weight_reshaped, output, \n                            x_reshaped.size(0), weight_reshaped.size(1), x_reshaped.size(1),\n                            x_reshaped.stride(0), x_reshaped.stride(1),\n                            weight_reshaped.stride(0), weight_reshaped.stride(1),\n                            output.stride(0), output.stride(1))\n        \n        output = output.view(n, h, w, -1).permute(0, 3, 1, 2)\n        return output\n\n@triton.jit\ndef _matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n        b_mask = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk * stride_bn\n    \n    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                TritonConv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "a52f9c18-acc3-45b2-a401-884496313d50", "generation": 1, "timestamp": 1754583453.3018737, "iteration_found": 2, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_19_MobileNetV1", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpy1_rfccf.py\", line 120, in forward\n    x = self.model(x)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\n    input = module(input)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\n    input = module(input)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpy1_rfccf.py\", line 32, in forward\n    _matmul_kernel[grid](x_reshaped, weight_reshaped, output,\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 561, in run\n    bound_args, specialization, options = binder(*args, **kwargs)\nTypeError: dynamic_func() missing 3 required positional arguments: 'BLOCK_M', 'BLOCK_N', and 'BLOCK_K'\n", "runtime_error_name": "builtins.TypeError", "reference_timing_ms": 1.36, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 1.63, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.36, "speed_up": 0.834, "custom_timing": 1.63}}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: reference code\n- passed: 0.0000\n- score: 0.0000\n- speed_up: 1.0000\n- runtime_ms: 1.6300\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.834, 'custom_timing': 1.63}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: reference code, passed: 0.0000, score: 0.0000, speed_up: 1.0000, runtime_ms: 1.6300, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.834, 'custom_timing': 1.63}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0000)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        \"\"\"\n        MobileNetV1 architecture implementation.\n\n        :param num_classes: The number of output classes (default: 1000)\n        :param input_channels: The number of input channels (default: 3 for RGB images)\n        :param alpha: Width multiplier (default: 1.0)\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_channels, height, width)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (reference code), Performs well on passed (0.0000), Performs well on score (0.0000), Performs well on speed_up (1.0000), Performs well on runtime_ms (1.6300), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:7', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.834, 'custom_timing': 1.63})\n\n\n\n\n# Current Program\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        \"\"\"\n        MobileNetV1 architecture implementation.\n\n        :param num_classes: The number of output classes (default: 1000)\n        :param input_channels: The number of input channels (default: 3 for RGB images)\n        :param alpha: Width multiplier (default: 1.0)\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_channels, height, width)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a PyTorch model (MobileNetV1) and we are to rewrite it to improve performance.\n However, note that the current program is a pure PyTorch model and the task is to rewrite it for Triton.\n But wait: the context says that we are to generate Triton implementations (custom_code) for kernel functions.\n However, the current program is a full model. This suggests that we are to focus on rewriting specific kernels (like convolution) in Triton.\n\n But note: the example provided was a kernel (per_row_quantize_int8) rewritten in Triton.\n\n However, the current program is a full model. We cannot rewrite the entire model in one Triton kernel.\n\n Clarification from the context: \n   - The dataset contains 250 kernelbench/level1-level5 *.py files. Each file contains a PyTorch reference (reference_code) and we are to generate Triton implementations (custom_code).\n\n Therefore, we must identify a kernel within the current model that can be optimized with Triton.\n\n Looking at the model: \n   It consists of two types of layers: conv_bn and conv_dw.\n   - conv_bn: a convolution followed by batch norm and ReLU.\n   - conv_dw: a depthwise convolution followed by a pointwise convolution, each with batch norm and ReLU.\n\n The most computationally expensive part is the convolution. We can try to replace the convolution layers with optimized Triton kernels.\n\n However, note the constraints:\n   - We must not change the function signatures (like forward) or the model structure (like the number of layers).\n   - We are only allowed to change the Triton kernel source within the EVOLVE-BLOCK.\n\n But the current code does not contain any Triton kernel. It is entirely in PyTorch.\n\n How to proceed?\n\n We are in Phase 1 (Compilation Pass) or Phase 2 (Numerical Correctness) or Phase 3 (Performance Gain) of evolution.\n\n The current program is the reference PyTorch code. We are to replace it with a custom Triton implementation.\n\n However, the entire model is too big. We must break it down.\n\n Since the task is to rewrite a kernel, we should focus on one kernel at a time.\n\n The most common approach is to replace the convolution operations with Triton kernels.\n\n But note: the model uses two kinds of convolutions:\n   1. Standard convolution (in conv_bn and the pointwise part of conv_dw)\n   2. Depthwise convolution (in conv_dw)\n\n We can start by replacing the standard convolution with a Triton kernel.\n\n However, the problem states: \"Modify ONLY the Triton kernel source within this block - all other code is LOCKED.\"\n\n But the current code does not have any Triton kernel. So we are allowed to introduce a Triton kernel?\n\n The example provided introduced a Triton kernel and a new function.\n\n Therefore, we can do the following:\n\n   Step 1: Replace the nn.Conv2d in the conv_bn and the pointwise convolution in conv_dw with a custom Triton convolution kernel.\n\n   However, note that the depthwise convolution is a special case and might require a different kernel.\n\n   Alternatively, we can focus on the most time-consuming convolution in the model.\n\n   But the problem says: \"small iterative steps\".\n\n   We are to make a minimal change that can improve performance.\n\n   Given the constraints, we are allowed to change the internal implementation of the convolution.\n\n   We are going to write a Triton kernel for the standard convolution and use it in the conv_bn and the pointwise part of conv_dw.\n\n   However, the depthwise convolution might also be replaced by a Triton kernel, but that is a separate step.\n\n   We are going to do one step at a time.\n\n   Step 2: We note that the model is defined using nn.Sequential and the conv_bn and conv_dw functions.\n\n   We are going to create a custom Triton-based convolution module that can replace the nn.Conv2d in the conv_bn and the pointwise convolution in conv_dw.\n\n   But note: the conv_bn and conv_dw functions also include batch norm and ReLU. We are only replacing the convolution.\n\n   We must be cautious: the convolution in Triton must support the same parameters (stride, padding, groups, etc).\n\n   However, the current groups=1 for standard convolution and groups=inp for depthwise.\n\n   Our Triton convolution kernel must at least support groups=1 (for standard) and we can leave the depthwise (groups>1) for later.\n\n   Alternatively, we can write a Triton kernel that supports both, but that is complex.\n\n   Given the complexity, we might start by replacing only the standard convolutions (groups=1) with a Triton kernel.\n\n   How about the pointwise convolution in conv_dw? It is groups=1? Yes, because the groups parameter is not set (default=1).\n\n   So both the conv_bn convolution and the pointwise convolution in conv_dw are standard (groups=1).\n\n   The depthwise convolution (groups=inp) is a separate issue.\n\n   We can leave the depthwise convolution as nn.Conv2d for now and only replace the standard ones.\n\n   Step 3: We design a Triton kernel for standard convolution.\n\n   We note that the convolution parameters are:\n      kernel_size=3, stride= (either 1 or 2), padding=1.\n\n   We can write a Triton kernel for a 3x3 convolution with stride 1 or 2 and padding 1.\n\n   However, writing a general convolution in Triton is complex. We can start with a fixed kernel size and padding.\n\n   Alternatively, we can use a matrix multiplication approach by im2col. But note that Triton has an example for convolution.\n\n   There is an official Triton convolution example: https://github.com/openai/triton/blob/master/python/tutorials/05-layer-norm.py\n\n   But wait, that's for layer norm. Actually, there is a convolution example in the Triton repository: \n        https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py\n\n   However, that is for attention. There is also an example for convolution in the Triton blog: \n        https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html\n\n   Actually, convolution can be reduced to matrix multiplication by unrolling the input (im2col).\n\n   We can write a Triton kernel that does im2col followed by matrix multiplication.\n\n   However, we are constrained by the fact that we must not change the interface of the model.\n\n   We are going to create a new module for convolution:\n\n        class TritonConv2d(nn.Module):\n            def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n                ...\n\n            def forward(self, x):\n                ...\n\n   Then, in the conv_bn and conv_dw, we replace the nn.Conv2d (for standard convolution) with our TritonConv2d.\n\n   But note: the current model uses bias=False. So we can ignore bias for now.\n\n   Step 4: We must implement the forward pass of TritonConv2d using a Triton kernel.\n\n   However, the problem says: \"Return ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.\"\n\n   We are going to write the Triton kernel and the TritonConv2d class within the EVOLVE-BLOCK.\n\n   But note: the current model has two functions (conv_bn and conv_dw) and the main model. We are going to modify these functions to use TritonConv2d for the standard convolutions.\n\n   However, we must be cautious: the depthwise convolution in conv_dw is not standard (groups=inp). We are not replacing that with TritonConv2d. We leave it as nn.Conv2d.\n\n   Specifically, in conv_dw, we have:\n        - First layer: nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False) -> depthwise, we leave as is.\n        - Second layer: nn.Conv2d(inp, oup, 1, 1, 0, bias=False) -> pointwise (groups=1), we replace with TritonConv2d.\n\n   And in conv_bn, we have:\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False) -> standard convolution (groups=1), we replace with TritonConv2d.\n\n   Step 5: We design the TritonConv2d.\n\n   We will assume kernel_size=3 or 1? Because we have both: in conv_bn and conv_dw pointwise we have 3x3 and 1x1.\n\n   We can write two different kernels? Or one kernel that handles both?\n\n   Alternatively, we can write a general kernel for any kernel size? But that is complex.\n\n   Given the constraints, we can write a kernel for 3x3 and a separate one for 1x1? But the problem says small iterative steps.\n\n   We start with 1x1 convolution because it is simpler (it is just a matrix multiplication).\n\n   But note: the pointwise convolution in conv_dw is 1x1 and the first convolution in conv_bn is 3x3.\n\n   We can write a 1x1 convolution first and then in a later step write the 3x3.\n\n   However, the problem says we are to improve performance. We must replace both to see a significant improvement.\n\n   Alternatively, we can write a kernel that works for 1x1 and 3x3? We can do by im2col.\n\n   We decide to do im2col for 3x3 and for 1x1 we can skip the im2col (since it is just reshaping).\n\n   However, note that the 1x1 convolution does not require im2col.\n\n   We can do:\n\n        if kernel_size == 1:\n            # do matrix multiplication without im2col\n        else:\n            # do im2col then matrix multiplication\n\n   But note: the current model also uses stride=2 in some places. We must support stride.\n\n   Given the complexity and the fact that we are in an iterative process, we start by replacing only the 1x1 convolutions.\n\n   Why? Because they are easier and might be a big part of the computation (in the pointwise convolutions).\n\n   We leave the 3x3 convolutions as nn.Conv2d for now.\n\n   We will replace only the pointwise convolutions (1x1) in conv_dw with our TritonConv2d.\n\n   Then, in a future step, we can replace the 3x3.\n\n   Step 6: Implementation of TritonConv2d for 1x1.\n\n   The 1x1 convolution is: \n        y = x * W   where W is (out_channels, in_channels, 1, 1)\n\n   We can reshape x to (batch_size * height * width, in_channels) and W to (out_channels, in_channels) and then do matrix multiplication.\n\n   Then reshape the result to (batch_size, out_channels, height, width).\n\n   We can do this in PyTorch without Triton? But that would not use a Triton kernel.\n\n   We are to write a Triton kernel for the matrix multiplication.\n\n   We note that the matrix multiplication is: \n        y = x @ W.T   [because W is (out_channels, in_channels) and x is (n, in_channels) -> (n, out_channels)]\n\n   We can use the Triton matrix multiplication example.\n\n   However, note: we are not allowed to change the grid configuration? But we are writing a new kernel.\n\n   We design a Triton kernel for matrix multiplication that is optimized for the specific sizes we have.\n\n   But the sizes vary. The input x has shape (batch_size, in_channels, height, width). The weight has shape (out_channels, in_channels).\n\n   The matrix multiplication is: \n        n = batch_size * height * width\n        k = in_channels\n        m = out_channels\n\n   We can use a Triton kernel for matmul that works for any n, m, k.\n\n   We use the standard matmul kernel from the Triton tutorial.\n\n   However, note: the kernel must be efficient for the specific sizes we have.\n\n   The sizes in MobileNetV1: \n        The pointwise convolutions are used in conv_dw to change the number of channels. The channel sizes are multiples of alpha (32, 64, 128, ...).\n\n   Step 7: Write the Triton kernel for matmul.\n\n   We will write a kernel that takes:\n        a_ptr: pointer to input tensor (x) of shape (n, k)\n        b_ptr: pointer to weight tensor (W) of shape (m, k)  [note: we need to transpose it to (k, m) for efficient access?]\n        c_ptr: pointer to output tensor (y) of shape (n, m)\n\n   But note: we are going to transpose the weight matrix once in the initialization so that it is in the shape (k, m).\n\n   Then the matmul: y = x @ W_t, where W_t is of shape (k, m) and stored in column-major? Actually, we can store the weight in column-major for efficient access.\n\n   Alternatively, we can use the kernel that expects both A and B to be row-major and then do A @ B^T? That is the standard.\n\n   We use: \n        y[i, j] = sum_k x[i, k] * W[j, k]   = x[i, :] @ W[j, :]^T\n\n   So if we store W in row-major (each row is a filter of length k), then we can do:\n\n        a (n, k) in row-major\n        b (m, k) in row-major\n\n        then we can compute y = a @ b^T.\n\n   The kernel from the Triton tutorial does: \n        A (n, k) and B (k, m) -> C (n, m) = A @ B.\n\n   But we have B being (m, k). So we can transpose B to (k, m) and then do A @ B.\n\n   However, transposing a weight matrix of size (m, k) to (k, m) is an overhead.\n\n   Alternatively, we can write a kernel that does A @ B^T.\n\n   There is a kernel in the Triton tutorial for that? Actually, the tutorial kernel can be adjusted.\n\n   We can set the stride of B to be (1, k) for the row-major (m, k) matrix? Then we can access B in a transposed way without actually transposing.\n\n   But note: the efficiency of memory access.\n\n   We decide to transpose the weight matrix once in __init__ and store it as (k, m) in column-major? Then we can use the standard matmul kernel.\n\n   Step 8: We write the standard matmul kernel from the Triton tutorial.\n\n   However, note that the tutorial kernel is for square matrices. We need to handle arbitrary sizes.\n\n   We use the kernel from: https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html\n\n   We will write a kernel that breaks the matrices into blocks.\n\n   Step 9: We must also consider the im2col for 3x3 convolutions? We skip for now because we are only doing 1x1.\n\n   But note: the conv_bn uses 3x3. We are not replacing that. We are only replacing the 1x1 in conv_dw.\n\n   Therefore, we change the conv_dw function to use TritonConv2d for the pointwise convolution (1x1) and leave the depthwise and the conv_bn as is.\n\n   We must be cautious: the conv_dw function returns a Sequential that includes two convolutions. We are replacing the second convolution.\n\n   Step 10: We design the TritonConv2d module.\n\n   We assume that the kernel_size is either 1 or 3. But for now, we only support 1x1.\n\n   We will check in the __init__: if kernel_size != (1,1) and padding != 0, then we fall back to nn.Conv2d? \n        But note: the problem says we must not change the output. We are allowed to leave some convolutions as PyTorch if we haven't implemented.\n\n   However, we are only going to use TritonConv2d for 1x1 convolutions. For others, we use nn.Conv2d.\n\n   But the conv_bn uses 3x3. So we leave it as nn.Conv2d.\n\n   Therefore, we change the conv_dw function:\n\n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),   # depthwise, leave as nn.Conv2d\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                # Replace the next nn.Conv2d with TritonConv2d for 1x1\n                TritonConv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n\n   And in conv_bn, we leave as nn.Conv2d because it is 3x3.\n\n   Step 11: We write the TritonConv2d module.\n\n   We assume the following parameters: \n        in_channels, out_channels, kernel_size, stride, padding, bias.\n\n   We only support kernel_size=1, stride=1, padding=0.\n\n   For other cases, we fall back to nn.Conv2d.\n\n   We initialize:\n\n        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(out_channels))\n        else:\n            self.bias = None\n\n        We initialize the weight and bias as in nn.Conv2d.\n\n   Then, in forward:\n\n        if kernel_size != (1,1) or stride != (1,1) or padding != (0,0):\n            # Fall back to PyTorch conv2d\n            return F.conv2d(x, self.weight, self.bias, stride, padding, ...)\n\n        else:\n            # Reshape x: (batch_size, in_channels, height, width) -> (batch_size * height * width, in_channels)\n            n, c, h, w = x.shape\n            x_reshaped = x.permute(0, 2, 3, 1).contiguous().view(n * h * w, c)\n            # Weight is (out_channels, in_channels) -> we want to do: y = x_reshaped @ self.weight.T\n            # We will call a Triton kernel for this matmul.\n\n   Step 12: Write the Triton kernel for matmul.\n\n   We write a kernel that does: \n        C = A @ B^T, where A is (n, k), B is (m, k), and C is (n, m)\n\n   But note: we can also do C = A @ B^T by having B stored as (m, k) and then we read it row by row.\n\n   We design the kernel to use blocks.\n\n   We use the standard matmul kernel from the tutorial.\n\n   However, we must support arbitrary sizes.\n\n   We write the kernel with autotuning.\n\n   But note: the problem constraints: \n        - We must not change the grid configuration? Actually, we are writing a new kernel so we define the grid.\n\n   We define the grid as (triton.cdiv(n, BLOCK_M), triton.cdiv(m, BLOCK_N)).\n\n   We are allowed to change the block sizes for tuning.\n\n   Step 13: We write the kernel.\n\n   However, note that the current program must be self-contained. We are going to include the Triton kernel inside the EVOLVE-BLOCK.\n\n   We are going to write:\n\n        @triton.jit\n        def matmul_kernel( ... )\n\n   But note: the entire model definition and the kernel must be in the same block.\n\n   We must also import triton and triton.language.\n\n   We add the necessary imports.\n\n   Step 14: We must consider the performance.\n\n   The current performance: speed_up=0.834 (which is worse than the reference). We aim to improve it.\n\n   We hope that the Triton kernel for 1x1 convolution will be faster than the PyTorch one.\n\n   Step 15: We note that the model has many 1x1 convolutions. For example, the conv_dw layers have one 1x1 convolution per layer.\n\n   We hope to see a speedup.\n\n   However, note that the depthwise convolutions are still in PyTorch. They might dominate the runtime.\n\n   This is a first step.\n\n   Given the complexity, we decide to do only the 1x1 convolutions in Triton.\n\n   We write the code accordingly.\n\n   We must be cautious: the Triton kernel must be numerically equivalent.\n\n   We use float32 for now.\n\n   Step 16: We write the Triton matmul kernel.\n\n   We base it on the tutorial.\n\n   We note that the tutorial kernel does: \n        C = A @ B, where A (n,k) and B (k,m)\n\n   We want C = A @ B^T.\n\n   How? We can set B to be of shape (k, m) and then do A @ B? Then we are computing A @ B, which is (n,k) @ (k,m) -> (n,m).\n\n   But we have the weight matrix as (m, k). We can transpose it to (k, m) and then do A @ B.\n\n   So in the module, we store the weight as (k, m) and then use the standard matmul kernel.\n\n   Alternatively, we can write a kernel that does A @ B^T without transposing B.\n\n   We choose to transpose the weight in the module to avoid writing a new kernel.\n\n   We do:\n\n        self.weight_t = self.weight.t().contiguous()\n\n   Then, in the kernel, we use the standard matmul kernel with:\n        A = x_reshaped (n, k)\n        B = self.weight_t (k, m)\n\n        then C = A @ B = x_reshaped @ self.weight_t = x_reshaped @ (self.weight)^T = same as x_reshaped @ self.weight^T.\n\n   So we can use the standard matmul kernel.\n\n   We use the kernel from the tutorial.\n\n   Step 17: We write the Triton kernel for matmul.\n\n   We use the kernel from: https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html\n\n   We copy the kernel and adjust the parameters.\n\n   We note that the tutorial kernel uses:\n        a_ptr, b_ptr, c_ptr, \n        M, N, K, \n        stride_am, stride_ak, \n        stride_bk, stride_bn, \n        stride_cm, stride_cn, ...\n\n   We set:\n        A: (M, K) -> M = n, K = k\n        B: (K, N) -> N = m\n        C: (M, N)\n\n   We set the strides accordingly.\n\n   Step 18: We call the kernel with the grid and block.\n\n   We set BLOCK_M, BLOCK_N, BLOCK_K as tunable.\n\n   We use autotuning.\n\n   Step 19: We must also handle the bias.\n\n   After the matmul, we add the bias if present.\n\n   We do that on the host (in PyTorch) because it is a simple add.\n\n   Step 20: We reshape the output to (batch_size, out_channels, height, width).\n\n   Then return.\n\n   Step 21: We must test the code.\n\n   But note: the problem says we are to return the code. We hope it compiles.\n\n   Given the complexity, we write the code for the EVOLVE-BLOCK.\n\n   We note that the current code in the EVOLVE-BLOCK is the entire model. We are going to replace the conv_dw to use TritonConv2d for the 1x1 convolution and write the TritonConv2d class and the matmul kernel.\n\n   We also add the necessary imports.\n\n   We note that the original code had:\n        import torch\n        import torch.nn as nn\n        import torch.nn.functional as F\n\n   We add:\n        import triton\n        import triton.language as tl\n\n   We write the kernel and the class.\n\n   Step 22: We must be cautious about the memory layout.\n\n   We use contiguous tensors.\n\n   Step 23: We write the code.\n\n   However, note: the problem says to make small changes. We are changing about 100 lines? \n\n   We hope that it is acceptable.\n\n   Alternatively, we could have started with a simpler kernel? But we are replacing the 1x1 convolution.\n\n   We write the code.\n\n   IMPORTANT: We must not change the existing code outside the EVOLVE-BLOCK? But the entire code is within the block.\n\n   We are replacing the entire block.\n\n   We output the new code.\n\n   Note: We are leaving the 3x3 convolutions as nn.Conv2d. We only replace the 1x1 in conv_dw.\n\n   We hope that the performance improves.\n\n   If it doesn't, we can try to optimize the kernel further.\n\n   But for now, we do this.\n\n   Let's write the code.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\nclass TritonConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_channels))\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        if self.kernel_size != 1 or self.stride != 1 or self.padding != 0:\n            return F.conv2d(x, self.weight, self.bias, self.stride, self.padding)\n        \n        n, c, h, w = x.shape\n        x_reshaped = x.permute(0, 2, 3, 1).contiguous().view(n * h * w, c)\n        weight_reshaped = self.weight.view(self.weight.size(0), -1).t().contiguous()\n        output = torch.empty(n * h * w, self.weight.size(0), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(x_reshaped.size(0), META['BLOCK_M']), \n                            triton.cdiv(weight_reshaped.size(1), META['BLOCK_N']))\n        \n        _matmul_kernel[grid](x_reshaped, weight_reshaped, output, \n                            x_reshaped.size(0), weight_reshaped.size(1), x_reshaped.size(1),\n                            x_reshaped.stride(0), x_reshaped.stride(1),\n                            weight_reshaped.stride(0), weight_reshaped.stride(1),\n                            output.stride(0), output.stride(1))\n        \n        output = output.view(n, h, w, -1).permute(0, 3, 1, 2)\n        return output\n\n@triton.jit\ndef _matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n        b_mask = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk * stride_bn\n    \n    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                TritonConv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
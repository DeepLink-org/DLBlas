{"id": "fc5e3924-0753-4dcd-88cd-19a55b8f8ea7", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width, spatial_size,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    \n    total = 0.0\n    num_elements = spatial_size\n    for i in range(0, num_elements, BLOCK_SIZE):\n        spatial_indices = i + tl.arange(0, BLOCK_SIZE)\n        mask = spatial_indices < num_elements\n        h = spatial_indices // width\n        w = spatial_indices % width\n        \n        spatial_offsets = h * input_height_stride + w * input_width_stride\n        pointers = input_ptr + input_offset + spatial_offsets\n        \n        values = tl.load(pointers, mask=mask, other=0.0)\n        total += tl.sum(values)\n    \n    channel_avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        spatial_size = x.shape[2] * x.shape[3]\n        \n        # Optimize BLOCK_SIZE for 7x7 input (49 elements)\n        BLOCK_SIZE = 64  # Reduced from 1024 to match typical 7x7 feature maps\n        \n        grid = (batch_size, channels)\n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3], spatial_size,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * extreme), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "98dc58b1-9e15-418f-84de-abe367ebed90", "generation": 3, "timestamp": 1754588597.0072496, "iteration_found": 27, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "name 'extreme' is not defined\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/evaluator.py\", line 173, in _verification_worker\n    result: KernelExecResult = eval_kernel_against_ref(\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 484, in eval_kernel_against_ref\n    custom_model = ModelNew(*init_args, **init_kwargs)\n  File \"/tmp/tmpym99538c.py\", line 86, in __init__\n    conv_dw(int(256 * alpha), int(256 * extreme), 1),\nNameError: name 'extreme' is not defined\n", "exec_log": "[Child] \u7ed1\u5b9a GPU 5, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 9, in <module>\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 844, in jit\n    return decorator(fn)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 832, in decorator\n    return JITFunction(\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 635, in __init__\n    self.starting_line_number = inspect.getsourcelines(fn)[1]\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/inspect.py\", line 1121, in getsourcelines\n    lines, lnum = findsource(object)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/inspect.py\", line 958, in findsource\n    raise OSError('could not get source code')\nOSError: could not get source code\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.805, "runtime_ms": 1.69, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:4", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.36, "speed_up": 0.805, "custom_timing": 1.69}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.8050\n- runtime_ms: 1.6900\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:4', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.805, 'custom_timing': 1.69}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: speed_up. Consider continuing with similar changes.\n- Metrics showing regression: runtime_ms. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.2140, runtime_ms: 6.3700, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.214, 'custom_timing': 6.37}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7560, runtime_ms: 1.8000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.756, 'custom_timing': 1.8}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.3560, runtime_ms: 3.8200, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.356, 'custom_timing': 3.82}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef conv1x1_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    input_batch, input_channels, output_channels, input_height, input_width,\n    stride_ib, stride_ic, stride_ih, stride_iw,\n    stride_wc,\n    stride_ob, stride_oc, stride_oh, stride_ow,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pixels = input_height * input_width\n    num_blocks = tl.cdiv(num_pixels, BLOCK_SIZE)\n    batch_idx = pid // (output_channels * num_blocks)\n    channel_idx = (pid % (output_channels * num_blocks)) // num_blocks\n    block_idx = pid % num_blocks\n\n    # Calculate spatial offsets\n    pixel_start = block_idx * BLOCK_SIZE\n    pixel_offsets = pixel_start + tl.arange(0, BLOCK_SIZE)\n    valid_pixels = pixel_offsets < num_pixels\n    h = pixel_offsets // input_width\n    w = pixel_offsets % input_width\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Loop over input channels\n    for c in range(0, input_channels):\n        # Load input and weight\n        input_val = tl.load(\n            input_ptr + batch_idx * stride_ib + c * stride_ic + h * stride_ih + w * stride_iw,\n            mask=valid_pixels\n        )\n        weight_val = tl.load(weight_ptr + channel_idx * stride_wc + c)\n        \n        # Accumulate\n        acc += input_val * weight_val\n\n    # Store results\n    output_offset = (\n        batch_idx * stride_ob + \n        channel_idx * stride_oc + \n        h * stride_oh + \n        w * stride_ow\n    )\n    tl.store(output_ptr + output_offset, acc, mask=valid_pixels)\n\nclass Conv1x1Triton(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels))\n        nn.init.kaiming_uniform_(self.weight)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        batch, _, height, width = x.shape\n        output = torch.empty((batch, self.out_channels, height, width), \n                             device=x.device, dtype=x.dtype)\n        \n        # Calculate strides\n        stride_ib = x.stride(0)\n        stride_ic = x.stride(1)\n        stride_ih = x.stride(2)\n        stride_iw = x.stride(3)\n        stride_wc = self.weight.stride(0)\n        stride_ob = output.stride(0)\n        stride_oc = output.stride(1)\n        stride_oh = output.stride(2)\n        stride_ow = output.stride(3)\n\n        # Launch kernel\n        BLOCK_SIZE = 128\n        num_pixels = height * width\n        num_blocks = triton.cdiv(num_pixels, BLOCK_SIZE)\n        grid = (batch * self.out_channels * num_blocks,)\n        \n        conv1x1_kernel[grid](\n            x, self.weight, output,\n            batch, self.in_channels, self.out_channels, height, width,\n            stride_ib, stride_ic, stride_ih, stride_iw,\n            stride_wc,\n            stride_ob, stride_oc, stride_oh, stride_ow,\n            BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                Conv1x1Triton(inp, oup),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.3560), Performs well on runtime_ms (3.8200), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:6', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.356, 'custom_timing': 3.82})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, \n    output_ptr,\n    input_batch_stride, \n    input_channel_stride, \n    input_height_stride, \n    input_width_stride,\n    output_batch_stride, \n    output_channel_stride,\n    height, \n    width,\n    P2: tl.constexpr,   # the next power of two for the spatial dimension\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    spatial_size = height * width\n    spatial_indices = tl.arange(0, P2)\n    mask = spatial_indices < spatial_size\n\n    h = spatial_indices // width\n    w = spatial_indices % width\n    spatial_offsets = h * input_height_stride + w * input_width_stride\n    pointers = input_ptr + input_offset + spatial_offsets\n    \n    values = tl.load(pointers, mask=mask, other=0.0)\n    channel_sum = tl.sum(values, axis=0)\n    channel_avg = channel_sum / spatial_size\n    \n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg)\n\nclass TritonGlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, x):\n        N, C = x.shape[0], x.shape[1]\n        output = torch.empty((N, C), device=x.device, dtype=x.dtype)\n        \n        if x.is_contiguous():\n            x = x.contiguous()\n            \n        strides = x.stride()\n        spatial_size = x.shape[2] * x.shape[3]\n        P2 = 1 << (spatial_size - 1).bit_length()  # Next power of two\n        \n        grid = lambda _: (N, C)\n        global_avg_pool_kernel[grid](\n            x,\n            output,\n            strides[0],\n            strides[1],\n            strides[2],\n            strides[3],\n            output.stride(0),\n            output.stride(1),\n            x.shape[2],\n            x.shape[3],\n            P2=P2\n        )\n        return output\n\nclass DepthwiseConv2d(nn.Module):\n    def __init__(self, inp, oup, stride):\n        super().__init__()\n        self.conv = nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False)\n        self.bn = nn.BatchNorm2d(inp)\n        self.relu = nn.ReLU(inplace=True)\n        self.pointwise = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n        self.bn2 = nn.BatchNorm2d(oup)\n        self.relu2 = nn.ReLU(inplace=True)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.pointwise(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        return x\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            DepthwiseConv2d(int(32 * alpha), int(64 * alpha), 1),\n            DepthwiseConv2d(int(64 * alpha), int(128 * alpha), 2),\n            DepthwiseConv2d(int(128 * alpha), int(128 * alpha), 1),\n            DepthwiseConv2d(int(128 * alpha), int(256 * alpha), 2),\n            DepthwiseConv2d(int(256 * alpha), int(256 * alpha), 1),\n            DepthwiseConv2d(int(256 * alpha), int(512 * alpha), 2),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(1024 * alpha), 2),\n            DepthwiseConv2d(int(1024 * alpha), int(1024 * alpha), 1),\n            TritonGlobalAvgPool2d(),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7560), Performs well on runtime_ms (1.8000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.756, 'custom_timing': 1.8})\n\n\n### Program 3 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef conv1x1_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    input_batch, input_channels, output_channels, input_height, input_width,\n    stride_ib, stride_ic, stride_ih, stride_iw,\n    stride_wc,\n    stride_ob, stride_oc, stride_oh, stride_ow,\n    BLOCK_SIZE: tl.constexpr,\n    BLOCK_C: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pixels = input_height * input_width\n    num_blocks = tl.cdiv(num_pixels, BLOCK_SIZE)\n    batch_idx = pid // (output_channels * num_blocks)\n    channel_idx = (pid % (output_channels * num_blocks)) // num_blocks\n    block_idx = pid % num_blocks\n\n    # Calculate spatial offsets\n    pixel_start = block_idx * BLOCK_SIZE\n    pixel_offsets = pixel_start + tl.arange(0, BLOCK_SIZE)\n    valid_pixels = pixel_offsets < num_pixels\n    h = pixel_offsets // input_width\n    w = pixel_offsets % input_width\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    c_offsets = tl.arange(0, BLOCK_C)\n    \n    # Loop over input channels in blocks\n    for c_start in range(0, input_channels, BLOCK_C):\n        c_mask = c_offsets < (input_channels - c_start)\n        \n        # Loop within channel block\n        for i in range(0, BLOCK_C):\n            c = c_start + i\n            if c < input_channels:\n                # Load input and weight\n                input_val = tl.load(\n                    input_ptr + batch_idx * stride_ib + c * stride_ic + h * stride_ih + w * stride_iw,\n                    mask=valid_pixels\n                )\n                weight_val = tl.load(weight_ptr + channel_idx * stride_wc + c)\n                acc += input_val * weight_val\n\n    # Store results\n    output_offset = (\n        batch_idx * stride_ob + \n        channel_idx * stride_oc + \n        h * stride_oh + \n        w * stride_ow\n    )\n    tl.store(output_ptr + output_offset, acc, mask=valid_pixels)\n\nclass Conv1x1Triton(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels))\n        nn.init.kaiming_uniform_(self.weight)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        batch, _, height, width = x.shape\n        output = torch.empty((batch, self.out_channels, height, width), \n                             device=x.device, dtype=x.dtype)\n        \n        # Calculate strides\n        stride_ib = x.stride(0)\n        stride_ic = x.stride(1)\n        stride_ih = x.stride(2)\n        stride_iw = x.stride(3)\n        stride_wc = self.weight.stride(0)\n        stride_ob = output.stride(0)\n        stride_oc = output.stride(1)\n        stride_oh = output.stride(2)\n        stride_ow = output.stride(3)\n\n        # Launch kernel\n        BLOCK_SIZE = 128\n        BLOCK_C = 16\n        num_pixels = height * width\n        num_blocks = triton.cdiv(num_pixels, BLOCK_SIZE)\n        grid = (batch * self.out_channels * num_blocks,)\n        \n        conv1x1_kernel[grid](\n            x, self.weight, output,\n            batch, self.in_channels, self.out_channels, height, width,\n            stride_ib, stride_ic, stride_ih, stride_iw,\n            stride_wc,\n            stride_ob, stride_oc, stride_oh, stride_ow,\n            BLOCK_SIZE, BLOCK_C\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                Conv1x1Triton(inp, oup),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.2140), Performs well on runtime_ms (6.3700), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:5', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.214, 'custom_timing': 6.37})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width, spatial_size,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    \n    total = 0.0\n    num_elements = spatial_size\n    for i in range(0, num_elements, BLOCK_SIZE):\n        spatial_indices = i + tl.arange(0, BLOCK_SIZE)\n        mask = spatial_indices < num_elements\n        h = spatial_indices // width\n        w = spatial_indices % width\n        \n        spatial_offsets = h * input_height_stride + w * input_width_stride\n        pointers = input_ptr + input_offset + spatial_offsets\n        \n        values = tl.load(pointers, mask=mask, other=0.0)\n        total += tl.sum(values)\n    \n    channel_avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        spatial_size = x.shape[2] * x.shape[3]\n        \n        # Use fixed block size for tiled processing\n        BLOCK_SIZE = 1024\n        \n        grid = (batch_size, channels)\n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3], spatial_size,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_conv_bn_relu(\n    x_ptr, weight_ptr, bias_ptr, gamma_ptr, beta_ptr, \n    mean_ptr, var_ptr, output_ptr,\n    stride: tl.constexpr, groups: tl.constexpr,\n    H: tl.constexpr, W: tl.constexpr, eps: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < H * W\n    \n    # Load inputs\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    weight = tl.load(weight_ptr + offsets, mask=mask, other=0)\n    bias = tl.load(bias_ptr + offsets, mask=mask, other=0)\n    gamma = tl.load(gamma_ptr + offsets, mask=mask, other=0)\n    beta = tl.load(beta_ptr + offsets, mask=mask, other=0)\n    mean = tl.load(mean_ptr + offsets, mask=mask, other=0)\n    var = tl.load(var_ptr + offsets, mask=mask, other=0)\n    \n    # Fused convolution-batchnorm-relu\n    conv_out = x * weight + bias\n    bn_out = gamma * (conv_out - mean) / tl.sqrt(var + eps) + beta\n    relu_out = tl.maximum(bn_out, 0)\n    \n    tl.store(output_ptr + offsets, relu_out, mask=mask)\n\nclass DepthwiseConv2d(nn.Module):\n    def __init__(self, inp, oup, stride):\n        super().__init__()\n        self.conv = nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False)\n        self.bn = nn.BatchNorm2d(inp)\n        self.relu = nn.ReLU(inplace=True)\n        self.pointwise = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n        self.bn2 = nn.BatchNorm2d(oup)\n        self.relu2 = nn.ReLU(inplace=True)\n        \n    def forward(self, x):\n        # Depthwise convolution fused with Triton\n        N, C, H, W = x.shape\n        weight = self.conv.weight\n        bias = torch.zeros_like(self.bn.bias)  # Conv has no bias\n        \n        # Prepare inputs for Triton kernel\n        x_flat = x.view(N, C, -1)\n        out = torch.empty_like(x_flat)\n        grid = lambda _: (C,)\n        \n        fused_conv_bn_relu[grid](\n            x_flat, weight.view(C, -1), bias, \n            self.bn.weight, self.bn.bias,\n            self.bn.running_mean, self.bn.running_var,\n            out, self.conv.stride[0], self.conv.groups,\n            H, W, self.bn.eps, BLOCK_SIZE=H*W\n        )\n        \n        x = out.view(N, C, H, W)\n        x = self.pointwise(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        return x\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            DepthwiseConv2d(int(32 * alpha), int(64 * alpha), 1),\n            DepthwiseConv2d(int(64 * alpha), int(128 * alpha), 2),\n            DepthwiseConv2d(int(128 * alpha), int(128 * alpha), 1),\n            DepthwiseConv2d(int(128 * alpha), int(256 * alpha), 2),\n            DepthwiseConv2d(int(256 * alpha), int(256 * alpha), 1),\n            DepthwiseConv2d(int(256 * alpha), int(512 * alpha), 2),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(1024 * alpha), 2),\n            DepthwiseConv2d(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef conv1x1_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    input_batch, input_channels, output_channels, input_height, input_width,\n    stride_ib, stride_ic, stride_ih, stride_iw,\n    stride_wc,\n    stride_ob, stride_oc, stride_oh, stride_ow,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pixels = input_height * input_width\n    num_blocks = tl.cdiv(num_pixels, BLOCK_SIZE)\n    batch_idx = pid // (output_channels * num_blocks)\n    channel_idx = (pid % (output_channels * num_blocks)) // num_blocks\n    block_idx = pid % num_blocks\n\n    # Calculate spatial offsets\n    pixel_start = block_idx * BLOCK_SIZE\n    pixel_offsets = pixel_start + tl.arange(0, BLOCK_SIZE)\n    valid_pixels = pixel_offsets < num_pixels\n    h = pixel_offsets // input_width\n    w = pixel_offsets % input_width\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Loop over input channels\n    for c in range(0, input_channels):\n        # Load input and weight\n        input_val = tl.load(\n            input_ptr + batch_idx * stride_ib + c * stride_ic + h * stride_ih + w * stride_iw,\n            mask=valid_pixels\n        )\n        weight_val = tl.load(weight_ptr + channel_idx * stride_wc + c)\n        \n        # Accumulate\n        acc += input_val * weight_val\n\n    # Store results\n    output_offset = (\n        batch_idx * stride_ob + \n        channel_idx * stride_oc + \n        h * stride_oh + \n        w * stride_ow\n    )\n    tl.store(output_ptr + output_offset, acc, mask=valid_pixels)\n\nclass Conv1x1Triton(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels))\n        nn.init.kaiming_uniform_(self.weight)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        batch, _, height, width = x.shape\n        output = torch.empty((batch, self.out_channels, height, width), \n                             device=x.device, dtype=x.dtype)\n        \n        # Calculate strides\n        stride_ib = x.stride(0)\n        stride_ic = x.stride(1)\n        stride_ih = x.stride(2)\n        stride_iw = x.stride(3)\n        stride_wc = self.weight.stride(0)\n        stride_ob = output.stride(0)\n        stride_oc = output.stride(1)\n        stride_oh = output.stride(2)\n        stride_ow = output.stride(3)\n\n        # Launch kernel\n        BLOCK_SIZE = 128\n        num_pixels = height * width\n        num_blocks = triton.cdiv(num_pixels, BLOCK_SIZE)\n        grid = (batch * self.out_channels * num_blocks,)\n        \n        conv1x1_kernel[grid](\n            x, self.weight, output,\n            batch, self.in_channels, self.out_channels, height, width,\n            stride_ib, stride_ic, stride_ih, stride_iw,\n            stride_wc,\n            stride_ob, stride_oc, stride_oh, stride_ow,\n            BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                Conv1x1Triton(inp, oup),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_conv_bn_relu(\n    x_ptr, weight_ptr, bias_ptr, gamma_ptr, beta_ptr, \n    mean_ptr, var_ptr, output_ptr,\n    stride: tl.constexpr, groups: tl.constexpr,\n    H: tl.constexpr, W: tl.constexpr, eps: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < H * W\n    \n    # Load inputs\n    x = tl.load(x_ptr + offsets, mask=mask, other=0)\n    weight = tl.load(weight_ptr + offsets, mask=mask, other=0)\n    bias = tl.load(bias_ptr + offsets, mask=mask, other=0)\n    gamma = tl.load(gamma_ptr + offsets, mask=mask, other=0)\n    beta = tl.load(beta_ptr + offsets, mask=mask, other=0)\n    mean = tl.load(mean_ptr + offsets, mask=mask, other=0)\n    var = tl.load(var_ptr + offsets, mask=mask, other=0)\n    \n    # Fused convolution-batchnorm-relu\n    conv_out = x * weight + bias\n    bn_out = gamma * (conv_out - mean) / tl.sqrt(var + eps) + beta\n    relu_out = tl.maximum(bn_out, 0)\n    \n    tl.store(output_ptr + offsets, relu_out, mask=mask)\n\nclass DepthwiseConv2d(nn.Module):\n    def __init__(self, inp, oup, stride):\n        super().__init__()\n        self.conv = nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False)\n        self.bn = nn.BatchNorm2d(inp)\n        self.relu = nn.ReLU(inplace=True)\n        self.pointwise = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n        self.bn2 = nn.BatchNorm2d(oup)\n        self.relu2 = nn.ReLU(inplace=True)\n        \n    def forward(self, x):\n        # Depthwise convolution fused with Triton\n        N, C, H, W = x.shape\n        weight = self.conv.weight\n        bias = torch.zeros_like(self.bn.bias)  # Conv has no bias\n        \n        # Prepare inputs for Triton kernel\n        x_flat = x.view(N, C, -1)\n        out = torch.empty_like(x_flat)\n        grid = lambda _: (C,)\n        \n        fused_conv_bn_relu[grid](\n            x_flat, weight.view(C, -1), bias, \n            self.bn.weight, self.bn.bias,\n            self.bn.running_mean, self.bn.running_var,\n            out, self.conv.stride[0], self.conv.groups,\n            H, W, self.bn.eps, BLOCK_SIZE=H*W\n        )\n        \n        x = out.view(N, C, H, W)\n        x = self.pointwise(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        return x\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            DepthwiseConv2d(int(32 * alpha), int(64 * alpha), 1),\n            DepthwiseConv2d(int(64 * alpha), int(128 * alpha), 2),\n            DepthwiseConv2d(int(128 * alpha), int(128 * alpha), 1),\n            DepthwiseConv2d(int(128 * alpha), int(256 * alpha), 2),\n            DepthwiseConv2d(int(256 * alpha), int(256 * alpha), 1),\n            DepthwiseConv2d(int(256 * alpha), int(512 * alpha), 2),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(512 * alpha), 1),\n            DepthwiseConv2d(int(512 * alpha), int(1024 * alpha), 2),\n            DepthwiseConv2d(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef conv1x1_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    input_batch, input_channels, output_channels, input_height, input_width,\n    stride_ib, stride_ic, stride_ih, stride_iw,\n    stride_wc,\n    stride_ob, stride_oc, stride_oh, stride_ow,\n    BLOCK_SIZE: tl.constexpr,\n    BLOCK_C: tl.constexpr\n):\n    pid = tl.program_id(0)\n    num_pixels = input_height * input_width\n    num_blocks = tl.cdiv(num_pixels, BLOCK_SIZE)\n    batch_idx = pid // (output_channels * num_blocks)\n    channel_idx = (pid % (output_channels * num_blocks)) // num_blocks\n    block_idx = pid % num_blocks\n\n    # Calculate spatial offsets\n    pixel_start = block_idx * BLOCK_SIZE\n    pixel_offsets = pixel_start + tl.arange(0, BLOCK_SIZE)\n    valid_pixels = pixel_offsets < num_pixels\n    h = pixel_offsets // input_width\n    w = pixel_offsets % input_width\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    c_offsets = tl.arange(0, BLOCK_C)\n    \n    # Loop over input channels in blocks\n    for c_start in range(0, input_channels, BLOCK_C):\n        c_mask = c_offsets < (input_channels - c_start)\n        \n        # Loop within channel block\n        for i in range(0, BLOCK_C):\n            c = c_start + i\n            if c < input_channels:\n                # Load input and weight\n                input_val = tl.load(\n                    input_ptr + batch_idx * stride_ib + c * stride_ic + h * stride_ih + w * stride_iw,\n                    mask=valid_pixels\n                )\n                weight_val = tl.load(weight_ptr + channel_idx * stride_wc + c)\n                acc += input_val * weight_val\n\n    # Store results\n    output_offset = (\n        batch_idx * stride_ob + \n        channel_idx * stride_oc + \n        h * stride_oh + \n        w * stride_ow\n    )\n    tl.store(output_ptr + output_offset, acc, mask=valid_pixels)\n\nclass Conv1x1Triton(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels))\n        nn.init.kaiming_uniform_(self.weight)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        batch, _, height, width = x.shape\n        output = torch.empty((batch, self.out_channels, height, width), \n                             device=x.device, dtype=x.dtype)\n        \n        # Calculate strides\n        stride_ib = x.stride(0)\n        stride_ic = x.stride(1)\n        stride_ih = x.stride(2)\n        stride_iw = x.stride(3)\n        stride_wc = self.weight.stride(0)\n        stride_ob = output.stride(0)\n        stride_oc = output.stride(1)\n        stride_oh = output.stride(2)\n        stride_ow = output.stride(3)\n\n        # Launch kernel\n        BLOCK_SIZE = 128\n        BLOCK_C = 16\n        num_pixels = height * width\n        num_blocks = triton.cdiv(num_pixels, BLOCK_SIZE)\n        grid = (batch * self.out_channels * num_blocks,)\n        \n        conv1x1_kernel[grid](\n            x, self.weight, output,\n            batch, self.in_channels, self.out_channels, height, width,\n            stride_ib, stride_ic, stride_ih, stride_iw,\n            stride_wc,\n            stride_ob, stride_oc, stride_oh, stride_ow,\n            BLOCK_SIZE, BLOCK_C\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                Conv1x1Triton(inp, oup),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 4 (Score: 1.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width, num_channels,\n    GROUP_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_group = tl.program_id(1)\n    \n    spatial_size = height * width\n    channel_base = pid_group * GROUP_SIZE\n    channel_offsets = tl.arange(0, GROUP_SIZE)\n    channel_mask = channel_offsets < (num_channels - channel_base)\n    \n    # Compute spatial indices and offsets\n    spatial_idx = tl.arange(0, spatial_size)\n    h = spatial_idx // width\n    w = spatial_idx % width\n    spatial_offsets = h * input_height_stride + w * input_width_stride\n    \n    # Base pointer for the batch\n    batch_offset = pid_batch * input_batch_stride\n    # Pointers: [GROUP_SIZE, spatial_size]\n    channel_ptr = input_ptr + batch_offset + (channel_base + channel_offsets[:, None]) * input_channel_stride\n    ptrs = channel_ptr + spatial_offsets[None, :]\n    \n    # Load all spatial values for the group of channels\n    values = tl.load(ptrs, mask=channel_mask[:, None], other=0.0)\n    channel_sums = tl.sum(values, axis=1)\n    channel_avg = channel_sums / spatial_size\n    \n    # Store the results\n    output_ptr_base = output_ptr + pid_batch * output_batch_stride + (channel_base + channel_offsets) * output_channel_stride\n    tl.store(output_ptr_base, channel_avg, mask=channel_mask)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        \n        # Use channel grouping to reduce launch overhead\n        GROUP_SIZE = 4\n        grid = (batch_size, triton.cdiv(channels, GROUP_SIZE))\n        \n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3], channels,\n            GROUP_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * extreme), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 5 (Score: -100.0000, Type: Exploratory)\n```python\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, \n    output_ptr,\n    input_batch_stride, \n    input_channel_stride, \n    input_height_stride, \n    input_width_stride,\n    output_batch_stride, \n    output_channel_stride,\n    height, \n    width,\n    P2: tl.constexpr,   # the next power of two for the spatial dimension (block size)\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    spatial_size = height * width\n    spatial_indices = tl.arange(0, P2)\n    mask = spatial_indices < spatial_size   # we only have spatial_size elements\n\n    # Compute h and w for each spatial index\n    h = spatial_indices // width\n    w = spatial_indices % width\n\n    spatial_offsets = h * input_height_stride + w * input_width_stride\n    pointers = input_ptr + input_offset + spatial_offsets\n    \n    values = tl.load(pointers, mask=mask, other=0.0)\n    channel_sum = tl.sum(values)\n    channel_avg = channel_sum / spatial_size   # note: we use the actual spatial_size, not P2\n    \n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg)\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width, spatial_size,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    \n    total = 0.0\n    num_elements = spatial_size\n    for i in range(0, num_elements, BLOCK_SIZE):\n        spatial_indices = i + tl.arange(0, BLOCK_SIZE)\n        mask = spatial_indices < num_elements\n        h = spatial_indices // width\n        w = spatial_indices % width\n        \n        spatial_offsets = h * input_height_stride + w * input_width_stride\n        pointers = input_ptr + input_offset + spatial_offsets\n        \n        values = tl.load(pointers, mask=mask, other=0.0)\n        total += tl.sum(values)\n    \n    channel_avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        spatial_size = x.shape[2] * x.shape[3]\n        \n        # Use fixed block size for tiled processing\n        BLOCK_SIZE = 1024\n        \n        grid = (batch_size, channels)\n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3], spatial_size,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program which uses a Triton kernel for global average pooling.\n The current performance shows a speed_up of 0.805 (which is less than 1, meaning slower than the reference) and a runtime of 1.69 ms (reference is 1.36 ms).\n\n The kernel in the current program (global_avg_pool_kernel) uses a tiled approach with a fixed BLOCK_SIZE (1024) and iterates over the spatial dimension in blocks.\n However, note that the grid is launched with (batch_size, channels) and each program iterates over the entire spatial dimension for one channel in one batch.\n\n The problem might be that the kernel is doing a lot of work per channel (iterating over 224*224=50176 elements in blocks of 1024, which is 49 iterations per channel).\n Also, the kernel uses a scalar accumulation (total) and then a scalar store at the end.\n\n We can try to optimize by:\n  1. Using a reduction tree within the block to compute the sum more efficiently.\n  2. Adjusting the BLOCK_SIZE to better utilize the GPU.\n\n However, note that the current kernel does not take advantage of parallel reduction within the spatial dimension because each channel is handled by a single program.\n\n Alternative approach from the inspiration programs:\n  - Program D1 (GlobalAvgPool2dTriton) uses a similar tiled approach but with a fixed block size and grid over (batch, channels).\n  - Inspiration 4 (global_avg_pool_kernel) uses a grouped approach over channels (GROUP_SIZE) and processes multiple channels per program.\n\n But note: the current program's kernel already has two-dimensional grid (batch, channels). The problem is that each program processes one channel and one batch, and the spatial dimension is processed in a loop.\n\n We can change the kernel to process multiple channels per program? However, the current structure of the model uses the GlobalAvgPool2dTriton at the end of the model, and the output is (batch, channels). So we have one value per channel per batch.\n\n Alternatively, we can restructure the kernel to use a reduction tree in shared memory? But note: the spatial dimension is 50176, which is too large to load all at once.\n\n Another idea: instead of iterating over the spatial dimension in a loop, we can launch a kernel that uses a two-dimensional grid (batch, channel) and then within the kernel, we use a reduction tree over the spatial dimension. However, the spatial dimension is large and we must use a reduction tree that spans multiple blocks? That would complicate.\n\n Alternatively, we can use a different strategy: let each program handle a block of spatial elements and a block of channels, and then use atomic operations? But atomic operations for float might be slow.\n\n However, note that the current kernel is already doing a reduction per channel per batch. The issue is that it is using a sequential reduction over the spatial dimension. We can change the inner loop to do a reduction in parallel within the block? But note: the block in Triton is one-dimensional and we are using a loop over the spatial blocks. The reduction is across the entire spatial dimension, so we cannot do it in one block.\n\n Therefore, we might consider changing the kernel to use a two-stage reduction? That would be too heavy for just a global average pooling.\n\n Alternatively, we can try to increase the parallelism by processing multiple channels per program? But note: the grid is (batch, channels). We are already launching one program per channel per batch. That is a lot of programs (batch * channels). For batch=10 and channels=1024, that is 10240 programs. That should be enough to saturate the GPU.\n\n Why is it slow then?\n  - Each program is doing 49 iterations (for 50176 elements with block size 1024). Each iteration loads a block of 1024 elements and sums them. Then the partial sums are added to a scalar. The scalar reduction at the end is not the bottleneck.\n\n Another possibility: the memory access pattern. The kernel loads a block of 1024 elements at a time. The input is of shape [N, C, H, W]. The kernel accesses:\n      pointers = input_ptr + input_offset + spatial_offsets\n  where input_offset = batch_idx * stride_batch + channel_idx * stride_channel.\n\n  The spatial_offsets are computed as h * stride_height + w * stride_width. The indices h and w are computed from spatial_indices (which are consecutive in the linearized space). However, note that consecutive spatial_indices might not be consecutive in memory because the image is stored in row-major. But that is the natural order.\n\n  However, the load is for consecutive spatial_indices? Actually, the spatial_offsets for consecutive spatial_indices are consecutive in memory? Not exactly: because h = index // width, w = index % width. So for consecutive indices, we get consecutive rows? Actually, consecutive indices in the same row would be consecutive, but when we wrap to the next row, we jump. So the load of 1024 consecutive indices in the linearized space would be 1024 consecutive pixels? Only if the row length is a multiple of 1024? Otherwise, it would be one row and then the next.\n\n  But note: the image is 224x224, and 224 is not divisible by 1024? Actually, we are loading 1024 elements at a time, but they are consecutive in the linearized space. That means we are loading a contiguous block of 1024 elements in the flattened spatial dimension. That is contiguous in memory? The image is stored as [H, W] and flattened in row-major. So if we have 224*224=50176, then the flattened array is contiguous. Therefore, the load of 1024 consecutive elements is contiguous and should be efficient.\n\n  What else? The current BLOCK_SIZE=1024 might be too large? Or too small? We can try to tune it.\n\n  Alternatively, we can try to use a different approach: instead of a loop, we can use a reduction tree that is done in one shot by having a large number of threads per program. But note: the spatial dimension is 50176, which is too big for one block. Therefore, we have to use a loop.\n\n  Another idea: use the fact that we have many channels and batches and do a two-dimensional reduction? We could launch a kernel with a two-dimensional grid (batch, channel) and a one-dimensional block (BLOCK_SIZE) and then use shared memory to reduce the block and then do a second reduction? But Triton doesn't have shared memory? Actually, Triton has shared memory via `tl.static` and `tl.make_block_ptr`? But we are not using it.\n\n  Alternatively, we can use the reduction primitives provided by Triton? But the example in the documentation does a reduction in one dimension by using a loop over the reduction dimension and accumulating in registers.\n\n  Actually, the current approach is the standard way: using a loop to reduce over a large dimension. The problem might be that the grid is too large? Or that the kernel is not optimized for memory access.\n\n  Let's try to change the BLOCK_SIZE to a smaller value? Why? Because 1024 might be too large and we are not having enough parallelism? Actually, the grid is (batch, channels) and we are processing one channel per program. The number of programs is batch * channels. For batch=10 and channels=1024, we have 10240 programs. That should be enough.\n\n  But note: the H100 has many cores, but 10240 programs is sufficient.\n\n  Another possibility: the kernel is memory bound? The global average pool requires reading the entire input and then writing a much smaller output. The input is [10, 1024, 7, 7] (because the model ends with a 7x7 feature map) ??? Wait, in the model the last conv_dw outputs [batch, 1024, 7, 7] and then we do global average pooling? Actually, in the model we have:\n\n        conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        nn.AvgPool2d(7),\n\n  But wait, the current Triton kernel is for global average pooling? However, the model in the current program uses:\n\n        self.avgpool = GlobalAvgPool2dTriton()\n\n  and the input to this layer is [batch, 1024, 7, 7]? Because the previous layer is conv_dw that does not change the spatial size? Actually, the last conv_dw has stride=1? And then we have an average pool of 7x7? That is, the spatial size is 7x7? Then the spatial_size is 49.\n\n  Why then the kernel uses BLOCK_SIZE=1024? And the spatial_size is 49? Then in the kernel, the loop will run only once? Because 0 to 49 in steps of 1024 -> one iteration.\n\n  So the kernel is actually only doing one iteration? Then why is it slow?\n\n  Let me check the model: the input image is 224x224, and the model has several downsampling layers (strides). The last conv_dw before the avgpool has stride=1 and the previous one had stride=2, so the spatial size at the end is 7x7.\n\n  Therefore, the spatial_size is 49. The kernel sets BLOCK_SIZE=1024, which is larger than 49, so the loop runs only once.\n\n  Then the kernel does:\n      spatial_indices = tl.arange(0, 1024)  [but we have mask for indices < 49]\n      h = spatial_indices // 7, w = spatial_indices % 7 -> but note: the width is 7? Actually, the input to the kernel is height=7, width=7.\n\n  Then the kernel loads 1024 elements? But we only have 49. It loads 49 elements and then zeros for the rest? Then it sums the 49 elements and then divides by 49.\n\n  The problem: we are loading 1024 elements but only 49 are valid. That is inefficient? Because we are loading 1024 elements but only 49 are used. The memory access is not coalesced? Actually, the 49 elements are contiguous in the linearized space? Because the spatial dimension is contiguous. So the first 49 elements are contiguous and the rest are masked.\n\n  But note: the load of 1024 elements might be done in a few cache lines? It's not too bad. However, we are using a lot of threads (1024 per program) to process only 49 elements? That is inefficient.\n\n  We can change the BLOCK_SIZE to be the next power of two of 49? Or set it to 64? Then we would have one block of 64, and the mask would be for the first 49.\n\n  How to set BLOCK_SIZE? We can set it to 64 (which is 2^6) and then we have one iteration. Then each program uses 64 threads? But the grid is (batch, channels) and for batch=10, channels=1024, that is 10240 programs, and each program uses 64 threads? That might be too many threads? Actually, the H100 has a lot of cores, but we are limited by the number of threads per SM.\n\n  Alternatively, we can set BLOCK_SIZE to the exact spatial_size? But that is 49, which is not a power of two. Triton requires BLOCK_SIZE to be a power of two? Actually, no, we can use any size? But the mask is computed correctly.\n\n  But note: the kernel uses `tl.arange(0, BLOCK_SIZE)` and then a mask. We can set BLOCK_SIZE=49? Then we would have one iteration and exactly 49 threads per program? Then we are launching 10240 programs, each with 49 threads? That would be 501760 threads? That might be acceptable.\n\n  However, the performance might be better if we set BLOCK_SIZE to a multiple of the warp size (32) and at least 64? Because 49 is not aligned.\n\n  Alternatively, we can change the kernel to not use a loop and set the BLOCK_SIZE to the next power of two of the spatial_size? But then we are still wasting threads.\n\n  Another idea: we can change the kernel to use a fixed BLOCK_SIZE that is the minimum of the spatial_size and 1024? But in our case, spatial_size is 49, so we set BLOCK_SIZE=64.\n\n  Let me change the kernel to set BLOCK_SIZE to 64 for this specific case? But the kernel is designed to work for any spatial_size. We can compute the BLOCK_SIZE in the kernel? Actually, we can pass it as a parameter.\n\n  Currently, the BLOCK_SIZE is set to 1024 in the Python code. We can change it to be the minimum of 1024 and the spatial_size? But that doesn't help because we are already only doing one iteration.\n\n  Alternatively, we can avoid the loop entirely when the spatial_size is less than or equal to BLOCK_SIZE? That is already the case.\n\n  The real problem might be that we are launching too many threads (10240 * 64) and the work per thread is very little? The kernel only does 49 loads and one store? And the reduction is done by the `tl.sum` over a block of 49? The `tl.sum` might be implemented with a reduction tree and requires log2(49) steps? That is acceptable.\n\n  But note: the `tl.sum` is over the entire BLOCK_SIZE (64) but we have a mask? The masked elements are zero. So it does 64 elements? That is 6 steps? That is acceptable.\n\n  Why is the kernel slow? Maybe because the grid is too large? 10240 blocks? And each block has 64 threads? That is 655360 threads? But the H100 has 14592 CUDA cores? That is more than enough.\n\n  Another possibility: the kernel is not the bottleneck? The model has many layers and the global average pooling is only a small part? Then why is the Triton version slower? Because the rest of the model is the same? The current program replaces the last AvgPool2d with a Triton kernel? But the rest of the model is standard PyTorch.\n\n  We must note: the current program uses a custom Triton global average pooling, but the reference uses `nn.AvgPool2d(7)`. The reference might be faster because it is optimized in PyTorch? And our Triton kernel might be slower because it is doing a generic reduction? And the input is [10,1024,7,7]? That is 10*1024*7*7 = 501760 elements. The reduction is over 49 elements per channel per batch.\n\n  The reference `nn.AvgPool2d(7)` might be implemented with a highly optimized CUDA kernel? And our Triton kernel is a naive reduction? So we might not be able to beat it? But the goal is to speed up the entire model? The current program only replaces the global average pooling? Then the rest of the model is the same? So the comparison is between the Triton global average pooling and the PyTorch one.\n\n  The current Triton kernel is slower? How can we make it faster?\n\n  We can try to change the kernel to process multiple channels per program? Then we would have fewer programs and more work per program? And better utilization of the threads? For example, we can set a GROUP_SIZE (like in Inspiration 4) and process multiple channels at once.\n\n  Let me try to restructure the kernel to process a group of channels per program? We'll change the grid to (batch, triton.cdiv(channels, GROUP_SIZE)) and then in the kernel, we process GROUP_SIZE channels.\n\n  Steps:\n    - The kernel will have an additional program_id for the channel group.\n    - We'll load multiple channels at once? And then do the reduction for each channel in the group.\n\n  We'll set GROUP_SIZE to 4 or 8? Then the grid becomes (batch, ceil(channels/GROUP_SIZE)).\n\n  Then each program will process GROUP_SIZE channels. We'll loop over the spatial dimension? But note: the spatial dimension is small (49). We can load all the spatial elements for one channel? Actually, we can load the entire channel in one go? Because 49 is small.\n\n  We can do:\n      for c in range(GROUP_SIZE):\n          if c < actual_channels_in_this_group:\n              for s in range(0, spatial_size, BLOCK_SIZE):\n                  ... load a block of spatial elements for channel c ...\n\n  But that would be two loops? And we are still doing a reduction per channel? We can use a vector of accumulators? One per channel in the group.\n\n  Alternatively, we can use a two-dimensional block: the first dimension over the group and the second over the spatial? But Triton doesn't have two-dimensional blocks? We can use a 1D block and then use modulo arithmetic.\n\n  Actually, we can do:\n\n      pid_batch = tl.program_id(0)\n      pid_group = tl.program_id(1)\n\n      channel_base = pid_group * GROUP_SIZE\n      channel_offsets = tl.arange(0, GROUP_SIZE)\n      channel_mask = channel_offsets < (channels - channel_base)\n\n      spatial_indices = tl.arange(0, spatial_size)   # but spatial_size might be 49, which is small? We can have a block of size GROUP_SIZE * spatial_size? That would be 4*49=196, which is acceptable.\n\n      # But note: the maximum block size in Triton is 1024? And 4*49=196 < 1024.\n\n  So we can do:\n\n      block = GROUP_SIZE * spatial_size   -> but we want to process all spatial elements for all channels in the group? We can set the block size to the next power of two of (GROUP_SIZE * spatial_size)? Or we can set it to GROUP_SIZE and then loop over the spatial dimension? But then we are back to a loop.\n\n  Alternatively, we can set the block size to spatial_size? Then we have GROUP_SIZE channels and each channel is processed by one thread? That would be inefficient because we have 49 threads per channel? But then we have 49*GROUP_SIZE threads? That might be too many? Actually, we are limited by the block size (max 1024). So if GROUP_SIZE=4 and spatial_size=49, then 4*49=196, which is acceptable.\n\n  We can do:\n\n      block_size = GROUP_SIZE * spatial_size   # but we can set it to the next power of two? Or just set it to 196? But Triton requires block size to be a power of two? Actually, no, we can set any block size? But the mask will take care of the boundaries.\n\n  However, the kernel will be launched with one block per batch and per channel group? And the block has 196 threads? Then the grid would be (batch, ceil(channels/GROUP_SIZE)) and each block has 196 threads.\n\n  Steps in the kernel:\n\n      pid_batch = tl.program_id(0)\n      pid_group = tl.program_id(1)\n\n      idx = tl.program_id(2)   # but we don't have a third dimension? We can use a 1D block and then use:\n\n      tid = tl.program_id(2)   # but the grid is 3D? Actually, we can set the grid to be 3D? But the kernel only has 2D grid? We are using (batch, group) as the grid, and then the block has 196 threads? Actually, in Triton, the grid is for the number of blocks? And each block has a fixed number of threads? But in Triton, the block is implicit? Actually, the kernel is written in a way that the block is defined by the `tl.arange` and the block size.\n\n  How about:\n\n      We set the grid to (batch, ceil(channels/GROUP_SIZE)) and then the block size to the next power of two of (GROUP_SIZE * spatial_size)? But we don't need to because we can use a 1D block and then use:\n\n      pid = tl.program_id(0)   # we are not using the block index? Actually, the kernel is written for a 2D grid (batch, channel) and no block? Now we want a 2D grid (batch, group) and a 1D block of size BLOCK (which is the group and spatial combined).\n\n  Actually, we can restructure the kernel to use a 2D grid and a 1D block? But the current kernel doesn't use the block index? It uses the program_id for the batch and channel, and then the block for the spatial tiling.\n\n  We are going to write a new kernel:\n\n      @triton.jit\n      def global_avg_pool_kernel(\n          input_ptr, output_ptr,\n          input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n          output_batch_stride, output_channel_stride,\n          height, width, channels,\n          spatial_size: tl.constexpr, GROUP_SIZE: tl.constexpr, BLOCK_SIZE: tl.constexpr\n      ):\n          pid_batch = tl.program_id(0)\n          pid_group = tl.program_id(1)\n          # We are going to have a block that spans the group and the spatial dimension? But we want to reduce the spatial dimension per channel.\n\n          # Let the block be over the spatial dimension and the group dimension? We can use a 1D block of size BLOCK_SIZE, and then we map the block to (GROUP_SIZE, spatial_size) by:\n          #   group_index = tid // spatial_size   -> but we don't have enough threads? We want to cover GROUP_SIZE * spatial_size.\n\n          # Instead, we can use a loop over the spatial dimension? But we want to avoid that.\n\n          # Alternative: we can use the block to cover the entire group and spatial? Then BLOCK_SIZE must be at least GROUP_SIZE * spatial_size? But that might be too big? We can set BLOCK_SIZE = GROUP_SIZE * spatial_size? And then use one block? But then we are limited by the maximum block size? The maximum block size in Triton is 1024? And GROUP_SIZE * spatial_size might be 4*49=196, which is less than 1024.\n\n          # So we can do:\n\n          tid = tl.arange(0, BLOCK_SIZE)\n          channel_in_group = tid // spatial_size   # [0, GROUP_SIZE-1]\n          spatial_idx = tid % spatial_size\n\n          # Then we need to check if this tid is within the valid range: channel_in_group < GROUP_SIZE and spatial_idx < spatial_size? Actually, we set BLOCK_SIZE = GROUP_SIZE * spatial_size, so we cover exactly.\n\n          # But note: we might have a group that is not full? Then we need a mask for the channels that are beyond the actual number of channels? We do:\n\n          channel_base = pid_group * GROUP_SIZE\n          channel_idx = channel_base + channel_in_group\n          channel_mask = (channel_idx < channels) & (tid < BLOCK_SIZE)   # and also the spatial_idx is always < spatial_size by modulo?\n\n          # Actually, spatial_idx = tid % spatial_size, so it's always in [0, spatial_size-1]. But we need to mask the channels that are beyond.\n\n          # Now, compute h and w:\n          h = spatial_idx // width\n          w = spatial_idx % width\n\n          # Compute the pointer for this element:\n          input_offset = pid_batch * input_batch_stride + channel_idx * input_channel_stride + h * input_height_stride + w * input_width_stride\n          # But note: the input_offset is for one element? Then we load one element.\n\n          # But we want to load the entire group and spatial? We can load with:\n          values = tl.load(input_ptr + input_offset, mask=channel_mask)\n\n          # Then we want to reduce over the spatial dimension per channel? We can use tl.atomic_add? But we want to avoid atomics.\n\n          # Instead, we can use a reduction using tl.associative_scan? Or we can use shared memory? But Triton doesn't have shared memory? We can use static arrays?\n\n          # We can do a reduction per channel in the group? We can use:\n\n          #   Create an array of accumulators for the group? We can use a static array of size GROUP_SIZE? But note: the block is 196, and we have 4 channels? Then we can do:\n\n          #   Use tl.reshape to group by channel_in_group? Then we have a 2D array [GROUP_SIZE, spatial_size]? Then we can do a reduction over the second dimension.\n\n          # But we have to do it with a loop? Because we are limited by the block size? Actually, we can use tl.sum over the spatial dimension for each channel? But we don't have a direct way to do that.\n\n          # Alternatively, we can use a reduction tree? We can use:\n\n          #   First, we store the values in a 2D array? But we don't have 2D arrays? We can use a 1D array and then use a loop to reduce over the spatial dimension per channel.\n\n          # We can do:\n\n          #   accum = tl.zeros((GROUP_SIZE,), dtype=tl.float32)\n          #   for i in range(spatial_size):\n          #       idx = channel_in_group * spatial_size + i   # but we don't have a loop? We are already using a block that covers the entire group and spatial.\n\n          #   Actually, we have all the values loaded? Then we can do:\n\n          #   We can use a reduction across the spatial dimension for each channel? We can use tl.associative_scan with the axis=1? But we don't have 2D arrays.\n\n          #   Alternatively, we can use a two-step reduction? First, we reduce within the block for each channel? Then we store the result? But note: the entire channel is in the block? So we can do:\n\n          #   accum = tl.zeros((GROUP_SIZE,), dtype=tl.float32)\n          #   accum = tl.where(channel_mask, accum + values, accum)   # but this is not a reduction tree.\n\n          #   Actually, we can use tl.reshape to make a 2D array? Then do a reduction along the second dimension? But Triton doesn't support 2D arrays natively? We can use:\n\n          #   values_2d = tl.reshape(values, (GROUP_SIZE, spatial_size))\n          #   Then we can do:\n          #       channel_sum = tl.sum(values_2d, axis=1)\n          #   But Triton doesn't have a 2D array type? And the reshape is not supported? Actually, we can use:\n\n          #   We can use a loop over the spatial dimension for each channel? That would be:\n\n          #       for i in range(spatial_size):\n          #           idx = channel_in_group * spatial_size + i\n          #           value = values[idx]   -> but we don't have the entire array in a contiguous buffer? We have loaded the values in a 1D array of size BLOCK_SIZE? But we have the `values` array? Actually, we have loaded one value per thread? And we have 196 threads? Then we cannot access the entire array from one thread.\n\n          # This approach is too complex.\n\n  Given the complexity, and the fact that the spatial_size is small (49), we can instead do:\n\n      We set the block size to spatial_size? Then we have one block per channel per batch? But that is the same as the current approach? And then we do a reduction over the block? That is what we are doing now? But we are using a scalar accumulation? We can use a reduction tree within the block? The current code uses `tl.sum(values)` which is a reduction tree? And it's efficient.\n\n  So what is the problem? The current kernel is:\n\n      total = 0.0\n      num_elements = spatial_size\n      for i in range(0, num_elements, BLOCK_SIZE):\n          ... load a block ...\n          total += tl.sum(values)\n\n  For spatial_size=49 and BLOCK_SIZE=1024, we do one iteration. The `tl.sum` is over 49 elements? But the block size is 1024? That means we are launching 1024 threads but only 49 are active? The other 975 threads are idle? That is a waste.\n\n  We should set BLOCK_SIZE to 64 (the next power of two of 49 is 64) and then we have one iteration. Then we only launch 64 threads per program? And only 49 are active? That is better than 1024.\n\n  Let me change the BLOCK_SIZE to 64 in the Python code? And see if it helps.\n\n  We are currently setting:\n        BLOCK_SIZE = 1024\n\n  We can change it to:\n        BLOCK_SIZE = 64\n\n  But note: the spatial_size might be larger in other models? The kernel is general. We want to handle any spatial_size. So we can set:\n\n        BLOCK_SIZE = min(64, spatial_size)   # but we cannot because the kernel requires BLOCK_SIZE to be a compile-time constant.\n\n  Alternatively, we can pass spatial_size to the kernel and then set BLOCK_SIZE to the next power of two of spatial_size? But that might be too big? Or we can set it to a fixed value that is the maximum we want? But we want to avoid wasting threads.\n\n  Another idea: set BLOCK_SIZE to 128? And then if spatial_size is small, we only have one iteration? But for 49, we still use 128 threads? That is better than 1024.\n\n  Let's try setting BLOCK_SIZE to 128.\n\n  We'll change in the Python code:\n        BLOCK_SIZE = 128   # instead of 1024\n\n  This change might reduce the number of inactive threads and improve performance.\n\n  We'll make this change and hope for a speedup.\n\n  Note: we must be cautious because the kernel might be used with larger spatial sizes? But in this model, the spatial_size is 49 at the last layer. But the kernel is general? It might be used in other contexts? But the task is to optimize for this model.\n\n  Alternatively, we can dynamically choose the BLOCK_SIZE based on the spatial_size? But we cannot because it is a constant. We can have multiple kernels? But that's complex.\n\n  We'll change the BLOCK_SIZE to 128.\n\n  Let's rewrite the current program with that change.\n\n  Only change one line: in the `GlobalAvgPool2dTriton.forward` method, change `BLOCK_SIZE = 1024` to `BLOCK_SIZE = 128`.\n\n  We hope that this reduces the number of inactive threads and speeds up the kernel.\n\n  We are not changing the kernel code, only the parameter.\n\n  This is a small change.\n\n  If it doesn't work, we might try a more sophisticated approach.\n\n  Let's output the code with that change.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width, spatial_size,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    \n    total = 0.0\n    num_elements = spatial_size\n    for i in range(0, num_elements, BLOCK_SIZE):\n        spatial_indices = i + tl.arange(0, BLOCK_SIZE)\n        mask = spatial_indices < num_elements\n        h = spatial_indices // width\n        w = spatial_indices % width\n        \n        spatial_offsets = h * input_height_stride + w * input_width_stride\n        pointers = input_ptr + input_offset + spatial_offsets\n        \n        values = tl.load(pointers, mask=mask, other=0.0)\n        total += tl.sum(values)\n    \n    channel_avg = total / num_elements\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        spatial_size = x.shape[2] * x.shape[3]\n        \n        # Optimize BLOCK_SIZE for 7x7 input (49 elements)\n        BLOCK_SIZE = 64  # Reduced from 1024 to match typical 7x7 feature maps\n        \n        grid = (batch_size, channels)\n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3], spatial_size,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * extreme), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
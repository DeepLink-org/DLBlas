{"id": "9af0e2a5-95c1-434e-8e92-49fe5c9feb52", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width, num_channels,\n    GROUP_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_group = tl.program_id(1)\n    \n    spatial_size = height * width\n    channel_base = pid_group * GROUP_SIZE\n    channel_offsets = tl.arange(0, GROUP_SIZE)\n    channel_mask = channel_offsets < (num_channels - channel_base)\n    \n    # Compute spatial indices and offsets\n    spatial_idx = tl.arange(0, spatial_size)\n    h = spatial_idx // width\n    w = spatial_idx % width\n    spatial_offsets = h * input_height_stride + w * input_width_stride\n    \n    # Base pointer for the batch\n    batch_offset = pid_batch * input_batch_stride\n    # Pointers: [GROUP_SIZE, spatial_size]\n    channel_ptr = input_ptr + batch_offset + (channel_base + channel_offsets[:, None]) * input_channel_stride\n    ptrs = channel_ptr + spatial_offsets[None, :]\n    \n    # Load all spatial values for the group of channels\n    values = tl.load(ptrs, mask=channel_mask[:, None], other=0.0)\n    channel_sums = tl.sum(values, axis=1)\n    channel_avg = channel_sums / spatial_size\n    \n    # Store the results\n    output_ptr_base = output_ptr + pid_batch * output_batch_stride + (channel_base + channel_offsets) * output_channel_stride\n    tl.store(output_ptr_base, channel_avg, mask=channel_mask)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        \n        # Use channel grouping to reduce launch overhead\n        GROUP_SIZE = 4\n        grid = (batch_size, triton.cdiv(channels, GROUP_SIZE))\n        \n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3], channels,\n            GROUP_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * extreme), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "65732aa1-3ea1-438e-a54c-5548886ed56a", "generation": 3, "timestamp": 1754587294.9718497, "iteration_found": 22, "metrics": {"compiled": false, "correctness": false, "score": 1, "stage": "compile_error", "compile_log": "name 'extreme' is not defined\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/evaluator.py\", line 173, in _verification_worker\n    result: KernelExecResult = eval_kernel_against_ref(\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 484, in eval_kernel_against_ref\n    custom_model = ModelNew(*init_args, **init_kwargs)\n  File \"/tmp/tmp2or_4_0y.py\", line 94, in __init__\n    conv_dw(int(512 * extreme), int(512 * alpha), 1),\nNameError: name 'extreme' is not defined\n", "exec_log": "[Child] \u7ed1\u5b9a GPU 1, torch sees 1 device(s)\n\n[Exception]\nTraceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/code_executor.py\", line 71, in _exec_worker\n    exec(textwrap.dedent(code_src), mod.__dict__)\n  File \"<string>\", line 9, in <module>\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 844, in jit\n    return decorator(fn)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 832, in decorator\n    return JITFunction(\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 635, in __init__\n    self.starting_line_number = inspect.getsourcelines(fn)[1]\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/inspect.py\", line 1121, in getsourcelines\n    lines, lnum = findsource(object)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/inspect.py\", line 958, in findsource\n    raise OSError('could not get source code')\nOSError: could not get source code\n", "exit_code": "1"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.791, "runtime_ms": 1.72, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.36, "speed_up": 0.791, "custom_timing": 1.72}}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.7910\n- runtime_ms: 1.7200\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.791, 'custom_timing': 1.72}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level3_19_MobileNetV1', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpy1_rfccf.py\", line 120, in forward\\n    x = self.model(x)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\\n    input = module(input)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\\n    input = module(input)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpy1_rfccf.py\", line 32, in forward\\n    _matmul_kernel[grid](x_reshaped, weight_reshaped, output,\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 561, in run\\n    bound_args, specialization, options = binder(*args, **kwargs)\\nTypeError: dynamic_func() missing 3 required positional arguments: \\'BLOCK_M\\', \\'BLOCK_N\\', and \\'BLOCK_K\\'\\n', 'runtime_error_name': 'builtins.TypeError', 'reference_timing_ms': 1.36, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7910, runtime_ms: 1.7200, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.791, 'custom_timing': 1.72}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.3580, runtime_ms: 3.8000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.358, 'custom_timing': 3.8}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef conv1x1_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    input_batch, input_channels, output_channels, input_height, input_width,\n    stride_ib, stride_ic, stride_ih, stride_iw,  # Added stride_ib\n    stride_wc,\n    stride_ob, stride_oc, stride_oh, stride_ow,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pixels = input_height * input_width\n    num_blocks = tl.cdiv(num_pixels, BLOCK_SIZE)\n    batch_idx = pid // (output_channels * num_blocks)\n    channel_idx = (pid % (output_channels * num_blocks)) // num_blocks\n    block_idx = pid % num_blocks\n\n    # Calculate spatial offsets\n    pixel_start = block_idx * BLOCK_SIZE\n    pixel_offsets = pixel_start + tl.arange(0, BLOCK_SIZE)\n    valid_pixels = pixel_offsets < num_pixels\n    h = pixel_offsets // input_width\n    w = pixel_offsets % input_width\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Loop over input channels\n    for c in range(0, input_channels):\n        # Load input and weight\n        input_val = tl.load(\n            input_ptr + batch_idx * stride_ib + c * stride_ic + h * stride_ih + w * stride_iw,\n            mask=valid_pixels\n        )\n        weight_val = tl.load(weight_ptr + channel_idx * stride_wc + c)\n        \n        # Accumulate\n        acc += input_val * weight_val\n\n    # Store results\n    output_offset = (\n        batch_idx * stride_ob + \n        channel_idx * stride_oc + \n        h * stride_oh + \n        w * stride_ow\n    )\n    tl.store(output_ptr + output_offset, acc, mask=valid_pixels)\n\nclass Conv1x1Triton(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels))\n        nn.init.kaiming_uniform_(self.weight)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        batch, _, height, width = x.shape\n        output = torch.empty((batch, self.out_channels, height, width), \n                             device=x.device, dtype=x.dtype)\n        \n        # Calculate strides\n        stride_ib = x.stride(0)\n        stride_ic = x.stride(1)\n        stride_ih = x.stride(2)\n        stride_iw = x.stride(3)\n        stride_wc = self.weight.stride(0)\n        stride_ob = output.stride(0)\n        stride_oc = output.stride(1)\n        stride_oh = output.stride(2)\n        stride_ow = output.stride(3)\n\n        # Launch kernel\n        BLOCK_SIZE = 128\n        num_pixels = height * width\n        num_blocks = triton.cdiv(num_pixels, BLOCK_SIZE)\n        grid = (batch * self.out_channels * num_blocks,)\n        \n        conv1x1_kernel[grid](\n            x, self.weight, output,\n            batch, self.in_channels, self.out_channels, height, width,\n            stride_ib, stride_ic, stride_ih, stride_iw,  # Added stride_ib\n            stride_wc,\n            stride_ob, stride_oc, stride_oh, stride_ow,\n            BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                Conv1x1Triton(inp, oup),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.3580), Performs well on runtime_ms (3.8000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.358, 'custom_timing': 3.8})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    spatial_indices = tl.arange(0, BLOCK_SIZE)\n    h = spatial_indices // width\n    w = spatial_indices % width\n    mask = spatial_indices < (height * width)\n    \n    spatial_offsets = h * input_height_stride + w * input_width_stride\n    pointers = input_ptr + input_offset + spatial_offsets\n    \n    values = tl.load(pointers, mask=mask, other=0.0)\n    channel_sum = tl.sum(values)\n    channel_avg = channel_sum / (height * width)\n    \n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size, channels)\n        spatial_size = x.shape[2] * x.shape[3]\n        # Compute next power of two for BLOCK_SIZE\n        block_size_pow2 = 1\n        while block_size_pow2 < spatial_size:\n            block_size_pow2 *= 2\n\n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3],\n            BLOCK_SIZE=block_size_pow2\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7910), Performs well on runtime_ms (1.7200), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.791, 'custom_timing': 1.72})\n\n\n### Program 3 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\nclass TritonConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_channels))\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        if self.kernel_size != 1 or self.stride != 1 or self.padding != 0:\n            return F.conv2d(x, self.weight, self.bias, self.stride, self.padding)\n        \n        n, c, h, w = x.shape\n        x_reshaped = x.permute(0, 2, 3, 1).contiguous().view(n * h * w, c)\n        weight_reshaped = self.weight.view(self.weight.size(0), -1).t().contiguous()\n        output = torch.empty(n * h * w, self.weight.size(0), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(x_reshaped.size(0), META['BLOCK_M']), \n                            triton.cdiv(weight_reshaped.size(1), META['BLOCK_N']))\n        \n        _matmul_kernel[grid](x_reshaped, weight_reshaped, output, \n                            x_reshaped.size(0), weight_reshaped.size(1), x_reshaped.size(1),\n                            x_reshaped.stride(0), x_reshaped.stride(1),\n                            weight_reshaped.stride(0), weight_reshaped.stride(1),\n                            output.stride(0), output.stride(1))\n        \n        output = output.view(n, h, w, -1).permute(0, 3, 1, 2)\n        return output\n\n@triton.jit\ndef _matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n        b_mask = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk * stride_bn\n    \n    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                TritonConv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level3_19_MobileNetV1', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpy1_rfccf.py\", line 120, in forward\\n    x = self.model(x)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\\n    input = module(input)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\\n    input = module(input)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpy1_rfccf.py\", line 32, in forward\\n    _matmul_kernel[grid](x_reshaped, weight_reshaped, output,\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 561, in run\\n    bound_args, specialization, options = binder(*args, **kwargs)\\nTypeError: dynamic_func() missing 3 required positional arguments: \\'BLOCK_M\\', \\'BLOCK_N\\', and \\'BLOCK_K\\'\\n', 'runtime_error_name': 'builtins.TypeError', 'reference_timing_ms': 1.36, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef conv1x1_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    input_batch, input_channels, output_channels, input_height, input_width,\n    stride_ib, stride_ic, stride_ih, stride_iw,\n    stride_wc,\n    stride_ob, stride_oc, stride_oh, stride_ow,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pixels = input_height * input_width\n    num_blocks = tl.cdiv(num_pixels, BLOCK_SIZE)\n    batch_idx = pid // (output_channels * num_blocks)\n    channel_idx = (pid % (output_channels * num_blocks)) // num_blocks\n    block_idx = pid % num_blocks\n\n    # Calculate spatial offsets\n    pixel_start = block_idx * BLOCK_SIZE\n    pixel_offsets = pixel_start + tl.arange(0, BLOCK_SIZE)\n    valid_pixels = pixel_offsets < num_pixels\n    h = pixel_offsets // input_width\n    w = pixel_offsets % input_width\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    TILE_CHANNELS = 4\n\n    # Loop over input channels with tiling\n    for c in range(0, input_channels, TILE_CHANNELS):\n        tile_channel_idx = tl.arange(0, TILE_CHANNELS)\n        channel_mask = (c + tile_channel_idx) < input_channels\n\n        # Load weight tile\n        weight_val = tl.load(\n            weight_ptr + channel_idx * stride_wc + c + tile_channel_idx,\n            mask=channel_mask,\n            other=0.0\n        )\n        \n        # Load input tile using broadcasting\n        base_ptr = input_ptr + batch_idx * stride_ib + (c + tile_channel_idx[:, None]) * stride_ic\n        spatial_offsets = h[None, :] * stride_ih + w[None, :] * stride_iw\n        input_val = tl.load(\n            base_ptr + spatial_offsets,\n            mask=channel_mask[:, None] & valid_pixels[None, :],\n            other=0.0\n        )\n        \n        # Accumulate with broadcasted multiplication\n        acc += tl.sum(weight_val[:, None] * input_val, axis=0)\n\n    # Store results\n    output_offset = (\n        batch_idx * stride_ob + \n        channel_idx * stride_oc + \n        h * stride_oh + \n        w * stride_ow\n    )\n    tl.store(output_ptr + output_offset, acc, mask=valid_pixels)\n\nclass Conv1x1Triton(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels))\n        nn.init.kaiming_uniform_(self.weight)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        batch, _, height, width = x.shape\n        output = torch.empty((batch, self.out_channels, height, width), \n                             device=x.device, dtype=x.dtype)\n        \n        # Calculate strides\n        stride_ib = x.stride(0)\n        stride_ic = x.stride(1)\n        stride_ih = x.stride(2)\n        stride_iw = x.stride(3)\n        stride_wc = self.weight.stride(0)\n        stride_ob = output.stride(0)\n        stride_oc = output.stride(1)\n        stride_oh = output.stride(2)\n        stride_ow = output.stride(3)\n\n        # Launch kernel with larger block size\n        BLOCK_SIZE = 256\n        num_pixels = height * width\n        num_blocks = triton.cdiv(num_pixels, BLOCK_SIZE)\n        grid = (batch * self.out_channels * num_blocks,)\n        \n        conv1x1_kernel[grid](\n            x, self.weight, output,\n            batch, self.in_channels, self.out_channels, height, width,\n            stride_ib, stride_ic, stride_ih, stride_iw,\n            stride_wc,\n            stride_ob, stride_oc, stride_oh, stride_ow,\n            BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                Conv1x1Triton(inp, oup),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\nclass TritonConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_channels))\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        if self.kernel_size != 1 or self.stride != 1 or self.padding != 0:\n            return F.conv2d(x, self.weight, self.bias, self.stride, self.padding)\n        \n        n, c, h, w = x.shape\n        x_reshaped = x.permute(0, 2, 3, 1).contiguous().view(n * h * w, c)\n        weight_reshaped = self.weight.view(self.weight.size(0), -1).t().contiguous()\n        output = torch.empty(n * h * w, self.weight.size(0), device=x.device, dtype=x.dtype)\n        \n        M, N, K = x_reshaped.size(0), weight_reshaped.size(1), x_reshaped.size(1)\n        \n        grid = lambda meta: (\n            triton.cdiv(M, meta['BLOCK_M']), \n            triton.cdiv(N, meta['BLOCK_N'])\n        )\n        \n        _matmul_kernel[grid](x_reshaped, weight_reshaped, output, \n                            M, N, K,\n                            x_reshaped.stride(0), x_reshaped.stride(1),\n                            weight_reshaped.stride(0), weight_reshaped.stride(1),\n                            output.stride(0), output.stride(1))\n        \n        output = output.view(n, h, w, -1).permute(0, 3, 1, 2)\n        return output\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n        b_mask = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    \n    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                TritonConv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef conv1x1_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    input_batch, input_channels, output_channels, input_height, input_width,\n    stride_ib, stride_ic, stride_ih, stride_iw,  # Added stride_ib\n    stride_wc,\n    stride_ob, stride_oc, stride_oh, stride_ow,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pixels = input_height * input_width\n    num_blocks = tl.cdiv(num_pixels, BLOCK_SIZE)\n    batch_idx = pid // (output_channels * num_blocks)\n    channel_idx = (pid % (output_channels * num_blocks)) // num_blocks\n    block_idx = pid % num_blocks\n\n    # Calculate spatial offsets\n    pixel_start = block_idx * BLOCK_SIZE\n    pixel_offsets = pixel_start + tl.arange(0, BLOCK_SIZE)\n    valid_pixels = pixel_offsets < num_pixels\n    h = pixel_offsets // input_width\n    w = pixel_offsets % input_width\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Loop over input channels\n    for c in range(0, input_channels):\n        # Load input and weight\n        input_val = tl.load(\n            input_ptr + batch_idx * stride_ib + c * stride_ic + h * stride_ih + w * stride_iw,\n            mask=valid_pixels\n        )\n        weight_val = tl.load(weight_ptr + channel_idx * stride_wc + c)\n        \n        # Accumulate\n        acc += input_val * weight_val\n\n    # Store results\n    output_offset = (\n        batch_idx * stride_ob + \n        channel_idx * stride_oc + \n        h * stride_oh + \n        w * stride_ow\n    )\n    tl.store(output_ptr + output_offset, acc, mask=valid_pixels)\n\nclass Conv1x1Triton(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels))\n        nn.init.kaiming_uniform_(self.weight)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        batch, _, height, width = x.shape\n        output = torch.empty((batch, self.out_channels, height, width), \n                             device=x.device, dtype=x.dtype)\n        \n        # Calculate strides\n        stride_ib = x.stride(0)\n        stride_ic = x.stride(1)\n        stride_ih = x.stride(2)\n        stride_iw = x.stride(3)\n        stride_wc = self.weight.stride(0)\n        stride_ob = output.stride(0)\n        stride_oc = output.stride(1)\n        stride_oh = output.stride(2)\n        stride_ow = output.stride(3)\n\n        # Launch kernel\n        BLOCK_SIZE = 128\n        num_pixels = height * width\n        num_blocks = triton.cdiv(num_pixels, BLOCK_SIZE)\n        grid = (batch * self.out_channels * num_blocks,)\n        \n        conv1x1_kernel[grid](\n            x, self.weight, output,\n            batch, self.in_channels, self.out_channels, height, width,\n            stride_ib, stride_ic, stride_ih, stride_iw,  # Added stride_ib\n            stride_wc,\n            stride_ob, stride_oc, stride_oh, stride_ow,\n            BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                Conv1x1Triton(inp, oup),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    spatial_indices = tl.arange(0, BLOCK_SIZE)\n    h = spatial_indices // width\n    w = spatial_indices % width\n    mask = spatial_indices < (height * width)\n    \n    spatial_offsets = h * input_height_stride + w * input_width_stride\n    pointers = input_ptr + input_offset + spatial_offsets\n    \n    values = tl.load(pointers, mask=mask, other=0.0)\n    channel_sum = tl.sum(values)\n    channel_avg = channel_sum / (height * width)\n    \n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size, channels)\n        spatial_size = x.shape[2] * x.shape[3]\n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3],\n            BLOCK_SIZE=spatial_size\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef conv1x1_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    input_batch, input_channels, output_channels, input_height, input_width,\n    stride_ib, stride_ic, stride_ih, stride_iw,\n    stride_wc,\n    stride_ob, stride_oc, stride_oh, stride_ow,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pixels = input_height * input_width\n    num_blocks = tl.cdiv(num_pixels, BLOCK_SIZE)\n    batch_idx = pid // (output_channels * num_blocks)\n    channel_idx = (pid % (output_channels * num_blocks)) // num_blocks\n    block_idx = pid % num_blocks\n\n    # Calculate spatial offsets\n    pixel_start = block_idx * BLOCK_SIZE\n    pixel_offsets = pixel_start + tl.arange(0, BLOCK_SIZE)\n    valid_pixels = pixel_offsets < num_pixels\n    h = pixel_offsets // input_width\n    w = pixel_offsets % input_width\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    TILE_CHANNELS = 4\n\n    # Loop over input channels with tiling\n    for c in range(0, input_channels, TILE_CHANNELS):\n        tile_channel_idx = tl.arange(0, TILE_CHANNELS)\n        channel_mask = (c + tile_channel_idx) < input_channels\n\n        # Load weight tile\n        weight_val = tl.load(\n            weight_ptr + channel_idx * stride_wc + c + tile_channel_idx,\n            mask=channel_mask,\n            other=0.0\n        )\n        \n        # Load input tile using broadcasting\n        base_ptr = input_ptr + batch_idx * stride_ib + (c + tile_channel_idx[:, None]) * stride_ic\n        spatial_offsets = h[None, :] * stride_ih + w[None, :] * stride_iw\n        input_val = tl.load(\n            base_ptr + spatial_offsets,\n            mask=channel_mask[:, None] & valid_pixels[None, :],\n            other=0.0\n        )\n        \n        # Accumulate with broadcasted multiplication\n        acc += tl.sum(weight_val[:, None] * input_val, axis=0)\n\n    # Store results\n    output_offset = (\n        batch_idx * stride_ob + \n        channel_idx * stride_oc + \n        h * stride_oh + \n        w * stride_ow\n    )\n    tl.store(output_ptr + output_offset, acc, mask=valid_pixels)\n\nclass Conv1x1Triton(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels))\n        nn.init.kaiming_uniform_(self.weight)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        batch, _, height, width = x.shape\n        output = torch.empty((batch, self.out_channels, height, width), \n                             device=x.device, dtype=x.dtype)\n        \n        # Calculate strides\n        stride_ib = x.stride(0)\n        stride_ic = x.stride(1)\n        stride_ih = x.stride(2)\n        stride_iw = x.stride(3)\n        stride_wc = self.weight.stride(0)\n        stride_ob = output.stride(0)\n        stride_oc = output.stride(1)\n        stride_oh = output.stride(2)\n        stride_ow = output.stride(3)\n\n        # Launch kernel with larger block size\n        BLOCK_SIZE = 256\n        num_pixels = height * width\n        num_blocks = triton.cdiv(num_pixels, BLOCK_SIZE)\n        grid = (batch * self.out_channels * num_blocks,)\n        \n        conv1x1_kernel[grid](\n            x, self.weight, output,\n            batch, self.in_channels, self.out_channels, height, width,\n            stride_ib, stride_ic, stride_ih, stride_iw,\n            stride_wc,\n            stride_ob, stride_oc, stride_oh, stride_ow,\n            BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                Conv1x1Triton(inp, oup),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 4 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\nclass TritonConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_channels))\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        if self.kernel_size != 1 or self.stride != 1 or self.padding != 0:\n            return F.conv2d(x, self.weight, self.bias, self.stride, self.padding)\n        \n        n, c, h, w = x.shape\n        x_reshaped = x.permute(0, 2, 3, 1).contiguous().view(n * h * w, c)\n        weight_reshaped = self.weight.view(self.weight.size(0), -1).t().contiguous()\n        output = torch.empty(n * h * w, self.weight.size(0), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(x_reshaped.size(0), META['BLOCK_M']), \n                            triton.cdiv(weight_reshaped.size(1), META['BLOCK_N']))\n        \n        _matmul_kernel[grid](x_reshaped, weight_reshaped, output, \n                            x_reshaped.size(0), weight_reshaped.size(1), x_reshaped.size(1),\n                            x_reshaped.stride(0), x_reshaped.stride(1),\n                            weight_reshaped.stride(0), weight_reshaped.stride(1),\n                            output.stride(0), output.stride(1))\n        \n        output = output.view(n, h, w, -1).permute(0, 3, 1, 2)\n        return output\n\n@triton.jit\ndef _matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n        b_mask = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk * stride_bn\n    \n    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                TritonConv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        \"\"\"\n        MobileNetV1 architecture implementation.\n\n        :param num_classes: The number of output classes (default: 1000)\n        :param input_channels: The number of input channels (default: 3 for RGB images)\n        :param alpha: Width multiplier (default: 1.0)\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_channels, height, width)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nUnique approach: Excellent compiled (1.000), Excellent correctness (1.000), Alternative passed approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    spatial_indices = tl.arange(0, BLOCK_SIZE)\n    h = spatial_indices // width\n    w = spatial_indices % width\n    mask = spatial_indices < (height * width)\n    \n    spatial_offsets = h * input_height_stride + w * input_width_stride\n    pointers = input_ptr + input_offset + spatial_offsets\n    \n    values = tl.load(pointers, mask=mask, other=0.0)\n    channel_sum = tl.sum(values)\n    channel_avg = channel_sum / (height * width)\n    \n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size, channels)\n        spatial_size = x.shape[2] * x.shape[3]\n        # Compute next power of two for BLOCK_SIZE\n        block_size_pow2 = 1\n        while block_size_pow2 < spatial_size:\n            block_size_pow2 *= 2\n\n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3],\n            BLOCK_SIZE=block_size_pow2\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given the current program which uses a Triton kernel for global average pooling.\n The current performance: speed_up = 0.7910 (which is less than 1, meaning slower than the reference)\n We are to improve the performance.\n\n The current Triton kernel for global average pooling:\n   - Uses a 2D grid (batch, channels)\n   - Each program loads a whole feature map (for one batch and one channel) and sums it.\n\n However, note that the current implementation uses a BLOCK_SIZE that is the next power of two of the spatial size.\n This might lead to loading more elements than necessary (if spatial size is not a power of two) and also might not be efficient.\n\n Proposed improvements:\n 1. Instead of having each program load the entire feature map (which might be large, e.g., 7x7=49 at the end of MobileNetV1, but earlier layers are larger),\n    we can use tiling: each program loads a tile and then we do a reduction across tiles.\n\n However, note that the spatial size at the point of global average pooling in MobileNetV1 is 7x7 (because after the last conv_dw we have a stride 2 and then an avgpool of 7).\n So the spatial size is 7x7 = 49. This is small.\n\n 2. Alternatively, we can change the kernel to use a two-stage reduction:\n    - First, each program in a 2D grid (batch, channel) reduces a tile of the feature map (if the feature map is large, but 49 is small, so we can do it in one block).\n    But note: in the current model, the input to the global average pooling is (batch, 1024, 7, 7). So the spatial size is 49.\n\n 3. The current kernel loads the entire feature map in one go. However, the number of elements (49) is less than the typical block size (e.g., 1024). So it is acceptable.\n\n 4. The problem might be that the kernel is launched with a grid of (batch, channels) and each kernel does 49 loads and then one store. The number of channels is 1024 and batch is 10 (in test). \n    This might not be efficient because the work per kernel is small (49 operations) and we have 10*1024=10240 kernels. We can try to use a vectorized load and reduce in a more efficient way.\n\n 5. Alternatively, we can change the kernel to have each program handle multiple channels? But note: the grid is (batch, channels) so each kernel is for one channel.\n\n 6. Another idea: use a 1D grid and have each program handle multiple channels? But then we might have to use shared memory for reduction? Not necessary because each channel is independent.\n\n 7. The current kernel uses a loop over the spatial indices? Actually, no: it uses a single load with a mask. This should be efficient.\n\n 8. However, note that the current kernel uses `tl.sum` over the entire feature map. The Triton compiler might generate efficient code for that.\n\n 9. Why is it slow? The reference implementation of global average pooling in PyTorch is very optimized. Our Triton kernel might be launching too many small kernels.\n\n 10. We can try to fuse the entire model? But the task is only to rewrite the global average pooling? Actually, the model has been modified to use a Triton global average pooling.\n\n 11. Alternatively, we can avoid using Triton for the global average pooling because the spatial size is small and the overhead of launching the kernel might be high.\n\n But note: the current program is already using Triton for global average pooling and we are to improve it.\n\n 12. Let's try to optimize the kernel by:\n     - Using a vectorized load: but the spatial size is 49, which is not a power of two? Actually, we are using a block size that is the next power of two (64). So we are loading 64 elements but only 49 are valid. This is acceptable.\n\n 13. However, we can change the block size to be exactly 49? But 49 is not a power of two. But Triton allows any block size? Actually, the kernel uses a BLOCK_SIZE that we set to the next power of two. We can set it to 49? But note the kernel uses:\n        spatial_indices = tl.arange(0, BLOCK_SIZE)\n        h = spatial_indices // width\n        w = spatial_indices % width\n        mask = spatial_indices < (height * width)\n\n     This works for any BLOCK_SIZE. We don't require it to be a power of two. So we can set BLOCK_SIZE = height * width.\n\n 14. Why did we use the next power of two? Probably to have a fixed block size that is efficient. But for 49, the next power of two is 64, so we are loading 15 extra zeros. This is not too bad.\n\n 15. The main issue might be the overhead of launching 10 * 1024 = 10240 kernels, each doing 49 loads and one store. The work per kernel is too small.\n\n 16. We can change the kernel to process multiple channels per program? For example, let each program handle a group of channels (e.g., 4 or 8) for one batch. Then we can reduce the grid size.\n\n Revised plan:\n   - Change the grid to be (batch, channels / GROUP_SIZE) and let each program handle GROUP_SIZE channels.\n   - We will use GROUP_SIZE=4 or 8 (a power of two) and then use vectorized operations.\n\n However, note: the output is (batch, channels). We are storing one value per channel.\n\n We can rewrite the kernel as:\n\n   pid_batch = tl.program_id(0)\n   pid_group = tl.program_id(1)   # group of channels\n\n   We then process GROUP_SIZE channels.\n\n   We'll use:\n      channel_base = pid_group * GROUP_SIZE\n      channel_offsets = tl.arange(0, GROUP_SIZE)\n      channel_mask = channel_offsets < num_channels - channel_base   # if we have incomplete group\n\n   Then for each channel in the group, we load the entire feature map? But note: the feature map for one channel is 49 elements. So for 4 channels, we would load 196 elements. This is acceptable.\n\n   We can then do:\n        for i in range(0, spatial_size, BLOCK_SIZE):\n            ... load a block of spatial indices for all channels in the group\n\n   However, note that the feature maps for different channels are contiguous in memory? Actually, they are separated by the channel stride.\n\n   Alternatively, we can load the entire feature map for multiple channels at once? We can use a 2D array of [GROUP_SIZE, spatial_size] and then reduce along the spatial dimension.\n\n   Steps in the kernel:\n\n        input_ptr + batch_idx * batch_stride + (channel_base + channel_offsets) * channel_stride + spatial_offset\n\n   We can load a block of spatial data for all channels in the group with a mask that accounts for both the channel mask and the spatial mask.\n\n   Then we can accumulate the sum for each channel.\n\n   But note: the spatial dimension is 49, which is small. We can load the entire spatial dimension at once? We are limited by the number of registers. For 4 channels and 49 elements, we would need 4*49=196 registers? That might be acceptable.\n\n   However, we can do it in tiles? But 49 is small, so we can do it without tiling.\n\n   Proposed kernel:\n\n        pid_batch = tl.program_id(0)\n        pid_group = tl.program_id(1)\n\n        GROUP_SIZE = 4   # for example\n\n        num_channels = ...  # we need to pass this as argument\n        spatial_size = height * width\n\n        channel_base = pid_group * GROUP_SIZE\n        channel_offsets = tl.arange(0, GROUP_SIZE)\n        channel_mask = channel_offsets < (num_channels - channel_base)\n\n        # We are going to load the entire spatial dimension for the group of channels\n        spatial_offsets = tl.arange(0, spatial_size)\n        spatial_mask = spatial_offsets < spatial_size\n\n        # We need to compute the pointers for each channel and each spatial element\n        batch_offset = pid_batch * input_batch_stride\n        channel_ptr = input_ptr + batch_offset + (channel_base + channel_offsets[:, None]) * input_channel_stride\n        spatial_ptr = spatial_offsets[None, :] * input_spatial_stride   # but note: the spatial dimension is 2D, so we have to compute h and w? \n\n        Actually, we cannot avoid computing h and w? But we can precompute the spatial offsets as in the original.\n\n        Alternatively, we can use the same method as the original to compute h and w? But note: the original kernel used:\n            spatial_indices = tl.arange(0, BLOCK_SIZE)\n            h = spatial_indices // width\n            w = spatial_indices % width\n            spatial_offsets = h * input_height_stride + w * input_width_stride\n\n        We can do the same for the entire spatial_size? But spatial_size might be 49, which is fixed? But we don't know at compile time.\n\n        So we can do:\n\n            spatial_idx = tl.arange(0, spatial_size)\n            h = spatial_idx // width\n            w = spatial_idx % width\n            spatial_offsets = h * input_height_stride + w * input_width_stride\n\n        Then the pointers for the group are:\n            ptrs = channel_ptr + spatial_offsets[None, :]\n\n        Then we load a [GROUP_SIZE, spatial_size] tensor.\n\n        Then we do:\n            values = tl.load(ptrs, mask=channel_mask[:, None] & spatial_mask[None, :], other=0.0)\n            sums = tl.sum(values, axis=1)   # reduce over the spatial dimension\n            averages = sums / spatial_size\n\n        Then we store the group of channels.\n\n   However, note: the spatial_size might be 0? We avoid by clamping? But in our case it is 49.\n\n   The grid would be:\n        grid = (batch, triton.cdiv(num_channels, GROUP_SIZE))\n\n   This reduces the grid size by a factor of GROUP_SIZE, which might help with launch overhead.\n\n   We choose GROUP_SIZE=4 or 8? Let's choose 4.\n\n   We must pass the num_channels and spatial_size (or height and width) to the kernel.\n\n   Also, note that the output must be stored at:\n        output_ptr + batch_idx * output_batch_stride + (channel_base + channel_offsets) * output_channel_stride\n\n 17. But note: the current model uses a variable `alpha` that scales the number of channels. The number of channels at the last layer is `int(1024 * alpha)`. We must pass the actual number of channels to the kernel.\n\n 18. We can try this approach and see if it improves performance.\n\n 19. Alternatively, we can note that the spatial size is small and the entire feature map for one batch and one channel is 49 elements. We can try to load the entire feature map for one channel in one vectorized load? We are already doing that.\n\n 20. The main bottleneck might be the launch overhead. Therefore, reducing the number of programs (by grouping channels) might help.\n\n Implementation:\n\n   We change the kernel and the wrapper as follows.\n\n   Kernel parameters: we add `num_channels` and `GROUP_SIZE` as a constant expression.\n\n   We also remove the BLOCK_SIZE and instead use the actual spatial_size.\n\n   However, note: the spatial_size might be large in other applications? But in our model it is 7x7=49. But we want the kernel to be general.\n\n   We can pass `spatial_size` and then use it as the block size for the spatial dimension? But then we are limited by the maximum block size? In Triton, the maximum number of elements in a dimension is 1024. 49 is safe.\n\n   But note: if we set the spatial dimension to 49, then we are using a block of 49. This is acceptable.\n\n   Let's write the kernel:\n\n        @triton.jit\n        def global_avg_pool_kernel(\n            input_ptr, output_ptr,\n            input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n            output_batch_stride, output_channel_stride,\n            height, width, num_channels,\n            GROUP_SIZE: tl.constexpr\n        ):\n            pid_batch = tl.program_id(0)\n            pid_group = tl.program_id(1)\n\n            spatial_size = height * width\n            channel_base = pid_group * GROUP_SIZE\n            channel_offsets = tl.arange(0, GROUP_SIZE)\n            channel_mask = channel_offsets < (num_channels - channel_base)\n\n            # Compute spatial indices and offsets\n            spatial_idx = tl.arange(0, spatial_size)\n            h = spatial_idx // width\n            w = spatial_idx % width\n            spatial_offsets = h * input_height_stride + w * input_width_stride\n\n            # Base pointer for the batch\n            batch_offset = pid_batch * input_batch_stride\n            # Pointers: [GROUP_SIZE, spatial_size]\n            channel_ptr = input_ptr + batch_offset + (channel_base + channel_offsets[:, None]) * input_channel_stride\n            ptrs = channel_ptr + spatial_offsets[None, :]\n\n            # Load all spatial values for the group of channels\n            values = tl.load(ptrs, mask=channel_mask[:, None] & (spatial_idx[None, :] < spatial_size), other=0.0)\n            channel_sums = tl.sum(values, axis=1)\n            channel_avg = channel_sums / spatial_size\n\n            # Store the results\n            output_ptr_base = output_ptr + pid_batch * output_batch_stride + (channel_base + channel_offsets) * output_channel_stride\n            tl.store(output_ptr_base, channel_avg, mask=channel_mask)\n\n   Then in the wrapper:\n\n        class GlobalAvgPool2dTriton(nn.Module):\n            def forward(self, x):\n                batch_size, channels = x.shape[0], x.shape[1]\n                output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n\n                # We'll use GROUP_SIZE=4\n                GROUP_SIZE = 4\n                grid = (batch_size, triton.cdiv(channels, GROUP_SIZE))\n\n                global_avg_pool_kernel[grid](\n                    x, output,\n                    x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n                    output.stride(0), output.stride(1),\n                    x.shape[2], x.shape[3], channels,\n                    GROUP_SIZE\n                )\n                return output\n\n 21. This reduces the grid size from (batch_size, channels) to (batch_size, ceil(channels/4)). For 1024 channels, we have 1024/4=256 programs in the channel dimension. Total grid size: batch_size * 256. For batch_size=10, that's 2560 programs (vs 10240 previously). This might reduce launch overhead.\n\n 22. Also, we are now doing 4 times the work per program, which might help with utilization.\n\n 23. We must be cautious: the kernel now uses more registers because it loads a [4,49] tensor. But 4*49=196 elements is acceptable.\n\n 24. We also changed the way we compute the spatial offsets: we compute them for the entire spatial_size. This is done once per program.\n\n 25. Let's try this and see if it improves the speed.\n\n 26. Note: we are passing `num_channels` as an argument to the kernel.\n\n 27. Also, note: the mask for the spatial dimension: `spatial_idx[None, :] < spatial_size` is always true because we set `spatial_idx = tl.arange(0, spatial_size)`. So we can skip that mask? But we pass `spatial_size = height * width` and then set `spatial_idx` from 0 to spatial_size-1? Actually, we set `spatial_idx = tl.arange(0, spatial_size)`, which goes from 0 to spatial_size-1. So we don't need the spatial mask? We do need if the spatial_size passed is less than the actual? But we set it to height*width. So it's exact. So we can remove the spatial mask? But note: we are using `spatial_size` as the size, so the indices are in [0, spatial_size-1]. So we don't need the mask for the spatial dimension? However, we have to mask the channels that are beyond the last group.\n\n 28. We'll keep the mask for the channels and remove the spatial mask? Actually, we are using the same spatial_size for the entire tensor. So it's safe. But we can keep the mask for the spatial dimension for correctness in case of non-divisible? Actually, we set `spatial_idx = tl.arange(0, spatial_size)` and then `spatial_size = height * width`, so it's exact. So we can remove the spatial mask.\n\n 29. We'll change the load to:\n        values = tl.load(ptrs, mask=channel_mask[:, None], other=0.0)\n\n   But wait: what if the spatial_offsets are not contiguous? We are computing them correctly. So the pointers are valid? Yes.\n\n 30. However, note: the spatial_offsets are computed as:\n        spatial_offsets = h * input_height_stride + w * input_width_stride\n      and then we add to the channel_ptr. This is correct.\n\n 31. Let's write the new kernel.\n\n 32. We'll change the kernel and the wrapper accordingly.\n\n 33. Also, note: the output storage: we store one value per channel. We are storing at the correct location.\n\n 34. We hope this will reduce the launch overhead and improve performance.\n\n 35. We must also note: the original kernel used a BLOCK_SIZE that was the next power of two. We are now using the exact spatial_size. This might be more efficient because we are not loading extra zeros.\n\n 36. We'll change the current program accordingly.\n\n 37. We are only changing the global_avg_pool_kernel and the GlobalAvgPool2dTriton class.\n\n 38. We leave the rest of the model unchanged.\n\n Let's rewrite the code accordingly.\n</think>\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width, num_channels,\n    GROUP_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_group = tl.program_id(1)\n    \n    spatial_size = height * width\n    channel_base = pid_group * GROUP_SIZE\n    channel_offsets = tl.arange(0, GROUP_SIZE)\n    channel_mask = channel_offsets < (num_channels - channel_base)\n    \n    # Compute spatial indices and offsets\n    spatial_idx = tl.arange(0, spatial_size)\n    h = spatial_idx // width\n    w = spatial_idx % width\n    spatial_offsets = h * input_height_stride + w * input_width_stride\n    \n    # Base pointer for the batch\n    batch_offset = pid_batch * input_batch_stride\n    # Pointers: [GROUP_SIZE, spatial_size]\n    channel_ptr = input_ptr + batch_offset + (channel_base + channel_offsets[:, None]) * input_channel_stride\n    ptrs = channel_ptr + spatial_offsets[None, :]\n    \n    # Load all spatial values for the group of channels\n    values = tl.load(ptrs, mask=channel_mask[:, None], other=0.0)\n    channel_sums = tl.sum(values, axis=1)\n    channel_avg = channel_sums / spatial_size\n    \n    # Store the results\n    output_ptr_base = output_ptr + pid_batch * output_batch_stride + (channel_base + channel_offsets) * output_channel_stride\n    tl.store(output_ptr_base, channel_avg, mask=channel_mask)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        \n        # Use channel grouping to reduce launch overhead\n        GROUP_SIZE = 4\n        grid = (batch_size, triton.cdiv(channels, GROUP_SIZE))\n        \n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3], channels,\n            GROUP_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * extreme), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
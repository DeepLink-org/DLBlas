{"id": "e72731e1-6793-4b11-8b7f-793ad56ec98a", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    pid_thread = tl.arange(0, BLOCK_SIZE)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    spatial_size = height * width\n    \n    # Initialize accumulator\n    accum = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Process spatial elements in chunks\n    num_chunks = tl.cdiv(spatial_size, BLOCK_SIZE)\n    for chunk_idx in range(num_chunks):\n        idx = chunk_idx * BLOCK_SIZE + pid_thread\n        mask = idx < spatial_size\n        \n        # Compute spatial indices\n        h = idx // width\n        w = idx % width\n        spatial_offset = h * input_height_stride + w * input_width_stride\n        ptr = input_ptr + input_offset + spatial_offset\n        \n        # Load and accumulate\n        values = tl.load(ptr, mask=mask, other=0.0)\n        accum += values\n    \n    # Tree reduction within block\n    s = BLOCK_SIZE // 2\n    while s >= 1:\n        accum += tl.shuffle(accum, pid_thread + s, BLOCK_SIZE)\n        s //= 2\n    \n    # Store final result (only thread 0)\n    if pid_thread == 0:\n        channel_avg = accum[0] / spatial_size\n        output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n        tl.store(output_ptr + output_offset, channel_avg)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size, channels)\n        BLOCK_SIZE = 1024  # Fixed block size for efficient parallel reduction\n        \n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3],\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "65732aa1-3ea1-438e-a54c-5548886ed56a", "generation": 3, "timestamp": 1754586492.0773048, "iteration_found": 16, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_19_MobileNetV1", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp4yynrh4c.py\", line 113, in forward\n    x = self.avgpool(x)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp4yynrh4c.py\", line 62, in forward\n    global_avg_pool_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'shuffle'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 1.36, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.791, "runtime_ms": 1.72, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:2", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.36, "speed_up": 0.791, "custom_timing": 1.72}}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are a Triton GPU core optimization engineer deeply familiar with NVIDIA H100/H800 architecture.\n\n==================== Context ====================\n\u2022 Dataset: 250 kernelbench/level1-level5 *.py files  \n  - Level1: simplest, Level5: most complex  \n\u2022 Each file contains PyTorch reference (reference_code)  \n\u2022 Goal: Generate functionally equivalent but faster Triton implementations (custom_code)  \n\u2022 Evaluation pipeline:\n    1) /verify_plus - Verify compilation & correctness  \n    2) /execute (optional) - Return terminal logs if compilation fails  \n    3) Scoring:  \n      - Speedup > 1 required for points  \n      - Score = (speed_up - 1) \u00d7 100  \n\u2022 GPU Hardware: NVIDIA L20Y / H800 (same SM8.9 as H100)  \n  - 3 TB/s HBM3 bandwidth  \n  - FP16 Tensor Core 32\u00d716\u00d716  \n\n==================== Evolution Phases ====================\n\u2460 Phase 1 [Compilation Pass]  \n  - Make Triton code compile (performance irrelevant)  \n\u2461 Phase 2 [Numerical Correctness]  \n  - Output must match PyTorch reference within float32 tolerance  \n\u2462 Phase 3 [Performance Gain]  \n  - Target \u22655% speedup (\u226515% = exceptional)  \n  - Official scoring based on speed_up field  \n\nThe Evolution system will pass the EVOLVE-BLOCK from the previous version.  \nModify ONLY the Triton kernel source within this block - all other code is LOCKED.\n\n==================== Constraints (MUST OBEY) ====================\n\u274c NEVER change @triton.jit function signatures or parameter names  \n\u274c NEVER modify grid configuration, output tensor shapes, or PID logic  \n\u274c NEVER remove boundary checks or out-of-bound masks  \n\u274c NEVER introduce race conditions, uninitialized reads, or sync gaps  \n\n\u2705 Allowed Optimizations (Examples):  \n  \u2022 Memory coalescing optimizations & vectorized tl.load/tl.store  \n  \u2022 Replace staged softmax with online/FlashAttention variants  \n  \u2022 Adaptive tuning: BLOCK_M/BLOCK_N/BLOCK_K dimensions  \n  \u2022 Multi-query-head batch processing (GQA, 5:1 patterns)  \n  \u2022 Shared/L2 buffering, Tensor Core alignment (FP16/bf16/tf32)  \n  \u2022 Stride precomputation & pointer caching  \n\n==================== Output Format ====================\nReturn ONLY the complete updated code containing the EVOLVE-BLOCK comment marker.  \nNO explanations, NO markdown, NO additional text.\nName the output structure class as ModelNew.\n\n==================== Optimization Tips ====================\n\u2022 If previous compile failed: Fix syntax/mask/stride errors first  \n\u2022 If correctness fails: Prioritize bounds mask & dtype precision  \n\u2022 When speed_up < 1: Focus on memory access & block sizing  \n\u2022 Small iterative steps: Modify \u226420 lines per revision  \n\u2022 Maintain reversibility between versions\nHere is an example to show you how to rewrite a PyTorch implementation that quantizes a high-precision tensor to int8 dtype with row-wise granularity into a Triton implementation: \nThe original PyTorch implementation is as follows: \n``` \nimport torch\ndef per_row_quantize_int8(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n    x_amax = x.abs().float().amax(dim=1).clamp(min=1e-4)\n    scale = 127 / x_amax\n    output = (x * scale[:, None]).round().clamp(-127, 127).to(torch.int8)\n    output_scale = x_amax / 127\n\n    return output, output_scale\n``` \nThe corresponding Triton-rewritten implementation is as follows: \n``` \nimport math\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra import libdevice\n\n# rowwise quantize\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_scale,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_scale + pid, max_val / 127)\n\n\ndef per_row_quantize_int8(x: torch.Tensor):\n    x = x.contiguous()\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_scale = torch.empty(x.shape[0], device=x.device, dtype=torch.float32)\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n    grid = lambda x: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_scale, BLOCK_SIZE=x.shape[1], P2=P2)\n\n    return output, output_scale\n\n\n    \n``` \n", "user": "# Current Program Information\n- Current performance metrics: - compiled: 1.0000\n- correctness: 1.0000\n- stage: success\n- passed: 0.0000\n- score: 20.0000\n- speed_up: 0.7910\n- runtime_ms: 1.7200\n- meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.791, 'custom_timing': 1.72}\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 0.0000, score: 10.0000, stage: wrong_answer, verify_payload: {'id': 'level3_19_MobileNetV1', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpy1_rfccf.py\", line 120, in forward\\n    x = self.model(x)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\\n    input = module(input)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\\n    input = module(input)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpy1_rfccf.py\", line 32, in forward\\n    _matmul_kernel[grid](x_reshaped, weight_reshaped, output,\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 561, in run\\n    bound_args, specialization, options = binder(*args, **kwargs)\\nTypeError: dynamic_func() missing 3 required positional arguments: \\'BLOCK_M\\', \\'BLOCK_N\\', and \\'BLOCK_K\\'\\n', 'runtime_error_name': 'builtins.TypeError', 'reference_timing_ms': 1.36, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}}\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.7910, runtime_ms: 1.7200, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.791, 'custom_timing': 1.72}\n- Outcome: Mixed results\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: compiled: 1.0000, correctness: 1.0000, stage: success, passed: 0.0000, score: 20.0000, speed_up: 0.3580, runtime_ms: 3.8000, meta: {'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.358, 'custom_timing': 3.8}\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef conv1x1_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    input_batch, input_channels, output_channels, input_height, input_width,\n    stride_ib, stride_ic, stride_ih, stride_iw,  # Added stride_ib\n    stride_wc,\n    stride_ob, stride_oc, stride_oh, stride_ow,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pixels = input_height * input_width\n    num_blocks = tl.cdiv(num_pixels, BLOCK_SIZE)\n    batch_idx = pid // (output_channels * num_blocks)\n    channel_idx = (pid % (output_channels * num_blocks)) // num_blocks\n    block_idx = pid % num_blocks\n\n    # Calculate spatial offsets\n    pixel_start = block_idx * BLOCK_SIZE\n    pixel_offsets = pixel_start + tl.arange(0, BLOCK_SIZE)\n    valid_pixels = pixel_offsets < num_pixels\n    h = pixel_offsets // input_width\n    w = pixel_offsets % input_width\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Loop over input channels\n    for c in range(0, input_channels):\n        # Load input and weight\n        input_val = tl.load(\n            input_ptr + batch_idx * stride_ib + c * stride_ic + h * stride_ih + w * stride_iw,\n            mask=valid_pixels\n        )\n        weight_val = tl.load(weight_ptr + channel_idx * stride_wc + c)\n        \n        # Accumulate\n        acc += input_val * weight_val\n\n    # Store results\n    output_offset = (\n        batch_idx * stride_ob + \n        channel_idx * stride_oc + \n        h * stride_oh + \n        w * stride_ow\n    )\n    tl.store(output_ptr + output_offset, acc, mask=valid_pixels)\n\nclass Conv1x1Triton(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels))\n        nn.init.kaiming_uniform_(self.weight)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        batch, _, height, width = x.shape\n        output = torch.empty((batch, self.out_channels, height, width), \n                             device=x.device, dtype=x.dtype)\n        \n        # Calculate strides\n        stride_ib = x.stride(0)\n        stride_ic = x.stride(1)\n        stride_ih = x.stride(2)\n        stride_iw = x.stride(3)\n        stride_wc = self.weight.stride(0)\n        stride_ob = output.stride(0)\n        stride_oc = output.stride(1)\n        stride_oh = output.stride(2)\n        stride_ow = output.stride(3)\n\n        # Launch kernel\n        BLOCK_SIZE = 128\n        num_pixels = height * width\n        num_blocks = triton.cdiv(num_pixels, BLOCK_SIZE)\n        grid = (batch * self.out_channels * num_blocks,)\n        \n        conv1x1_kernel[grid](\n            x, self.weight, output,\n            batch, self.in_channels, self.out_channels, height, width,\n            stride_ib, stride_ic, stride_ih, stride_iw,  # Added stride_ib\n            stride_wc,\n            stride_ob, stride_oc, stride_oh, stride_ow,\n            BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                Conv1x1Triton(inp, oup),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.3580), Performs well on runtime_ms (3.8000), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:0', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.358, 'custom_timing': 3.8})\n\n\n### Program 2 (Score: 20.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    spatial_indices = tl.arange(0, BLOCK_SIZE)\n    h = spatial_indices // width\n    w = spatial_indices % width\n    mask = spatial_indices < (height * width)\n    \n    spatial_offsets = h * input_height_stride + w * input_width_stride\n    pointers = input_ptr + input_offset + spatial_offsets\n    \n    values = tl.load(pointers, mask=mask, other=0.0)\n    channel_sum = tl.sum(values)\n    channel_avg = channel_sum / (height * width)\n    \n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size, channels)\n        spatial_size = x.shape[2] * x.shape[3]\n        # Compute next power of two for BLOCK_SIZE\n        block_size_pow2 = 1\n        while block_size_pow2 < spatial_size:\n            block_size_pow2 *= 2\n\n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3],\n            BLOCK_SIZE=block_size_pow2\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (1.0000), Performs well on stage (success), Performs well on passed (0.0000), Performs well on score (20.0000), Performs well on speed_up (0.7910), Performs well on runtime_ms (1.7200), Performs well on meta ({'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'correctness_trials': '(5 / 5)', 'reference_timing_ms': 1.36, 'speed_up': 0.791, 'custom_timing': 1.72})\n\n\n### Program 3 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\nclass TritonConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_channels))\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        if self.kernel_size != 1 or self.stride != 1 or self.padding != 0:\n            return F.conv2d(x, self.weight, self.bias, self.stride, self.padding)\n        \n        n, c, h, w = x.shape\n        x_reshaped = x.permute(0, 2, 3, 1).contiguous().view(n * h * w, c)\n        weight_reshaped = self.weight.view(self.weight.size(0), -1).t().contiguous()\n        output = torch.empty(n * h * w, self.weight.size(0), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(x_reshaped.size(0), META['BLOCK_M']), \n                            triton.cdiv(weight_reshaped.size(1), META['BLOCK_N']))\n        \n        _matmul_kernel[grid](x_reshaped, weight_reshaped, output, \n                            x_reshaped.size(0), weight_reshaped.size(1), x_reshaped.size(1),\n                            x_reshaped.stride(0), x_reshaped.stride(1),\n                            weight_reshaped.stride(0), weight_reshaped.stride(1),\n                            output.stride(0), output.stride(1))\n        \n        output = output.view(n, h, w, -1).permute(0, 3, 1, 2)\n        return output\n\n@triton.jit\ndef _matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n        b_mask = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk * stride_bn\n    \n    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                TritonConv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Performs well on compiled (1.0000), Performs well on correctness (0.0000), Performs well on score (10.0000), Performs well on stage (wrong_answer), Performs well on verify_payload ({'id': 'level3_19_MobileNetV1', 'result': {'compiled': True, 'correctness': False, 'metadata': {'hardware': 'NVIDIA L20Y', 'device': 'cuda:2', 'runtime_error': 'Traceback (most recent call last):\\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\\n    output_new = model_new(*inputs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpy1_rfccf.py\", line 120, in forward\\n    x = self.model(x)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\\n    input = module(input)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 244, in forward\\n    input = module(input)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/tmp/tmpy1_rfccf.py\", line 32, in forward\\n    _matmul_kernel[grid](x_reshaped, weight_reshaped, output,\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 561, in run\\n    bound_args, specialization, options = binder(*args, **kwargs)\\nTypeError: dynamic_func() missing 3 required positional arguments: \\'BLOCK_M\\', \\'BLOCK_N\\', and \\'BLOCK_K\\'\\n', 'runtime_error_name': 'builtins.TypeError', 'reference_timing_ms': 1.36, 'speed_up': -1.0, 'custom_timing': -1.0}, 'runtime': -1.0, 'runtime_stats': {}}})\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 1.0000)\n```python\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, \n    output_ptr,\n    input_batch_stride, \n    input_channel_stride, \n    input_height_stride, \n    input_width_stride,\n    output_batch_stride, \n    output_channel_stride,\n    height, \n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    spatial_size = height * width\n    thread_idx = tl.arange(0, BLOCK_SIZE)\n    num_steps = tl.cdiv(spatial_size, BLOCK_SIZE)\n    \n    acc = tl.zeros((1,), dtype=tl.float32)\n    for step in range(num_steps):\n        element_idx = step * BLOCK_SIZE + thread_idx\n        mask = element_idx < spatial_size\n        h = element_idx // width\n        w = element_idx % width\n        spatial_offset = h * input_height_stride + w * input_width_stride\n        ptr = input_ptr + input_offset + spatial_offset\n        value = tl.load(ptr, mask=mask, other=0.0)\n        acc += value\n    \n    total_sum = tl.sum(acc, axis=0)\n    channel_avg = total_sum / spatial_size\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg, mask=thread_idx == 0)\n\ndef global_avg_pool(input: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = input.shape\n    output = torch.empty((batch, channels), device=input.device, dtype=input.dtype)\n    \n    if input.is_cuda:\n        BLOCK_SIZE = 1024\n        grid = (batch, channels)\n        global_avg_pool_kernel[grid](\n            input_ptr=input,\n            output_ptr=output,\n            input_batch_stride=input.stride(0),\n            input_channel_stride=input.stride(1),\n            input_height_stride=input.stride(2),\n            input_width_stride=input.stride(3),\n            output_batch_stride=output.stride(0),\n            output_channel_stride=output.stride(1),\n            height=height,\n            width=width,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n    else:\n        # CPU fallback\n        output = input.mean(dim=[2, 3])\n        \n    return output\n\nclass ModelNew:\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def global_avg_pool(input: torch.Tensor) -> torch.Tensor:\n        return global_avg_pool(input)\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n\n### Program D2 (Score: 10.0000)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    spatial_indices = tl.arange(0, BLOCK_SIZE)\n    h = spatial_indices // width\n    w = spatial_indices % width\n    mask = spatial_indices < (height * width)\n    \n    spatial_offsets = h * input_height_stride + w * input_width_stride\n    pointers = input_ptr + input_offset + spatial_offsets\n    \n    values = tl.load(pointers, mask=mask, other=0.0)\n    channel_sum = tl.sum(values)\n    channel_avg = channel_sum / (height * width)\n    \n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size, channels)\n        spatial_size = x.shape[2] * x.shape[3]\n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3],\n            BLOCK_SIZE=spatial_size\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nKey features: Alternative approach to compiled, Alternative approach to correctness\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 20.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef conv1x1_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    input_batch, input_channels, output_channels, input_height, input_width,\n    stride_ib, stride_ic, stride_ih, stride_iw,  # Added stride_ib\n    stride_wc,\n    stride_ob, stride_oc, stride_oh, stride_ow,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pixels = input_height * input_width\n    num_blocks = tl.cdiv(num_pixels, BLOCK_SIZE)\n    batch_idx = pid // (output_channels * num_blocks)\n    channel_idx = (pid % (output_channels * num_blocks)) // num_blocks\n    block_idx = pid % num_blocks\n\n    # Calculate spatial offsets\n    pixel_start = block_idx * BLOCK_SIZE\n    pixel_offsets = pixel_start + tl.arange(0, BLOCK_SIZE)\n    valid_pixels = pixel_offsets < num_pixels\n    h = pixel_offsets // input_width\n    w = pixel_offsets % input_width\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Loop over input channels\n    for c in range(0, input_channels):\n        # Load input and weight\n        input_val = tl.load(\n            input_ptr + batch_idx * stride_ib + c * stride_ic + h * stride_ih + w * stride_iw,\n            mask=valid_pixels\n        )\n        weight_val = tl.load(weight_ptr + channel_idx * stride_wc + c)\n        \n        # Accumulate\n        acc += input_val * weight_val\n\n    # Store results\n    output_offset = (\n        batch_idx * stride_ob + \n        channel_idx * stride_oc + \n        h * stride_oh + \n        w * stride_ow\n    )\n    tl.store(output_ptr + output_offset, acc, mask=valid_pixels)\n\nclass Conv1x1Triton(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.empty(out_channels, in_channels))\n        nn.init.kaiming_uniform_(self.weight)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        batch, _, height, width = x.shape\n        output = torch.empty((batch, self.out_channels, height, width), \n                             device=x.device, dtype=x.dtype)\n        \n        # Calculate strides\n        stride_ib = x.stride(0)\n        stride_ic = x.stride(1)\n        stride_ih = x.stride(2)\n        stride_iw = x.stride(3)\n        stride_wc = self.weight.stride(0)\n        stride_ob = output.stride(0)\n        stride_oc = output.stride(1)\n        stride_oh = output.stride(2)\n        stride_ow = output.stride(3)\n\n        # Launch kernel\n        BLOCK_SIZE = 128\n        num_pixels = height * width\n        num_blocks = triton.cdiv(num_pixels, BLOCK_SIZE)\n        grid = (batch * self.out_channels * num_blocks,)\n        \n        conv1x1_kernel[grid](\n            x, self.weight, output,\n            batch, self.in_channels, self.out_channels, height, width,\n            stride_ib, stride_ic, stride_ih, stride_iw,  # Added stride_ib\n            stride_wc,\n            stride_ob, stride_oc, stride_oh, stride_ow,\n            BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                Conv1x1Triton(inp, oup),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Excellent correctness (1.000)\n\n\n### Inspiration 2 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    spatial_indices = tl.arange(0, BLOCK_SIZE)\n    h = spatial_indices // width\n    w = spatial_indices % width\n    mask = spatial_indices < (height * width)\n    \n    spatial_offsets = h * input_height_stride + w * input_width_stride\n    pointers = input_ptr + input_offset + spatial_offsets\n    \n    values = tl.load(pointers, mask=mask, other=0.0)\n    channel_sum = tl.sum(values)\n    channel_avg = channel_sum / (height * width)\n    \n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size, channels)\n        spatial_size = x.shape[2] * x.shape[3]\n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3],\n            BLOCK_SIZE=spatial_size\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n### Inspiration 3 (Score: 1.0000, Type: High-Performer)\n```python\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, \n    output_ptr,\n    input_batch_stride, \n    input_channel_stride, \n    input_height_stride, \n    input_width_stride,\n    output_batch_stride, \n    output_channel_stride,\n    height, \n    width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    spatial_size = height * width\n    thread_idx = tl.arange(0, BLOCK_SIZE)\n    num_steps = tl.cdiv(spatial_size, BLOCK_SIZE)\n    \n    acc = tl.zeros((1,), dtype=tl.float32)\n    for step in range(num_steps):\n        element_idx = step * BLOCK_SIZE + thread_idx\n        mask = element_idx < spatial_size\n        h = element_idx // width\n        w = element_idx % width\n        spatial_offset = h * input_height_stride + w * input_width_stride\n        ptr = input_ptr + input_offset + spatial_offset\n        value = tl.load(ptr, mask=mask, other=0.0)\n        acc += value\n    \n    total_sum = tl.sum(acc, axis=0)\n    channel_avg = total_sum / spatial_size\n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg, mask=thread_idx == 0)\n\ndef global_avg_pool(input: torch.Tensor) -> torch.Tensor:\n    batch, channels, height, width = input.shape\n    output = torch.empty((batch, channels), device=input.device, dtype=input.dtype)\n    \n    if input.is_cuda:\n        BLOCK_SIZE = 1024\n        grid = (batch, channels)\n        global_avg_pool_kernel[grid](\n            input_ptr=input,\n            output_ptr=output,\n            input_batch_stride=input.stride(0),\n            input_channel_stride=input.stride(1),\n            input_height_stride=input.stride(2),\n            input_width_stride=input.stride(3),\n            output_batch_stride=output.stride(0),\n            output_channel_stride=output.stride(1),\n            height=height,\n            width=width,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n    else:\n        # CPU fallback\n        output = input.mean(dim=[2, 3])\n        \n    return output\n\nclass ModelNew:\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def global_avg_pool(input: torch.Tensor) -> torch.Tensor:\n        return global_avg_pool(input)\n```\nUnique approach: Modification: Full rewrite, Alternative compiled approach, Alternative correctness approach\n\n\n### Inspiration 4 (Score: 0.0000, Type: Exploratory)\n```python\n\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        \"\"\"\n        MobileNetV1 architecture implementation.\n\n        :param num_classes: The number of output classes (default: 1000)\n        :param input_channels: The number of input channels (default: 3 for RGB images)\n        :param alpha: Width multiplier (default: 1.0)\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        :param x: The input tensor, shape (batch_size, input_channels, height, width)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n\n```\nUnique approach: Excellent compiled (1.000), Excellent correctness (1.000), Alternative passed approach\n\n\n### Inspiration 5 (Score: 10.0000, Type: High-Performer)\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\nclass TritonConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_channels))\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        if self.kernel_size != 1 or self.stride != 1 or self.padding != 0:\n            return F.conv2d(x, self.weight, self.bias, self.stride, self.padding)\n        \n        n, c, h, w = x.shape\n        x_reshaped = x.permute(0, 2, 3, 1).contiguous().view(n * h * w, c)\n        weight_reshaped = self.weight.view(self.weight.size(0), -1).t().contiguous()\n        output = torch.empty(n * h * w, self.weight.size(0), device=x.device, dtype=x.dtype)\n        \n        grid = lambda META: (triton.cdiv(x_reshaped.size(0), META['BLOCK_M']), \n                            triton.cdiv(weight_reshaped.size(1), META['BLOCK_N']))\n        \n        _matmul_kernel[grid](x_reshaped, weight_reshaped, output, \n                            x_reshaped.size(0), weight_reshaped.size(1), x_reshaped.size(1),\n                            x_reshaped.stride(0), x_reshaped.stride(1),\n                            weight_reshaped.stride(0), weight_reshaped.stride(1),\n                            output.stride(0), output.stride(1))\n        \n        output = output.view(n, h, w, -1).permute(0, 3, 1, 2)\n        return output\n\n@triton.jit\ndef _matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)\n        b_mask = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk * stride_bn\n    \n    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                \n                TritonConv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\nUnique approach: Modification: Full rewrite, Excellent compiled (1.000), Alternative correctness approach\n\n\n\n# Current Program\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    spatial_indices = tl.arange(0, BLOCK_SIZE)\n    h = spatial_indices // width\n    w = spatial_indices % width\n    mask = spatial_indices < (height * width)\n    \n    spatial_offsets = h * input_height_stride + w * input_width_stride\n    pointers = input_ptr + input_offset + spatial_offsets\n    \n    values = tl.load(pointers, mask=mask, other=0.0)\n    channel_sum = tl.sum(values)\n    channel_avg = channel_sum / (height * width)\n    \n    output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size, channels)\n        spatial_size = x.shape[2] * x.shape[3]\n        # Compute next power of two for BLOCK_SIZE\n        block_size_pow2 = 1\n        while block_size_pow2 < spatial_size:\n            block_size_pow2 *= 2\n\n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3],\n            BLOCK_SIZE=block_size_pow2\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["<think>\nWe are given a Triton kernel for global average pooling and a model (MobileNetV1) that uses it.\n The current performance shows a speed_up of 0.791 (which is less than 1, meaning slower than the reference).\n We are to optimize the Triton kernel for global average pooling.\n\n The current kernel uses a block size that is the next power of two of the spatial size (height * width).\n However, the kernel loads all spatial elements in one go and then sums them. This might not be efficient for large spatial sizes (like 224x224=50176) because:\n   - The BLOCK_SIZE (which is 50176) is too large and might not fit in the available registers or cause inefficient memory access.\n\n Instead, we can break the reduction into multiple blocks. We can have each block compute a partial sum and then use a second kernel to combine them.\n However, to keep it simple and because the spatial size is large, we can change the kernel to use a two-dimensional grid and reduce within the block.\n\n Alternatively, we can use a two-step reduction:\n   Step 1: Each block computes a partial sum over a chunk of the spatial dimension.\n   Step 2: Another kernel combines the partial sums.\n\n But note: the current kernel is launched with grid (batch_size, channels) and each program does one channel.\n\n We can restructure the kernel to use multiple blocks per channel? Actually, the current kernel uses one block per channel and the entire spatial dimension in one block.\n\n We can change the kernel to use multiple blocks per channel? But note the output is per channel. We want one value per channel.\n\n Proposed approach:\n   We can have the kernel use multiple blocks per channel, each block summing a portion of the spatial data, and then use atomic operations to accumulate in global memory.\n\n However, atomic operations for floating point might be expensive and non-deterministic.\n\n Alternatively, we can do:\n   - Launch with grid (batch_size, channels, num_blocks) where num_blocks = ceil(spatial_size / BLOCK_SIZE)\n   - Each block computes the sum for its portion and stores in a temporary array.\n   - Then launch a second kernel to reduce the temporary array per channel.\n\n But that would require two kernels and temporary memory.\n\n We want to do it in one kernel? But note: Triton does not support dynamic parallelism.\n\n Alternatively, we can use one kernel and do the reduction in two stages within the same kernel by having shared memory? \n However, the reduction across blocks requires synchronization and shared memory is per block.\n\n Therefore, we change the kernel to use a two-stage reduction without atomic operations? Actually, we can use a single kernel with atomics? \n But note: Triton does not support atomic add for float32 on all GPUs? Actually, it does support atomic_add for float32.\n\n But let's check the Triton documentation: tl.atomic_add is available.\n\n However, using atomics might lead to non-determinism and might be slower due to contention.\n\n Alternatively, we can use a tree reduction within the kernel? But the spatial size is too large to fit in one block.\n\n We break the reduction by having multiple blocks per channel. We can do:\n\n   grid = (batch_size, channels, num_blocks)   [num_blocks = ceil(spatial_size / BLOCK_SIZE)]\n\n   Step 1: Each block computes a local sum for its portion.\n   Step 2: Store the local sum in a temporary array of shape [batch_size, channels, num_blocks].\n   Step 3: Then we need a second kernel to reduce the temporary array along the num_blocks dimension.\n\n Since the problem requires a single kernel? We cannot do two kernels in one Triton kernel.\n\n Alternatively, we can use one kernel and atomics to accumulate the sum for each channel.\n\n We choose to use atomics because it simplifies the code and might be acceptable if the number of blocks is not too high.\n\n However, note that the number of blocks per channel is ceil(spatial_size / BLOCK_SIZE). For 224x224=50176 and BLOCK_SIZE=1024, we have about 49 blocks per channel. \n The atomic operations might be acceptable.\n\n Proposed kernel:\n\n   We change the kernel to:\n     pid_batch = tl.program_id(0)\n     pid_channel = tl.program_id(1)\n     pid_block = tl.program_id(2)   # which block in the spatial dimension for this channel\n\n     spatial_size = height * width\n     start_idx = pid_block * BLOCK_SIZE\n     offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n     mask = offsets < spatial_size\n\n     h = offsets // width\n     w = offsets % width\n\n     ... load the values ...\n\n     local_sum = tl.sum(values)\n\n     Then we use atomic_add to accumulate local_sum to the output[pid_batch, pid_channel].\n\n   But note: we also need to compute the average? We can do the atomic_add for the sum and then after the kernel, we can do a division? \n   However, we cannot do that because the kernel writes one value per channel and we don't know when all blocks are done.\n\n   Therefore, we must do the division in a separate kernel? That is not acceptable.\n\n   Alternatively, we can do the division in the same kernel? But note: multiple blocks are writing to the same output location. \n   We cannot do the division until all blocks have added their partial sums.\n\n   We can do:\n\n     Step 1: Initialize a temporary output tensor to zeros.\n     Step 2: Run the kernel that does atomic_add to the temporary tensor.\n     Step 3: Run a second kernel that does: output = temporary / (height*width)\n\n   That would be two kernels.\n\n   Alternatively, we can use a single kernel and have the last block do the division? But how do we know which block is the last? We cannot synchronize across blocks.\n\n   Therefore, we stick to the original one-block-per-channel if the spatial size is small? But 50176 is too big.\n\n   Alternatively, we can use a reduction tree within a block for the entire channel? But that would require a block size of 50176, which is too big.\n\n   We can use multiple blocks per channel and then use a second kernel to combine? That is two kernels.\n\n   Given the complexity of two kernels and the need for temporary memory, and since the current kernel is slow, we might try to optimize the one-block version by making the reduction more efficient?\n\n   The current one-block version does:\n\n     spatial_indices = tl.arange(0, BLOCK_SIZE)   # BLOCK_SIZE is the next power of two (e.g., 65536 for 50176)\n     Then it loads the entire channel with a mask for the valid ones.\n\n   The problem is that BLOCK_SIZE is 65536, which is too large and causes the kernel to use too many registers and not run efficiently.\n\n   We can try to break the channel into multiple segments and do the reduction in multiple steps within the same block? \n\n   We can change the kernel to loop over the spatial dimension in chunks of size BLOCK_SIZE_LOAD (which we set to 1024 or 4096). \n\n   Revised plan for one block per channel:\n\n     We set a fixed BLOCK_SIZE_LOAD (e.g., 4096) and loop over the spatial dimension in chunks of BLOCK_SIZE_LOAD.\n\n     For each chunk:\n        load a block of data (size BLOCK_SIZE_LOAD) with masking for the last chunk.\n        accumulate the sum.\n\n     Then we have one block per channel that does the entire reduction in a loop.\n\n   This way, the block size is fixed (to BLOCK_SIZE_LOAD) and we can have a reasonable number of threads.\n\n   However, note: the kernel is launched with one block per channel? How do we parallelize within a block? \n\n   Actually, the current kernel uses a 1D block of size BLOCK_SIZE (which was the entire spatial). Now we want to use a fixed block size (say 1024) and have each thread in the block process multiple elements? \n\n   We can do:\n\n     pid = tl.program_id(0)   # batch index\n     pid1 = tl.program_id(1)   # channel index\n\n     We don't need a third dimension because we will loop.\n\n     We can have:\n\n        num_elements = height * width\n        num_steps = tl.cdiv(num_elements, BLOCK_SIZE_LOAD)\n\n        accumulator = 0.0\n        for step in range(num_steps):\n            offset = step * BLOCK_SIZE_LOAD + tl.arange(0, BLOCK_SIZE_LOAD)\n            mask = offset < num_elements\n            h = offset // width\n            w = offset % width\n            ptr = input_ptr + ... + h * stride_h + w * stride_w\n            values = tl.load(ptr, mask=mask, other=0.0)\n            step_sum = tl.sum(values)\n            accumulator += step_sum\n\n        average = accumulator / num_elements\n        store ...\n\n   But note: the entire block is doing the same loop? We are not parallelizing the reduction within the block? \n\n   We want to parallelize the reduction within the block? We can use the entire block to load one chunk and then do a parallel reduction within the block? \n\n   How about:\n\n        We break the spatial dimension into chunks of size BLOCK_SIZE_LOAD (which is the number of threads in the block).\n        Each thread in the block will load one element per chunk and then we do a tree reduction within the block for that chunk.\n\n        Then we use shared memory to store the partial sum for the chunk and then accumulate across chunks.\n\n   However, we cannot have dynamic shared memory? And we want to avoid shared memory if possible.\n\n   Alternatively, we can use the tl.sum function on a vector of size BLOCK_SIZE_LOAD? That would be efficient.\n\n   So the kernel becomes:\n\n        pid_batch = tl.program_id(0)\n        pid_channel = tl.program_id(1)\n\n        input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n        num_elements = height * width\n        accumulator = 0.0\n\n        BLOCK = BLOCK_SIZE_LOAD   # number of threads in the block, we set it to 1024\n\n        for step in range(0, num_elements, BLOCK):\n            offsets = step + tl.arange(0, BLOCK)\n            mask = offsets < num_elements\n            h = offsets // width\n            w = offsets % width\n            spatial_offsets = h * input_height_stride + w * input_width_stride\n            pointers = input_ptr + input_offset + spatial_offsets\n            values = tl.load(pointers, mask=mask, other=0.0)\n            chunk_sum = tl.sum(values)\n            accumulator += chunk_sum\n\n        channel_avg = accumulator / num_elements\n        output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n        tl.store(output_ptr + output_offset, channel_avg)\n\n   We note that the entire block is doing the same loop? But we want to use the entire block to load a chunk and then do a parallel reduction for the chunk? \n\n   Actually, the above code is for one thread per channel? That would be inefficient because we are not utilizing the whole block for the reduction of one chunk.\n\n   We need to change the kernel to use the block for parallel reduction within each chunk.\n\n   We can restructure:\n\n        We have a 1D block of size BLOCK (say 1024). \n        We assign one channel per block? Then we have grid (batch_size * channels, 1)? But note: we have batch_size * channels blocks.\n\n        Actually, we can have grid (batch_size, channels) and each block has BLOCK threads? Then we can parallelize the reduction within the chunk and across chunks.\n\n   How about:\n\n        pid_batch = tl.program_id(0)\n        pid_channel = tl.program_id(1)\n        pid_thread = tl.program_id(2)   # but note: triton only has 2D grid? Actually, we can have 3D grid.\n\n        Actually, Triton supports 3D grid: we can set grid = (batch_size, channels, num_blocks_per_channel) but we don't want that.\n\n        We want one block per channel? Then we can set grid = (batch_size, channels) and block = (BLOCK, 1, 1) where BLOCK is the number of threads.\n\n        Then within the block, we can split the spatial dimension into chunks of size BLOCK? Actually, we can split the entire spatial dimension into chunks of size BLOCK, and each thread in the block will process multiple chunks? \n\n        Alternatively, we can have each thread process multiple elements in a chunk? \n\n        We do:\n\n            total_elements = height * width\n            num_chunks = tl.cdiv(total_elements, BLOCK)\n\n            accumulator = 0.0\n            for chunk_idx in range(num_chunks):\n                element_idx = chunk_idx * BLOCK + pid_thread\n                if element_idx < total_elements:\n                    h = element_idx // width\n                    w = element_idx % width\n                    ptr = input_ptr + ... + h * stride_h + w * stride_w\n                    value = tl.load(ptr)\n                    # Now we need to sum the values in the block for this chunk? \n                    # But note: we are accumulating the entire channel. We want to sum all the values in the block for this chunk and then add to a global accumulator? \n\n            We can do a shared memory reduction for each chunk? \n\n        Steps:\n\n            shared = tl.zeros((BLOCK,), dtype=tl.float32)\n\n            for chunk_idx in range(num_chunks):\n                element_idx = chunk_idx * BLOCK + pid_thread\n                if element_idx < total_elements:\n                    ... load value ...\n                else:\n                    value = 0.0\n                shared[pid_thread] = value\n                tl.barrier()\n                # do a tree reduction in shared memory for the chunk\n                s = 1\n                while s < BLOCK:\n                    if pid_thread % (2*s) == 0:\n                        other = pid_thread + s\n                        if other < BLOCK:\n                            shared[pid_thread] += shared[other]\n                    s *= 2\n                    tl.barrier()\n                if pid_thread == 0:\n                    accumulator += shared[0]\n\n            if pid_thread == 0:\n                channel_avg = accumulator / total_elements\n                store ...\n\n        But note: we are using barriers and shared memory. Also, the tree reduction might be expensive.\n\n        Alternatively, we can use tl.sum for a vector of values? But we are loading one value per thread.\n\n        We can do: chunk_sum = tl.sum(shared)   but that requires the entire shared array? Actually, tl.sum is not available for shared memory? We have to do tree reduction.\n\n   This approach is complex and might not be faster.\n\n   Given the time, let's try the simpler approach: one thread per channel and loop over chunks without parallelization? But that would be slow.\n\n   Alternatively, we can use the fact that the spatial size is large and we have many channels and batches. We can use the original one-block-per-channel but with a more efficient reduction by breaking into chunks without parallel reduction? \n\n   But note: one thread per channel is not utilizing the GPU. We want to use many threads per channel.\n\n   We decide to use one block per channel and use the entire block to parallelize the reduction within the channel.\n\n   We change the kernel to:\n\n        pid_batch = tl.program_id(0)\n        pid_channel = tl.program_id(1)\n        pid = tl.arange(0, BLOCK_SIZE)   # within the block, we have BLOCK_SIZE threads\n\n        input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n        num_elements = height * width\n\n        # We will break the spatial dimension into segments that we process in parallel? Actually, we can have each thread process multiple elements.\n\n        accumulator = tl.zeros((1,), dtype=tl.float32)   # one accumulator per thread? or per block? We want per block.\n\n        # Instead, we use a shared memory for the entire block? But we want to avoid shared memory.\n\n        # We can do:\n        #   total = 0.0\n        #   for i in range(0, num_elements, BLOCK_SIZE):\n        #       idx = i + pid\n        #       mask = idx < num_elements\n        #       h = idx // width\n        #       w = idx % width\n        #       ptr = ... + h * stride_h + w * stride_w\n        #       value = tl.load(ptr, mask=mask, other=0.0)\n        #       total += tl.sum(value)   # but tl.sum is over the entire vector? That would be the same for every thread.\n\n        Actually, we want each thread to accumulate its own partial sum? Then we do a tree reduction?\n\n        How about:\n\n            accum = tl.zeros((1,), dtype=tl.float32)[0]   # scalar per thread\n            num_iters = tl.cdiv(num_elements, BLOCK_SIZE)\n            for i in range(num_iters):\n                idx = i * BLOCK_SIZE + pid\n                if idx < num_elements:\n                    h = idx // width\n                    w = idx % width\n                    ptr = input_ptr + input_offset + h * input_height_stride + w * input_width_stride\n                    value = tl.load(ptr)\n                    accum += value\n\n            # Now do a tree reduction in the block to sum all the accumulators?\n            # We use shared memory for reduction.\n\n            shared = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n            shared[pid] = accum\n            tl.barrier()\n\n            # tree reduction in shared memory\n            s = 1\n            while s < BLOCK_SIZE:\n                if pid % (2*s) == 0:\n                    if pid + s < BLOCK_SIZE:\n                        shared[pid] += shared[pid+s]\n                s *= 2\n                tl.barrier()\n\n            if pid == 0:\n                total_sum = shared[0]\n                channel_avg = total_sum / num_elements\n                output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n                tl.store(output_ptr + output_offset, channel_avg)\n\n   This uses shared memory and barriers. It is more complex but should be efficient.\n\n   We choose BLOCK_SIZE appropriately (like 1024) so that the tree reduction is efficient.\n\n   Steps:\n      - Each thread accumulates a private sum over a subset of the spatial elements (stride by BLOCK_SIZE).\n      - Then we do a tree reduction in shared memory to get the total sum for the channel.\n      - Then thread 0 writes the average.\n\n   We note that the tree reduction is efficient for BLOCK_SIZE=1024 (10 steps).\n\n   We set BLOCK_SIZE=1024.\n\n   The grid will be (batch_size, channels) and each block has 1024 threads.\n\n   We must note: the number of blocks in the grid might be large (batch_size * channels) and we have 1024 threads per block. \n   This should be acceptable for the L20Y/H800.\n\n   Let's rewrite the kernel accordingly.\n\n   We also note: the input might not be contiguous? We use the strides.\n\n   We'll try this approach.\n\n   Important: We must initialize the shared memory and use barriers.\n\n   We also note: the tree reduction code must be efficient.\n\n   Alternatively, we can use the tl.reduce function? But Triton does not have a built-in block reduce? We have to write our own.\n\n   We write a tree reduction for power-of-two block size.\n\n   Since we set BLOCK_SIZE=1024 (power of two), we can do:\n\n        offset = tl.program_id(2)   # not needed, because we use a 1D block? Actually, we are in a 1D block.\n        pid = tl.arange(0, BLOCK_SIZE)   # we don't have an equivalent in the kernel? Actually, we have pid = tl.arange(0, BLOCK_SIZE) for the entire block? \n\n        Instead, we use the local thread index: we can get it by tl.arange(0, BLOCK_SIZE) but that gives a vector. We want a scalar index.\n\n        Actually, we can use:\n\n            pid = tl.program_id(axis=2)   # but our grid is 2D (batch, channel) and the block is 1D of size BLOCK_SIZE.\n\n        How do we get the local thread index? In Triton, we can use:\n\n            pid = tl.arange(0, BLOCK_SIZE)   # returns a vector of indices [0,1,...,BLOCK_SIZE-1] for the entire block? \n\n        But note: the kernel is written for a 1D block. We can use:\n\n            idx = tl.program_id(0)   # batch\n            idy = tl.program_id(1)   # channel\n            idz = tl.arange(0, BLOCK_SIZE)   # thread index in the block? Actually, this is a vector of size BLOCK_SIZE.\n\n        But the kernel is launched with a 2D grid and 1D block? We can do:\n\n            grid = (batch, channels)\n            block = (BLOCK_SIZE,)\n\n        Then in the kernel:\n\n            pid_batch = tl.program_id(0)\n            pid_channel = tl.program_id(1)\n            pid_thread = tl.arange(0, BLOCK_SIZE)   # This is a vector of thread indices? Actually, no, we want a scalar for each thread.\n\n        We need to use:\n\n            pid_thread = tl.arange(0, BLOCK_SIZE)   # returns a vector of [0, 1, 2, ...] for the entire block? But then we are in a SIMD model.\n\n        How to do per-thread private accumulation? \n\n        Actually, in Triton, the unit of execution is the block, and the block is vectorized. We cannot have each thread do independent work? \n\n        We can use:\n\n            accum = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)   # each element for a thread\n\n        Then for the loop:\n\n            for i in range(num_iters):\n                base = i * BLOCK_SIZE\n                idx = base + pid_thread   # [base, base+1, ... , base+BLOCK_SIZE-1]\n                mask = idx < num_elements\n                h = idx // width\n                w = idx % width\n                ptr = input_ptr + input_offset + h * input_height_stride + w * input_width_stride\n                values = tl.load(ptr, mask=mask, other=0.0)   # vector of BLOCK_SIZE\n                accum += values\n\n        Then we have a vector of BLOCK_SIZE with partial sums? Then we want to reduce this vector to a scalar? \n\n        We can do:\n\n            total_sum = tl.sum(accum)\n\n        But then we have one value for the entire block? Then we can store it? But note: we want to do a tree reduction? \n\n        Actually, we can just do:\n\n            total_sum = tl.sum(accum)   # this sums the entire vector\n\n        Then we store by one thread? But note: the entire block has the same total_sum? \n\n        How to store only once? We can do:\n\n            if pid_thread[0] == 0:   # but pid_thread[0] is 0, so it's always true? \n\n        Actually, we can do:\n\n            channel_avg = total_sum / num_elements\n            tl.store(output_ptr + output_offset, channel_avg)   # but every thread will store? \n\n        We want only one thread to store? We can do:\n\n            if tl.arange(0, BLOCK_SIZE)[0] == 0:   # but this is always true for the first element? \n\n        Actually, we can use:\n\n            tl.store(output_ptr + output_offset, channel_avg, mask=pid_thread == 0)\n\n        This will work.\n\n   Therefore, the kernel becomes:\n\n        pid_batch = tl.program_id(0)\n        pid_channel = tl.program_id(1)\n        pid_thread = tl.arange(0, BLOCK_SIZE)   # vector of indices [0,1,...,BLOCK_SIZE-1]\n\n        input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n        num_elements = height * width\n        num_iters = tl.cdiv(num_elements, BLOCK_SIZE)\n\n        accum = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n        for i in range(num_iters):\n            base = i * BLOCK_SIZE\n            idx = base + pid_thread\n            mask = idx < num_elements\n            h = idx // width\n            w = idx % width\n            spatial_offset = h * input_height_stride + w * input_width_stride\n            ptr = input_ptr + input_offset + spatial_offset\n            values = tl.load(ptr, mask=mask, other=0.0)\n            accum += values\n\n        total_sum = tl.sum(accum)\n        channel_avg = total_sum / num_elements\n\n        output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n        tl.store(output_ptr + output_offset, channel_avg, mask=pid_thread==0)\n\n   We note: the entire block is active and doing the same work? Actually, the block is vectorized: the load is coalesced and the arithmetic is vectorized.\n\n   We set BLOCK_SIZE to 1024 or 512.\n\n   We also note: the loop might be long if num_iters is large? For 50176 elements and BLOCK_SIZE=1024, num_iters=49.\n\n   This should be acceptable.\n\n   Let's try this version.\n\n   We change the kernel accordingly and adjust the launch parameters.\n\n   We remove the previous BLOCK_SIZE (which was the entire spatial) and use a fixed BLOCK_SIZE (like 1024) for the block of threads.\n\n   The grid remains (batch, channels).\n\n   We also remove the power-of-two computation for the spatial dimension? We don't need it because we are using a fixed BLOCK_SIZE for the block of threads.\n\n   We'll set BLOCK_SIZE=1024.\n\n   We'll change the kernel and the caller.\n\n   The caller changes:\n\n        grid = (batch_size, channels)\n        # We now specify the block size as (BLOCK_SIZE,) for the threads per block.\n        global_avg_pool_kernel[grid](x, output, ... , BLOCK_SIZE=1024)\n\n   But note: the kernel now uses a 1D block of size BLOCK_SIZE (1024). We must specify the number of threads per block.\n\n   In Triton, we specify the block size by the `num_warps` and `BLOCK_SIZE`? Actually, the block size is set by the `num_warps` and the `BLOCK_SIZE` is the vector size? \n\n   Wait, no: in Triton, the `num_warps` determines the number of warps and the total threads is num_warps * 32. We cannot set an arbitrary block size? \n\n   Actually, we can set the block size by the `BLOCK_SIZE` in the kernel? But note: the `BLOCK_SIZE` in the kernel is a constexpr and we use it for the vector operations.\n\n   How do we set the number of threads? \n\n   The number of threads is determined by the product of the dimensions in `tl.arange`? Actually, Triton abstracts the thread level? \n\n   In the kernel, we have:\n\n        pid_thread = tl.arange(0, BLOCK_SIZE)\n\n   This will create a vector of size BLOCK_SIZE. The kernel will be executed by a block of BLOCK_SIZE threads? \n\n   But note: Triton uses a single stream multiprocessor and the threads are virtual? \n\n   We think it will work.\n\n   We set BLOCK_SIZE=1024. But note: 1024 might be too large for the number of threads per block? The maximum threads per block is 1024 for modern GPUs.\n\n   We'll try BLOCK_SIZE=1024.\n\n   Alternatively, we can set BLOCK_SIZE=512.\n\n   We choose BLOCK_SIZE=1024.\n\n   Let's rewrite the kernel accordingly.\n\n   We also note: the mask for the load is computed per element.\n\n   We'll change the kernel to the new version.\n\n   We hope this will be faster than the previous one-block-per-channel that used a huge block size.\n\n   Let's code accordingly.\n\n   We also note: the previous kernel had a parameter named BLOCK_SIZE which was the entire spatial. We are changing its meaning.\n\n   We rename the constexpr to THREAD_BLOCK_SIZE to avoid confusion? But the current program uses BLOCK_SIZE. We'll change the name to avoid confusion.\n\n   Actually, we can keep the name BLOCK_SIZE but note that in the new kernel it means the number of threads per block.\n\n   We'll change the kernel parameter to THREAD_BLOCK_SIZE.\n\n   However, we want to keep the same signature? The problem says not to change the signature.\n\n   So we keep the name BLOCK_SIZE.\n\n   But note: the previous kernel used BLOCK_SIZE as the spatial block. The new kernel uses it as the thread block size.\n\n   We change the kernel to:\n\n        @triton.jit\n        def global_avg_pool_kernel(\n            input_ptr, output_ptr,\n            input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n            output_batch_stride, output_channel_stride,\n            height, width,\n            BLOCK_SIZE: tl.constexpr,   # now this is the number of threads per block\n        ):\n            pid_batch = tl.program_id(0)\n            pid_channel = tl.program_id(1)\n            pid_thread = tl.arange(0, BLOCK_SIZE)   # vector of thread indices\n\n            input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n            num_elements = height * width\n            num_iters = tl.cdiv(num_elements, BLOCK_SIZE)\n\n            accum = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n            for i in range(num_iters):\n                base = i * BLOCK_SIZE\n                idx = base + pid_thread\n                mask = idx < num_elements\n                h = idx // width\n                w = idx % width\n                spatial_offsets = h * input_height_stride + w * input_width_stride\n                pointers = input_ptr + input_offset + spatial_offsets\n                values = tl.load(pointers, mask=mask, other=0.0)\n                accum += values\n\n            total_sum = tl.sum(accum)\n            channel_avg = total_sum / num_elements\n\n            output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n            tl.store(output_ptr + output_offset, channel_avg, mask=pid_thread==0)\n\n   And in the caller:\n\n        # We set the block size (number of threads) by the triton heuristics? Actually, we set it as a constant.\n        # We use BLOCK_SIZE=1024.\n\n        # We must specify the number of warps? Triton will derive it from BLOCK_SIZE: number of warps = ceil(BLOCK_SIZE / 32)\n        # But we don't set num_warps explicitly? The kernel will use BLOCK_SIZE to size the vector.\n\n        grid = (batch_size, channels)\n\n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3],\n            BLOCK_SIZE=1024\n        )\n\n   We hope this will be faster.\n\n   Let's run with this kernel.\n\n   Note: the previous kernel had a BLOCK_SIZE that was the next power of two of the spatial size. We are changing that to 1024.\n\n   We remove the code that computes block_size_pow2.\n\n   We'll try this and hope for a speedup.\n\n   We also note: the kernel must be correct.\n\n   We output the entire code block.\n</think>\nTo improve the performance of the global average pooling operation, I've optimized the Triton kernel by implementing a parallel reduction approach. The key changes include:\n\n1. Using a fixed thread block size (1024) for efficient parallel processing\n2. Each thread accumulates a partial sum over a segment of the spatial dimension\n3. Performing an efficient tree reduction within the thread block\n4. Only thread 0 writes the final result to avoid redundant stores\n\nThis approach better utilizes the GPU's parallel processing capabilities and reduces memory access overhead.\n\n```python\n# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_channel = tl.program_id(1)\n    pid_thread = tl.arange(0, BLOCK_SIZE)\n    \n    input_offset = pid_batch * input_batch_stride + pid_channel * input_channel_stride\n    spatial_size = height * width\n    \n    # Initialize accumulator\n    accum = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Process spatial elements in chunks\n    num_chunks = tl.cdiv(spatial_size, BLOCK_SIZE)\n    for chunk_idx in range(num_chunks):\n        idx = chunk_idx * BLOCK_SIZE + pid_thread\n        mask = idx < spatial_size\n        \n        # Compute spatial indices\n        h = idx // width\n        w = idx % width\n        spatial_offset = h * input_height_stride + w * input_width_stride\n        ptr = input_ptr + input_offset + spatial_offset\n        \n        # Load and accumulate\n        values = tl.load(ptr, mask=mask, other=0.0)\n        accum += values\n    \n    # Tree reduction within block\n    s = BLOCK_SIZE // 2\n    while s >= 1:\n        accum += tl.shuffle(accum, pid_thread + s, BLOCK_SIZE)\n        s //= 2\n    \n    # Store final result (only thread 0)\n    if pid_thread == 0:\n        channel_avg = accum[0] / spatial_size\n        output_offset = pid_batch * output_batch_stride + pid_channel * output_channel_stride\n        tl.store(output_ptr + output_offset, channel_avg)\n\nclass GlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        \n        grid = (batch_size, channels)\n        BLOCK_SIZE = 1024  # Fixed block size for efficient parallel reduction\n        \n        global_avg_pool_kernel[grid](\n            x, output,\n            x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n            output.stride(0), output.stride(1),\n            x.shape[2], x.shape[3],\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.features = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n        )\n        self.avgpool = GlobalAvgPool2dTriton()\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================\n```"]}}}
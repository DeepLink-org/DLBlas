{"id": "b2e8afd1-fd13-4d4c-9b9d-37d77597adbc", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fast_global_avg_pool_kernel(\n    input_ptr, output_ptr,\n    input_batch_stride, input_channel_stride, input_height_stride, input_width_stride,\n    output_batch_stride, output_channel_stride,\n    height, width, n_channels,\n    BLOCK_CHANNELS: tl.constexpr\n):\n    pid_batch = tl.program_id(0)\n    pid_chunk = tl.program_id(1)\n    \n    # Create channel offsets and mask\n    channel_offsets = pid_chunk * BLOCK_CHANNELS + tl.arange(0, BLOCK_CHANNELS)\n    channel_mask = channel_offsets < n_channels\n    \n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_CHANNELS,), dtype=tl.float32)\n    num_elements = height * width\n    \n    # Loop over spatial dimensions\n    for h in range(0, height):\n        for w in range(0, width):\n            # Compute input pointer offset\n            input_offset = (pid_batch * input_batch_stride + \n                            channel_offsets * input_channel_stride + \n                            h * input_height_stride + \n                            w * input_width_stride)\n            \n            # Load values with channel mask\n            values = tl.load(input_ptr + input_offset, mask=channel_mask, other=0.0)\n            acc += values\n    \n    # Compute average\n    channel_avg = acc / num_elements\n    \n    # Compute output offset and store\n    output_offset = pid_batch * output_batch_stride + channel_offsets * output_channel_stride\n    tl.store(output_ptr + output_offset, channel_avg, mask=channel_mask)\n\nclass FastGlobalAvgPool2dTriton(nn.Module):\n    def forward(self, x):\n        x = x.contiguous()\n        batch_size, channels = x.shape[0], x.shape[1]\n        output = torch.empty((batch_size, channels), device=x.device, dtype=x.dtype)\n        \n        # Compute strides\n        stride_b = x.stride(0)\n        stride_c = x.stride(1)\n        stride_h = x.stride(2)\n        stride_w = x.stride(3)\n        out_stride_b = output.stride(0)\n        out_stride_c = output.stride(1)\n        \n        # Kernel configuration\n        BLOCK_CHANNELS = 128\n        grid = (batch_size, triton.cdiv(channels, BLOCK_CHANNELS))\n        \n        # Launch kernel\n        fast_global_avg_pool_kernel[grid](\n            x, output,\n            stride_b, stride_c, stride_h, stride_w,\n            out_stride_b, out_stride_c,\n            x.shape[2], x.shape[3], channels,\n            BLOCK_CHANNELS=BLOCK_CHANNELS\n        )\n        return output\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):\n        super(ModelNew, self).__init__()\n        \n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n        \n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n        \n        self.model = nn.Sequential(\n            conv_bn(input_channels, int(32 * alpha), 2),\n            conv_dw(int(32 * alpha), int(64 * alpha), 1),\n            conv_dw(int(64 * alpha), int(128 * alpha), 2),\n            conv_dw(int(128 * alpha), int(128 * alpha), 1),\n            conv_dw(int(128 * alpha), int(256 * alpha), 2),\n            conv_dw(int(256 * alpha), int(256 * alpha), 1),\n            conv_dw(int(256 * alpha), int(512 * alpha), 2),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(512 * alpha), 1),\n            conv_dw(int(512 * alpha), int(1024 * alpha), 2),\n            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),\n            FastGlobalAvgPool2dTriton()\n        )\n        self.fc = nn.Linear(int(1024 * alpha), num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Test code\nbatch_size = 10\ninput_channels = 3\nheight = 224\nwidth = 224\nnum_classes = 1000\nalpha = 1.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_channels, height, width)]\n\ndef get_init_inputs():\n    return [num_classes, input_channels, alpha]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "a52f9c18-acc3-45b2-a401-884496313d50", "generation": 1, "timestamp": 1754585765.8125799, "iteration_found": 13, "metrics": {"compiled": true, "correctness": true, "stage": "success", "passed": false, "score": 20, "speed_up": 0.8, "runtime_ms": 1.7, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.36, "speed_up": 0.8, "custom_timing": 1.7}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": true, "stage": "reference code", "passed": false, "score": 0.0, "speed_up": 1.0, "runtime_ms": 1.63, "meta": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "correctness_trials": "(5 / 5)", "reference_timing_ms": 1.36, "speed_up": 0.834, "custom_timing": 1.63}}, "island": 1}, "artifacts_json": null, "artifact_dir": null}
{"id": "8954e9b3-ea9a-46d9-b19f-24c756cc2b8a", "code": "import torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef lstm_forward_kernel(\n    x_ptr,\n    output_ptr,\n    h_ptr,\n    c_ptr,\n    w_ih_ptr,\n    w_hh_ptr,\n    b_ih_ptr,\n    b_hh_ptr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n    seq_len: tl.constexpr,\n    batch_size: tl.constexpr,\n    BLOCK_HS: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    off_h = pid * hidden_size + tl.arange(0, BLOCK_HS)\n    mask_h = tl.arange(0, BLOCK_HS) < hidden_size\n    \n    # Initialize hidden and cell states\n    h_cur = tl.zeros((BLOCK_HS,), dtype=tl.float32)\n    c_cur = tl.zeros((BLOCK_HS,), dtype=tl.float32)\n    \n    # Precompute gate offsets\n    gate_offsets = tl.arange(0, 4) * hidden_size\n    for step in range(seq_len):\n        # Load input for current timestep\n        off_x = (pid * seq_len + step) * input_size\n        x = tl.load(x_ptr + off_x + tl.arange(0, input_size))\n        \n        # Compute input transformation\n        gates = tl.zeros((4, BLOCK_HS), dtype=tl.float32)\n        for i in range(4):\n            w_ih = tl.load(w_ih_ptr + (gate_offsets[i] + tl.arange(0, BLOCK_HS))[:, None] * input_size + \n                          tl.arange(0, input_size)[None, :], mask=mask_h[:, None])\n            gates = tl.store(gates, i, tl.sum(w_ih * x, axis=1) + tl.load(b_ih_ptr + gate_offsets[i] + tl.arange(0, BLOCK_HS), mask=mask_h))\n        \n        # Compute hidden transformation\n        for i in range(4):\n            w_hh = tl.load(w_hh_ptr + (gate_offsets[i] + tl.arange(0, BLOCK_HS))[:, None] * hidden_size + \n                          tl.arange(0, BLOCK_HS)[None, :], mask=mask_h[:, None] & mask_h[None, :])\n            gates = tl.store(gates, i, gates[i] + tl.sum(w_hh * h_cur, axis=1) + tl.load(b_hh_ptr + gate_offsets[i] + tl.arange(0, BLOCK_HS), mask=mask_h))\n        \n        # Split gates and apply activations\n        i_gate = tl.sigmoid(gates[0])\n        f_gate = tl.sigmoid(gates[1])\n        g_gate = tl.tanh(gates[2])\n        o_gate = tl.sigmoid(gates[3])\n        \n        # Update cell and hidden states\n        c_cur = f_gate * c_cur + i_gate * g_gate\n        h_cur = o_gate * tl.tanh(c_cur)\n        \n        # Store output\n        tl.store(output_ptr + (pid * seq_len + step) * hidden_size + tl.arange(0, BLOCK_HS), \n                h_cur, mask=mask_h)\n    \n    # Store final states\n    tl.store(h_ptr + pid * hidden_size + tl.arange(0, BLOCK_HS), h_cur, mask=mask_h)\n    tl.store(c_ptr + pid * hidden_size + tl.arange(0, BLOCK_HS), c_cur, mask=mask_h)\n\n@triton.jit\ndef lstm_backward_kernel(\n    x_ptr,\n    output_ptr,\n    h_ptr,\n    c_ptr,\n    w_ih_ptr,\n    w_hh_ptr,\n    b_ih_ptr,\n    b_hh_ptr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n    seq_len: tl.constexpr,\n    batch_size: tl.constexpr,\n    BLOCK_HS: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    off_h = pid * hidden_size + tl.arange(0, BLOCK_HS)\n    mask_h = tl.arange(0, BLOCK_HS) < hidden_size\n    \n    # Initialize hidden and cell states\n    h_cur = tl.zeros((BLOCK_HS,), dtype=tl.float32)\n    c_cur = tl.zeros((BLOCK_HS,), dtype=tl.float32)\n    \n    # Precompute gate offsets\n    gate_offsets = tl.arange(0, 4) * hidden_size\n    for step in range(seq_len-1, -1, -1):\n        # Load input for current timestep\n        off_x = (pid * seq_len + step) * input_size\n        x = tl.load(x_ptr + off_x + tl.arange(0, input_size))\n        \n        # Compute input transformation\n        gates = tl.zeros((4, BLOCK_HS), dtype=tl.float32)\n        for i in range(4):\n            w_ih = tl.load(w_ih_ptr + (gate_offsets[i] + tl.arange(0, BLOCK_HS))[:, None] * input_size + \n                          tl.arange(0, input_size)[None, :], mask=mask_h[:, None])\n            gates = tl.store(gates, i, tl.sum(w_ih * x, axis=1) + tl.load(b_ih_ptr + gate_offsets[i] + tl.arange(0, BLOCK_HS), mask=mask_h))\n        \n        # Compute hidden transformation\n        for i in range(4):\n            w_hh = tl.load(w_hh_ptr + (gate_offsets[i] + tl.arange(0, BLOCK_HS))[:, None] * hidden_size + \n                          tl.arange(0, BLOCK_HS)[None, :], mask=mask_h[:, None] & mask_h[None, :])\n            gates = tl.store(gates, i, gates[i] + tl.sum(w_hh * h_cur, axis=1) + tl.load(b_hh_ptr + gate_offsets[i] + tl.arange(0, BLOCK_HS), mask=mask_h))\n        \n        # Split gates and apply activations\n        i_gate = tl.sigmoid(gates[0])\n        f_gate = tl.sigmoid(gates[1])\n        g_gate = tl.tanh(gates[2])\n        o_gate = tl.sigmoid(gates[3])\n        \n        # Update cell and hidden states\n        c_cur = f_gate * c_cur + i_gate * g_gate\n        h_cur = o_gate * tl.tanh(c_cur)\n        \n        # Store output\n        tl.store(output_ptr + (pid * seq_len + step) * hidden_size + tl.arange(0, BLOCK_HS), \n                h_cur, mask=mask_h)\n    \n    # Store final states\n    tl.store(h_ptr + pid * hidden_size + tl.arange(0, BLOCK_HS), h_cur, mask=mask_h)\n    tl.store(c_ptr + pid * hidden_size + tl.arange(0, BLOCK_HS), c_cur, mask=mask_h)\n\ndef bidirectional_lstm_triton(x, weight_ih_f, weight_hh_f, bias_ih_f, bias_hh_f,\n                              weight_ih_b, weight_hh_b, bias_ih_b, bias_hh_b, h0, c0):\n    batch_size, seq_len, input_size = x.shape\n    hidden_size = weight_ih_f.shape[0] // 4\n    \n    # Prepare outputs\n    output_f = torch.empty((batch_size, seq_len, hidden_size), device=x.device, dtype=x.dtype)\n    output_b = torch.empty((batch_size, seq_len, hidden_size), device=x.device, dtype=x.dtype)\n    hn_f = torch.empty((batch_size, hidden_size), device=x.device, dtype=x.dtype)\n    cn_f = torch.empty((batch_size, hidden_size), device=x.device, dtype=x.dtype)\n    hn_b = torch.empty((batch_size, hidden_size), device=x.device, dtype=x.dtype)\n    cn_b = torch.empty((batch_size, hidden_size), device=x.device, dtype=x.dtype)\n    \n    # Flatten tensors\n    x_flat = x.contiguous().view(-1)\n    output_f_flat = output_f.view(-1)\n    output_b_flat = output_b.view(-1)\n    \n    # Compute block size\n    BLOCK_HS = triton.next_power_of_2(hidden_size)\n    \n    # Launch forward kernel\n    lstm_forward_kernel[(batch_size,)](\n        x_flat, output_f_flat, hn_f, cn_f,\n        weight_ih_f, weight_hh_f, bias_ih_f, bias_hh_f,\n        input_size, hidden_size, seq_len, batch_size, BLOCK_HS\n    )\n    \n    # Launch backward kernel\n    lstm_backward_kernel[(batch_size,)](\n        x_flat, output_b_flat, hn_b, cn_b,\n        weight_ih_b, weight_hh_b, bias_ih_b, bias_hh_b,\n        input_size, hidden_size, seq_len, batch_size, BLOCK_HS\n    )\n    \n    # Combine outputs\n    output = torch.cat((output_f, output_b), dim=-1)\n    hn = torch.stack((hn_f, hn_b), dim=0)\n    cn = torch.stack((cn_f, cn_b), dim=0)\n    \n    return output, (hn, cn)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.output_size = output_size\n        self.dropout = dropout\n\n        # Forward weights\n        self.weight_ih_f = nn.Parameter(torch.randn(4 * hidden_size, input_size))\n        self.weight_hh_f = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))\n        self.bias_ih_f = nn.Parameter(torch.randn(4 * hidden_size))\n        self.bias_hh_f = nn.Parameter(torch.randn(4 * hidden_size))\n        \n        # Backward weights\n        self.weight_ih_b = nn.Parameter(torch.randn(4 * hidden_size, input_size))\n        self.weight_hh_b = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))\n        self.bias_ih_b = nn.Parameter(torch.randn(4 * hidden_size))\n        self.bias_hh_b = nn.Parameter(torch.randn(4 * hidden_size))\n\n        self.fc = nn.Linear(2 * hidden_size, output_size)\n\n    def forward(self, x):\n        batch_size, seq_len, _ = x.shape\n        \n        # Initialize hidden states\n        h0 = torch.zeros(2, batch_size, self.hidden_size, device=x.device)\n        c0 = torch.zeros(2, batch_size, self.hidden_size, device=x.device)\n        \n        # Process LSTM\n        out, _ = bidirectional_lstm_triton(\n            x,\n            self.weight_ih_f, self.weight_hh_f, self.bias_ih_f, self.bias_hh_f,\n            self.weight_ih_b, self.weight_hh_b, self.bias_ih_b, self.bias_hh_b,\n            h0, c0\n        )\n        \n        # Final output\n        out = self.fc(out[:, -1, :])\n        return out", "language": "python", "parent_id": "ae499dcb-a1d8-46b8-acbf-e5700492195f", "generation": 2, "timestamp": 1754591116.1849065, "iteration_found": 9, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_38_LTSMBidirectional", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:6", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmpdrl5xhb9.py\", line 203, in forward\n    out, _ = bidirectional_lstm_triton(\n  File \"/tmp/tmpdrl5xhb9.py\", line 152, in bidirectional_lstm_triton\n    lstm_forward_kernel[(batch_size,)](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 229, in visit_For\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 42.6, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_38_LTSMBidirectional", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "max_difference": ["1.622943", "1.869474", "1.447587", "1.391966", "1.518117"], "avg_difference": ["0.423477", "0.541965", "0.423403", "0.471572", "0.472528"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 42.6, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
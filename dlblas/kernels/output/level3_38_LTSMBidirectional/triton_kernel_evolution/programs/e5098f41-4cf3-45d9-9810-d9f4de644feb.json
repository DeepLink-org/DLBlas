{"id": "e5098f41-4cf3-45d9-9810-d9f4de644feb", "code": "import torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef lstm_kernel(\n    x_ptr,  # input tensor\n    output_ptr,  # output tensor\n    hidden_ptr,  # final hidden state\n    cell_ptr,   # final cell state\n    weight_ih_ptr,\n    weight_hh_ptr,\n    bias_ih_ptr,\n    bias_hh_ptr,\n    input_size: tl.constexpr,\n    hidden_size: tl.constexpr,\n    seq_length: tl.constexpr,\n    batch_size: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    hid_offsets = tl.arange(0, hidden_size)\n    input_offsets = tl.arange(0, input_size)\n    gate_offsets = tl.arange(0, 4 * hidden_size)\n    \n    # Load initial hidden and cell states\n    h_prev = tl.load(hidden_ptr + pid * hidden_size + hid_offsets)\n    c_prev = tl.load(cell_ptr + pid * hidden_size + hid_offsets)\n    \n    for step in range(seq_length):\n        # Load input for current time step\n        x_offset = pid * seq_length * input_size + step * input_size\n        x_t = tl.load(x_ptr + x_offset + input_offsets)\n        \n        # Compute input-hidden contribution\n        part1 = tl.zeros((4 * hidden_size,), dtype=tl.float32)\n        for j in range(4 * hidden_size):\n            w_row = tl.load(weight_ih_ptr + j * input_size + input_offsets)\n            part1 = tl.store(part1, j, tl.sum(x_t * w_row))\n        \n        # Compute hidden-hidden contribution\n        part2 = tl.zeros((4 * hidden_size,), dtype=tl.float32)\n        for j in range(4 * hidden_size):\n            w_row = tl.load(weight_hh_ptr + j * hidden_size + hid_offsets)\n            part2 = tl.store(part2, j, tl.sum(h_prev * w_row))\n        \n        # Add biases\n        bias_ih = tl.load(bias_ih_ptr + gate_offsets)\n        bias_hh = tl.load(bias_hh_ptr + gate_offsets)\n        gates = part1 + part2 + bias_ih + bias_hh\n        \n        # Split gates and apply activations\n        i = tl.sigmoid(gates[0:hidden_size])\n        f = tl.sigmoid(gates[hidden_size:2*hidden_size])\n        g = tl.tanh(gates[2*hidden_size:3*hidden_size])\n        o = tl.sigmoid(gates[3*hidden_size:4*hidden_size])\n        \n        # Update cell and hidden states\n        c_t = f * c_prev + i * g\n        h_t = o * tl.tanh(c_t)\n        \n        # Store output\n        out_offset = pid * seq_length * hidden_size + step * hidden_size\n        tl.store(output_ptr + out_offset + hid_offsets, h_t.to(output_ptr.dtype.element_ty))\n        \n        # Update for next step\n        h_prev = h_t\n        c_prev = c_t\n    \n    # Store final states\n    tl.store(hidden_ptr + pid * hidden_size + hid_offsets, h_prev.to(hidden_ptr.dtype.element_ty))\n    tl.store(cell_ptr + pid * hidden_size + hid_offsets, c_prev.to(cell_ptr.dtype.element_ty))\n\ndef bidirectional_lstm_triton(x, weight_ih, weight_hh, bias_ih, bias_hh, h0, c0):\n    batch_size, seq_length, input_size = x.shape\n    hidden_size = weight_ih.shape[0] // 4\n\n    # Prepare inputs\n    x_flat = x.contiguous().view(-1)\n    output = torch.empty((batch_size, seq_length, hidden_size), device=x.device, dtype=x.dtype)\n    output_flat = output.view(-1)\n    \n    # Prepare states (use initial states as input buffers)\n    h0_flat = h0.view(-1).contiguous()\n    c0_flat = c0.view(-1).contiguous()\n    \n    # Launch kernel\n    grid = (batch_size,)\n    lstm_kernel[grid](\n        x_flat, output_flat, h0_flat, c0_flat,\n        weight_ih, weight_hh, bias_ih, bias_hh,\n        input_size, hidden_size, seq_length, batch_size,\n        hidden_size\n    )\n    \n    # Return outputs and final states\n    hn = h0_flat.view(batch_size, hidden_size)\n    cn = c0_flat.view(batch_size, hidden_size)\n    return output, (hn.unsqueeze(0), cn.unsqueeze(0))\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.output_size = output_size\n        self.dropout = dropout\n\n        # Weight parameters\n        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size))\n        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))\n        self.bias_ih = nn.Parameter(torch.randn(4 * hidden_size))\n        self.bias_hh = nn.Parameter(torch.randn(4 * hidden_size))\n\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        batch_size, seq_length, input_size = x.shape\n\n        # Initialize states\n        h0 = torch.zeros(1, batch_size, self.hidden_size, device=x.device)\n        c0 = torch.zeros(1, batch_size, self.hidden_size, device=x.device)\n\n        # Compute LSTM\n        out, (hn, cn) = bidirectional_lstm_triton(\n            x,\n            self.weight_ih, self.weight_hh,\n            self.bias_ih, self.bias_hh,\n            h0, c0\n        )\n\n        # Final output\n        out = self.fc(out[:, -1, :])\n        return out", "language": "python", "parent_id": "ae499dcb-a1d8-46b8-acbf-e5700492195f", "generation": 2, "timestamp": 1754591818.6033223, "iteration_found": 12, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_38_LTSMBidirectional", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:7", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmp7tp5_ocz.py\", line 127, in forward\n    out, (hn, cn) = bidirectional_lstm_triton(\n  File \"/tmp/tmp7tp5_ocz.py\", line 90, in bidirectional_lstm_triton\n    lstm_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 229, in visit_For\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 42.6, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_38_LTSMBidirectional", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "max_difference": ["1.622943", "1.869474", "1.447587", "1.391966", "1.518117"], "avg_difference": ["0.423477", "0.541965", "0.423403", "0.471572", "0.472528"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 42.6, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 2}, "artifacts_json": null, "artifact_dir": null}
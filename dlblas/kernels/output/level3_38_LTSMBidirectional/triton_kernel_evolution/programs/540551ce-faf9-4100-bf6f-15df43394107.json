{"id": "540551ce-faf9-4100-bf6f-15df43394107", "code": "# ================== EVOLVE-BLOCK-START ==================\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef lstm_forward_kernel(\n    x_ptr, output_ptr, h_ptr, c_ptr,\n    weight_ih_ptr, weight_hh_ptr, bias_ih_ptr, bias_hh_ptr,\n    input_size: tl.constexpr, hidden_size: tl.constexpr,\n    seq_length: tl.constexpr, batch_size: tl.constexpr\n):\n    pid = tl.program_id(0)\n    \n    # Initialize state for this sequence\n    h_prev = tl.zeros((hidden_size,), dtype=tl.float32)\n    c_prev = tl.zeros((hidden_size,), dtype=tl.float32)\n    \n    # Iterate over sequence steps\n    for step in range(seq_length):\n        # Load input for current timestep\n        x_offset = pid * seq_length * input_size + step * input_size\n        x_t = tl.load(x_ptr + x_offset + tl.arange(0, input_size))\n        \n        # Compute gates\n        gates = tl.zeros((4 * hidden_size,), dtype=tl.float32)\n        \n        # Input-hidden transformation\n        for j in range(4 * hidden_size):\n            w_ih_row = tl.load(weight_ih_ptr + j * input_size + tl.arange(0, input_size))\n            gates = tl.store(gates, j, tl.sum(w_ih_row * x_t))\n            gates = tl.store(gates, j, gates[j] + tl.load(bias_ih_ptr + j))\n            \n        # Hidden-hidden transformation\n        for j in range(4 * hidden_size):\n            w_hh_row = tl.load(weight_hh_ptr + j * hidden_size + tl.arange(0, hidden_size))\n            gates = tl.store(gates, j, gates[j] + tl.sum(w_hh_row * h_prev))\n            gates = tl.store(gates, j, gates[j] + tl.load(bias_hh_ptr + j))\n        \n        # Split gates and apply activations\n        i_gate = tl.sigmoid(gates[0:hidden_size])\n        f_gate = tl.sigmoid(gates[hidden_size:2*hidden_size])\n        g_gate = tl.tanh(gates[2*hidden_size:3*hidden_size])\n        o_gate = tl.sigmoid(gates[3*hidden_size:4*hidden_size])\n        \n        # Update cell and hidden states\n        c_cur = f_gate * c_prev + i_gate * g_gate\n        h_cur = o_gate * tl.tanh(c_cur)\n        \n        # Store output\n        out_offset = pid * seq_length * hidden_size + step * hidden_size\n        tl.store(output_ptr + out_offset + tl.arange(0, hidden_size), h_cur)\n        \n        # Update state for next timestep\n        h_prev = h_cur\n        c_prev = c_cur\n    \n    # Store final state\n    tl.store(h_ptr + pid * hidden_size + tl.arange(0, hidden_size), h_cur)\n    tl.store(c_ptr + pid * hidden_size + tl.arange(0, hidden_size), c_cur)\n\ndef bidirectional_lstm_triton(x, weight_ih, weight_hh, bias_ih, bias_hh, \n                              h0_forward, c0_forward, h0_backward, c0_backward):\n    batch_size, seq_length, input_size = x.shape\n    hidden_size = weight_ih.shape[0] // 4\n    \n    # Process forward direction\n    output_forward = torch.empty((batch_size, seq_length, hidden_size), \n                                device=x.device, dtype=x.dtype)\n    grid = (batch_size,)\n    lstm_forward_kernel[grid](\n        x.contiguous().view(-1), output_forward.view(-1), \n        h0_forward, c0_forward,\n        weight_ih, weight_hh, bias_ih, bias_hh,\n        input_size, hidden_size, seq_length, batch_size\n    )\n    \n    # Process backward direction\n    x_reversed = torch.flip(x, [1])\n    output_backward = torch.empty((batch_size, seq_length, hidden_size), \n                                 device=x.device, dtype=x.dtype)\n    lstm_forward_kernel[grid](\n        x_reversed.contiguous().view(-1), output_backward.view(-1), \n        h0_backward, c0_backward,\n        weight_ih, weight_hh, bias_ih, bias_hh,\n        input_size, hidden_size, seq_length, batch_size\n    )\n    output_backward = torch.flip(output_backward, [1])\n    \n    # Combine directions\n    output = torch.cat([output_forward, output_backward], dim=-1)\n    hn = torch.stack([h0_forward, h0_backward], dim=0)\n    cn = torch.stack([c0_forward, c0_backward], dim=0)\n    \n    return output, (hn, cn)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        super(ModelNew, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.output_size = output_size\n        self.dropout = dropout\n        \n        # Create parameters for all layers\n        self.weights_ih = nn.ParameterList()\n        self.weights_hh = nn.ParameterList()\n        self.biases_ih = nn.ParameterList()\n        self.biases_hh = nn.ParameterList()\n        \n        for i in range(num_layers):\n            # Forward layer weights\n            self.weights_ih.append(nn.Parameter(torch.randn(4 * hidden_size, \n                                                           input_size if i == 0 else hidden_size * 2)))\n            self.weights_hh.append(nn.Parameter(torch.randn(4 * hidden_size, hidden_size)))\n            self.biases_ih.append(nn.Parameter(torch.randn(4 * hidden_size)))\n            self.biases_hh.append(nn.Parameter(torch.randn(4 * hidden_size)))\n            \n            # Backward layer weights\n            self.weights_ih.append(nn.Parameter(torch.randn(4 * hidden_size, \n                                                           input_size if i == 0 else hidden_size * 2)))\n            self.weights_hh.append(nn.Parameter(torch.randn(4 * hidden_size, hidden_size)))\n            self.biases_ih.append(nn.Parameter(torch.randn(4 * hidden_size)))\n            self.biases_hh.append(nn.Parameter(torch.randn(4 * hidden_size)))\n        \n        self.fc = nn.Linear(hidden_size * 2, output_size)\n    \n    def forward(self, x):\n        batch_size, seq_length, _ = x.shape\n        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size, device=x.device)\n        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size, device=x.device)\n        \n        # Process through all layers\n        for layer in range(self.num_layers):\n            # Get initial states for this layer\n            h0_forward = h0[layer * 2]\n            c0_forward = c0[layer * 2]\n            h0_backward = h0[layer * 2 + 1]\n            c0_backward = c0[layer * 2 + 1]\n            \n            # Get weights for this layer\n            weight_idx = layer * 2\n            out, (hn, cn) = bidirectional_lstm_triton(\n                x,\n                self.weights_ih[weight_idx], self.weights_hh[weight_idx],\n                self.biases_ih[weight_idx], self.biases_hh[weight_idx],\n                h0_forward, c0_forward, h0_backward, c0_backward\n            )\n            \n            # Update input for next layer\n            x = out\n            \n            # Store final states for next layer\n            if layer < self.num_layers - 1:\n                h0[(layer + 1) * 2] = hn[0]\n                h0[(layer + 1) * 2 + 1] = hn[1]\n                c0[(layer + 1) * 2] = cn[0]\n                c0[(layer + 1) * 2 + 1] = cn[1]\n        \n        # Final output\n        out = self.fc(out[:, -1, :])\n        return out\n\n# Test code\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, sequence_length, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]\n# =================== EVOLVE-BLOCK-END ===================", "language": "python", "parent_id": "901736b0-7002-4f28-83a0-80d37f86e4f9", "generation": 3, "timestamp": 1754594254.2959735, "iteration_found": 25, "metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_38_LTSMBidirectional", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:5", "runtime_error": "Traceback (most recent call last):\n  File \"/cpfs01/shared/llm_kernel/hujiakai/KernelLLM/validation_api_server/helpers/evaluations.py\", line 720, in run_and_check_correctness\n    output_new = model_new(*inputs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/tmphrnko4tw.py\", line 145, in forward\n    out, (hn, cn) = bidirectional_lstm_triton(\n  File \"/tmp/tmphrnko4tw.py\", line 72, in bidirectional_lstm_triton\n    lstm_forward_kernel[grid](\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 594, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 281, in compile\n    key = f\"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 78, in hash\n    key = f\"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}\"\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 696, in cache_key\n    dependencies_finder.visit(self.parse())\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 163, in visit_FunctionDef\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 229, in visit_For\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 426, in generic_visit\n    self.visit(item)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 217, in visit_Assign\n    self.generic_visit(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 428, in generic_visit\n    self.visit(value)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/ast.py\", line 418, in visit\n    return visitor(node)\n  File \"/home/lijiaxing/.conda/envs/api/lib/python3.10/site-packages/triton/runtime/jit.py\", line 156, in visit_Attribute\n    ret = getattr(lhs, node.attr)\nAttributeError: module 'triton.language' has no attribute 'tanh'\n", "runtime_error_name": "builtins.AttributeError", "reference_timing_ms": 42.6, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"compiled": true, "correctness": false, "score": 10, "stage": "wrong_answer", "verify_payload": {"id": "level3_38_LTSMBidirectional", "result": {"compiled": true, "correctness": false, "metadata": {"hardware": "NVIDIA L20Y", "device": "cuda:3", "max_difference": ["0.286796", "0.302321", "0.286816", "0.302320", "0.287174"], "avg_difference": ["0.078024", "0.078617", "0.077893", "0.078659", "0.077975"], "correctness_issue": "Output mismatch", "correctness_trials": "(0 / 5)", "reference_timing_ms": 42.6, "speed_up": -1.0, "custom_timing": -1.0}, "runtime": -1.0, "runtime_stats": {}}}}, "island": 0}, "artifacts_json": null, "artifact_dir": null}